# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import functools
import inspect
import sys
from abc import ABC
from collections.abc import Callable, Iterable
from functools import wraps
from typing import TYPE_CHECKING, Any, TypeVar, get_type_hints

from ..agentchat import Agent
from ..doc_utils import export_module
from ..fast_depends import Depends as FastDepends
from ..fast_depends import inject
from ..fast_depends.dependencies import model
from ..fast_depends.utils import is_coroutine_callable

if TYPE_CHECKING:
    from ..agentchat.conversable_agent import ConversableAgent

__all__ = [
    "BaseContext",
    "ChatContext",
    "Depends",
    "Field",
    "get_context_params",
    "inject_params",
    "on",
    "remove_params",
]


@export_module("autogen.tools")
class BaseContext(ABC):
    """Base class for context classes.

    This is the base class for defining various context types that may be used
    throughout the application. It serves as a parent for specific context classes.
    """

    pass


@export_module("autogen.tools")
class ChatContext(BaseContext):
    """ChatContext class that extends BaseContext.

    This class is used to represent a chat context that holds a list of messages.
    It inherits from `BaseContext` and adds the `messages` attribute.
    """

    def __init__(self, agent: "ConversableAgent") -> None:
        """Initializes the ChatContext with an agent.

        Args:
            agent: The agent to use for retrieving chat messages.
        """
        self._agent = agent

    @property
    def chat_messages(self) -> dict[Agent, list[dict[Any, Any]]]:
        """The messages in the chat.

        Returns:
            A dictionary of agents and their messages.
        """
        return self._agent.chat_messages

    @property
    def last_message(self) -> dict[str, Any] | None:
        """The last message in the chat.

        Returns:
            The last message in the chat.
        """
        return self._agent.last_message()


T = TypeVar("T")


def on(x: T) -> Callable[[], T]:
    def inner(ag2_x: T = x) -> T:
        return ag2_x

    return inner


@export_module("autogen.tools")
def Depends(x: Any) -> Any:  # noqa: N802
    """Creates a dependency for injection based on the provided context or type.

    Args:
        x: The context or dependency to be injected.

    Returns:
        A FastDepends object that will resolve the dependency for injection.
    """
    if isinstance(x, BaseContext):
        return FastDepends(lambda: x)

    return FastDepends(x)


def get_context_params(func: Callable[..., Any], subclass: type[BaseContext] | type[ChatContext]) -> list[str]:
    """Gets the names of the context parameters in a function signature.

    Args:
        func: The function to inspect for context parameters.
        subclass: The subclass to search for.

    Returns:
        A list of parameter names that are instances of the specified subclass.
    """
    sig = inspect.signature(func)
    return [p.name for p in sig.parameters.values() if _is_context_param(p, subclass=subclass)]


def _is_context_param(param: inspect.Parameter, subclass: type[BaseContext] | type[ChatContext] = BaseContext) -> bool:
    # param.annotation.__args__[0] is used to handle Annotated[MyContext, Depends(MyContext(b=2))]
    param_annotation = param.annotation.__args__[0] if hasattr(param.annotation, "__args__") else param.annotation
    try:
        return isinstance(param_annotation, type) and issubclass(param_annotation, subclass)
    except TypeError:
        return False


def _is_depends_param(param: inspect.Parameter) -> bool:
    return isinstance(param.default, model.Depends) or (
        hasattr(param.annotation, "__metadata__")
        and type(param.annotation.__metadata__) == tuple
        and isinstance(param.annotation.__metadata__[0], model.Depends)
    )


def remove_params(func: Callable[..., Any], sig: inspect.Signature, params: Iterable[str]) -> None:
    new_signature = sig.replace(parameters=[p for p in sig.parameters.values() if p.name not in params])
    func.__signature__ = new_signature  # type: ignore[attr-defined]


def _remove_injected_params_from_signature(func: Callable[..., Any]) -> Callable[..., Any]:
    func = _fix_staticmethod(func)
    sig = inspect.signature(func)
    params_to_remove = [p.name for p in sig.parameters.values() if _is_context_param(p) or _is_depends_param(p)]
    remove_params(func, sig, params_to_remove)
    return func


class Field:
    """Represents a description field for use in type annotations.

    This class is used to store a description for an annotated field, often used for
    documenting or validating fields in a context or data model.
    """

    def __init__(self, description: str) -> None:
        """Initializes the Field with a description.

        Args:
            description: The description text for the field.
        """
        self._description = description

    @property
    def description(self) -> str:
        return self._description


def _string_metadata_to_description_field(func: Callable[..., Any]) -> Callable[..., Any]:
    type_hints = get_type_hints(func, include_extras=True)

    for _, annotation in type_hints.items():
        # Check if the annotation itself has metadata (using __metadata__)
        if hasattr(annotation, "__metadata__"):
            metadata = annotation.__metadata__
            if metadata and isinstance(metadata[0], str):
                # Replace string metadata with Field
                annotation.__metadata__ = (Field(description=metadata[0]),)
        # For Python < 3.11, annotations like `Optional` are stored as `Union`, so metadata
        # would be in the first element of __args__ (e.g., `__args__[0]` for `int` in `Optional[int]`)
        elif hasattr(annotation, "__args__") and hasattr(annotation.__args__[0], "__metadata__"):
            metadata = annotation.__args__[0].__metadata__
            if metadata and isinstance(metadata[0], str):
                # Replace string metadata with Field
                annotation.__args__[0].__metadata__ = (Field(description=metadata[0]),)
    return func


def _fix_staticmethod(f: Callable[..., Any]) -> Callable[..., Any]:
    # This is a workaround for Python 3.9+ where staticmethod.__func__ is accessible
    if sys.version_info >= (3, 9) and isinstance(f, staticmethod) and hasattr(f, "__func__"):

        @wraps(f.__func__)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            return f.__func__(*args, **kwargs)  # type: ignore[attr-defined]

        wrapper.__name__ = f.__func__.__name__

        f = wrapper
    return f


def _set_return_annotation_to_any(f: Callable[..., Any]) -> Callable[..., Any]:
    if is_coroutine_callable(f):

        @functools.wraps(f)
        async def _a_wrapped_func(*args: Any, **kwargs: Any) -> Any:
            return await f(*args, **kwargs)

        wrapped_func = _a_wrapped_func

    else:

        @functools.wraps(f)
        def _wrapped_func(*args: Any, **kwargs: Any) -> Any:
            return f(*args, **kwargs)

        wrapped_func = _wrapped_func

    sig = inspect.signature(f)

    # Change the return annotation directly on the signature of the wrapper
    wrapped_func.__signature__ = sig.replace(return_annotation=Any)  # type: ignore[attr-defined]

    return wrapped_func


def inject_params(f: Callable[..., Any]) -> Callable[..., Any]:
    """Injects parameters into a function, removing injected dependencies from its signature.

    This function is used to modify a function by injecting dependencies and removing
    injected parameters from the function's signature.

    Args:
        f: The function to modify with dependency injection.

    Returns:
        The modified function with injected dependencies and updated signature.
    """
    # This is a workaround for Python 3.9+ where staticmethod.__func__ is accessible
    f = _fix_staticmethod(f)
    f = _string_metadata_to_description_field(f)
    f = _set_return_annotation_to_any(f)
    f = inject(f)
    f = _remove_injected_params_from_signature(f)

    return f
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .crawl4ai import Crawl4AITool

__all__ = ["Crawl4AITool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Annotated, Any, Optional

from pydantic import BaseModel

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ....interop import LiteLLmConfigFactory
from ....llm_config import LLMConfig
from ... import Tool
from ...dependency_injection import Depends, on

with optional_import_block():
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig
    from crawl4ai.extraction_strategy import LLMExtractionStrategy

__all__ = ["Crawl4AITool"]


@require_optional_import(["crawl4ai"], "crawl4ai")
@export_module("autogen.tools.experimental")
class Crawl4AITool(Tool):
    """Crawl a website and extract information using the crawl4ai library."""

    def __init__(
        self,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        extraction_model: type[BaseModel] | None = None,
        llm_strategy_kwargs: dict[str, Any] | None = None,
    ) -> None:
        """Initialize the Crawl4AITool.

        Args:
            llm_config: The config dictionary for the LLM model. If None, the tool will run without LLM.
            extraction_model: The Pydantic model to use for extraction. If None, the tool will use the default schema.
            llm_strategy_kwargs: The keyword arguments to pass to the LLM extraction strategy.
        """
        Crawl4AITool._validate_llm_strategy_kwargs(llm_strategy_kwargs, llm_config_provided=(llm_config is not None))

        async def crawl4ai_helper(  # type: ignore[no-any-unimported]
            url: str,
            browser_cfg: Optional["BrowserConfig"] = None,
            crawl_config: Optional["CrawlerRunConfig"] = None,
        ) -> Any:
            async with AsyncWebCrawler(config=browser_cfg) as crawler:
                result = await crawler.arun(
                    url=url,
                    config=crawl_config,
                )

            if crawl_config is None:
                response = result.markdown
            else:
                response = result.extracted_content if result.success else result.error_message

            return response

        async def crawl4ai_without_llm(
            url: Annotated[str, "The url to crawl and extract information from."],
        ) -> Any:
            return await crawl4ai_helper(url=url)

        async def crawl4ai_with_llm(
            url: Annotated[str, "The url to crawl and extract information from."],
            instruction: Annotated[str, "The instruction to provide on how and what to extract."],
            llm_config: Annotated[Any, Depends(on(llm_config))],
            llm_strategy_kwargs: Annotated[dict[str, Any] | None, Depends(on(llm_strategy_kwargs))],
            extraction_model: Annotated[type[BaseModel] | None, Depends(on(extraction_model))],
        ) -> Any:
            browser_cfg = BrowserConfig(headless=True)
            crawl_config = Crawl4AITool._get_crawl_config(
                llm_config=llm_config,
                instruction=instruction,
                extraction_model=extraction_model,
                llm_strategy_kwargs=llm_strategy_kwargs,
            )

            return await crawl4ai_helper(url=url, browser_cfg=browser_cfg, crawl_config=crawl_config)

        super().__init__(
            name="crawl4ai",
            description="Crawl a website and extract information.",
            func_or_tool=crawl4ai_without_llm if llm_config is None else crawl4ai_with_llm,
        )

    @staticmethod
    def _validate_llm_strategy_kwargs(llm_strategy_kwargs: dict[str, Any] | None, llm_config_provided: bool) -> None:
        if not llm_strategy_kwargs:
            return

        if not llm_config_provided:
            raise ValueError("llm_strategy_kwargs can only be provided if llm_config is also provided.")

        check_parameters_error_msg = "".join(
            f"'{key}' should not be provided in llm_strategy_kwargs. It is automatically set based on llm_config.\n"
            for key in ["provider", "api_token"]
            if key in llm_strategy_kwargs
        )

        check_parameters_error_msg += "".join(
            "'schema' should not be provided in llm_strategy_kwargs. It is automatically set based on extraction_model type.\n"
            if "schema" in llm_strategy_kwargs
            else ""
        )

        check_parameters_error_msg += "".join(
            "'instruction' should not be provided in llm_strategy_kwargs. It is provided at the time of calling the tool.\n"
            if "instruction" in llm_strategy_kwargs
            else ""
        )

        if check_parameters_error_msg:
            raise ValueError(check_parameters_error_msg)

    @staticmethod
    def _get_crawl_config(  # type: ignore[no-any-unimported]
        llm_config: LLMConfig | dict[str, Any],
        instruction: str,
        llm_strategy_kwargs: dict[str, Any] | None = None,
        extraction_model: type[BaseModel] | None = None,
    ) -> "CrawlerRunConfig":
        lite_llm_config = LiteLLmConfigFactory.create_lite_llm_config(llm_config)

        if llm_strategy_kwargs is None:
            llm_strategy_kwargs = {}

        schema = (
            extraction_model.model_json_schema()
            if (extraction_model and issubclass(extraction_model, BaseModel))
            else None
        )

        extraction_type = llm_strategy_kwargs.pop("extraction_type", "schema" if schema else "block")

        # 1. Define the LLM extraction strategy
        llm_strategy = LLMExtractionStrategy(
            **lite_llm_config,
            schema=schema,
            extraction_type=extraction_type,
            instruction=instruction,
            **llm_strategy_kwargs,
        )

        # 2. Build the crawler config
        crawl_config = CrawlerRunConfig(extraction_strategy=llm_strategy, cache_mode=CacheMode.BYPASS)

        return crawl_config
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

from .reliable import ReliableTool, ReliableToolError, SuccessfulExecutionParameters, ToolExecutionDetails

__all__ = ["ReliableTool", "ReliableToolError", "SuccessfulExecutionParameters", "ToolExecutionDetails"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

import asyncio
import contextlib
import copy
import functools
import inspect
import json
import logging
import time
import warnings
from collections.abc import Callable
from typing import Annotated, Any

from pydantic import BaseModel, ConfigDict, Field, ValidationError

from ....agentchat import ChatResult, initiate_group_chat
from ....agentchat.agent import Agent
from ....agentchat.conversable_agent import ConversableAgent
from ....agentchat.group import AgentTarget, ReplyResult, TerminateTarget
from ....agentchat.group.context_variables import ContextVariables
from ....agentchat.group.patterns import DefaultPattern
from ....doc_utils import export_module
from ....fast_depends.utils import is_coroutine_callable
from ....llm_config import LLMConfig
from ....tools.dependency_injection import Field as AG2Field
from ....tools.tool import Tool

__all__ = ("ReliableTool", "ReliableToolError", "SuccessfulExecutionParameters", "ToolExecutionDetails")

logger = logging.getLogger(__name__)

HYPOTHESIS_DESCRIPTION = (
    "A clear, concise statement about the expected outcome or result format of the function call "
    "based on the provided inputs. This helps in assessing the relevance and potential success "
    "of the call, and guides validation."
)


class ValidationResult(BaseModel):
    """Represents the outcome of a single validation step."""

    model_config = ConfigDict(extra="forbid")
    validation_result: bool
    justification: str

    def __str__(self) -> str:
        status = "Passed" if self.validation_result else "Failed"
        return f"Validation Result: {status}\nJustification: {self.justification}"

    def format(self) -> str:
        """Returns the JSON representation for AutoGen compatibility."""
        return self.model_dump_json()


class ExecutionAttempt(BaseModel):
    """Stores the state of a single attempt to execute and validate the function."""

    model_config = ConfigDict(arbitrary_types_allowed=True)
    timestamp: float = Field(default_factory=time.time)
    attempt_args: list[Any] = Field(default_factory=list)
    attempt_kwargs: dict[str, Any] = Field(default_factory=dict)
    hypothesis: str | None = None
    error: str | None = None
    result_data: Any | None = None
    result_str: str | None = None
    validation: ValidationResult | None = None

    @property
    def did_execute_successfully(self) -> bool:
        """Check if the attempt executed without raising an error."""
        return self.error is None

    @property
    def did_validate_successfully(self) -> bool:
        """Check if the attempt passed validation."""
        return self.validation is not None and self.validation.validation_result


class ReliableToolContext(BaseModel):
    """Main context object holding the overall state and history of attempts."""

    model_config = ConfigDict(arbitrary_types_allowed=True)
    task: str
    reliable_tool_name: str
    start_time: float = Field(default_factory=time.time)
    dynamic_validation_input: str | None = None
    attempts: list[ExecutionAttempt] = Field(default_factory=list)
    initial_messages: list[dict[str, Any]] | None = Field(
        default=None, description="Initial messages provided to the tool run."
    )
    initial_ground_truth: list[str] | None = Field(default=None, description="Initial ground truth strings provided.")

    @property
    def attempt_count(self) -> int:
        """Return the number of attempts made."""
        return len(self.attempts)

    @property
    def latest_attempt(self) -> ExecutionAttempt | None:
        """Return the most recent attempt, if any."""
        return self.attempts[-1] if self.attempts else None

    @property
    def is_complete_and_successful(self) -> bool:
        """Check if the process finished with a validated successful attempt."""
        latest = self.latest_attempt
        return latest is not None and latest.did_execute_successfully and latest.did_validate_successfully

    def get_final_result_data(self) -> Any:
        """Return the result_data from the successful and validated attempt."""
        if self.is_complete_and_successful and self.latest_attempt:
            return self.latest_attempt.result_data
        return None

    def get_final_result_str(self) -> Any:
        """Return the result_str from the successful and validated attempt."""
        if self.is_complete_and_successful and self.latest_attempt:
            return self.latest_attempt.result_str
        return None

    def get_failure_summary(self) -> str:
        """Provide a summary of why the overall execution failed."""
        latest = self.latest_attempt
        if latest is None:
            return "No execution attempts were made."
        if not latest.did_execute_successfully:
            return f"Execution failed: {latest.error}"
        if not latest.did_validate_successfully:
            justification = (
                latest.validation.justification if latest.validation else "Validation result missing or invalid"
            )
            return f"Execution succeeded but failed validation (Justification: {justification})"
        return "Execution completed but overall status indicates failure (Internal inconsistency)."


class SuccessfulExecutionParameters(BaseModel):
    """Holds the arguments of a successful tool function execution."""

    model_config = ConfigDict(arbitrary_types_allowed=True)
    attempt_args: list[Any]
    attempt_kwargs: dict[str, Any]


class ToolExecutionDetails(BaseModel):
    """Provides detailed information about a ReliableTool execution."""

    model_config = ConfigDict(arbitrary_types_allowed=True)
    task: str
    is_overall_successful: bool
    failure_reason: str | None = None
    successful_parameters: SuccessfulExecutionParameters | None = None
    final_tool_context: ReliableToolContext


def _configure_llm_for_structured_output(
    llm_config: LLMConfig | dict[str, Any] | None, structured_output_type: type[BaseModel]
) -> LLMConfig | dict[str, Any]:  # Return type changed, False is no longer a valid return
    """Configure LLM config for structured output using a Pydantic model."""
    if llm_config is None or llm_config is False:
        raise ValueError("LLMConfig cannot be None or False for structured output.")
    if not issubclass(structured_output_type, BaseModel):
        raise TypeError(f"{structured_output_type} must be a Pydantic BaseModel subclass.")

    llm_config_obj = ConversableAgent._validate_llm_config(llm_config)

    if llm_config_obj is False:  # Should not happen if input llm_config is not False
        raise ValueError("Validated LLMConfig resolved to False unexpectedly.")

    response_format_set = False

    def _set_format_and_remove_conflicts(config_item: LLMConfig | dict[str, Any]) -> None:
        nonlocal response_format_set
        conflicting_keys = ["tools", "tool_choice", "functions"]
        removed_keys = []

        if isinstance(config_item, dict):
            config_item["response_format"] = structured_output_type
            response_format_set = True
            for key in conflicting_keys:
                if key in config_item:
                    del config_item[key]
                    removed_keys.append(key)
        elif hasattr(config_item, "response_format"):  # LLMConfig object
            setattr(config_item, "response_format", structured_output_type)
            response_format_set = True
            for key in conflicting_keys:
                if hasattr(config_item, key) and getattr(config_item, key, None):
                    # Try setting to None or empty list/dict as appropriate
                    default_empty: list[str] | None = [] if key in ["tools", "functions"] else None
                    setattr(config_item, key, default_empty)
                    removed_keys.append(key)
        else:
            # This case implies llm_config_obj is an object not fitting LLMConfig ducktype for response_format
            # or not a dict, which should be caught by _validate_llm_config or earlier checks.
            raise TypeError(f"Unsupported LLM config item type for structured output: {type(config_item)}")

        if removed_keys:
            logger.debug(
                "Removed conflicting keys %s from LLM config for structured output (response_format=%s)",
                removed_keys,
                structured_output_type.__name__,
            )

    _set_format_and_remove_conflicts(llm_config_obj)

    if not response_format_set and not isinstance(llm_config_obj, dict):  # Double check if it's an object
        # if it's an object and response_format could not be set, it's an issue.
        # For dicts, it's assumed to be set by _set_format_and_remove_conflicts.
        raise ValueError(
            f"LLMConfig object type ({type(llm_config_obj).__name__}) "
            "could not have 'response_format' set. Structured output may fail."
        )

    # Handle config_list if present
    config_list_attr_name = "config_list"
    original_config_list = None

    if isinstance(llm_config_obj, dict):
        original_config_list = llm_config_obj.get(config_list_attr_name)
    elif hasattr(llm_config_obj, config_list_attr_name):
        original_config_list = getattr(llm_config_obj, config_list_attr_name, None)

    if isinstance(original_config_list, list):
        new_config_list = []
        for item in original_config_list:
            item_copy = copy.deepcopy(item)
            # Assuming items in config_list are dicts or LLMConfig-like objects
            _set_format_and_remove_conflicts(item_copy)
            new_config_list.append(item_copy)

        if isinstance(llm_config_obj, dict):
            llm_config_obj[config_list_attr_name] = new_config_list
        else:  # Must be an object if hasattr was true
            setattr(llm_config_obj, config_list_attr_name, new_config_list)

    logger.debug("Prepared LLM config for validator (response_format=%s)", structured_output_type.__name__)
    return llm_config_obj


def _get_last_non_empty_message_content(messages: list[dict[str, Any]] | None) -> str | None:
    """Get content of the last message with non-empty content."""
    if not messages:
        return None
    for message in reversed(messages):
        content = message.get("content")
        if isinstance(content, str) and content.strip():
            return content.strip()
        if isinstance(content, list) and content:  # Handle multimodal content
            # Prioritize text parts
            text_parts = [
                item["text"].strip()
                for item in content
                if isinstance(item, dict)
                and item.get("type") == "text"
                and isinstance(item.get("text"), str)
                and item["text"].strip()
            ]
            if text_parts:
                return "\n".join(text_parts)

            # If no text parts, serialize the first non-empty item
            for item in content:
                if item:  # Ensure item is not None or empty
                    if isinstance(item, dict):
                        return json.dumps(item)
                    else:
                        return str(item).strip()
    return None


def _get_reliable_tool_context(context_variables: ContextVariables, context_key: str) -> ReliableToolContext:
    """Retrieve and validate the ReliableToolContext from ContextVariables."""
    context_data = context_variables.get(context_key)
    if context_data is None:
        raise KeyError(f"ReliableToolContext key '{context_key}' not found in ContextVariables.")
    try:
        if isinstance(context_data, str):
            return ReliableToolContext.model_validate_json(context_data)
        raise TypeError(
            f"Unexpected type {type(context_data)} for context key '{context_key}'. Expected ReliableToolContext, str, or dict."
        )
    except (ValidationError, json.JSONDecodeError, TypeError) as e:
        preview = f" Preview: '{str(context_data)[:100]}...'" if isinstance(context_data, (str, dict)) else ""
        # Logged error level changed to warning as this function re-raises.
        logger.warning(
            "Failed loading ReliableToolContext '%s'. Error: %s. Type: %s.%s",
            context_key,
            e,
            type(context_data).__name__,
            preview,
        )
        raise ValueError(f"Failed loading ReliableToolContext key '{context_key}': {e}") from e


def _set_reliable_tool_context(
    context_variables: ContextVariables, context_key: str, context: ReliableToolContext
) -> None:
    """Serialize and store the ReliableToolContext in ContextVariables."""
    if not isinstance(context, ReliableToolContext):
        raise TypeError(f"Object to set must be a ReliableToolContext, got {type(context)}.")
    try:
        context_variables[context_key] = context.model_dump_json(warnings="warn")
    except (ValidationError, TypeError) as e:  # More specific exceptions
        context_dict_str = "N/A"
        try:  # Best effort to get some context info for logging
            context_dict_str = str(context.model_dump(warnings="warn", exclude={"attempts"}))[:500]
        except Exception:
            contextlib.suppress(Exception)
        logger.error(  # Log as error as this is a critical serialization failure
            "Failed serializing ReliableToolContext key '%s': %s. Context (partial): %s",
            context_key,
            e,
            context_dict_str,
        )
        raise ValueError(f"Critical error serializing ReliableToolContext: {e}") from e


def get_runner_prompt(task: str, agent_system_message: str, internal_tool_name: str) -> str:
    """Generate the system prompt for the internal runner agent."""
    return f"""
You are an AI assistant responsible for invoking a specific function based on the user's task and conversation history.
Function to call: '{internal_tool_name}'
Analyze the previous attempt's outcome (if any, visible in history) and adjust the function arguments accordingly for this retry. If this is the first attempt, determine the best initial arguments based on the task and initial context.

You MUST invoke the function '{internal_tool_name}' exactly one time per response using a tool call format that the system can execute.
Do NOT just output text explaining what you would do, or asking for confirmation. Directly make the tool call.
Analyze the task description and *full conversation history* carefully to determine the correct arguments for the function call.
You MUST provide a 'hypothesis' argument summarizing the expected outcome or result format of the function call based on the inputs.

Base Instructions:
{agent_system_message}

Current Task:
{task}
"""


def get_validator_prompt(
    task: str, base_validator_system_message: str, dynamic_validation_addition: str | None = None
) -> str:
    """Generate the system prompt for the internal validator agent."""
    dynamic_section = (
        f"\n\nAdditional Dynamic Requirements for This Specific Run:\n{dynamic_validation_addition.strip()}"
        if dynamic_validation_addition and dynamic_validation_addition.strip()
        else ""
    )
    return f"""
You are an AI validation assistant. You will receive a curated message list containing:
1. Initial context messages (original request, potentially prior conversation).
2. Provided ground truth information (if any).
3. The final result of a function call intended to accomplish the task.

Your goal is to validate if the *final function call result* meets ALL requirements based on the *entire context provided in the message list*. Consider the base task description, base validation rules, initial context/ground truth, and any dynamic requirements below.

Evaluate the *final function call result* (presented at the end of the message list) based on *all* information provided.

Base Validation Rules/Context:
{base_validator_system_message}{dynamic_section}

Base Task Description (for reference):
{task}
"""


def reliable_function_wrapper(
    tool_function: Callable[..., Any], validator: ConversableAgent, runner: ConversableAgent, context_variables_key: str
) -> Callable[..., Any]:
    """Wraps the target function, returning a sync or async wrapper.

    Adds 'hypothesis' and 'context_variables' keyword-only arguments.
    Returns a ReplyResult targeting the validator.
    """
    is_original_func_async = is_coroutine_callable(tool_function)
    tool_sig = inspect.signature(tool_function)
    wrapper_func: Callable[..., Any]  # Declare type for wrapper_func

    def _handle_execution_error(
        attempt: ExecutionAttempt, context_vars: ContextVariables, context: ReliableToolContext, e: Exception
    ) -> ReplyResult:
        """Shared logic to handle tool_function execution error."""
        err_msg = f"{type(e).__name__}: {e}"
        logger.error(  # Log the error from the wrapped function
            "Wrapped function '%s' execution error: %s",
            getattr(tool_function, "__name__", "unknown_func"),
            err_msg,
            exc_info=True,  # Include traceback for wrapped function error
        )
        attempt.error = err_msg
        if attempt not in context.attempts:
            context.attempts.append(attempt)

        _set_reliable_tool_context(context_vars, context_variables_key, context)

        # Go to runner in this scenario because an error can just be handled by the runner again
        return ReplyResult(
            context_variables=context_vars,
            target=AgentTarget(runner),
            message=f"Function execution failed with error: {err_msg}.",
        )

    def _process_successful_execution(
        attempt: ExecutionAttempt, result: Any, context_vars: ContextVariables, context: ReliableToolContext
    ) -> ReplyResult:
        value_to_stringify: Any = None

        if isinstance(result, tuple):
            if len(result) >= 2:
                attempt.result_data = result[0]
                value_to_stringify = result[1]
            elif len(result) == 1:
                attempt.result_data = result[0]
                value_to_stringify = result[0]
            else:
                attempt.result_data = None
                value_to_stringify = ""

        else:
            attempt.result_data = result
            value_to_stringify = result

        try:
            attempt.result_str = str(value_to_stringify) if value_to_stringify is not None else ""
        except Exception as str_e:
            logger.warning(
                "Could not convert result string part to string, using repr() \n %s",
                str_e,
            )
            attempt.result_str = repr(value_to_stringify)

        if attempt not in context.attempts:
            context.attempts.append(attempt)

        _set_reliable_tool_context(context_vars, context_variables_key, context)

        return ReplyResult(
            context_variables=context_vars,
            target=AgentTarget(validator),
            message=attempt.result_str,
        )

    if not is_original_func_async:

        @functools.wraps(tool_function)
        def sync_wrapper(
            *args: Any, hypothesis: str, context_variables: ContextVariables, **kwargs: Any
        ) -> ReplyResult:
            context = _get_reliable_tool_context(context_variables, context_variables_key)
            attempt = ExecutionAttempt(attempt_args=list(args), attempt_kwargs=kwargs, hypothesis=hypothesis)
            try:
                result = tool_function(*args, **kwargs)
                return _process_successful_execution(attempt, result, context_variables, context)
            except Exception as e:
                return _handle_execution_error(attempt, context_variables, context, e)

        wrapper_func = sync_wrapper
    else:

        @functools.wraps(tool_function)
        async def async_wrapper(
            *args: Any, hypothesis: str, context_variables: ContextVariables, **kwargs: Any
        ) -> ReplyResult:
            context = _get_reliable_tool_context(context_variables, context_variables_key)
            attempt = ExecutionAttempt(attempt_args=list(args), attempt_kwargs=kwargs, hypothesis=hypothesis)
            try:
                result = await tool_function(*args, **kwargs)
                return _process_successful_execution(attempt, result, context_variables, context)
            except Exception as e:
                return _handle_execution_error(attempt, context_variables, context, e)

        wrapper_func = async_wrapper

    params = list(tool_sig.parameters.values())
    pos_or_kw_params, kw_only_params, var_pos_param, var_kw_param = [], [], None, None
    for p in params:
        if p.kind == inspect.Parameter.POSITIONAL_ONLY or p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
            pos_or_kw_params.append(p)
        elif p.kind == inspect.Parameter.VAR_POSITIONAL:
            var_pos_param = p
        elif p.kind == inspect.Parameter.KEYWORD_ONLY:
            kw_only_params.append(p)
        elif p.kind == inspect.Parameter.VAR_KEYWORD:
            var_kw_param = p

    new_kw_only_params = [
        inspect.Parameter(
            "hypothesis",
            inspect.Parameter.KEYWORD_ONLY,
            annotation=Annotated[str, AG2Field(description=HYPOTHESIS_DESCRIPTION)],
            default=inspect.Parameter.empty,
        ),
        inspect.Parameter(
            "context_variables",
            inspect.Parameter.KEYWORD_ONLY,
            annotation=ContextVariables,
            default=inspect.Parameter.empty,
        ),
    ]

    wrapper_params = (
        pos_or_kw_params
        + ([var_pos_param] if var_pos_param else [])
        + kw_only_params
        + new_kw_only_params
        + ([var_kw_param] if var_kw_param else [])
    )
    setattr(wrapper_func, "__signature__", inspect.Signature(parameters=wrapper_params, return_annotation=ReplyResult))
    return wrapper_func


@export_module("autogen.tools.experimental")
class ReliableToolError(Exception):
    """Custom exception for errors during ReliableTool execution."""

    def __init__(self, message: str, final_context: ReliableToolContext | None = None):
        super().__init__(message)
        self.final_context = final_context


@export_module("autogen.tools.experimental")
class ReliableTool(Tool):
    INTERNAL_TOOL_NAME_PREFIX = "execute_"

    def __init__(
        self,
        name: str,
        func_or_tool: Callable[..., Any] | Tool,
        runner_llm_config: LLMConfig | dict[str, Any],
        validator_llm_config: LLMConfig | dict[str, Any],
        description: str | None = None,
        system_message_addition_for_tool_calling: str = "",
        system_message_addition_for_result_validation: str = "",
        max_tool_invocations: int = 3,
        enable_dynamic_validation: bool = False,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> None:
        """A ReliableTool wraps an existing function or tool.
        When the ReliableTool is invoked, it kicks off an internal Group Chat where a Runner
        and Validator agent will iteratively invoke the wrapped function or tool until
        *the output of a single invocation of the original function or tool satisfies the provided validation criteria.*
        Reliable Tools are best used when the LLM used or the function or tool itself is unreliable.
        Commonly this happens when using small, local LLMs, <32b params
        Or when functions/tools are used to "explore" (doing many web searches, exploring a database with SQL)
        The Reliable Tool allows the user to bake a result validation strategy into the tool itself
        so that the broader group chat/agentic system can be built more clearly around the intended flow
        instead of needing to focus so much on retry and validation loops.

        Additionally, the .run() and .a_run() methods serve as a way to use LLMs to invoke a specific tool outside
        of a Group Chat or similar structure to provide a more traditional programming method of using LLMs and tools in code.

        Args:
            name (str):
                A unique and descriptive name for this ReliableTool instance.
                This name is used for logging, internal context management, and can be
                how other agents or systems refer to this specific reliable capability.
                Example: `"AccurateWeatherForecaster"`, `"ValidatedCustomerLookup"`

            func_or_tool (Union[Callable[..., Any], Tool]):
                The core Python function or an existing AG2 `Tool` instance that this
                `ReliableTool` will manage and execute. This is the underlying capability
                you want to enhance with reliability features like retries and validation.
                The `ReliableTool` will handle calling this function with arguments
                determined by its internal Runner Agent based on the provided `task`.
                Example: `my_api_call_function`, `existing_search_tool_instance`

            runner_llm_config (Union[LLMConfig, dict[str, Any]]):
                The LLM configuration for the internal "Runner Agent". This agent is
                responsible for interpreting the high-level `task` provided when the
                `ReliableTool` is invoked, deciding the appropriate arguments for the
                `func_or_tool`, and initiating its execution.
                This configuration dictates the model, API keys, temperature, etc., for
                the LLM that attempts to call your function. It must support tool/function calling.
                Example: `LLMConfig(config_list=oai_config_list, model="gpt-4o-mini")`
                         `{"config_list": [{"model": "gpt-3.5-turbo", "api_key": "..."}], "temperature": 0.5}`

            validator_llm_config (Union[LLMConfig, dict[str, Any]]):
                The LLM configuration for the internal "Validator Agent". After the
                `func_or_tool` executes successfully, this agent receives its string output
                and assesses whether it meets defined validation criteria. It is
                configured for structured output (Pydantic model `ValidationResult`)
                to provide a boolean validation status and a justification.
                This configuration dictates the model, etc., for the LLM that validates
                the function's result. It can be the same as `runner_llm_config` or different.
                Example: `LLMConfig(config_list=oai_config_list, model="gpt-4o-mini")`

            description (Optional[str], default: None):
                A human-readable description of what this `ReliableTool` achieves.
                If `None`, the description is inferred from the docstring of the
                provided `func_or_tool`. This description is primarily for the public-facing
                `ReliableTool` (e.g., when registered with an outer agent for it to decide
                when to use this tool).
                Example: `"Reliably fetches and validates current weather information for a specified city."`

            system_message_addition_for_tool_calling (str, default: ""):
                Additional text appended to the system message of the internal "Runner Agent".
                This allows you to provide specific instructions, context, or constraints
                to the LLM responsible for deciding *how* to call your underlying `func_or_tool`.
                Use this when the Runner Agent needs more guidance than just the task
                description and the function's signature to correctly formulate arguments.
                Example: `"When calling 'search_products', if the task mentions 'budget', ensure the 'max_price' argument is set accordingly. Prioritize items in stock."`

            system_message_addition_for_result_validation (str, default: ""):
                Additional text appended to the system message of the internal "Validator Agent".
                This is where you define the *base* or *static* criteria for validating the
                *result* (string representation) of your `func_or_tool`. These criteria
                are applied on every validation attempt unless overridden or supplemented by
                dynamic validation.
                Example: `"The stock price must be a positive number. The company name in the result must match the one in the task. If data is unavailable, the result should explicitly state 'Data not found'."`

            max_tool_invocations (int, default: 3):
                The maximum number of times the internal "Runner Agent" can attempt to
                call the underlying `func_or_tool`. This limit includes the initial attempt
                and any subsequent retries that occur due to:
                1. Direct execution errors from `func_or_tool`.
                2. The Runner Agent failing to generate a valid tool call.
                3. The Validator Agent deeming a successful execution's result as invalid.
                Adjust this to control retries and prevent excessive LLM calls, considering
                the potential flakiness of the `func_or_tool` or complexity of parameterization.
                Example: `max_tool_invocations=2` (allows one initial attempt and one retry if needed).

            enable_dynamic_validation (bool, default: False):
                If `True`, the public-facing `run` (or `a_run`) method of this `ReliableTool`
                (accessible via its `func` attribute after initialization) will accept an
                additional optional argument: `validation_prompt_addition: Optional[str]`.
                If a string is provided for this argument during a call, it will be appended
                to the Validator Agent's system message *for that specific run*, allowing
                validation criteria to be tailored on-the-fly based on the task.
                Example: If `True`, `my_tool.func(task="search for AG2 examples", validation_prompt_addition="Result must include Python code snippets.")`

            messages (Optional[List[dict[str, Any]]], default: None):
                A list of initial messages (e.g., from a prior conversation history) to
                provide context to the internal Runner and Validator agents. These messages
                are prepended to the message history seen by these agents during their
                internal chat, helping them understand the `task` in a broader context.
                Use when the `task` for the `ReliableTool` might refer to entities or
                intentions established in preceding turns of a conversation.
                Example: `messages=[{"role": "user", "content": "I'm interested in large-cap tech stocks."}, {"role": "assistant", "content": "Okay, any specific ones?"}]`
                         (Then a task like "Fetch the latest price for 'the one we just discussed'.")

            ground_truth (Optional[List[str]], default: None):
                A list of strings representing factual information, examples, or specific
                constraints that should be considered by the internal Runner and Validator
                agents. These are injected into the conversation history as distinct user
                messages (e.g., "[[Provided Ground Truth 1]]: ...").
                Use to provide specific, factual data or strong hints that might not fit
                naturally into system messages or prior conversation history, guiding the
                agents towards correct interpretation or validation.
                Example: `ground_truth=["The API rate limit is 10 requests per minute.", "User preference: only show results from the last 7 days."]`
        """
        self._original_func, original_name, original_description = self._extract_func_details(func_or_tool)
        self._is_original_func_async = is_coroutine_callable(self._original_func)

        self._runner_llm_config = ConversableAgent._validate_llm_config(runner_llm_config)
        if self._runner_llm_config is False:
            raise ValueError("Runner LLM config failed validation.")
        # Validate validator_llm_config and store it. It can be LLMConfig | dict | False.
        self._validator_llm_config = ConversableAgent._validate_llm_config(validator_llm_config)
        if self._validator_llm_config is False:  # Check before use in _setup_validator_agent
            raise ValueError("Validator LLM config failed validation.")

        self._runner_system_message_addition = system_message_addition_for_tool_calling
        self._validator_system_message_addition = system_message_addition_for_result_validation
        self.max_tool_invocations = max_tool_invocations
        self._context_variables_key = f"{name}_ReliableToolContext_{id(self)}"

        self._original_func_name = original_name
        self.enable_dynamic_validation = enable_dynamic_validation

        self._init_messages = copy.deepcopy(messages) if messages is not None else None
        self._init_ground_truth = copy.deepcopy(ground_truth) if ground_truth else None

        self._tool_description = description if description is not None else original_description

        public_entry_point_func = self._define_public_entry_point(
            self._is_original_func_async, self.enable_dynamic_validation
        )

        super().__init__(
            name=name,
            description=self._tool_description,
            func_or_tool=public_entry_point_func,
        )

        self._validator_name = f"{self.name}_Validator"
        self._runner_name = f"{self.name}_Runner"

        self._validator = self._setup_validator_agent()
        self._runner = self._setup_runner_agent()
        self._reliable_func_wrapper = reliable_function_wrapper(
            self._original_func, self._validator, self._runner, self._context_variables_key
        )
        self._setup_runner_tool()
        self._register_internal_hooks()

    def _define_public_entry_point(self, is_async: bool, enable_dynamic: bool) -> Callable[..., Any]:
        if not is_async:
            if enable_dynamic:

                def sync_entry_point_with_validation(task: str, validation_prompt_addition: str | None = None) -> Any:
                    return self.run(task=task, validation_prompt_addition=validation_prompt_addition)

                return sync_entry_point_with_validation
            else:

                def sync_entry_point_without_validation(task: str) -> Any:
                    return self.run(task=task, validation_prompt_addition=None)

                return sync_entry_point_without_validation
        else:
            if enable_dynamic:

                async def async_entry_point_with_validation(
                    task: str, validation_prompt_addition: str | None = None
                ) -> Any:
                    return await self.a_run(task=task, validation_prompt_addition=validation_prompt_addition)

                return async_entry_point_with_validation
            else:

                async def async_entry_point_without_validation(task: str) -> Any:
                    return await self.a_run(task=task, validation_prompt_addition=None)

                return async_entry_point_without_validation

    def _extract_func_details(self, func_or_tool: Callable[..., Any] | Tool) -> tuple[Callable[..., Any], str, str]:
        default_desc_template = "Executes the '{name}' function."
        if isinstance(func_or_tool, Tool):
            func = getattr(func_or_tool, "func", None)
            if not callable(func):
                raise TypeError(
                    f"Tool '{func_or_tool.name}' provided but its 'func' attribute is not callable or missing."
                )
            name = func_or_tool.name
            desc = func_or_tool.description
            if not desc or desc == f"Tool '{name}'." or desc == "No description provided.":
                func_doc = inspect.getdoc(func)
                desc = func_doc.strip() if func_doc else f"{default_desc_template.format(name=name)}"
            return func, name, desc
        elif callable(func_or_tool):
            name = getattr(func_or_tool, "__name__", "callable_function")
            doc = inspect.getdoc(func_or_tool)
            desc = doc.strip() if doc else f"{default_desc_template.format(name=name)}"
            # For raw callables, we don't have a pre-computed schema like Tool object might
            return func_or_tool, name, desc
        raise TypeError(
            "Input 'func_or_tool' must be a callable or an autogen.Tool instance with a callable 'func' attribute."
        )

    def _setup_validator_agent(self) -> ConversableAgent:
        # _configure_llm_for_structured_output will raise ValueError if config is bad
        # Use a local variable for type narrowing after the False check.
        current_validator_config = self._validator_llm_config
        if current_validator_config is False:
            # This case should have been caught in __init__, but as a safeguard:
            raise ValueError("Validator LLM config is False, cannot proceed.")

        structured_llm_config = _configure_llm_for_structured_output(
            copy.deepcopy(current_validator_config),  # current_validator_config is not False here
            ValidationResult,
        )
        return ConversableAgent(
            name=self._validator_name,
            system_message="[Validator Prompt Updated Per Run]",
            llm_config=structured_llm_config,
            human_input_mode="NEVER",
        )

    def _setup_runner_agent(self) -> ConversableAgent:
        runner_llm_config_copy = copy.deepcopy(self._runner_llm_config)
        runner = ConversableAgent(
            name=self._runner_name,
            system_message="[Runner Prompt Updated Per Run]",
            llm_config=runner_llm_config_copy,
            human_input_mode="NEVER",
        )
        return runner

    def _setup_runner_tool(self) -> None:
        internal_tool_name = f"{self.INTERNAL_TOOL_NAME_PREFIX}{self._original_func_name}"
        internal_tool = Tool(
            name=internal_tool_name, description=self._tool_description, func_or_tool=self._reliable_func_wrapper
        )
        internal_tool.register_tool(self._runner)
        logger.info(
            "Successfully registered internal tool '%s' with runner '%s'", internal_tool_name, self._runner.name
        )

    def _register_internal_hooks(self) -> None:
        self._validator.register_hook(
            hookable_method="process_message_before_send", hook=self._validator_structured_output_hook
        )
        self._validator.register_hook(
            hookable_method="process_all_messages_before_reply", hook=self._validator_construct_context_hook
        )
        self._runner.register_hook(hookable_method="process_message_before_send", hook=self._ensure_function_call_hook)

    def _validator_structured_output_hook(
        self, sender: Agent, message: dict[str, Any] | str, recipient: Agent, silent: bool
    ) -> dict[str, Any] | str:
        if not isinstance(message, str):
            logger.error(
                f"Validator Hook: Expected a JSON string message from LLM, but got {type(message)}. Content: {str(message)[:200]}"
            )
            # This indicates a misconfiguration or unexpected LLM output format.
            raise TypeError(f"Validator hook expected str from LLM, got {type(message)}")

        validation_result_obj: ValidationResult = ValidationResult.model_validate_json(message)
        status = "PASSED" if validation_result_obj.validation_result else "FAILED"
        log_level = logging.INFO if status == "PASSED" else logging.WARNING
        logger.log(
            log_level,
            f"Validator Hook: Parsed Validation - {status}. Justification: {validation_result_obj.justification}",
        )

        self._try_update_context_validation(sender, validation_result_obj)
        # sender is self._validator in this hook context
        self._set_validator_handoff(self._validator, validation_result_obj.validation_result)
        return validation_result_obj.format()  # Return JSON string

    def _set_validator_handoff(self, validator_agent: ConversableAgent, validation_passed: bool) -> None:
        if not validation_passed:
            logger.info("Validation failed, setting handoff to runner: %s", self._runner_name)
            validator_agent.handoffs.set_after_work(target=AgentTarget(self._runner))
        else:
            logger.info("Validation passed, setting handoff to TerminateTarget.")
            validator_agent.handoffs.set_after_work(target=TerminateTarget())

    def _try_update_context_validation(self, sender: Agent, validation_result: ValidationResult) -> None:
        """Helper to attempt updating the validation state in the ReliableToolContext."""
        context_vars = getattr(sender, "context_variables")

        tool_context = _get_reliable_tool_context(context_vars, self._context_variables_key)
        latest_attempt = tool_context.latest_attempt

        if not latest_attempt:
            # This implies a logical error in the execution flow.
            raise RuntimeError(
                f"Validator hook: No execution attempt found in context '{self._context_variables_key}' to update validation for."
            )

        latest_attempt.validation = validation_result
        _set_reliable_tool_context(context_vars, self._context_variables_key, tool_context)
        logger.info(
            "Validator hook: Updated validation status in context: %s",
            "Passed" if validation_result.validation_result else "Failed",
        )

    def _validator_construct_context_hook(self, messages: list[dict[str, Any]], **kwargs: Any) -> list[dict[str, Any]]:
        sender = self._validator  # Assuming self._validator is the agent instance
        logger.debug("Validator Construct Context Hook running for agent %s.", sender.name)

        context_vars = getattr(sender, "context_variables")

        tool_context = _get_reliable_tool_context(context_vars, self._context_variables_key)
        initial_messages_to_inject = (
            copy.deepcopy(tool_context.initial_messages) if tool_context.initial_messages else []
        )

        ground_truth_messages_to_inject = []
        if tool_context.initial_ground_truth:
            for i, gt in enumerate(tool_context.initial_ground_truth):
                ground_truth_messages_to_inject.append({
                    "role": "user",
                    "content": f"[[Provided Ground Truth {i + 1}]]:\n{gt}",
                })

        last_content = _get_last_non_empty_message_content(messages)
        result_message_dict = {
            "role": "user",
            "content": f"--- Function Result to Validate ---\n```\n{last_content}\n```\n--- End of Result ---",
        }

        final_messages = initial_messages_to_inject + ground_truth_messages_to_inject + [result_message_dict]
        return final_messages

    def _ensure_function_call_hook(
        self, sender: Agent, message: dict[str, Any] | str, recipient: Agent, silent: bool
    ) -> dict[str, Any] | str:
        if sender.name != self._runner_name:
            return message

        tool_calls_list = None
        if isinstance(message, dict):
            tool_calls_list = message.get("tool_calls")

        tool_name_expected = f"{self.INTERNAL_TOOL_NAME_PREFIX}{self._original_func_name}"
        correct_tool_called = False
        if isinstance(tool_calls_list, list):
            for call in tool_calls_list:
                if (
                    isinstance(call, dict)
                    and call.get("type") == "function"
                    and isinstance(call.get("function"), dict)
                    and call["function"].get("name") == tool_name_expected
                ):
                    correct_tool_called = True
                    break

        if not correct_tool_called:
            if not hasattr(self._runner, "handoffs"):
                raise AttributeError(f"Runner agent '{self._runner.name}' missing 'handoffs' attribute for reminder.")
            self._runner.handoffs.set_after_work(target=AgentTarget(self._runner))  # Retry with runner

            logger.warning(
                "Runner '%s' did not generate required tool call for '%s'. Appending reminder.",
                self._runner_name,
                tool_name_expected,
            )
            reminder = (
                f"\n\n[[System Reminder: You MUST invoke the function '{tool_name_expected}' using a tool call. "
                "Provide all required arguments including 'hypothesis'.]]\n"
                "Correct your mistake and make a new attempt at invoking the tool."
            )

            current_content = ""
            if isinstance(message, str):
                current_content = message
            elif isinstance(message, dict):
                current_content = message.get("content") or ""

            # Return a new message dict to ensure it's processed correctly by the agent
            return {
                "role": "assistant",  # The LLM's previous turn was as assistant
                "content": (current_content or "") + reminder,
                "tool_calls": [] if isinstance(message, dict) else None,
            }
        return message

    def _execute_internal_group_chat(
        self,
        task: str,
        initial_context_vars: ContextVariables,  # Renamed for clarity
        dynamic_validation_str: str | None = None,
    ) -> tuple[ChatResult, ContextVariables, Agent]:
        internal_tool_name = f"{self.INTERNAL_TOOL_NAME_PREFIX}{self._original_func_name}"

        # update_system_message should not fail if agent is properly initialized
        runner_prompt = get_runner_prompt(task, self._runner_system_message_addition, internal_tool_name)
        self._runner.update_system_message(runner_prompt)

        validator_prompt = get_validator_prompt(task, self._validator_system_message_addition, dynamic_validation_str)
        self._validator.update_system_message(validator_prompt)

        # Store context ref on agents for hooks. Crucial for hooks to access shared state.
        self._validator.context_variables = initial_context_vars
        self._runner.context_variables = initial_context_vars

        messages_for_runner_history = []
        # Retrieve tool_context again to build runner history with potentially updated initial messages/GT
        # This is vital if _process_run (the caller) modifies them in initial_context_vars.
        tool_context = _get_reliable_tool_context(initial_context_vars, self._context_variables_key)

        if tool_context.initial_messages:
            messages_for_runner_history.extend(copy.deepcopy(tool_context.initial_messages))
        if tool_context.initial_ground_truth:
            for i, gt in enumerate(tool_context.initial_ground_truth):
                messages_for_runner_history.append({
                    "role": "user",
                    "content": f"[[Provided Ground Truth {i + 1}]]:\n{gt}",
                })

        task_message = {
            "role": "user",
            "content": f"[[Task Kickoff]]: Please execute the required function call for the task: {task}",
        }
        final_initial_messages_for_runner = messages_for_runner_history + [task_message]

        agent_pattern = DefaultPattern(
            agents=[self._runner, self._validator],
            initial_agent=self._runner,
            context_variables=initial_context_vars,
        )

        max_internal_rounds = 1 + (self.max_tool_invocations * 3)
        logger.debug(
            f"Setting max internal chat rounds to {max_internal_rounds} for {self.max_tool_invocations} tool invocations."
        )

        logger.info(
            f"--- Starting ReliableTool '{self.name}' Internal Chat (Max Invocations: {self.max_tool_invocations}) ---"
        )

        last_reply, final_context_vars, last_agent = initiate_group_chat(
            pattern=agent_pattern,
            messages=final_initial_messages_for_runner,
            max_rounds=max_internal_rounds,
        )
        logger.info(
            f"--- ReliableTool '{self.name}' Internal Chat Finished (Last Agent: {getattr(last_agent, 'name', 'N/A')}) ---"
        )
        if not isinstance(final_context_vars, ContextVariables):
            # This would be an unexpected issue with initiate_group_chat or pattern
            raise TypeError(f"Internal chat returned invalid context_variables type: {type(final_context_vars)}")
        return last_reply, final_context_vars, last_agent

    def _prepare_tool_context(
        self,
        task: str,
        current_context_variables: ContextVariables,
        validation_prompt_addition: str | None = None,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> ReliableToolContext:
        """Initializes or updates the ReliableToolContext for the current run."""
        effective_messages = copy.deepcopy(messages) if messages is not None else self._init_messages
        effective_ground_truth = copy.deepcopy(ground_truth) if ground_truth is not None else self._init_ground_truth

        tool_context = ReliableToolContext(task=task, reliable_tool_name=self.name)

        tool_context.task = task
        tool_context.dynamic_validation_input = validation_prompt_addition
        tool_context.initial_messages = effective_messages
        tool_context.initial_ground_truth = effective_ground_truth

        _set_reliable_tool_context(current_context_variables, self._context_variables_key, tool_context)
        return tool_context

    def _process_run(
        self,
        task: str,
        context_variables: ContextVariables | None = None,
        validation_prompt_addition: str | None = None,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> Any:
        current_context_variables = context_variables if context_variables is not None else ContextVariables()
        if not isinstance(current_context_variables, ContextVariables):
            raise TypeError(f"Expected context_variables as ContextVariables or None, got {type(context_variables)}")

        self._prepare_tool_context(task, current_context_variables, validation_prompt_addition, messages, ground_truth)

        final_tool_context: ReliableToolContext
        _, chat_context_variables, _ = self._execute_internal_group_chat(
            task=task,
            initial_context_vars=current_context_variables,
            dynamic_validation_str=validation_prompt_addition,
        )
        current_context_variables = chat_context_variables

        final_tool_context = _get_reliable_tool_context(current_context_variables, self._context_variables_key)
        latest_attempt_obj = final_tool_context.latest_attempt

        if not latest_attempt_obj:
            raise ReliableToolError(
                "Critical internal error: No execution attempt recorded after chat cycle.",
                final_context=final_tool_context,
            )

        # If execution was successful BUT validation is missing (e.g. validator hook failed to set it)
        if latest_attempt_obj.did_execute_successfully and latest_attempt_obj.validation is None:
            logger.warning(
                "[%s]: Validation result missing after successful execution. Assuming validation failed.", self.name
            )
            latest_attempt_obj.validation = ValidationResult(
                validation_result=False,
                justification="Validation result was not recorded after successful execution. Usually due to group chat reaching maximum runs",
            )
            _set_reliable_tool_context(current_context_variables, self._context_variables_key, final_tool_context)

        if final_tool_context.is_complete_and_successful:
            logger.info("ReliableTool '%s' succeeded.", self.name)
            return final_tool_context.get_final_result_data()
        else:
            failure_reason = final_tool_context.get_failure_summary()
            logger.warning("ReliableTool '%s' failed. Reason: %s", self.name, failure_reason)
            raise ReliableToolError(
                f"ReliableTool '{self.name}' failed. Last failure: {failure_reason}",
                final_context=final_tool_context,
            )

    def run(
        self,
        task: str,
        context_variables: ContextVariables | None = None,
        validation_prompt_addition: str | None = None,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> Any:
        if self._is_original_func_async:
            raise TypeError(f"Sync 'run()' called for async tool '{self.name}'. Use 'a_run()'.")
        return self._process_run(
            task=task,
            context_variables=context_variables,
            validation_prompt_addition=validation_prompt_addition,
            messages=messages,
            ground_truth=ground_truth,
        )

    async def a_run(
        self,
        task: str,
        context_variables: ContextVariables | None = None,
        validation_prompt_addition: str | None = None,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> Any:
        if not self._is_original_func_async:
            warnings.warn(
                f"Running sync function '{self._original_func_name}' wrapped by ReliableTool '{self.name}' "
                f"asynchronously using 'a_run()'. The underlying execution of _process_run will be synchronous "
                f"within an executor.",
                UserWarning,
            )

        loop = asyncio.get_running_loop()
        func_call = functools.partial(
            self._process_run,
            task=task,
            context_variables=context_variables,
            validation_prompt_addition=validation_prompt_addition,
            messages=messages,
            ground_truth=ground_truth,
        )
        return await loop.run_in_executor(None, func_call)

    def _process_run_with_details(
        self,
        task: str,
        context_variables: ContextVariables | None = None,
        validation_prompt_addition: str | None = None,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> ToolExecutionDetails:
        current_context_variables = context_variables if context_variables is not None else ContextVariables()
        if not isinstance(current_context_variables, ContextVariables):
            err_msg = f"Invalid ContextVariables type: {type(context_variables)}"
            # Create a minimal context for reporting
            err_ctx = ReliableToolContext(task=task, reliable_tool_name=self.name)
            err_ctx.attempts.append(ExecutionAttempt(error=f"Initialization error: {err_msg}"))
            return ToolExecutionDetails(
                task=task, is_overall_successful=False, failure_reason=err_msg, final_tool_context=err_ctx
            )

        tool_context_for_run: ReliableToolContext
        try:
            # Initialize or update tool context state. Raises on ser/de errors.
            tool_context_for_run = self._prepare_tool_context(
                task, current_context_variables, validation_prompt_addition, messages, ground_truth
            )
        except (ValueError, TypeError) as e_ctx_setup:
            err_msg = f"Error during ReliableToolContext setup: {e_ctx_setup}"
            logger.error("[%s] %s", self.name, err_msg, exc_info=True)
            err_ctx = ReliableToolContext(task=task, reliable_tool_name=self.name)
            err_ctx.attempts.append(ExecutionAttempt(error=f"Context setup error: {e_ctx_setup}"))
            return ToolExecutionDetails(
                task=task, is_overall_successful=False, failure_reason=err_msg, final_tool_context=err_ctx
            )

        # Variables for ToolExecutionDetails
        is_successful_val = False
        failure_reason_val = None
        successful_params_val = None
        final_tool_context_val: ReliableToolContext = tool_context_for_run  # Start with prepared context

        try:
            _, chat_context_variables, _ = self._execute_internal_group_chat(
                task=task,
                initial_context_vars=current_context_variables,  # This contains the prepared tool_context_for_run
                dynamic_validation_str=validation_prompt_addition,
            )
            current_context_variables = chat_context_variables  # Update with context from chat

            final_tool_context_val = _get_reliable_tool_context(current_context_variables, self._context_variables_key)
            latest_attempt = final_tool_context_val.latest_attempt

            if not latest_attempt:
                failure_reason_val = "Critical internal error: No execution attempt recorded after chat cycle."
                # final_tool_context_val already reflects this state if attempts list is empty
            elif latest_attempt.did_execute_successfully and latest_attempt.validation is None:
                logger.warning(
                    "[%s]: Validation result missing after successful execution. Assuming validation failed.", self.name
                )
                latest_attempt.validation = ValidationResult(
                    validation_result=False,
                    justification="Validation result was not recorded after successful execution.",
                )
                _set_reliable_tool_context(
                    current_context_variables, self._context_variables_key, final_tool_context_val
                )

            if final_tool_context_val.is_complete_and_successful:
                is_successful_val = True
                # latest_attempt must exist if is_complete_and_successful is true
                # Re-fetch or assert to help Mypy understand it's not None
                confirmed_latest_attempt = final_tool_context_val.latest_attempt
                assert confirmed_latest_attempt is not None, (
                    "Internal logic error: is_complete_and_successful is True but latest_attempt is None"
                )
                successful_params_val = SuccessfulExecutionParameters(
                    attempt_args=confirmed_latest_attempt.attempt_args,
                    attempt_kwargs=confirmed_latest_attempt.attempt_kwargs,
                )
            else:
                failure_reason_val = final_tool_context_val.get_failure_summary()

        except ReliableToolError as e:
            is_successful_val = False
            failure_reason_val = f"ReliableTool execution failed: {e}"
            logger.warning("[%s] %s", self.name, failure_reason_val)  # Log the failure reason from ReliableToolError
            final_tool_context_val = e.final_context or final_tool_context_val  # Use context from error if available
            if not final_tool_context_val.attempts:  # Ensure some attempt is logged if context is minimal
                final_tool_context_val.attempts.append(ExecutionAttempt(error=str(e)))

        except (KeyError, ValueError, TypeError) as e_ctx_final:  # Context errors after chat
            is_successful_val = False
            failure_reason_val = f"Critical error involving context after chat: {e_ctx_final}"
            logger.error("[%s] %s", self.name, failure_reason_val, exc_info=True)
            try:  # Try to get the latest context, otherwise use what we had
                final_tool_context_val = _get_reliable_tool_context(
                    current_context_variables, self._context_variables_key
                )
            except (
                Exception
            ):  # If still fails, final_tool_context_val remains as tool_context_for_run or from a prior partial update
                if not final_tool_context_val.attempts or final_tool_context_val.attempts[-1].error is None:
                    final_tool_context_val.attempts.append(ExecutionAttempt(error=failure_reason_val))

        except Exception as e_unexp:  # Unexpected errors during the process
            is_successful_val = False
            failure_reason_val = f"Unexpected error during reliable execution: {e_unexp}"
            logger.error("[%s] %s", self.name, failure_reason_val, exc_info=True)
            try:  # Try to get the latest context
                final_tool_context_val = _get_reliable_tool_context(
                    current_context_variables, self._context_variables_key
                )
            except (
                Exception
            ):  # If still fails, final_tool_context_val remains as tool_context_for_run or from a prior partial update
                if not final_tool_context_val.attempts or final_tool_context_val.attempts[-1].error is None:
                    final_tool_context_val.attempts.append(ExecutionAttempt(error=failure_reason_val))

        return ToolExecutionDetails(
            task=task,
            is_overall_successful=is_successful_val,
            failure_reason=failure_reason_val,
            successful_parameters=successful_params_val,
            final_tool_context=final_tool_context_val,
        )

    def run_and_get_details(
        self,
        task: str,
        context_variables: ContextVariables | None = None,
        validation_prompt_addition: str | None = None,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> ToolExecutionDetails:
        if self._is_original_func_async:
            raise TypeError(
                f"Synchronous 'run_and_get_details()' called for an async tool '{self.name}'. "
                f"Use 'a_run_and_get_details()' instead."
            )
        return self._process_run_with_details(
            task=task,
            context_variables=context_variables,
            validation_prompt_addition=validation_prompt_addition,
            messages=messages,
            ground_truth=ground_truth,
        )

    async def a_run_and_get_details(
        self,
        task: str,
        context_variables: ContextVariables | None = None,
        validation_prompt_addition: str | None = None,
        messages: list[dict[str, Any]] | None = None,
        ground_truth: list[str] | None = None,
    ) -> ToolExecutionDetails:
        if not self._is_original_func_async:
            warnings.warn(
                f"Running sync function '{self._original_func_name}' (wrapped by ReliableTool '{self.name}') "
                f"asynchronously using 'a_run_and_get_details()'. The underlying execution will be synchronous "
                f"within an executor.",
                UserWarning,
            )

        loop = asyncio.get_running_loop()
        try:
            func_call = functools.partial(
                self._process_run_with_details,
                task=task,
                context_variables=context_variables,
                validation_prompt_addition=validation_prompt_addition,
                messages=messages,
                ground_truth=ground_truth,
            )
            details: ToolExecutionDetails = await loop.run_in_executor(None, func_call)
            return details
        except Exception as e:
            logger.critical(
                "[%s] a_run_and_get_details encountered an unhandled exception from executor: %s",
                self.name,
                e,
                exc_info=True,
            )
            fallback_ctx = ReliableToolContext(task=task, reliable_tool_name=self.name)
            fallback_ctx.attempts.append(
                ExecutionAttempt(error=f"Unhandled executor/process error: {type(e).__name__}: {e}")
            )
            return ToolExecutionDetails(
                task=task,
                is_overall_successful=False,
                failure_reason=f"Critical unhandled exception during async execution: {type(e).__name__}: {e}",
                final_tool_context=fallback_ctx,
            )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .wikipedia import WikipediaPageLoadTool, WikipediaQueryRunTool

__all__ = ["WikipediaPageLoadTool", "WikipediaQueryRunTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

import requests
from pydantic import BaseModel

from autogen.import_utils import optional_import_block, require_optional_import
from autogen.tools import Tool

with optional_import_block():
    import wikipediaapi

# Maximum allowed length for a query string.
MAX_QUERY_LENGTH = 300
# Maximum number of pages to retrieve from a search.
MAX_PAGE_RETRIEVE = 100
# Maximum number of characters to return from a Wikipedia page.
MAX_ARTICLE_LENGTH = 10000


class Document(BaseModel):
    """Pydantic model representing a Wikipedia document.

    Attributes:
        page_content (str): Textual content of the Wikipedia page
            (possibly truncated).
        metadata (dict[str, str]): Additional info, including:
            - source URL
            - title
            - pageid
            - timestamp
            - word count
            - size
    """

    page_content: str
    metadata: dict[str, str]


class WikipediaClient:
    """Client for interacting with the Wikipedia API.

    Supports searching and page retrieval on a specified language edition.

    Public methods:
        search(query: str, limit: int) -> list[dict[str, Any]]
        get_page(title: str) -> Optional[wikipediaapi.WikipediaPage]

    Attributes:
        base_url (str): URL of the MediaWiki API endpoint.
        headers (dict[str, str]): HTTP headers, including User-Agent.
        wiki (wikipediaapi.Wikipedia): Low-level Wikipedia API client.
    """

    def __init__(self, language: str = "en", tool_name: str = "wikipedia-client") -> None:
        """Initialize the WikipediaClient.

        Args:
            language (str): ISO code of the Wikipedia edition (e.g., 'en', 'es').
            tool_name (str): Identifier for User-Agent header.
        """
        self.base_url = f"https://{language}.wikipedia.org/w/api.php"
        self.headers = {"User-Agent": f"autogen.Agent ({tool_name})"}
        self.wiki = wikipediaapi.Wikipedia(
            language=language,
            extract_format=wikipediaapi.ExtractFormat.WIKI,
            user_agent=f"autogen.Agent ({tool_name})",
        )

    def search(self, query: str, limit: int = 3) -> Any:
        """Search Wikipedia for pages matching a query string.

        Args:
            query (str): The search keywords.
            limit (int): Max number of results to return.

        Returns:
            list[dict[str, Any]]: Each dict has keys:
                - 'title' (str)
                - 'size' (int)
                - 'wordcount' (int)
                - 'timestamp' (str)

        Raises:
            requests.HTTPError: If the HTTP request to the API fails.
        """
        params = {
            "action": "query",
            "format": "json",
            "list": "search",
            "srsearch": query,
            "srlimit": str(limit),
            "srprop": "size|wordcount|timestamp",
        }

        response = requests.get(url=self.base_url, params=params, headers=self.headers)
        response.raise_for_status()
        data = response.json()
        search_data = data.get("query", {}).get("search", [])
        return search_data

    def get_page(self, title: str) -> Any | None:
        """Retrieve a WikipediaPage object by title.

        Args:
            title (str): Title of the Wikipedia page.

        Returns:
            wikipediaapi.WikipediaPage | None: The page object if it exists,
            otherwise None.

        Raises:
            wikipediaapi.WikipediaException: On lowerlevel API errors.
        """
        page = self.wiki.page(title)
        if not page.exists():
            return None
        return page


@require_optional_import(["wikipediaapi"], "wikipedia")
class WikipediaQueryRunTool(Tool):
    """Tool for querying Wikipedia and returning summarized page results.

    This tool uses the `wikipediaapi` package to perform searches
    against a specified language edition of Wikipedia and returns
    up to `top_k` page summaries.

    Public methods:
        query_run(query: str) -> list[str] | str

    Attributes:
        language (str): Language code for the Wikipedia edition (e.g., 'en', 'es').
        top_k (int): Max number of page summaries returned ( MAX_PAGE_RETRIEVE).
        verbose (bool): If True, enables debug logging to stdout.
        wiki_cli (WikipediaClient): Internal client for Wikipedia API calls.
    """

    def __init__(self, language: str = "en", top_k: int = 3, verbose: bool = False) -> None:
        """Initialize the WikipediaQueryRunTool.

        Args:
            language (str): ISO code of the Wikipedia edition to query.
            top_k (int): Desired number of summaries (capped by MAX_PAGE_RETRIEVE).
            verbose (bool): If True, print debug information during searches.
        """
        self.language = language
        self.tool_name = "wikipedia-query-run"
        self.wiki_cli = WikipediaClient(language, self.tool_name)
        self.top_k = min(top_k, MAX_PAGE_RETRIEVE)
        self.verbose = verbose
        super().__init__(
            name=self.tool_name,
            description="Run a Wikipedia query and return page summaries.",
            func_or_tool=self.query_run,
        )

    def query_run(self, query: str) -> list[str] | str:
        """Search Wikipedia and return formatted page summaries.

        Truncates `query` to MAX_QUERY_LENGTH before searching.

        Args:
            query (str): Search term(s) to look up in Wikipedia.

        Returns:
            list[str]: Each element is "Page: <title>\nSummary: <text>".
            str: Error message if no results are found or on exception.

        Note:
            Automatically handles API exceptions and returns error strings for robust operation
        """
        try:
            if self.verbose:
                print(f"INFO\t [{self.tool_name}] search query='{query[:MAX_QUERY_LENGTH]}' top_k={self.top_k}")
            search_results = self.wiki_cli.search(query[:MAX_QUERY_LENGTH], limit=self.top_k)
            summaries: list[str] = []
            for item in search_results:
                title = item["title"]
                page = self.wiki_cli.get_page(title)
                # Only format the summary if the page exists and has a summary.
                if page is not None and page.summary:
                    summary = f"Page: {title}\nSummary: {page.summary}"
                    summaries.append(summary)
            if not summaries:
                return "No good Wikipedia Search Result was found"
            return summaries
        except Exception as e:
            return f"wikipedia search failed: {str(e)}"


@require_optional_import(["wikipediaapi"], "wikipedia")
class WikipediaPageLoadTool(Tool):
    """A tool to load up to N characters of Wikipedia page content along with metadata.

    This tool uses a language-specific Wikipedia client to search for relevant articles
    and returns a list of Document objects containing truncated page content and metadata
    (source URL, title, page ID, timestamp, word count, and size). Ideal for agents
    requiring structured Wikipedia data for research, summarization, or contextual enrichment.

    Attributes:
        language (str): Wikipedia language code (default: "en").
        top_k (int): Maximum number of pages to retrieve per query (default: 3).
        truncate (int): Maximum number of characters of content per page (default: 4000).
        verbose (bool): If True, prints debug information (default: False).
        tool_name (str): Identifier used in User-Agent header.
        wiki_cli (WikipediaClient): Client for interacting with the Wikipedia API.
    """

    def __init__(self, language: str = "en", top_k: int = 3, truncate: int = 4000, verbose: bool = False) -> None:
        """Initializes the WikipediaPageLoadTool with configurable language, result count, and content length.

        Args:
            language (str): The language code for the Wikipedia edition (default is "en").
            top_k (int): The maximum number of pages to retrieve per query (default is 3;
                         capped at MAX_PAGE_RETRIEVE).
            truncate (int): The maximum number of characters to extract from each page (default is 4000;
                            capped at MAX_ARTICLE_LENGTH).
            verbose (bool): If True, enables verbose/debug logging (default is False).
        """
        self.language = language
        self.top_k = min(top_k, MAX_PAGE_RETRIEVE)
        self.truncate = min(truncate, MAX_ARTICLE_LENGTH)
        self.verbose = verbose
        self.tool_name = "wikipedia-page-load"
        self.wiki_cli = WikipediaClient(language, self.tool_name)
        super().__init__(
            name=self.tool_name,
            description=(
                "Search Wikipedia for relevant pages using a language-specific client. "
                "Returns a list of documents with truncated content and metadata including title, URL, "
                "page ID, timestamp, word count, and page size. Configure number of results with the 'top_k' parameter "
                "and content length with 'truncate'. Useful for research, summarization, or contextual enrichment."
            ),
            func_or_tool=self.content_search,
        )

    def content_search(self, query: str) -> list[Document] | str:
        """Executes a Wikipedia search and returns page content plus metadata.

        Args:
            query (str): The search term to query Wikipedia.

        Returns:
            Union[list[Document], str]:
                - list[Document]: Documents with up to `truncate` characters of page text
                  and metadata if pages are found.
                - str: Error message if the search fails or no pages are found.

        Notes:
            - Errors are caught internally and returned as strings.
            - If no matching pages have text content, returns
              "No good Wikipedia Search Result was found".
        """
        try:
            if self.verbose:
                print(f"INFO\t [{self.tool_name}] search query='{query[:MAX_QUERY_LENGTH]}' top_k={self.top_k}")
            search_results = self.wiki_cli.search(query[:MAX_QUERY_LENGTH], limit=self.top_k)
            docs: list[Document] = []
            for item in search_results:
                page = self.wiki_cli.get_page(item["title"])
                # Only process pages that exist and have text content.
                if page is not None and page.text:
                    document = Document(
                        page_content=page.text[: self.truncate],
                        metadata={
                            "source": f"https://{self.language}.wikipedia.org/?curid={item['pageid']}",
                            "title": item["title"],
                            "pageid": str(item["pageid"]),
                            "timestamp": str(item["timestamp"]),
                            "wordcount": str(item["wordcount"]),
                            "size": str(item["size"]),
                        },
                    )
                    docs.append(document)
            if not docs:
                return "No good Wikipedia Search Result was found"
            return docs

        except Exception as e:
            return f"wikipedia search failed: {str(e)}"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .duckduckgo_search import DuckDuckGoSearchTool

__all__ = ["DuckDuckGoSearchTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from typing import Annotated, Any

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ... import Tool

with optional_import_block():
    from duckduckgo_search import DDGS


@require_optional_import(
    [
        "duckduckgo_search",
    ],
    "duckduckgo_search",
)
def _execute_duckduckgo_query(
    query: str,
    num_results: int = 5,
) -> list[dict[str, Any]]:
    """Execute a search query using the DuckDuckGo Search API.

    Args:
        query (str): The search query string.
        num_results (int, optional): The maximum number of results to return. Defaults to 5.

    Returns:
        list[dict[str, Any]]: A list of search results from the DuckDuckGo API.
    """
    with DDGS() as ddgs:
        try:
            # region='wt-wt' means worldwide
            results = list(ddgs.text(query, region="wt-wt", max_results=num_results))
        except Exception as e:
            print(f"DuckDuckGo Search failed: {e}")
            results = []
    return results


def _duckduckgo_search(
    query: str,
    num_results: int = 5,
) -> list[dict[str, Any]]:
    """Perform a DuckDuckGo search and format the results.

    This function takes search parameters, executes the query using `_execute_duckduckgo_query`,
    and formats the results into a list of dictionaries containing title, link, and snippet.

    Args:
        query (str): The search query string.
        num_results (int, optional): The maximum number of results to return. Defaults to 5.

    Returns:
        list[dict[str, Any]]: A list of dictionaries, where each dictionary represents a search result
            with keys 'title', 'link', and 'snippet'. Returns an empty list if no results are found.
    """
    res = _execute_duckduckgo_query(
        query=query,
        num_results=num_results,
    )

    return [
        {"title": item.get("title", ""), "link": item.get("href", ""), "snippet": item.get("body", "")} for item in res
    ]


@export_module("autogen.tools.experimental")
class DuckDuckGoSearchTool(Tool):
    """DuckDuckGoSearchTool is a tool that uses DuckDuckGo to perform a search.

    This tool allows agents to leverage the DuckDuckGo search engine for information retrieval.
    DuckDuckGo does not require an API key, making it easy to use.
    """

    def __init__(self) -> None:
        """Initializes the DuckDuckGoSearchTool."""

        def duckduckgo_search(
            query: Annotated[str, "The search query."],
            num_results: Annotated[int, "The number of results to return."] = 5,
        ) -> list[dict[str, Any]]:
            """Performs a search using the DuckDuckGo Search API and returns formatted results.

            Args:
                query: The search query string.
                num_results: The maximum number of results to return. Defaults to 5.

            Returns:
                A list of dictionaries, each containing 'title', 'link', and 'snippet' of a search result.
            """
            return _duckduckgo_search(
                query=query,
                num_results=num_results,
            )

        super().__init__(
            name="duckduckgo_search",
            description="Use the DuckDuckGo Search API to perform a search.",
            func_or_tool=duckduckgo_search,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .firecrawl_tool import FirecrawlTool

__all__ = ["FirecrawlTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
from typing import Annotated, Any

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ....llm_config import LLMConfig
from ... import Depends, Tool
from ...dependency_injection import on

logger = logging.getLogger(__name__)

with optional_import_block():
    from firecrawl import FirecrawlApp, ScrapeOptions


@require_optional_import(
    [
        "firecrawl-py",
    ],
    "firecrawl",
)
def _execute_firecrawl_scrape(
    url: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    formats: list[str] | None = None,
    include_tags: list[str] | None = None,
    exclude_tags: list[str] | None = None,
    headers: dict[str, str] | None = None,
    wait_for: int | None = None,
    timeout: int | None = None,
) -> dict[str, Any]:
    """Execute a scrape operation using the Firecrawl API.

    Args:
        url (str): The URL to scrape.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None (uses default or env var).
        formats (list[str], optional): Output formats (e.g., ['markdown', 'html']). Defaults to ['markdown'].
        include_tags (list[str], optional): HTML tags to include. Defaults to None.
        exclude_tags (list[str], optional): HTML tags to exclude. Defaults to None.
        headers (dict[str, str], optional): HTTP headers to use. Defaults to None.
        wait_for (int, optional): Time to wait for page load in milliseconds. Defaults to None.
        timeout (int, optional): Request timeout in milliseconds. Defaults to None.

    Returns:
        dict[str, Any]: The scrape result from Firecrawl.
    """
    if formats is None:
        formats = ["markdown"]

    app = FirecrawlApp(api_key=firecrawl_api_key, api_url=firecrawl_api_url)

    result = app.scrape_url(
        url=url,
        formats=formats,
        include_tags=include_tags,
        exclude_tags=exclude_tags,
        headers=headers,
        wait_for=wait_for,
        timeout=timeout,
    )
    return dict(result)


@require_optional_import(
    [
        "firecrawl-py",
    ],
    "firecrawl",
)
def _execute_firecrawl_crawl(
    url: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    limit: int = 5,
    formats: list[str] | None = None,
    include_paths: list[str] | None = None,
    exclude_paths: list[str] | None = None,
    max_depth: int | None = None,
    allow_backward_crawling: bool = False,
    allow_external_content_links: bool = False,
) -> dict[str, Any]:
    """Execute a crawl operation using the Firecrawl API.

    Args:
        url (str): The starting URL to crawl.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None (uses default or env var).
        limit (int, optional): Maximum number of pages to crawl. Defaults to 5.
        formats (list[str], optional): Output formats (e.g., ['markdown', 'html']). Defaults to ['markdown'].
        include_paths (list[str], optional): URL patterns to include. Defaults to None.
        exclude_paths (list[str], optional): URL patterns to exclude. Defaults to None.
        max_depth (int, optional): Maximum crawl depth. Defaults to None.
        allow_backward_crawling (bool, optional): Allow crawling backward links. Defaults to False.
        allow_external_content_links (bool, optional): Allow external links. Defaults to False.

    Returns:
        dict[str, Any]: The crawl result from Firecrawl.
    """
    if formats is None:
        formats = ["markdown"]

    app = FirecrawlApp(api_key=firecrawl_api_key, api_url=firecrawl_api_url)

    # Build scrape options for crawling
    scrape_options = None
    if formats:
        scrape_options = ScrapeOptions(formats=formats)

    result = app.crawl_url(
        url=url,
        limit=limit,
        scrape_options=scrape_options,
        include_paths=include_paths,
        exclude_paths=exclude_paths,
        max_depth=max_depth,
        allow_backward_links=allow_backward_crawling,
        allow_external_links=allow_external_content_links,
    )
    return dict(result)


@require_optional_import(
    [
        "firecrawl-py",
    ],
    "firecrawl",
)
def _execute_firecrawl_map(
    url: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    search: str | None = None,
    ignore_sitemap: bool = False,
    include_subdomains: bool = False,
    limit: int = 5000,
) -> dict[str, Any]:
    """Execute a map operation using the Firecrawl API to get URLs from a website.

    Args:
        url (str): The website URL to map.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None (uses default or env var).
        search (str, optional): Search term to filter URLs. Defaults to None.
        ignore_sitemap (bool, optional): Whether to ignore the sitemap. Defaults to False.
        include_subdomains (bool, optional): Whether to include subdomains. Defaults to False.
        limit (int, optional): Maximum number of URLs to return. Defaults to 5000.

    Returns:
        dict[str, Any]: The map result from Firecrawl.
    """
    app = FirecrawlApp(api_key=firecrawl_api_key, api_url=firecrawl_api_url)

    result = app.map_url(
        url=url,
        search=search,
        ignore_sitemap=ignore_sitemap,
        include_subdomains=include_subdomains,
        limit=limit,
    )
    return dict(result)


@require_optional_import(
    [
        "firecrawl-py",
    ],
    "firecrawl",
)
def _execute_firecrawl_search(
    query: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    limit: int = 5,
    tbs: str | None = None,
    filter: str | None = None,
    lang: str = "en",
    country: str = "us",
    location: str | None = None,
    timeout: int | None = None,
) -> dict[str, Any]:
    """Execute a search operation using the Firecrawl API.

    Args:
        query (str): The search query string.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None (uses default or env var).
        limit (int, optional): Maximum number of results to return. Defaults to 5.
        tbs (str, optional): Time filter (e.g., "qdr:d" for past day). Defaults to None.
        filter (str, optional): Custom result filter. Defaults to None.
        lang (str, optional): Language code. Defaults to "en".
        country (str, optional): Country code. Defaults to "us".
        location (str, optional): Geo-targeting location. Defaults to None.
        timeout (int, optional): Request timeout in milliseconds. Defaults to None.

    Returns:
        dict[str, Any]: The search result from Firecrawl.
    """
    app = FirecrawlApp(api_key=firecrawl_api_key, api_url=firecrawl_api_url)

    result = app.search(
        query=query,
        limit=limit,
        tbs=tbs,
        filter=filter,
        lang=lang,
        country=country,
        location=location,
        timeout=timeout,
    )
    return dict(result)


@require_optional_import(
    [
        "firecrawl-py",
    ],
    "firecrawl",
)
def _execute_firecrawl_deep_research(
    query: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    max_depth: int = 7,
    time_limit: int = 270,
    max_urls: int = 20,
    analysis_prompt: str | None = None,
    system_prompt: str | None = None,
) -> dict[str, Any]:
    """Execute a deep research operation using the Firecrawl API.

    Args:
        query (str): The research query or topic to investigate.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None (uses default or env var).
        max_depth (int, optional): Maximum depth of research exploration. Defaults to 7.
        time_limit (int, optional): Time limit in seconds for research. Defaults to 270.
        max_urls (int, optional): Maximum number of URLs to process. Defaults to 20.
        analysis_prompt (str, optional): Custom prompt for analysis. Defaults to None.
        system_prompt (str, optional): Custom system prompt. Defaults to None.

    Returns:
        dict[str, Any]: The deep research result from Firecrawl.
    """
    app = FirecrawlApp(api_key=firecrawl_api_key, api_url=firecrawl_api_url)

    result = app.deep_research(
        query=query,
        max_depth=max_depth,
        time_limit=time_limit,
        max_urls=max_urls,
        analysis_prompt=analysis_prompt,
        system_prompt=system_prompt,
    )
    return dict(result)


def _firecrawl_scrape(
    url: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    formats: list[str] | None = None,
    include_tags: list[str] | None = None,
    exclude_tags: list[str] | None = None,
    headers: dict[str, str] | None = None,
    wait_for: int | None = None,
    timeout: int | None = None,
) -> list[dict[str, Any]]:
    """Perform a Firecrawl scrape and format the results.

    Args:
        url (str): The URL to scrape.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None.
        formats (list[str], optional): Output formats. Defaults to ['markdown'].
        include_tags (list[str], optional): HTML tags to include. Defaults to None.
        exclude_tags (list[str], optional): HTML tags to exclude. Defaults to None.
        headers (dict[str, str], optional): HTTP headers to use. Defaults to None.
        wait_for (int, optional): Time to wait for page load in milliseconds. Defaults to None.
        timeout (int, optional): Request timeout in milliseconds. Defaults to None.

    Returns:
        list[dict[str, Any]]: A list containing the scraped content.
    """
    try:
        result = _execute_firecrawl_scrape(
            url=url,
            firecrawl_api_key=firecrawl_api_key,
            firecrawl_api_url=firecrawl_api_url,
            formats=formats,
            include_tags=include_tags,
            exclude_tags=exclude_tags,
            headers=headers,
            wait_for=wait_for,
            timeout=timeout,
        )

        # Format the result to match expected output
        formatted_result = {
            "title": result.get("metadata", {}).get("title", ""),
            "url": url,
            "content": result.get("markdown", result.get("html", "")),
            "metadata": result.get("metadata", {}),
        }

        return [formatted_result]
    except Exception as e:
        logger.error(f"Firecrawl scrape failed: {e}")
        return []


def _firecrawl_crawl(
    url: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    limit: int = 5,
    formats: list[str] | None = None,
    include_paths: list[str] | None = None,
    exclude_paths: list[str] | None = None,
    max_depth: int | None = None,
    allow_backward_crawling: bool = False,
    allow_external_content_links: bool = False,
) -> list[dict[str, Any]]:
    """Perform a Firecrawl crawl and format the results.

    Args:
        url (str): The starting URL to crawl.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None.
        limit (int, optional): Maximum number of pages to crawl. Defaults to 5.
        formats (list[str], optional): Output formats. Defaults to ['markdown'].
        include_paths (list[str], optional): URL patterns to include. Defaults to None.
        exclude_paths (list[str], optional): URL patterns to exclude. Defaults to None.
        max_depth (int, optional): Maximum crawl depth. Defaults to None.
        allow_backward_crawling (bool, optional): Allow crawling backward links. Defaults to False.
        allow_external_content_links (bool, optional): Allow external links. Defaults to False.

    Returns:
        list[dict[str, Any]]: A list of crawled pages with content.
    """
    try:
        result = _execute_firecrawl_crawl(
            url=url,
            firecrawl_api_key=firecrawl_api_key,
            firecrawl_api_url=firecrawl_api_url,
            limit=limit,
            formats=formats,
            include_paths=include_paths,
            exclude_paths=exclude_paths,
            max_depth=max_depth,
            allow_backward_crawling=allow_backward_crawling,
            allow_external_content_links=allow_external_content_links,
        )

        # Format the results
        formatted_results = []
        data = result.get("data", [])

        for item in data:
            formatted_result = {
                "title": item.get("metadata", {}).get("title", ""),
                "url": item.get("metadata", {}).get("sourceURL", ""),
                "content": item.get("markdown", item.get("html", "")),
                "metadata": item.get("metadata", {}),
            }
            formatted_results.append(formatted_result)

        return formatted_results
    except Exception as e:
        logger.error(f"Firecrawl crawl failed: {e}")
        return []


def _firecrawl_map(
    url: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    search: str | None = None,
    ignore_sitemap: bool = False,
    include_subdomains: bool = False,
    limit: int = 5000,
) -> list[dict[str, Any]]:
    """Perform a Firecrawl map operation and format the results.

    Args:
        url (str): The website URL to map.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None.
        search (str, optional): Search term to filter URLs. Defaults to None.
        ignore_sitemap (bool, optional): Whether to ignore the sitemap. Defaults to False.
        include_subdomains (bool, optional): Whether to include subdomains. Defaults to False.
        limit (int, optional): Maximum number of URLs to return. Defaults to 5000.

    Returns:
        list[dict[str, Any]]: A list of URLs found on the website.
    """
    try:
        result = _execute_firecrawl_map(
            url=url,
            firecrawl_api_key=firecrawl_api_key,
            firecrawl_api_url=firecrawl_api_url,
            search=search,
            ignore_sitemap=ignore_sitemap,
            include_subdomains=include_subdomains,
            limit=limit,
        )

        # Format the results
        formatted_results = []
        links = result.get("links", [])

        for link in links:
            formatted_result = {
                "url": link,
                "title": "",  # Map operation doesn't provide titles
                "content": "",  # Map operation doesn't provide content
            }
            formatted_results.append(formatted_result)

        return formatted_results
    except Exception as e:
        logger.error(f"Firecrawl map failed: {e}")
        return []


def _firecrawl_search(
    query: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    limit: int = 5,
    tbs: str | None = None,
    filter: str | None = None,
    lang: str = "en",
    country: str = "us",
    location: str | None = None,
    timeout: int | None = None,
) -> list[dict[str, Any]]:
    """Perform a Firecrawl search and format the results.

    Args:
        query (str): The search query string.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None.
        limit (int, optional): Maximum number of results to return. Defaults to 5.
        tbs (str, optional): Time filter (e.g., "qdr:d" for past day). Defaults to None.
        filter (str, optional): Custom result filter. Defaults to None.
        lang (str, optional): Language code. Defaults to "en".
        country (str, optional): Country code. Defaults to "us".
        location (str, optional): Geo-targeting location. Defaults to None.
        timeout (int, optional): Request timeout in milliseconds. Defaults to None.

    Returns:
        list[dict[str, Any]]: A list of search results with content.
    """
    try:
        result = _execute_firecrawl_search(
            query=query,
            firecrawl_api_key=firecrawl_api_key,
            firecrawl_api_url=firecrawl_api_url,
            limit=limit,
            tbs=tbs,
            filter=filter,
            lang=lang,
            country=country,
            location=location,
            timeout=timeout,
        )

        # Format the results
        formatted_results = []
        data = result.get("data", [])

        for item in data:
            formatted_result = {
                "title": item.get("title", ""),
                "url": item.get("url", ""),
                "content": item.get("markdown", item.get("html", "")),
                "description": item.get("description", ""),
                "metadata": item.get("metadata", {}),
            }
            formatted_results.append(formatted_result)

        return formatted_results
    except Exception as e:
        logger.error(f"Firecrawl search failed: {e}")
        return []


def _firecrawl_deep_research(
    query: str,
    firecrawl_api_key: str,
    firecrawl_api_url: str | None = None,
    max_depth: int = 7,
    time_limit: int = 270,
    max_urls: int = 20,
    analysis_prompt: str | None = None,
    system_prompt: str | None = None,
) -> dict[str, Any]:
    """Perform a Firecrawl deep research operation and format the results.

    Args:
        query (str): The research query or topic to investigate.
        firecrawl_api_key (str): The API key for Firecrawl.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None.
        max_depth (int, optional): Maximum depth of research exploration. Defaults to 7.
        time_limit (int, optional): Time limit in seconds for research. Defaults to 270.
        max_urls (int, optional): Maximum number of URLs to process. Defaults to 20.
        analysis_prompt (str, optional): Custom prompt for analysis. Defaults to None.
        system_prompt (str, optional): Custom system prompt. Defaults to None.

    Returns:
        dict[str, Any]: The deep research result with analysis, sources, and summaries.
    """
    try:
        result = _execute_firecrawl_deep_research(
            query=query,
            firecrawl_api_key=firecrawl_api_key,
            firecrawl_api_url=firecrawl_api_url,
            max_depth=max_depth,
            time_limit=time_limit,
            max_urls=max_urls,
            analysis_prompt=analysis_prompt,
            system_prompt=system_prompt,
        )

        # Format the result - deep research returns a comprehensive analysis
        formatted_result = {
            "query": query,
            "status": result.get("status", ""),
            "data": result.get("data", {}),
            "sources": result.get("sources", []),
            "summaries": result.get("summaries", []),
            "activities": result.get("activities", []),
            "success": result.get("success", False),
            "error": result.get("error", ""),
        }

        return formatted_result
    except Exception as e:
        logger.error(f"Firecrawl deep research failed: {e}")
        return {
            "query": query,
            "status": "failed",
            "data": {},
            "sources": [],
            "summaries": [],
            "activities": [],
            "success": False,
            "error": str(e),
        }


@export_module("autogen.tools.experimental")
class FirecrawlTool(Tool):
    """FirecrawlTool is a tool that uses the Firecrawl API to scrape, crawl, map, search, and research websites.

    This tool allows agents to leverage Firecrawl for web content extraction, discovery, and research.
    It requires a Firecrawl API key, which can be provided during initialization or set as
    an environment variable `FIRECRAWL_API_KEY`.

    The tool provides five main functionalities:
    - Scrape: Extract content from a single URL
    - Crawl: Recursively crawl a website starting from a URL
    - Map: Discover URLs from a website
    - Search: Search the web for content using Firecrawl's search capabilities
    - Deep Research: Perform comprehensive research on a topic with analysis and insights

    Attributes:
        firecrawl_api_key (str): The API key used for authenticating with the Firecrawl API.
        firecrawl_api_url (str, optional): The base URL for the Firecrawl API. Defaults to None.
    """

    def __init__(
        self,
        *,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        firecrawl_api_key: str | None = None,
        firecrawl_api_url: str | None = None,
    ):
        """Initializes the FirecrawlTool.

        Args:
            llm_config (Optional[Union[LLMConfig, dict[str, Any]]]): LLM configuration. (Currently unused but kept for potential future integration).
            firecrawl_api_key (Optional[str]): The API key for the Firecrawl API. If not provided,
                it attempts to read from the `FIRECRAWL_API_KEY` environment variable.
            firecrawl_api_url (Optional[str]): The base URL for the Firecrawl API. If not provided,
                it attempts to read from the `FIRECRAWL_API_URL` environment variable, or defaults
                to the public Firecrawl API. Use this parameter to connect to self-hosted Firecrawl instances.

        Raises:
            ValueError: If `firecrawl_api_key` is not provided either directly or via the environment variable.
        """
        self.firecrawl_api_key = firecrawl_api_key or os.getenv("FIRECRAWL_API_KEY")
        self.firecrawl_api_url = firecrawl_api_url or os.getenv("FIRECRAWL_API_URL")

        if self.firecrawl_api_key is None:
            raise ValueError(
                "firecrawl_api_key must be provided either as an argument or via FIRECRAWL_API_KEY env var"
            )

        def firecrawl_scrape(
            url: Annotated[str, "The URL to scrape."],
            firecrawl_api_key: Annotated[str | None, Depends(on(self.firecrawl_api_key))],
            firecrawl_api_url: Annotated[str | None, Depends(on(self.firecrawl_api_url))],
            formats: Annotated[list[str] | None, "Output formats (e.g., ['markdown', 'html'])"] = None,
            include_tags: Annotated[list[str] | None, "HTML tags to include"] = None,
            exclude_tags: Annotated[list[str] | None, "HTML tags to exclude"] = None,
            headers: Annotated[dict[str, str] | None, "HTTP headers to use"] = None,
            wait_for: Annotated[int | None, "Time to wait for page load in milliseconds"] = None,
            timeout: Annotated[int | None, "Request timeout in milliseconds"] = None,
        ) -> list[dict[str, Any]]:
            """Scrapes a single URL and returns the content.

            Args:
                url: The URL to scrape.
                firecrawl_api_key: The API key for Firecrawl (injected dependency).
                firecrawl_api_url: The base URL for the Firecrawl API (injected dependency).
                formats: Output formats (e.g., ['markdown', 'html']). Defaults to ['markdown'].
                include_tags: HTML tags to include. Defaults to None.
                exclude_tags: HTML tags to exclude. Defaults to None.
                headers: HTTP headers to use. Defaults to None.
                wait_for: Time to wait for page load in milliseconds. Defaults to None.
                timeout: Request timeout in milliseconds. Defaults to None.

            Returns:
                A list containing the scraped content with title, url, content, and metadata.

            Raises:
                ValueError: If the Firecrawl API key is not available.
            """
            if firecrawl_api_key is None:
                raise ValueError("Firecrawl API key is missing.")
            return _firecrawl_scrape(
                url=url,
                firecrawl_api_key=firecrawl_api_key,
                firecrawl_api_url=firecrawl_api_url,
                formats=formats,
                include_tags=include_tags,
                exclude_tags=exclude_tags,
                headers=headers,
                wait_for=wait_for,
                timeout=timeout,
            )

        def firecrawl_crawl(
            url: Annotated[str, "The starting URL to crawl."],
            firecrawl_api_key: Annotated[str | None, Depends(on(self.firecrawl_api_key))],
            firecrawl_api_url: Annotated[str | None, Depends(on(self.firecrawl_api_url))],
            limit: Annotated[int, "Maximum number of pages to crawl"] = 5,
            formats: Annotated[list[str] | None, "Output formats (e.g., ['markdown', 'html'])"] = None,
            include_paths: Annotated[list[str] | None, "URL patterns to include"] = None,
            exclude_paths: Annotated[list[str] | None, "URL patterns to exclude"] = None,
            max_depth: Annotated[int | None, "Maximum crawl depth"] = None,
            allow_backward_crawling: Annotated[bool | None, "Allow crawling backward links"] = False,
            allow_external_content_links: Annotated[bool | None, "Allow external links"] = False,
        ) -> list[dict[str, Any]]:
            """Crawls a website starting from a URL and returns the content from multiple pages.

            Args:
                url: The starting URL to crawl.
                firecrawl_api_key: The API key for Firecrawl (injected dependency).
                firecrawl_api_url: The base URL for the Firecrawl API (injected dependency).
                limit: Maximum number of pages to crawl. Defaults to 5.
                formats: Output formats (e.g., ['markdown', 'html']). Defaults to ['markdown'].
                include_paths: URL patterns to include. Defaults to None.
                exclude_paths: URL patterns to exclude. Defaults to None.
                max_depth: Maximum crawl depth. Defaults to None.
                allow_backward_crawling: Allow crawling backward links. Defaults to False.
                allow_external_content_links: Allow external links. Defaults to False.

            Returns:
                A list of crawled pages with title, url, content, and metadata for each page.

            Raises:
                ValueError: If the Firecrawl API key is not available.
            """
            if firecrawl_api_key is None:
                raise ValueError("Firecrawl API key is missing.")
            return _firecrawl_crawl(
                url=url,
                firecrawl_api_key=firecrawl_api_key,
                firecrawl_api_url=firecrawl_api_url,
                limit=limit,
                formats=formats,
                include_paths=include_paths,
                exclude_paths=exclude_paths,
                max_depth=max_depth,
                allow_backward_crawling=allow_backward_crawling or False,
                allow_external_content_links=allow_external_content_links or False,
            )

        def firecrawl_map(
            url: Annotated[str, "The website URL to map."],
            firecrawl_api_key: Annotated[str | None, Depends(on(self.firecrawl_api_key))],
            firecrawl_api_url: Annotated[str | None, Depends(on(self.firecrawl_api_url))],
            search: Annotated[str | None, "Search term to filter URLs"] = None,
            ignore_sitemap: Annotated[bool | None, "Whether to ignore the sitemap"] = False,
            include_subdomains: Annotated[bool | None, "Whether to include subdomains"] = False,
            limit: Annotated[int, "Maximum number of URLs to return"] = 5000,
        ) -> list[dict[str, Any]]:
            """Maps a website to discover URLs.

            Args:
                url: The website URL to map.
                firecrawl_api_key: The API key for Firecrawl (injected dependency).
                firecrawl_api_url: The base URL for the Firecrawl API (injected dependency).
                search: Search term to filter URLs. Defaults to None.
                ignore_sitemap: Whether to ignore the sitemap. Defaults to False.
                include_subdomains: Whether to include subdomains. Defaults to False.
                limit: Maximum number of URLs to return. Defaults to 5000.

            Returns:
                A list of URLs found on the website.

            Raises:
                ValueError: If the Firecrawl API key is not available.
            """
            if firecrawl_api_key is None:
                raise ValueError("Firecrawl API key is missing.")
            return _firecrawl_map(
                url=url,
                firecrawl_api_key=firecrawl_api_key,
                firecrawl_api_url=firecrawl_api_url,
                search=search,
                ignore_sitemap=ignore_sitemap or False,
                include_subdomains=include_subdomains or False,
                limit=limit,
            )

        def firecrawl_search(
            query: Annotated[str, "The search query string."],
            firecrawl_api_key: Annotated[str | None, Depends(on(self.firecrawl_api_key))],
            firecrawl_api_url: Annotated[str | None, Depends(on(self.firecrawl_api_url))],
            limit: Annotated[int, "Maximum number of results to return"] = 5,
            tbs: Annotated[str | None, "Time filter (e.g., 'qdr:d' for past day)"] = None,
            filter: Annotated[str | None, "Custom result filter"] = None,
            lang: Annotated[str | None, "Language code"] = "en",
            country: Annotated[str | None, "Country code"] = "us",
            location: Annotated[str | None, "Geo-targeting location"] = None,
            timeout: Annotated[int | None, "Request timeout in milliseconds"] = None,
        ) -> list[dict[str, Any]]:
            """Executes a search operation using the Firecrawl API.

            Args:
                query: The search query string.
                firecrawl_api_key: The API key for Firecrawl (injected dependency).
                firecrawl_api_url: The base URL for the Firecrawl API (injected dependency).
                limit: Maximum number of results to return. Defaults to 5.
                tbs: Time filter (e.g., "qdr:d" for past day). Defaults to None.
                filter: Custom result filter. Defaults to None.
                lang: Language code. Defaults to "en".
                country: Country code. Defaults to "us".
                location: Geo-targeting location. Defaults to None.
                timeout: Request timeout in milliseconds. Defaults to None.

            Returns:
                A list of search results with title, url, content, and metadata.

            Raises:
                ValueError: If the Firecrawl API key is not available.
            """
            if firecrawl_api_key is None:
                raise ValueError("Firecrawl API key is missing.")
            return _firecrawl_search(
                query=query,
                firecrawl_api_key=firecrawl_api_key,
                firecrawl_api_url=firecrawl_api_url,
                limit=limit,
                tbs=tbs,
                filter=filter,
                lang=lang or "en",
                country=country or "us",
                location=location,
                timeout=timeout,
            )

        def firecrawl_deep_research(
            query: Annotated[str, "The research query or topic to investigate."],
            firecrawl_api_key: Annotated[str | None, Depends(on(self.firecrawl_api_key))],
            firecrawl_api_url: Annotated[str | None, Depends(on(self.firecrawl_api_url))],
            max_depth: Annotated[int, "Maximum depth of research exploration"] = 7,
            time_limit: Annotated[int, "Time limit in seconds for research"] = 270,
            max_urls: Annotated[int, "Maximum number of URLs to process"] = 20,
            analysis_prompt: Annotated[str | None, "Custom prompt for analysis"] = None,
            system_prompt: Annotated[str | None, "Custom system prompt"] = None,
        ) -> dict[str, Any]:
            """Executes a deep research operation using the Firecrawl API.

            Args:
                query: The research query or topic to investigate.
                firecrawl_api_key: The API key for Firecrawl (injected dependency).
                firecrawl_api_url: The base URL for the Firecrawl API (injected dependency).
                max_depth: Maximum depth of research exploration. Defaults to 7.
                time_limit: Time limit in seconds for research. Defaults to 270.
                max_urls: Maximum number of URLs to process. Defaults to 20.
                analysis_prompt: Custom prompt for analysis. Defaults to None.
                system_prompt: Custom system prompt. Defaults to None.

            Returns:
                The deep research result from Firecrawl.

            Raises:
                ValueError: If the Firecrawl API key is not available.
            """
            if firecrawl_api_key is None:
                raise ValueError("Firecrawl API key is missing.")
            return _execute_firecrawl_deep_research(
                query=query,
                firecrawl_api_key=firecrawl_api_key,
                firecrawl_api_url=firecrawl_api_url,
                max_depth=max_depth,
                time_limit=time_limit,
                max_urls=max_urls,
                analysis_prompt=analysis_prompt,
                system_prompt=system_prompt,
            )

        # Default to scrape functionality for the main tool
        super().__init__(
            name="firecrawl_scrape",
            description="Use the Firecrawl API to scrape content from a single URL.",
            func_or_tool=firecrawl_scrape,
        )

        # Store additional methods for manual access
        self.scrape = firecrawl_scrape
        self.crawl = firecrawl_crawl
        self.map = firecrawl_map
        self.search = firecrawl_search
        self.deep_research = firecrawl_deep_research
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .authentication import GoogleCredentialsLocalProvider, GoogleCredentialsProvider
from .drive import GoogleDriveToolkit
from .toolkit_protocol import GoogleToolkitProtocol

__all__ = [
    "GoogleCredentialsLocalProvider",
    "GoogleCredentialsProvider",
    "GoogleDriveToolkit",
    "GoogleToolkitProtocol",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Annotated

from pydantic import BaseModel, Field

__all__ = [
    "GoogleFileInfo",
]


class GoogleFileInfo(BaseModel):
    name: Annotated[str, Field(description="The name of the file.")]
    id: Annotated[str, Field(description="The ID of the file.")]
    mime_type: Annotated[str, Field(alias="mimeType", description="The MIME type of the file.")]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Protocol, runtime_checkable

__all__ = [
    "GoogleToolkitProtocol",
]


@runtime_checkable
class GoogleToolkitProtocol(Protocol):
    """A protocol for Google tool maps."""

    @classmethod
    def recommended_scopes(cls) -> list[str]:
        """Defines a required static method without implementation."""
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from pathlib import Path
from typing import Annotated, Literal

from .....doc_utils import export_module
from .....import_utils import optional_import_block
from .... import Toolkit, tool
from ..model import GoogleFileInfo
from ..toolkit_protocol import GoogleToolkitProtocol
from .drive_functions import download_file, list_files_and_folders

with optional_import_block():
    from google.oauth2.credentials import Credentials
    from googleapiclient.discovery import build

__all__ = [
    "GoogleDriveToolkit",
]


@export_module("autogen.tools.experimental.google.drive")
class GoogleDriveToolkit(Toolkit, GoogleToolkitProtocol):
    """A tool map for Google Drive."""

    def __init__(  # type: ignore[no-any-unimported]
        self,
        *,
        credentials: "Credentials",
        download_folder: Path | str,
        exclude: list[Literal["list_drive_files_and_folders", "download_file_from_drive"]] | None = None,
        api_version: str = "v3",
    ) -> None:
        """Initialize the Google Drive tool map.

        Args:
            credentials: The Google OAuth2 credentials.
            download_folder: The folder to download files to.
            exclude: The tool names to exclude.
            api_version: The Google Drive API version to use."
        """
        self.service = build(serviceName="drive", version=api_version, credentials=credentials)

        if isinstance(download_folder, str):
            download_folder = Path(download_folder)
        download_folder.mkdir(parents=True, exist_ok=True)

        @tool(description="List files and folders in a Google Drive")
        def list_drive_files_and_folders(
            page_size: Annotated[int, "The number of files to list per page."] = 10,
            folder_id: Annotated[
                str | None,
                "The ID of the folder to list files from. If not provided, lists all files in the root folder.",
            ] = None,
        ) -> list[GoogleFileInfo]:
            return list_files_and_folders(service=self.service, page_size=page_size, folder_id=folder_id)

        @tool(description="download a file from Google Drive")
        def download_file_from_drive(
            file_info: Annotated[GoogleFileInfo, "The file info to download."],
            subfolder_path: Annotated[
                str | None,
                "The subfolder path to save the file in. If not provided, saves in the main download folder.",
            ] = None,
        ) -> str:
            return download_file(
                service=self.service,
                file_id=file_info.id,
                file_name=file_info.name,
                mime_type=file_info.mime_type,
                download_folder=download_folder,
                subfolder_path=subfolder_path,
            )

        if exclude is None:
            exclude = []

        tools = [tool for tool in [list_drive_files_and_folders, download_file_from_drive] if tool.name not in exclude]
        super().__init__(tools=tools)

    @classmethod
    def recommended_scopes(cls) -> list[str]:
        """Return the recommended scopes manatory for using tools from this tool map."""
        return [
            "https://www.googleapis.com/auth/drive.readonly",
        ]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import io
from pathlib import Path
from typing import Any

from .....import_utils import optional_import_block, require_optional_import
from ..model import GoogleFileInfo

with optional_import_block():
    from googleapiclient.http import MediaIoBaseDownload


__all__ = [
    "download_file",
    "list_files_and_folders",
]


@require_optional_import(
    [
        "googleapiclient",
    ],
    "google-api",
)
def list_files_and_folders(service: Any, page_size: int, folder_id: str | None) -> list[GoogleFileInfo]:
    kwargs = {
        "pageSize": page_size,
        "fields": "nextPageToken, files(id, name, mimeType)",
    }
    if folder_id:
        kwargs["q"] = f"'{folder_id}' in parents and trashed=false"  # Search for files in the folder
    response = service.files().list(**kwargs).execute()
    result = response.get("files", [])
    if not isinstance(result, list):
        raise ValueError(f"Expected a list of files, but got {result}")
    result = [GoogleFileInfo(**file_info) for file_info in result]
    return result


def _get_file_extension(mime_type: str) -> str | None:
    """Returns the correct file extension for a given MIME type."""
    mime_extensions = {
        "application/vnd.google-apps.document": "docx",  # Google Docs  Word
        "application/vnd.google-apps.spreadsheet": "csv",  # Google Sheets  CSV
        "application/vnd.google-apps.presentation": "pptx",  # Google Slides  PowerPoint
        "video/quicktime": "mov",
        "application/vnd.google.colaboratory": "ipynb",
        "application/pdf": "pdf",
        "image/jpeg": "jpg",
        "image/png": "png",
        "text/plain": "txt",
        "application/zip": "zip",
    }

    return mime_extensions.get(mime_type)


@require_optional_import(
    [
        "googleapiclient",
    ],
    "google-api",
)
def download_file(
    service: Any,
    file_id: str,
    file_name: str,
    mime_type: str,
    download_folder: Path,
    subfolder_path: str | None = None,
) -> str:
    """Download or export file based on its MIME type, optionally saving to a subfolder."""
    file_extension = _get_file_extension(mime_type)
    if file_extension and (not file_name.lower().endswith(file_extension.lower())):
        file_name = f"{file_name}.{file_extension}"

    # Define export formats for Google Docs, Sheets, and Slides
    export_mime_types = {
        "application/vnd.google-apps.document": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",  # Google Docs  Word
        "application/vnd.google-apps.spreadsheet": "text/csv",  # Google Sheets  CSV
        "application/vnd.google-apps.presentation": "application/vnd.openxmlformats-officedocument.presentationml.presentation",  # Google Slides  PowerPoint
    }

    # Google Docs, Sheets, and Slides cannot be downloaded directly using service.files().get_media() because they are Google-native files
    if mime_type in export_mime_types:
        request = service.files().export(fileId=file_id, mimeType=export_mime_types[mime_type])
    else:
        # Download normal files (videos, images, etc.)
        request = service.files().get_media(fileId=file_id)

    # Determine the final destination directory
    destination_dir = download_folder
    if subfolder_path:
        destination_dir = download_folder / subfolder_path
        # Ensure the subfolder exists, create it if necessary
        destination_dir.mkdir(parents=True, exist_ok=True)

    # Construct the full path for the file
    file_path = destination_dir / file_name

    # Save file
    try:
        with io.BytesIO() as buffer:
            downloader = MediaIoBaseDownload(buffer, request)
            done = False
            while not done:
                _, done = downloader.next_chunk()

            buffer.seek(0)

            with open(file_path, "wb") as f:
                f.write(buffer.getvalue())

        # Print out the relative path of the downloaded file
        relative_path = Path(subfolder_path) / file_name if subfolder_path else Path(file_name)
        return f" Downloaded: {relative_path}"

    except Exception as e:
        # Error message if unable to download
        relative_path = Path(subfolder_path) / file_name if subfolder_path else Path(file_name)
        return f" FAILED to download {relative_path}: {e}"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .toolkit import GoogleDriveToolkit

__all__ = [
    "GoogleDriveToolkit",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import os
from typing import Optional

from .....doc_utils import export_module
from .....import_utils import optional_import_block, require_optional_import
from .credentials_provider import GoogleCredentialsProvider

with optional_import_block():
    from google.auth.transport.requests import Request
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import InstalledAppFlow


__all__ = ["GoogleCredentialsLocalProvider"]


@export_module("autogen.tools.experimental.google.authentication")
class GoogleCredentialsLocalProvider(GoogleCredentialsProvider):
    def __init__(
        self,
        client_secret_file: str,
        scopes: list[str],  # e.g. ['https://www.googleapis.com/auth/drive/readonly']
        token_file: str | None = None,
        port: int = 8080,
    ) -> None:
        """A Google credentials provider that gets the credentials locally.

        Args:
            client_secret_file (str): The path to the client secret file.
            scopes (list[str]): The scopes to request.
            token_file (str): Optional path to the token file. If not provided, the token will not be saved.
            port (int): The port from which to get the credentials.
        """
        self.client_secret_file = client_secret_file
        self.scopes = scopes
        self.token_file = token_file
        self._port = port

    @property
    def host(self) -> str:
        """Localhost is the default host."""
        return "localhost"

    @property
    def port(self) -> int:
        """The port from which to get the credentials."""
        return self._port

    @require_optional_import(
        [
            "google_auth_httplib2",
            "google_auth_oauthlib",
        ],
        "google-api",
    )
    def _refresh_or_get_new_credentials(self, creds: Optional["Credentials"]) -> "Credentials":  # type: ignore[no-any-unimported]
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())  # type: ignore[no-untyped-call]
        else:
            flow = InstalledAppFlow.from_client_secrets_file(self.client_secret_file, self.scopes)
            creds = flow.run_local_server(host=self.host, port=self.port)
        return creds  # type: ignore[return-value]

    @require_optional_import(
        [
            "google_auth_httplib2",
            "google_auth_oauthlib",
        ],
        "google-api",
    )
    def get_credentials(self) -> "Credentials":  # type: ignore[no-any-unimported]
        """Get the Google credentials."""
        creds = None
        if self.token_file and os.path.exists(self.token_file):
            creds = Credentials.from_authorized_user_file(self.token_file)  # type: ignore[no-untyped-call]

        # If there are no (valid) credentials available, let the user log in.
        if not creds or not creds.valid:
            creds = self._refresh_or_get_new_credentials(creds)

            if self.token_file:
                # Save the credentials for the next run
                with open(self.token_file, "w") as token:
                    token.write(creds.to_json())  # type: ignore[no-untyped-call]

        return creds  # type: ignore[no-any-return]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from .....doc_utils import export_module
from .....import_utils import optional_import_block
from .credentials_provider import GoogleCredentialsProvider

with optional_import_block():
    from google.oauth2.credentials import Credentials


__all__ = ["GoogleCredenentialsHostedProvider"]


@export_module("autogen.tools.experimental.google.authentication")
class GoogleCredenentialsHostedProvider(GoogleCredentialsProvider):
    def __init__(
        self,
        host: str,
        port: int = 8080,
        *,
        kwargs: dict[str, str],
    ) -> None:
        self._host = host
        self._port = port
        self._kwargs = kwargs

        raise NotImplementedError("This class is not implemented yet.")

    @property
    def host(self) -> str:
        """The host from which to get the credentials."""
        return self._host

    @property
    def port(self) -> int:
        """The port from which to get the credentials."""
        return self._port

    def get_credentials(self) -> "Credentials":  # type: ignore[no-any-unimported]
        raise NotImplementedError("This class is not implemented yet.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .credentials_local_provider import GoogleCredentialsLocalProvider
from .credentials_provider import GoogleCredentialsProvider

__all__ = [
    "GoogleCredentialsLocalProvider",
    "GoogleCredentialsProvider",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from typing import Optional, Protocol, runtime_checkable

from .....doc_utils import export_module
from .....import_utils import optional_import_block

with optional_import_block():
    from google.oauth2.credentials import Credentials


__all__ = ["GoogleCredentialsProvider"]


@runtime_checkable
@export_module("autogen.tools.experimental.google.authentication")
class GoogleCredentialsProvider(Protocol):
    """A protocol for Google credentials provider."""

    def get_credentials(self) -> Optional["Credentials"]:  # type: ignore[no-any-unimported]
        """Get the Google credentials."""
        ...

    @property
    def host(self) -> str:
        """The host from which to get the credentials."""
        ...

    @property
    def port(self) -> int:
        """The port from which to get the credentials."""
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .browser_use import BrowserUseTool
from .code_execution import PythonCodeExecutionTool
from .crawl4ai import Crawl4AITool
from .deep_research import DeepResearchTool
from .duckduckgo import DuckDuckGoSearchTool
from .firecrawl import FirecrawlTool
from .google_search import GoogleSearchTool, YoutubeSearchTool
from .messageplatform import (
    DiscordRetrieveTool,
    DiscordSendTool,
    SlackRetrieveRepliesTool,
    SlackRetrieveTool,
    SlackSendTool,
    TelegramRetrieveTool,
    TelegramSendTool,
)
from .perplexity import PerplexitySearchTool
from .reliable import ReliableTool, ReliableToolError, SuccessfulExecutionParameters, ToolExecutionDetails
from .searxng import SearxngSearchTool
from .tavily import TavilySearchTool
from .web_search_preview import WebSearchPreviewTool
from .wikipedia import WikipediaPageLoadTool, WikipediaQueryRunTool

__all__ = [
    "BrowserUseTool",
    "Crawl4AITool",
    "DeepResearchTool",
    "DiscordRetrieveTool",
    "DiscordSendTool",
    "DuckDuckGoSearchTool",
    "FirecrawlTool",
    "GoogleSearchTool",
    "PerplexitySearchTool",
    "PythonCodeExecutionTool",
    "ReliableTool",
    "ReliableToolError",
    "SearxngSearchTool",
    "SlackRetrieveRepliesTool",
    "SlackRetrieveTool",
    "SlackSendTool",
    "SuccessfulExecutionParameters",
    "TavilySearchTool",
    "TelegramRetrieveTool",
    "TelegramSendTool",
    "ToolExecutionDetails",
    "WebSearchPreviewTool",
    "WikipediaPageLoadTool",
    "WikipediaQueryRunTool",
    "YoutubeSearchTool",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .searxng_search import SearxngSearchTool

__all__ = ["SearxngSearchTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
"""SearxNG Search Tool
A simple tool to perform web searches using a SearxNG instance.
"""

import logging
from typing import Annotated, Any

import requests

from autogen.doc_utils import export_module
from autogen.tools import Tool

logger = logging.getLogger(__name__)


def _execute_searxng_query(
    query: str,
    max_results: int = 5,
    categories: list[str] | None = None,
    language: str | None = None,
    base_url: str = "https://searxng.site/search",
) -> list[dict[str, Any]]:
    """Execute a search query using a SearxNG instance.

    Args:
        query (str): The search query string.
        max_results (int, optional): The maximum number of results to return. Defaults to 5.
        categories (Optional[List[str]]): List of categories to search in.
        language (Optional[str]): Language code.
        base_url (str): SearxNG instance URL.

    Returns:
        list[dict[str, Any]]: A list of search results from SearxNG.
    """
    params = {
        "q": query,
        "format": "json",
        "language": language or "en-US",
        "categories": ",".join(categories) if categories else None,
        "count": max_results,
    }
    params = {k: v for k, v in params.items() if v is not None}
    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        results = data.get("results", [])
        if not isinstance(results, list):
            return []
        # Ensure each result is a dict before returning
        typed_results: list[dict[str, Any]] = []
        for item in results:
            if isinstance(item, dict):
                typed_results.append(item)
        return typed_results
    except Exception as e:
        logger.error(f"SearxNG Search failed: {e}")
        return []


def _searxng_search(
    query: str,
    max_results: int = 5,
    categories: list[str] | None = None,
    language: str | None = None,
    base_url: str = "https://searxng.site/search",
) -> list[dict[str, Any]]:
    """Perform a SearxNG search and format the results.

    Args:
        query (str): The search query string.
        max_results (int, optional): The maximum number of results to return. Defaults to 5.
        categories (Optional[List[str]]): List of categories to search in.
        language (Optional[str]): Language code.
        base_url (str): SearxNG instance URL.

    Returns:
        list[dict[str, Any]]: A list of dictionaries with 'title', 'link', and 'snippet'.
    """
    res = _execute_searxng_query(
        query=query,
        max_results=max_results,
        categories=categories,
        language=language,
        base_url=base_url,
    )
    formatted_results: list[dict[str, Any]] = [
        {"title": item.get("title", ""), "link": item.get("url", ""), "snippet": item.get("content", "")}
        for item in res
    ]
    return formatted_results


@export_module("autogen.tools.experimental")
class SearxngSearchTool(Tool):
    """SearxngSearchTool is a tool that uses SearxNG to perform a search.

    This tool allows agents to leverage the SearxNG search engine for information retrieval.
    SearxNG does not require an API key by default, making it easy to use.
    """

    def __init__(self, base_url: str = "https://searxng.site/search") -> None:
        """Initializes the SearxngSearchTool.

        Args:
            base_url (str): The SearxNG instance URL.
        """
        self.base_url = base_url
        super().__init__(
            name="searxng_search",
            description="Use the SearxNG API to perform a search.",
            func_or_tool=self.searxng_search,
        )

    def searxng_search(
        self,
        query: Annotated[str, "The search query."],
        max_results: Annotated[int, "The number of results to return."] = 5,
        categories: Annotated[list[str] | None, "List of categories to search in."] = None,
        language: Annotated[str | None, "Language code (e.g., 'en-US')."] = None,
    ) -> list[dict[str, Any]]:
        """Performs a search using the SearxNG API and returns formatted results.

        Args:
            query: The search query string.
            max_results: The maximum number of results to return. Defaults to 5.
            categories: List of categories to search in.
            language: Language code.

        Returns:
            A list of dictionaries, each containing 'title', 'link', and 'snippet' of a search result.
        """
        return _searxng_search(
            query=query,
            max_results=max_results,
            categories=categories,
            language=language,
            base_url=self.base_url,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .google_search import GoogleSearchTool
from .youtube_search import YoutubeSearchTool

__all__ = ["GoogleSearchTool", "YoutubeSearchTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import logging
from typing import Annotated, Any

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ... import Depends, Tool
from ...dependency_injection import on

with optional_import_block():
    from googleapiclient.discovery import build


@require_optional_import(
    [
        "googleapiclient",
    ],
    "google-search",
)
def _execute_query(query: str, search_api_key: str, search_engine_id: str, num_results: int) -> Any:
    service = build("customsearch", "v1", developerKey=search_api_key)
    return service.cse().list(q=query, cx=search_engine_id, num=num_results).execute()


def _google_search(
    query: str,
    search_api_key: str,
    search_engine_id: str,
    num_results: int,
) -> list[dict[str, Any]]:
    res = _execute_query(
        query=query, search_api_key=search_api_key, search_engine_id=search_engine_id, num_results=num_results
    )

    return [
        {"title": item.get("title", ""), "link": item.get("link", ""), "snippet": item.get("snippet", "")}
        for item in res.get("items", [])
    ]


@export_module("autogen.tools.experimental")
class GoogleSearchTool(Tool):
    """GoogleSearchTool is a tool that uses the Google Search API to perform a search."""

    def __init__(
        self,
        *,
        search_api_key: str | None = None,
        search_engine_id: str | None = None,
        use_internal_llm_tool_if_available: bool = True,
    ):
        """GoogleSearchTool is a tool that uses the Google Search API to perform a search.

        Args:
            search_api_key: The API key for the Google Search API.
            search_engine_id: The search engine ID for the Google Search API.
            use_internal_llm_tool_if_available: Whether to use the predefined (e.g. Gemini GenaAI) search tool. Currently, this can only be used for agents with the Gemini (GenAI) configuration.
        """
        self.search_api_key = search_api_key
        self.search_engine_id = search_engine_id
        self.use_internal_llm_tool_if_available = use_internal_llm_tool_if_available

        if not use_internal_llm_tool_if_available and (search_api_key is None or search_engine_id is None):
            raise ValueError(
                "search_api_key and search_engine_id must be provided if use_internal_llm_tool_if_available is False"
            )

        if use_internal_llm_tool_if_available and (search_api_key is not None or search_engine_id is not None):
            logging.warning("search_api_key and search_engine_id will be ignored if internal LLM tool is available")

        def google_search(
            query: Annotated[str, "The search query."],
            search_api_key: Annotated[str | None, Depends(on(search_api_key))],
            search_engine_id: Annotated[str | None, Depends(on(search_engine_id))],
            num_results: Annotated[int, "The number of results to return."] = 10,
        ) -> list[dict[str, Any]]:
            if search_api_key is None or search_engine_id is None:
                raise ValueError(
                    "Your LLM is not configured to use prebuilt google-search tool.\n"
                    "Please provide search_api_key and search_engine_id.\n"
                )
            return _google_search(query, search_api_key, search_engine_id, num_results)

        super().__init__(
            # GeminiClient will look for a tool with the name "prebuilt_google_search"
            name="prebuilt_google_search" if use_internal_llm_tool_if_available else "google_search",
            description="Use the Google Search API to perform a search.",
            func_or_tool=google_search,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import logging
from typing import Annotated, Any

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ... import Depends, Tool
from ...dependency_injection import on

with optional_import_block():
    import googleapiclient.errors
    from googleapiclient.discovery import build


@require_optional_import(
    ["googleapiclient"],
    "google-search",
)
def _execute_search_query(query: str, youtube_api_key: str, max_results: int) -> Any:
    """Execute a YouTube search query using the YouTube Data API.

    Args:
        query: The search query string.
        youtube_api_key: The API key for the YouTube Data API.
        max_results: The maximum number of results to return.

    Returns:
        The search response from the YouTube Data API.
    """
    youtube = build("youtube", "v3", developerKey=youtube_api_key)

    try:
        search_response = (
            youtube.search().list(q=query, part="id,snippet", maxResults=max_results, type="video").execute()
        )

        return search_response
    except googleapiclient.errors.HttpError as e:
        logging.error(f"An HTTP error occurred: {e}")
        raise


@require_optional_import(
    ["googleapiclient"],
    "google-search",
)
def _get_video_details(video_ids: list[str], youtube_api_key: str) -> Any:
    """Get detailed information about specific YouTube videos.

    Args:
        video_ids: List of YouTube video IDs.
        youtube_api_key: The API key for the YouTube Data API.

    Returns:
        The video details response from the YouTube Data API.
    """
    if not video_ids:
        return {"items": []}

    youtube = build("youtube", "v3", developerKey=youtube_api_key)

    try:
        videos_response = (
            youtube.videos().list(id=",".join(video_ids), part="snippet,contentDetails,statistics").execute()
        )

        return videos_response
    except googleapiclient.errors.HttpError as e:
        logging.error(f"An HTTP error occurred: {e}")
        raise


def _youtube_search(
    query: str,
    youtube_api_key: str,
    max_results: int,
    include_video_details: bool = True,
) -> list[dict[str, Any]]:
    """Search YouTube videos based on a query.

    Args:
        query: The search query string.
        youtube_api_key: The API key for the YouTube Data API.
        max_results: The maximum number of results to return.
        include_video_details: Whether to include detailed video information.

    Returns:
        A list of dictionaries containing information about the videos.
    """
    search_response = _execute_search_query(query=query, youtube_api_key=youtube_api_key, max_results=max_results)

    results = []
    video_ids = []

    # Extract basic info from search results
    for item in search_response.get("items", []):
        if item["id"]["kind"] == "youtube#video":
            video_ids.append(item["id"]["videoId"])
            video_info = {
                "title": item["snippet"]["title"],
                "description": item["snippet"]["description"],
                "publishedAt": item["snippet"]["publishedAt"],
                "channelTitle": item["snippet"]["channelTitle"],
                "videoId": item["id"]["videoId"],
                "url": f"https://www.youtube.com/watch?v={item['id']['videoId']}",
            }
            results.append(video_info)

    # If detailed info requested, get it
    if include_video_details and video_ids:
        video_details = _get_video_details(video_ids, youtube_api_key)

        # Create a mapping of videoId to details
        details_map = {item["id"]: item for item in video_details.get("items", [])}

        # Update results with additional details
        for result in results:
            video_id = result["videoId"]
            if video_id in details_map:
                details = details_map[video_id]
                result.update({
                    "viewCount": details["statistics"].get("viewCount"),
                    "likeCount": details["statistics"].get("likeCount"),
                    "commentCount": details["statistics"].get("commentCount"),
                    "duration": details["contentDetails"].get("duration"),
                    "definition": details["contentDetails"].get("definition"),
                })

    return results


@export_module("autogen.tools.experimental")
class YoutubeSearchTool(Tool):
    """YoutubeSearchTool is a tool that uses the YouTube Data API to search for videos."""

    def __init__(
        self,
        *,
        youtube_api_key: str | None = None,
    ):
        """Initialize a YouTube search tool.

        Args:
            youtube_api_key: The API key for the YouTube Data API.
        """
        self.youtube_api_key = youtube_api_key

        if youtube_api_key is None:
            raise ValueError("youtube_api_key must be provided")

        def youtube_search(
            query: Annotated[str, "The search query for YouTube videos."],
            youtube_api_key: Annotated[str, Depends(on(youtube_api_key))],
            max_results: Annotated[int, "The maximum number of results to return."] = 5,
            include_video_details: Annotated[bool, "Whether to include detailed video information."] = True,
        ) -> list[dict[str, Any]]:
            """Search for YouTube videos based on a query.

            Args:
                query: The search query string.
                youtube_api_key: The API key for the YouTube Data API.
                max_results: The maximum number of results to return.
                include_video_details: Whether to include detailed video information.

            Returns:
                A list of dictionaries containing information about the videos.
            """
            if youtube_api_key is None:
                raise ValueError("YouTube API key is required")

            return _youtube_search(query, youtube_api_key, max_results, include_video_details)

        super().__init__(
            name="youtube_search",
            description="Search for YouTube videos based on a query, optionally including detailed information.",
            func_or_tool=youtube_search,
        )
"""Module: perplexity_search_tool
Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
SPDX-License-Identifier: Apache-2.0

This module provides classes for interacting with the Perplexity AI search API.
It defines data models for responses and a tool for executing web and conversational searches.
"""

import json
import os
from typing import Any

import requests
from pydantic import BaseModel, ValidationError

from autogen.tools import Tool


class Message(BaseModel):
    """Represents a message in the chat conversation.

    Attributes:
        role (str): The role of the message sender (e.g., "system", "user").
        content (str): The text content of the message.
    """

    role: str
    content: str


class Usage(BaseModel):
    """Model representing token usage details.

    Attributes:
        prompt_tokens (int): The number of tokens used for the prompt.
        completion_tokens (int): The number of tokens generated in the completion.
        total_tokens (int): The total number of tokens (prompt + completion).
        search_context_size (str): The size context used in the search (e.g., "high").
    """

    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    search_context_size: str


class Choice(BaseModel):
    """Represents one choice in the response from the Perplexity API.

    Attributes:
        index (int): The index of this choice.
        finish_reason (str): The reason why the API finished generating this choice.
        message (Message): The message object containing the response text.
    """

    index: int
    finish_reason: str
    message: Message


class PerplexityChatCompletionResponse(BaseModel):
    """Represents the full chat completion response from the Perplexity API.

    Attributes:
        id (str): Unique identifier for the response.
        model (str): The model name used for generating the response.
        created (int): Timestamp when the response was created.
        usage (Usage): Token usage details.
        citations (list[str]): list of citation strings included in the response.
        object (str): Type of the response object.
        choices (list[Choice]): list of choices returned by the API.
    """

    id: str
    model: str
    created: int
    usage: Usage
    citations: list[str]
    object: str
    choices: list[Choice]


class SearchResponse(BaseModel):
    """Represents the response from a search query.

    Attributes:
        content (Optional[str]): The textual content returned from the search.
        citations (Optional[list[str]]): A list of citation URLs relevant to the search result.
        error (Optional[str]): An error message if the search failed.
    """

    content: str | None
    citations: list[str] | None
    error: str | None


class PerplexitySearchTool(Tool):
    """Tool for interacting with the Perplexity AI search API.

    This tool uses the Perplexity API to perform web search, news search,
    and conversational search, returning concise and precise responses.

    Attributes:
        url (str): API endpoint URL.
        model (str): Name of the model to be used.
        api_key (str): API key for authenticating with the Perplexity API.
        max_tokens (int): Maximum tokens allowed for the API response.
        search_domain_filters (Optional[list[str]]): Optional list of domain filters for the search.
    """

    def __init__(
        self,
        model: str = "sonar",
        api_key: str | None = None,
        max_tokens: int = 1000,
        search_domain_filter: list[str] | None = None,
    ):
        """Initializes a new instance of the PerplexitySearchTool.

        Args:
            model (str, optional): The model to use. Defaults to "sonar".
            api_key (Optional[str], optional): API key for authentication.
            max_tokens (int, optional): Maximum number of tokens for the response. Defaults to 1000.
            search_domain_filter (Optional[list[str]], optional): list of domain filters to restrict search.

        Raises:
            ValueError: If the API key is missing, the model is empty, max_tokens is not positive,
                        or if search_domain_filter is not a list when provided.
        """
        self.api_key = api_key or os.getenv("PERPLEXITY_API_KEY")
        self._validate_tool_config(model, self.api_key, max_tokens, search_domain_filter)
        self.url = "https://api.perplexity.ai/chat/completions"
        self.model = model
        self.max_tokens = max_tokens
        self.search_domain_filters = search_domain_filter
        super().__init__(
            name="perplexity-search",
            description="Perplexity AI search tool for web search, news search, and conversational search "
            "for finding answers to everyday questions, conducting in-depth research and analysis.",
            func_or_tool=self.search,
        )

    @staticmethod
    def _validate_tool_config(
        model: str, api_key: str | None, max_tokens: int, search_domain_filter: list[str] | None
    ) -> None:
        """Validates the configuration parameters for the search tool.

        Args:
            model (str): The model to use.
            api_key (Union[str, None]): The API key for authentication.
            max_tokens (int): Maximum tokens allowed.
            search_domain_filter (Union[list[str], None]): Domain filters for search.

        Raises:
            ValueError: If the API key is missing, model is empty, max_tokens is not positive,
                        or search_domain_filter is not a list.
        """
        if not api_key:
            raise ValueError("Perplexity API key is missing")
        if not model:
            raise ValueError("model cannot be empty")
        if max_tokens <= 0:
            raise ValueError("max_tokens must be positive")
        if search_domain_filter is not None and not isinstance(search_domain_filter, list):
            raise ValueError("search_domain_filter must be a list")

    def _execute_query(self, payload: dict[str, Any]) -> "PerplexityChatCompletionResponse":
        """Executes a query by sending a POST request to the Perplexity API.

        Args:
            payload (dict[str, Any]): The payload to send in the API request.

        Returns:
            PerplexityChatCompletionResponse: Parsed response from the Perplexity API.

        Raises:
            RuntimeError: If there is a network error, HTTP error, JSON parsing error, or if the response
                          cannot be parsed into a PerplexityChatCompletionResponse.
        """
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        response = requests.request("POST", self.url, json=payload, headers=headers, timeout=10)
        try:
            response.raise_for_status()
        except requests.exceptions.Timeout as e:
            raise RuntimeError(
                f"Perplexity API => Request timed out: {response.text}. Status code: {response.status_code}"
            ) from e
        except requests.exceptions.HTTPError as e:
            raise RuntimeError(
                f"Perplexity API => HTTP error occurred: {response.text}. Status code: {response.status_code}"
            ) from e
        except requests.exceptions.RequestException as e:
            raise RuntimeError(
                f"Perplexity API => Error during request: {response.text}. Status code: {response.status_code}"
            ) from e

        try:
            response_json = response.json()
        except json.JSONDecodeError as e:
            raise RuntimeError(f"Perplexity API => Invalid JSON response received. Error: {e}") from e

        try:
            # This may raise a pydantic.ValidationError if the response structure is not as expected.
            perp_resp = PerplexityChatCompletionResponse(**response_json)
        except ValidationError as e:
            raise RuntimeError("Perplexity API => Validation error when parsing API response: " + str(e)) from e
        except Exception as e:
            raise RuntimeError(
                "Perplexity API => Failed to parse API response into PerplexityChatCompletionResponse: " + str(e)
            ) from e

        return perp_resp

    def search(self, query: str) -> "SearchResponse":
        """Perform a search query using the Perplexity AI API.

        Constructs the payload, executes the query, and parses the response to return
        a concise search result along with any provided citations.

        Args:
            query (str): The search query.

        Returns:
            SearchResponse: A model containing the search result content and citations.

        Raises:
            ValueError: If the search query is invalid.
            RuntimeError: If there is an error during the search process.
        """
        if not query or not isinstance(query, str):
            raise ValueError("A valid non-empty query string must be provided.")

        payload = {
            "model": self.model,
            "messages": [{"role": "system", "content": "Be precise and concise."}, {"role": "user", "content": query}],
            "max_tokens": self.max_tokens,
            "search_domain_filter": self.search_domain_filters,
            "web_search_options": {"search_context_size": "high"},
        }

        try:
            perplexity_response = self._execute_query(payload)
            content = perplexity_response.choices[0].message.content
            citations = perplexity_response.citations
            return SearchResponse(content=content, citations=citations, error=None)
        except Exception as e:
            # Return a SearchResponse with an error message if something goes wrong.
            return SearchResponse(content=None, citations=None, error=f"{e}")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .perplexity_search import PerplexitySearchTool

__all__ = ["PerplexitySearchTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import copy
import logging
import os
from typing import Annotated, Any, Literal

from pydantic import BaseModel

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ....llm_config import LLMConfig
from ... import Tool

with optional_import_block():
    from openai import OpenAI
    from openai.types.responses import WebSearchToolParam
    from openai.types.responses.web_search_tool import UserLocation


@require_optional_import("openai>=1.66.2", "openai")
@export_module("autogen.tools.experimental")
class WebSearchPreviewTool(Tool):
    """WebSearchPreviewTool is a tool that uses OpenAI's web_search_preview tool to perform a search."""

    def __init__(
        self,
        *,
        llm_config: LLMConfig | dict[str, Any],
        search_context_size: Literal["low", "medium", "high"] = "medium",
        user_location: dict[str, str] | None = None,
        instructions: str | None = None,
        text_format: type[BaseModel] | None = None,
    ):
        """Initialize the WebSearchPreviewTool.

        Args:
            llm_config: The LLM configuration to use. This should be a dictionary
                containing the model name and other parameters.
            search_context_size: The size of the search context. One of `low`, `medium`, or `high`.
                `medium` is the default.
            user_location: The location of the user. This should be a dictionary containing
                the city, country, region, and timezone.
            instructions: Inserts a system (or developer) message as the first item in the model's context.
            text_format: The format of the text to be returned. This should be a subclass of `BaseModel`.
                The default is `None`, which means the text will be returned as a string.
        """
        self.web_search_tool_param = WebSearchToolParam(
            type="web_search",
            search_context_size=search_context_size,
            user_location=UserLocation(**user_location) if user_location else None,  # type: ignore[typeddict-item]
        )
        self.instructions = instructions
        self.text_format = text_format

        if isinstance(llm_config, LLMConfig):
            llm_config = llm_config.model_dump()

        llm_config = copy.deepcopy(llm_config)

        if "config_list" not in llm_config:
            raise ValueError("llm_config must contain 'config_list' key")

        # Find first OpenAI model which starts with "gpt-4"
        self.model = None
        self.api_key = None
        for model in llm_config["config_list"]:
            if model["model"].startswith("gpt-4") and model.get("api_type", "openai") == "openai":
                self.model = model["model"]
                self.api_key = model.get("api_key", os.getenv("OPENAI_API_KEY"))
                break
        if self.model is None:
            raise ValueError(
                "No OpenAI model starting with 'gpt-4' found in llm_config, other models do not support web_search_preview"
            )

        if not self.model.startswith("gpt-4.1") and not self.model.startswith("gpt-4o-search-preview"):
            logging.warning(
                f"We recommend using a model starting with 'gpt-4.1' or 'gpt-4o-search-preview' for web_search_preview, but found {self.model}. "
                "This may result in suboptimal performance."
            )

        def web_search_preview(
            query: Annotated[str, "The search query. Add all relevant context to the query."],
        ) -> str | BaseModel | None:
            client = OpenAI()

            if not self.text_format:
                response = client.responses.create(
                    model=self.model,  # type: ignore[arg-type]
                    tools=[self.web_search_tool_param],
                    input=query,
                    instructions=self.instructions,
                )
                text = []
                for output in response.output:
                    if output.type == "message":
                        for content in output.content:
                            if content.type == "output_text":
                                text.append(content.text)
                return "\n".join(text)

            else:
                response = client.responses.parse(
                    model=self.model,  # type: ignore[arg-type]
                    tools=[self.web_search_tool_param],
                    input=query,
                    instructions=self.instructions,
                    text_format=self.text_format,
                )
                return response.output_parsed

        super().__init__(
            name="web_search_preview",
            description="Tool used to perform a web search. It can be used as google search or directly searching a specific website.",
            func_or_tool=web_search_preview,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .web_search_preview import WebSearchPreviewTool

__all__ = ["WebSearchPreviewTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
from datetime import datetime, timezone
from typing import Annotated, Any

from .....doc_utils import export_module
from .....import_utils import optional_import_block, require_optional_import
from .... import Tool
from ....dependency_injection import Depends, on

__all__ = ["DiscordRetrieveTool", "DiscordSendTool"]

with optional_import_block():
    from discord import Client, Intents, utils

MAX_MESSAGE_LENGTH = 2000
MAX_BATCH_RETRIEVE_MESSAGES = 100  # Discord's max per request


@require_optional_import(["discord"], "commsagent-discord")
@export_module("autogen.tools.experimental")
class DiscordSendTool(Tool):
    """Sends a message to a Discord channel."""

    def __init__(self, *, bot_token: str, channel_name: str, guild_name: str) -> None:
        """Initialize the DiscordSendTool.

        Args:
            bot_token: The bot token to use for sending messages.
            channel_name: The name of the channel to send messages to.
            guild_name: The name of the guild for the channel.
        """

        # Function that sends the message, uses dependency injection for bot token / channel / guild
        async def discord_send_message(
            message: Annotated[str, "Message to send to the channel."],
            bot_token: Annotated[str, Depends(on(bot_token))],
            guild_name: Annotated[str, Depends(on(guild_name))],
            channel_name: Annotated[str, Depends(on(channel_name))],
        ) -> Any:
            """Sends a message to a Discord channel.

            Args:
                message: The message to send to the channel.
                bot_token: The bot token to use for Discord. (uses dependency injection)
                guild_name: The name of the server. (uses dependency injection)
                channel_name: The name of the channel. (uses dependency injection)
            """
            intents = Intents.default()
            intents.message_content = True
            intents.guilds = True
            intents.guild_messages = True

            client = Client(intents=intents)
            result_future: asyncio.Future[str] = asyncio.Future()  # Stores the result of the send

            # When the client is ready, we'll send the message
            @client.event  # type: ignore[misc]
            async def on_ready() -> None:
                try:
                    # Server
                    guild = utils.get(client.guilds, name=guild_name)
                    if guild:
                        # Channel
                        channel = utils.get(guild.text_channels, name=channel_name)
                        if channel:
                            # Send the message
                            if len(message) > MAX_MESSAGE_LENGTH:
                                chunks = [
                                    message[i : i + (MAX_MESSAGE_LENGTH - 1)]
                                    for i in range(0, len(message), (MAX_MESSAGE_LENGTH - 1))
                                ]
                                for i, chunk in enumerate(chunks):
                                    sent = await channel.send(chunk)

                                    # Store ID for the first chunk
                                    if i == 0:
                                        sent_message_id = str(sent.id)

                                result_future.set_result(
                                    f"Message sent successfully ({len(chunks)} chunks, first ID: {sent_message_id}):\n{message}"
                                )
                            else:
                                sent = await channel.send(message)
                                result_future.set_result(f"Message sent successfully (ID: {sent.id}):\n{message}")
                        else:
                            result_future.set_result(f"Message send failed, could not find channel: {channel_name}")
                    else:
                        result_future.set_result(f"Message send failed, could not find guild: {guild_name}")

                except Exception as e:
                    result_future.set_exception(e)
                finally:
                    try:
                        await client.close()
                    except Exception as e:
                        raise Exception(f"Unable to close Discord client: {e}")

            # Start the client and when it's ready it'll send the message in on_ready
            try:
                await client.start(bot_token)

                # Capture the result of the send
                return await result_future
            except Exception as e:
                raise Exception(f"Failed to start Discord client: {e}")

        super().__init__(
            name="discord_send",
            description="Sends a message to a Discord channel.",
            func_or_tool=discord_send_message,
        )


@require_optional_import(["discord"], "commsagent-discord")
@export_module("autogen.tools.experimental")
class DiscordRetrieveTool(Tool):
    """Retrieves messages from a Discord channel."""

    def __init__(self, *, bot_token: str, channel_name: str, guild_name: str) -> None:
        """Initialize the DiscordRetrieveTool.

        Args:
            bot_token: The bot token to use for retrieving messages.
            channel_name: The name of the channel to retrieve messages from.
            guild_name: The name of the guild for the channel.
        """

        async def discord_retrieve_messages(
            bot_token: Annotated[str, Depends(on(bot_token))],
            guild_name: Annotated[str, Depends(on(guild_name))],
            channel_name: Annotated[str, Depends(on(channel_name))],
            messages_since: Annotated[
                str | None,
                "Date to retrieve messages from (ISO format) OR Discord snowflake ID. If None, retrieves latest messages.",
            ] = None,
            maximum_messages: Annotated[
                int | None, "Maximum number of messages to retrieve. If None, retrieves all messages since date."
            ] = None,
        ) -> Any:
            """Retrieves messages from a Discord channel.

            Args:
                bot_token: The bot token to use for Discord. (uses dependency injection)
                guild_name: The name of the server. (uses dependency injection)
                channel_name: The name of the channel. (uses dependency injection)
                messages_since: ISO format date string OR Discord snowflake ID, to retrieve messages from. If None, retrieves latest messages.
                maximum_messages: Maximum number of messages to retrieve. If None, retrieves all messages since date.
            """
            intents = Intents.default()
            intents.message_content = True
            intents.guilds = True
            intents.guild_messages = True

            client = Client(intents=intents)
            result_future: asyncio.Future[list[dict[str, Any]]] = asyncio.Future()

            messages_since_date: str | None = None
            if messages_since is not None:
                if DiscordRetrieveTool._is_snowflake(messages_since):
                    messages_since_date = DiscordRetrieveTool._snowflake_to_iso(messages_since)
                else:
                    messages_since_date = messages_since

            @client.event  # type: ignore[misc]
            async def on_ready() -> None:
                try:
                    messages = []

                    # Get guild and channel
                    guild = utils.get(client.guilds, name=guild_name)
                    if not guild:
                        result_future.set_result([{"error": f"Could not find guild: {guild_name}"}])
                        return

                    channel = utils.get(guild.text_channels, name=channel_name)
                    if not channel:
                        result_future.set_result([{"error": f"Could not find channel: {channel_name}"}])
                        return

                    # Setup retrieval parameters
                    last_message_id = None
                    messages_retrieved = 0

                    # Convert to ISO format
                    after_date = None
                    if messages_since_date:
                        try:
                            from datetime import datetime

                            after_date = datetime.fromisoformat(messages_since_date)
                        except ValueError:
                            result_future.set_result([
                                {"error": f"Invalid date format: {messages_since_date}. Use ISO format."}
                            ])
                            return

                    while True:
                        # Setup fetch options
                        fetch_options = {
                            "limit": MAX_BATCH_RETRIEVE_MESSAGES,
                            "before": last_message_id if last_message_id else None,
                            "after": after_date if after_date else None,
                        }

                        # Fetch batch of messages
                        message_batch = []
                        async for message in channel.history(**fetch_options):  # type: ignore[arg-type]
                            message_batch.append(message)
                            messages_retrieved += 1

                            # Check if we've reached the maximum
                            if maximum_messages and messages_retrieved >= maximum_messages:
                                break

                        if not message_batch:
                            break

                        # Process messages
                        for msg in message_batch:
                            messages.append({
                                "id": str(msg.id),
                                "content": msg.content,
                                "author": str(msg.author),
                                "timestamp": msg.created_at.isoformat(),
                            })

                        # Update last message ID for pagination
                        last_message_id = message_batch[-1]  # Use message object directly as 'before' parameter

                        # Break if we've reached the maximum
                        if maximum_messages and messages_retrieved >= maximum_messages:
                            break

                    result_future.set_result(messages)

                except Exception as e:
                    result_future.set_exception(e)
                finally:
                    try:
                        await client.close()
                    except Exception as e:
                        raise Exception(f"Unable to close Discord client: {e}")

            try:
                await client.start(bot_token)
                return await result_future
            except Exception as e:
                raise Exception(f"Failed to start Discord client: {e}")

        super().__init__(
            name="discord_retrieve",
            description="Retrieves messages from a Discord channel based datetime/message ID and/or number of latest messages.",
            func_or_tool=discord_retrieve_messages,
        )

    @staticmethod
    def _is_snowflake(value: str) -> bool:
        """Check if a string is a valid Discord snowflake ID."""
        # Must be numeric and 17-20 digits
        if not value.isdigit():
            return False

        digit_count = len(value)
        return 17 <= digit_count <= 20

    @staticmethod
    def _snowflake_to_iso(snowflake: str) -> str:
        """Convert a Discord snowflake ID to ISO timestamp string."""
        if not DiscordRetrieveTool._is_snowflake(snowflake):
            raise ValueError(f"Invalid snowflake ID: {snowflake}")

        # Discord epoch (2015-01-01)
        discord_epoch = 1420070400000

        # Convert ID to int and shift right 22 bits to get timestamp
        timestamp_ms = (int(snowflake) >> 22) + discord_epoch

        # Convert to datetime and format as ISO string
        dt = datetime.fromtimestamp(timestamp_ms / 1000.0, tz=timezone.utc)
        return dt.isoformat()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .discord import DiscordRetrieveTool, DiscordSendTool

__all__ = ["DiscordRetrieveTool", "DiscordSendTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .discord import DiscordRetrieveTool, DiscordSendTool
from .slack import SlackRetrieveRepliesTool, SlackRetrieveTool, SlackSendTool
from .telegram import TelegramRetrieveTool, TelegramSendTool

__all__ = [
    "DiscordRetrieveTool",
    "DiscordSendTool",
    "SlackRetrieveRepliesTool",
    "SlackRetrieveTool",
    "SlackSendTool",
    "TelegramRetrieveTool",
    "TelegramSendTool",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from datetime import datetime
from typing import Annotated, Any, Union

from .....doc_utils import export_module
from .....import_utils import optional_import_block, require_optional_import
from .... import Tool
from ....dependency_injection import Depends, on

__all__ = ["TelegramRetrieveTool", "TelegramSendTool"]

with optional_import_block():
    from telethon import TelegramClient
    from telethon.tl.types import InputMessagesFilterEmpty, Message, PeerChannel, PeerChat, PeerUser

MAX_MESSAGE_LENGTH = 4096


@require_optional_import(["telethon", "telethon.tl.types"], "commsagent-telegram")
@export_module("autogen.tools.experimental")
class BaseTelegramTool:
    """Base class for Telegram tools containing shared functionality."""

    def __init__(self, api_id: str, api_hash: str, session_name: str) -> None:
        self._api_id = api_id
        self._api_hash = api_hash
        self._session_name = session_name

    def _get_client(self) -> "TelegramClient":  # type: ignore[no-any-unimported]
        """Get a fresh TelegramClient instance."""
        return TelegramClient(self._session_name, self._api_id, self._api_hash)

    @staticmethod
    def _get_peer_from_id(chat_id: str) -> Union["PeerChat", "PeerChannel", "PeerUser"]:  # type: ignore[no-any-unimported]
        """Convert a chat ID string to appropriate Peer type."""
        try:
            # Convert string to integer
            id_int = int(chat_id)

            # Channel/Supergroup: -100 prefix
            if str(chat_id).startswith("-100"):
                channel_id = int(str(chat_id)[4:])  # Remove -100 prefix
                return PeerChannel(channel_id)

            # Group: negative number without -100 prefix
            elif id_int < 0:
                group_id = -id_int  # Remove the negative sign
                return PeerChat(group_id)

            # User/Bot: positive number
            else:
                return PeerUser(id_int)

        except ValueError as e:
            raise ValueError(f"Invalid chat_id format: {chat_id}. Error: {str(e)}")

    async def _initialize_entity(self, client: "TelegramClient", chat_id: str) -> Any:  # type: ignore[no-any-unimported]
        """Initialize and cache the entity by trying different methods."""
        peer = self._get_peer_from_id(chat_id)

        try:
            # Try direct entity resolution first
            entity = await client.get_entity(peer)
            return entity
        except ValueError:
            try:
                # Get all dialogs (conversations)
                async for dialog in client.iter_dialogs():
                    # For users/bots, we need to find the dialog with the user
                    if (
                        isinstance(peer, PeerUser)
                        and dialog.entity.id == peer.user_id
                        or dialog.entity.id == getattr(peer, "channel_id", getattr(peer, "chat_id", None))
                    ):
                        return dialog.entity

                # If we get here, we didn't find the entity in dialogs
                raise ValueError(f"Could not find entity {chat_id} in dialogs")
            except Exception as e:
                raise ValueError(
                    f"Could not initialize entity for {chat_id}. "
                    f"Make sure you have access to this chat. Error: {str(e)}"
                )


@require_optional_import(["telethon"], "commsagent-telegram")
@export_module("autogen.tools.experimental")
class TelegramSendTool(BaseTelegramTool, Tool):
    """Sends a message to a Telegram channel, group, or user."""

    def __init__(self, *, api_id: str, api_hash: str, chat_id: str) -> None:
        """Initialize the TelegramSendTool.

        Args:
            api_id: Telegram API ID from https://my.telegram.org/apps.
            api_hash: Telegram API hash from https://my.telegram.org/apps.
            chat_id: The ID of the destination (Channel, Group, or User ID).
        """
        BaseTelegramTool.__init__(self, api_id, api_hash, "telegram_send_session")

        async def telegram_send_message(
            message: Annotated[str, "Message to send to the chat."],
            chat_id: Annotated[str, Depends(on(chat_id))],
        ) -> Any:
            """Sends a message to a Telegram chat.

            Args:
                message: The message to send.
                chat_id: The ID of the destination. (uses dependency injection)
            """
            try:
                client = self._get_client()
                async with client:
                    # Initialize and cache the entity
                    entity = await self._initialize_entity(client, chat_id)

                    if len(message) > MAX_MESSAGE_LENGTH:
                        chunks = [
                            message[i : i + (MAX_MESSAGE_LENGTH - 1)]
                            for i in range(0, len(message), (MAX_MESSAGE_LENGTH - 1))
                        ]
                        first_message: Message | None = None  # type: ignore[no-any-unimported]

                        for i, chunk in enumerate(chunks):
                            sent = await client.send_message(
                                entity=entity,
                                message=chunk,
                                parse_mode="html",
                                reply_to=first_message.id if first_message else None,
                            )

                            # Store the first message to chain replies
                            if i == 0:
                                first_message = sent
                                sent_message_id = str(sent.id)

                        return (
                            f"Message sent successfully ({len(chunks)} chunks, first ID: {sent_message_id}):\n{message}"
                        )
                    else:
                        sent = await client.send_message(entity=entity, message=message, parse_mode="html")
                        return f"Message sent successfully (ID: {sent.id}):\n{message}"

            except Exception as e:
                return f"Message send failed, exception: {str(e)}"

        Tool.__init__(
            self,
            name="telegram_send",
            description="Sends a message to a personal channel, bot channel, group, or channel.",
            func_or_tool=telegram_send_message,
        )


@require_optional_import(["telethon"], "commsagent-telegram")
@export_module("autogen.tools.experimental")
class TelegramRetrieveTool(BaseTelegramTool, Tool):
    """Retrieves messages from a Telegram channel."""

    def __init__(self, *, api_id: str, api_hash: str, chat_id: str) -> None:
        """Initialize the TelegramRetrieveTool.

        Args:
            api_id: Telegram API ID from https://my.telegram.org/apps.
            api_hash: Telegram API hash from https://my.telegram.org/apps.
            chat_id: The ID of the chat to retrieve messages from (Channel, Group, Bot Chat ID).
        """
        BaseTelegramTool.__init__(self, api_id, api_hash, "telegram_retrieve_session")
        self._chat_id = chat_id

        async def telegram_retrieve_messages(
            chat_id: Annotated[str, Depends(on(chat_id))],
            messages_since: Annotated[
                str | None,
                "Date to retrieve messages from (ISO format) OR message ID. If None, retrieves latest messages.",
            ] = None,
            maximum_messages: Annotated[
                int | None, "Maximum number of messages to retrieve. If None, retrieves all messages since date."
            ] = None,
            search: Annotated[str | None, "Optional string to search for in messages."] = None,
        ) -> Any:
            """Retrieves messages from a Telegram chat.

            Args:
                chat_id: The ID of the chat. (uses dependency injection)
                messages_since: ISO format date string OR message ID to retrieve messages from.
                maximum_messages: Maximum number of messages to retrieve.
                search: Optional string to search for in messages.
            """
            try:
                client = self._get_client()
                async with client:
                    # Initialize and cache the entity
                    entity = await self._initialize_entity(client, chat_id)

                    # Setup retrieval parameters
                    params = {
                        "entity": entity,
                        "limit": maximum_messages if maximum_messages else None,
                        "search": search if search else None,
                        "filter": InputMessagesFilterEmpty(),
                        "wait_time": None,  # No wait time between requests
                    }

                    # Handle messages_since parameter
                    if messages_since:
                        try:
                            # Try to parse as message ID first
                            msg_id = int(messages_since)
                            params["min_id"] = msg_id
                        except ValueError:
                            # Not a message ID, try as ISO date
                            try:
                                date = datetime.fromisoformat(messages_since.replace("Z", "+00:00"))
                                params["offset_date"] = date
                                params["reverse"] = (
                                    True  # Need this because the date gets messages before a certain date by default
                                )
                            except ValueError:
                                return {
                                    "error": "Invalid messages_since format. Please provide either a message ID or ISO format date (e.g., '2025-01-25T00:00:00Z')"
                                }

                    # Retrieve messages
                    messages = []
                    count = 0
                    # For bot users, we need to get both sent and received messages
                    if isinstance(self._get_peer_from_id(chat_id), PeerUser):
                        print(f"Retrieving messages for bot chat {chat_id}")

                    async for message in client.iter_messages(**params):
                        count += 1
                        messages.append({
                            "id": str(message.id),
                            "date": message.date.isoformat(),
                            "from_id": str(message.from_id) if message.from_id else None,
                            "text": message.text,
                            "reply_to_msg_id": str(message.reply_to_msg_id) if message.reply_to_msg_id else None,
                            "forward_from": str(message.forward.from_id) if message.forward else None,
                            "edit_date": message.edit_date.isoformat() if message.edit_date else None,
                            "media": bool(message.media),
                            "entities": [
                                {"type": e.__class__.__name__, "offset": e.offset, "length": e.length}
                                for e in message.entities
                            ]
                            if message.entities
                            else None,
                        })

                        # Check if we've hit the maximum
                        if maximum_messages and len(messages) >= maximum_messages:
                            break

                    return {
                        "message_count": len(messages),
                        "messages": messages,
                        "start_time": messages_since or "latest",
                    }

            except Exception as e:
                return f"Message retrieval failed, exception: {str(e)}"

        Tool.__init__(
            self,
            name="telegram_retrieve",
            description="Retrieves messages from a Telegram chat based on datetime/message ID and/or number of latest messages.",
            func_or_tool=telegram_retrieve_messages,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .telegram import TelegramRetrieveTool, TelegramSendTool

__all__ = ["TelegramRetrieveTool", "TelegramSendTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .slack import SlackRetrieveRepliesTool, SlackRetrieveTool, SlackSendTool

__all__ = ["SlackRetrieveRepliesTool", "SlackRetrieveTool", "SlackSendTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
from datetime import datetime, timedelta
from typing import Annotated, Any

from .....doc_utils import export_module
from .....import_utils import optional_import_block, require_optional_import
from .... import Tool
from ....dependency_injection import Depends, on

__all__ = ["SlackSendTool"]

with optional_import_block():
    from slack_sdk import WebClient
    from slack_sdk.errors import SlackApiError

MAX_MESSAGE_LENGTH = 40000


@require_optional_import(["slack_sdk"], "commsagent-slack")
@export_module("autogen.tools.experimental")
class SlackSendTool(Tool):
    """Sends a message to a Slack channel."""

    def __init__(self, *, bot_token: str, channel_id: str) -> None:
        """Initialize the SlackSendTool.

        Args:
            bot_token: Bot User OAuth Token starting with "xoxb-".
            channel_id: Channel ID where messages will be sent.
        """

        # Function that sends the message, uses dependency injection for bot token / channel / guild
        async def slack_send_message(
            message: Annotated[str, "Message to send to the channel."],
            bot_token: Annotated[str, Depends(on(bot_token))],
            channel_id: Annotated[str, Depends(on(channel_id))],
        ) -> Any:
            """Sends a message to a Slack channel.

            Args:
                message: The message to send to the channel.
                bot_token: The bot token to use for Slack. (uses dependency injection)
                channel_id: The ID of the channel. (uses dependency injection)
            """
            try:
                web_client = WebClient(token=bot_token)

                # Send the message
                if len(message) > MAX_MESSAGE_LENGTH:
                    chunks = [
                        message[i : i + (MAX_MESSAGE_LENGTH - 1)]
                        for i in range(0, len(message), (MAX_MESSAGE_LENGTH - 1))
                    ]
                    for i, chunk in enumerate(chunks):
                        response = web_client.chat_postMessage(channel=channel_id, text=chunk)

                        if not response["ok"]:
                            return f"Message send failed on chunk {i + 1}, Slack response error: {response['error']}"

                        # Store ID for the first chunk
                        if i == 0:
                            sent_message_id = response["ts"]

                    return f"Message sent successfully ({len(chunks)} chunks, first ID: {sent_message_id}):\n{message}"
                else:
                    response = web_client.chat_postMessage(channel=channel_id, text=message)

                    if not response["ok"]:
                        return f"Message send failed, Slack response error: {response['error']}"

                    return f"Message sent successfully (ID: {response['ts']}):\n{message}"
            except SlackApiError as e:
                return f"Message send failed, Slack API exception: {e.response['error']} (See https://api.slack.com/automation/cli/errors#{e.response['error']})"
            except Exception as e:
                return f"Message send failed, exception: {e}"

        super().__init__(
            name="slack_send",
            description="Sends a message to a Slack channel.",
            func_or_tool=slack_send_message,
        )


@require_optional_import(["slack_sdk"], "commsagent-slack")
@export_module("autogen.tools.experimental")
class SlackRetrieveTool(Tool):
    """Retrieves messages from a Slack channel."""

    def __init__(self, *, bot_token: str, channel_id: str) -> None:
        """Initialize the SlackRetrieveTool.

        Args:
            bot_token: Bot User OAuth Token starting with "xoxb-".
            channel_id: Channel ID where messages will be sent.
        """

        async def slack_retrieve_messages(
            bot_token: Annotated[str, Depends(on(bot_token))],
            channel_id: Annotated[str, Depends(on(channel_id))],
            messages_since: Annotated[
                str | None,
                "Date to retrieve messages from (ISO format) OR Slack message ID. If None, retrieves latest messages.",
            ] = None,
            maximum_messages: Annotated[
                int | None, "Maximum number of messages to retrieve. If None, retrieves all messages since date."
            ] = None,
        ) -> Any:
            """Retrieves messages from a Discord channel.

            Args:
                bot_token: The bot token to use for Discord. (uses dependency injection)
                channel_id: The ID of the channel. (uses dependency injection)
                messages_since: ISO format date string OR Slack message ID, to retrieve messages from. If None, retrieves latest messages.
                maximum_messages: Maximum number of messages to retrieve. If None, retrieves all messages since date.
            """
            try:
                web_client = WebClient(token=bot_token)

                # Convert ISO datetime to Unix timestamp if needed
                oldest = None
                if messages_since:
                    if "." in messages_since:  # Likely a Slack message ID
                        oldest = messages_since
                    else:  # Assume ISO format
                        try:
                            dt = datetime.fromisoformat(messages_since.replace("Z", "+00:00"))
                            oldest = str(dt.timestamp())
                        except ValueError as e:
                            return f"Invalid date format. Please provide either a Slack message ID or ISO format date (e.g., '2025-01-25T00:00:00Z'). Error: {e}"

                messages = []
                cursor = None

                while True:
                    try:
                        # Prepare API call parameters
                        params = {
                            "channel": channel_id,
                            "limit": min(1000, maximum_messages) if maximum_messages else 1000,
                        }
                        if oldest:
                            params["oldest"] = oldest
                        if cursor:
                            params["cursor"] = cursor

                        # Make API call
                        response = web_client.conversations_history(**params)  # type: ignore[arg-type]

                        if not response["ok"]:
                            return f"Message retrieval failed, Slack response error: {response['error']}"

                        # Add messages to our list
                        messages.extend(response["messages"])

                        # Check if we've hit our maximum
                        if maximum_messages and len(messages) >= maximum_messages:
                            messages = messages[:maximum_messages]
                            break

                        # Check if there are more messages
                        if not response["has_more"]:
                            break

                        cursor = response["response_metadata"]["next_cursor"]

                    except SlackApiError as e:
                        return f"Message retrieval failed on pagination, Slack API error: {e.response['error']}"

                return {
                    "message_count": len(messages),
                    "messages": messages,
                    "start_time": oldest or "latest",
                }

            except SlackApiError as e:
                return f"Message retrieval failed, Slack API exception: {e.response['error']} (See https://api.slack.com/automation/cli/errors#{e.response['error']})"
            except Exception as e:
                return f"Message retrieval failed, exception: {e}"

        super().__init__(
            name="slack_retrieve",
            description="Retrieves messages from a Slack channel based datetime/message ID and/or number of latest messages.",
            func_or_tool=slack_retrieve_messages,
        )


@require_optional_import(["slack_sdk"], "commsagent-slack")
@export_module("autogen.tools.experimental")
class SlackRetrieveRepliesTool(Tool):
    """Retrieves replies to a specific Slack message from both threads and the channel."""

    def __init__(self, *, bot_token: str, channel_id: str) -> None:
        """Initialize the SlackRetrieveRepliesTool.

        Args:
            bot_token: Bot User OAuth Token starting with "xoxb-".
            channel_id: Channel ID where the parent message exists.
        """

        async def slack_retrieve_replies(
            message_ts: Annotated[str, "Timestamp (ts) of the parent message to retrieve replies for."],
            bot_token: Annotated[str, Depends(on(bot_token))],
            channel_id: Annotated[str, Depends(on(channel_id))],
            min_replies: Annotated[
                int | None,
                "Minimum number of replies to wait for before returning (thread + channel). If None, returns immediately.",
            ] = None,
            timeout_seconds: Annotated[
                int, "Maximum time in seconds to wait for the requested number of replies."
            ] = 60,
            poll_interval: Annotated[int, "Time in seconds between polling attempts when waiting for replies."] = 5,
            include_channel_messages: Annotated[
                bool, "Whether to include messages in the channel after the original message."
            ] = True,
        ) -> Any:
            """Retrieves replies to a specific Slack message, from both threads and the main channel.

            Args:
                message_ts: The timestamp (ts) identifier of the parent message.
                bot_token: The bot token to use for Slack. (uses dependency injection)
                channel_id: The ID of the channel. (uses dependency injection)
                min_replies: Minimum number of combined replies to wait for before returning. If None, returns immediately.
                timeout_seconds: Maximum time in seconds to wait for the requested number of replies.
                poll_interval: Time in seconds between polling attempts when waiting for replies.
                include_channel_messages: Whether to include messages posted in the channel after the original message.
            """
            try:
                web_client = WebClient(token=bot_token)

                # Function to get current thread replies
                async def get_thread_replies() -> tuple[list[dict[str, Any]] | None, str | None]:
                    try:
                        response = web_client.conversations_replies(
                            channel=channel_id,
                            ts=message_ts,
                        )

                        if not response["ok"]:
                            return None, f"Thread reply retrieval failed, Slack response error: {response['error']}"

                        # The first message is the parent message itself, so exclude it when counting replies
                        replies = response["messages"][1:] if len(response["messages"]) > 0 else []
                        return replies, None

                    except SlackApiError as e:
                        return None, f"Thread reply retrieval failed, Slack API exception: {e.response['error']}"
                    except Exception as e:
                        return None, f"Thread reply retrieval failed, exception: {e}"

                # Function to get messages in the channel after the original message
                async def get_channel_messages() -> tuple[list[dict[str, Any]] | None, str | None]:
                    try:
                        response = web_client.conversations_history(
                            channel=channel_id,
                            oldest=message_ts,  # Start from the original message timestamp
                            inclusive=False,  # Don't include the original message
                        )

                        if not response["ok"]:
                            return None, f"Channel message retrieval failed, Slack response error: {response['error']}"

                        # Return all messages in the channel after the original message
                        # We need to filter out any that are part of the thread we're already getting
                        messages = []
                        for msg in response["messages"]:
                            # Skip if the message is part of the thread we're already retrieving
                            if "thread_ts" in msg and msg["thread_ts"] == message_ts:
                                continue
                            messages.append(msg)

                        return messages, None

                    except SlackApiError as e:
                        return None, f"Channel message retrieval failed, Slack API exception: {e.response['error']}"
                    except Exception as e:
                        return None, f"Channel message retrieval failed, exception: {e}"

                # Function to get all replies (both thread and channel)
                async def get_all_replies() -> tuple[
                    list[dict[str, Any]] | None, list[dict[str, Any]] | None, str | None
                ]:
                    thread_replies, thread_error = await get_thread_replies()
                    if thread_error:
                        return None, None, thread_error

                    channel_messages: list[dict[str, Any]] = []
                    channel_error = None

                    if include_channel_messages:
                        channel_results, channel_error = await get_channel_messages()
                        if channel_error:
                            return thread_replies, None, channel_error
                        channel_messages = channel_results if channel_results is not None else []

                    return thread_replies, channel_messages, None

                # If no waiting is required, just get replies and return
                if min_replies is None:
                    thread_replies, channel_messages, error = await get_all_replies()
                    if error:
                        return error

                    thread_replies_list: list[dict[str, Any]] = [] if thread_replies is None else thread_replies
                    channel_messages_list: list[dict[str, Any]] = [] if channel_messages is None else channel_messages

                    # Combine replies for counting but keep them separate in the result
                    total_reply_count = len(thread_replies_list) + len(channel_messages_list)

                    return {
                        "parent_message_ts": message_ts,
                        "total_reply_count": total_reply_count,
                        "thread_replies": thread_replies_list,
                        "thread_reply_count": len(thread_replies_list),
                        "channel_messages": channel_messages_list if include_channel_messages else None,
                        "channel_message_count": len(channel_messages_list) if include_channel_messages else None,
                    }

                # Wait for the required number of replies with timeout
                start_time = datetime.now()
                end_time = start_time + timedelta(seconds=timeout_seconds)

                while datetime.now() < end_time:
                    thread_replies, channel_messages, error = await get_all_replies()
                    if error:
                        return error

                    thread_replies_current: list[dict[str, Any]] = [] if thread_replies is None else thread_replies
                    channel_messages_current: list[dict[str, Any]] = (
                        [] if channel_messages is None else channel_messages
                    )

                    # Combine replies for counting
                    total_reply_count = len(thread_replies_current) + len(channel_messages_current)

                    # If we have enough total replies, return them
                    if total_reply_count >= min_replies:
                        return {
                            "parent_message_ts": message_ts,
                            "total_reply_count": total_reply_count,
                            "thread_replies": thread_replies_current,
                            "thread_reply_count": len(thread_replies_current),
                            "channel_messages": channel_messages_current if include_channel_messages else None,
                            "channel_message_count": len(channel_messages_current)
                            if include_channel_messages
                            else None,
                            "waited_seconds": (datetime.now() - start_time).total_seconds(),
                        }

                    # Wait before checking again
                    await asyncio.sleep(poll_interval)

                # If we reach here, we timed out waiting for replies
                thread_replies, channel_messages, error = await get_all_replies()
                if error:
                    return error

                # Combine replies for counting
                total_reply_count = len(thread_replies or []) + len(channel_messages or [])

                return {
                    "parent_message_ts": message_ts,
                    "total_reply_count": total_reply_count,
                    "thread_replies": thread_replies or [],
                    "thread_reply_count": len(thread_replies or []),
                    "channel_messages": channel_messages or [] if include_channel_messages else None,
                    "channel_message_count": len(channel_messages or []) if include_channel_messages else None,
                    "timed_out": True,
                    "waited_seconds": timeout_seconds,
                    "requested_replies": min_replies,
                }

            except SlackApiError as e:
                return f"Reply retrieval failed, Slack API exception: {e.response['error']} (See https://api.slack.com/automation/cli/errors#{e.response['error']})"
            except Exception as e:
                return f"Reply retrieval failed, exception: {e}"

        super().__init__(
            name="slack_retrieve_replies",
            description="Retrieves replies to a specific Slack message, checking both thread replies and messages in the channel after the original message.",
            func_or_tool=slack_retrieve_replies,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .browser_use import BrowserUseResult, BrowserUseTool, ExtractedContent

__all__ = ["BrowserUseResult", "BrowserUseTool", "ExtractedContent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Annotated, Any, Optional

from pydantic import BaseModel, field_validator

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ....llm_config import LLMConfig
from ... import Depends, Tool
from ...dependency_injection import on

with optional_import_block():
    from browser_use import Agent, Controller
    from browser_use.browser.browser import Browser, BrowserConfig

    from ....interop.langchain.langchain_chat_model_factory import LangChainChatModelFactory


__all__ = ["BrowserUseResult", "BrowserUseTool", "ExtractedContent"]


@export_module("autogen.tools.experimental.browser_use")
class ExtractedContent(BaseModel):
    """Extracted content from the browser.

    Attributes:
        content: The extracted content.
        url: The URL of the extracted content
    """

    content: str
    url: str | None

    @field_validator("url")
    @classmethod
    def check_url(cls, v: str) -> str | None:
        """Check if the URL is about:blank and return None if it is.

        Args:
            v: The URL to check.
        """
        if v == "about:blank":
            return None
        return v


@export_module("autogen.tools.experimental.browser_use")
class BrowserUseResult(BaseModel):
    """The result of using the browser to perform a task.

    Attributes:
        extracted_content: List of extracted content.
        final_result: The final result.
    """

    extracted_content: list[ExtractedContent]
    final_result: str | None


@require_optional_import(
    [
        "langchain_anthropic",
        "langchain_google_genai",
        "langchain_ollama",
        "langchain_openai",
        "langchain_core",
        "browser_use",
    ],
    "browser-use",
)
@export_module("autogen.tools.experimental")
class BrowserUseTool(Tool):
    """BrowserUseTool is a tool that uses the browser to perform a task."""

    def __init__(  # type: ignore[no-any-unimported]
        self,
        *,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        browser: Optional["Browser"] = None,
        agent_kwargs: dict[str, Any] | None = None,
        browser_config: dict[str, Any] | None = None,
    ):
        """Use the browser to perform a task.

        Args:
            llm_config: The LLM configuration. If None, the current LLMConfig from context is used.
            browser: The browser to use. If defined, browser_config must be None
            agent_kwargs: Additional keyword arguments to pass to the Agent
            browser_config: The browser configuration to use. If defined, browser must be None
        """
        if llm_config is None:
            llm_config = LLMConfig.current
        if agent_kwargs is None:
            agent_kwargs = {}
        if browser_config is None:
            browser_config = {}
        if browser is not None and browser_config:
            raise ValueError(
                f"Cannot provide both browser and additional keyword parameters: {browser=}, {browser_config=}"
            )

        async def browser_use(  # type: ignore[no-any-unimported]
            task: Annotated[str, "The task to perform."],
            llm_config: Annotated[LLMConfig | dict[str, Any], Depends(on(llm_config))],
            browser: Annotated[Browser | None, Depends(on(browser))],
            agent_kwargs: Annotated[dict[str, Any], Depends(on(agent_kwargs))],
            browser_config: Annotated[dict[str, Any], Depends(on(browser_config))],
        ) -> BrowserUseResult:
            agent_kwargs = agent_kwargs.copy()
            browser_config = browser_config.copy()
            if browser is None:
                # set default value for headless
                headless = browser_config.pop("headless", True)
                browser_config = BrowserConfig(headless=headless, **browser_config)
                browser = Browser(config=browser_config)
            # set default value for generate_gif
            if "generate_gif" not in agent_kwargs:
                agent_kwargs["generate_gif"] = False
            llm = LangChainChatModelFactory.create_base_chat_model(llm_config)
            max_steps = agent_kwargs.pop("max_steps", 100)
            agent = Agent(
                task=task,
                llm=llm,
                browser=browser,
                controller=BrowserUseTool._get_controller(llm_config),
                **agent_kwargs,
            )
            result = await agent.run(max_steps=max_steps)
            extracted_content = [
                ExtractedContent(content=content, url=url)
                for content, url in zip(result.extracted_content(), result.urls())
            ]
            return BrowserUseResult(
                extracted_content=extracted_content,
                final_result=result.final_result(),
            )

        super().__init__(
            name="browser_use",
            description="Use the browser to perform a task.",
            func_or_tool=browser_use,
        )

    @staticmethod
    def _get_controller(llm_config: LLMConfig | dict[str, Any]) -> Any:
        response_format = (
            llm_config["config_list"][0].get("response_format", None)
            if "config_list" in llm_config
            else llm_config.get("response_format")
        )
        return Controller(output_model=response_format)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .deep_research import DeepResearchTool

__all__ = ["DeepResearchTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import copy
from collections.abc import Callable
from typing import Annotated, Any

from pydantic import BaseModel, Field

from ....agentchat import ConversableAgent
from ....doc_utils import export_module
from ....llm_config import LLMConfig
from ... import Depends, Tool
from ...dependency_injection import on

__all__ = ["DeepResearchTool"]


class Subquestion(BaseModel):
    question: Annotated[str, Field(description="The original question.")]

    def format(self) -> str:
        return f"Question: {self.question}\n"


class SubquestionAnswer(Subquestion):
    answer: Annotated[str, Field(description="The answer to the question.")]

    def format(self) -> str:
        return f"Question: {self.question}\n{self.answer}\n"


class Task(BaseModel):
    question: Annotated[str, Field(description="The original question.")]
    subquestions: Annotated[list[Subquestion], Field(description="The subquestions that need to be answered.")]

    def format(self) -> str:
        return f"Task: {self.question}\n\n" + "\n".join(
            "Subquestion " + str(i + 1) + ":\n" + subquestion.format()
            for i, subquestion in enumerate(self.subquestions)
        )


class CompletedTask(BaseModel):
    question: Annotated[str, Field(description="The original question.")]
    subquestions: Annotated[list[SubquestionAnswer], Field(description="The subquestions and their answers")]

    def format(self) -> str:
        return f"Task: {self.question}\n\n" + "\n".join(
            "Subquestion " + str(i + 1) + ":\n" + subquestion.format()
            for i, subquestion in enumerate(self.subquestions)
        )


class InformationCrumb(BaseModel):
    source_url: str
    source_title: str
    source_summary: str
    relevant_info: str


class GatheredInformation(BaseModel):
    information: list[InformationCrumb]

    def format(self) -> str:
        return "Here is the gathered information: \n" + "\n".join(
            f"URL: {info.source_url}\nTitle: {info.source_title}\nSummary: {info.source_summary}\nRelevant Information: {info.relevant_info}\n\n"
            for info in self.information
        )


@export_module("autogen.tools.experimental")
class DeepResearchTool(Tool):
    """A tool that delegates a web research task to the subteams of agents."""

    ANSWER_CONFIRMED_PREFIX = "Answer confirmed:"

    def __init__(
        self,
        llm_config: LLMConfig | dict[str, Any],
        max_web_steps: int = 30,
    ):
        """Initialize the DeepResearchTool.

        Args:
            llm_config (LLMConfig, dict[str, Any]): The LLM configuration.
            max_web_steps (int, optional): The maximum number of web steps. Defaults to 30.
        """
        self.llm_config = llm_config

        self.summarizer_agent = ConversableAgent(
            name="SummarizerAgent",
            system_message=(
                "You are an agent with a task of answering the question provided by the user."
                "First you need to split the question into subquestions by calling the 'split_question_and_answer_subquestions' method."
                "Then you need to sintesize the answers the original question by combining the answers to the subquestions."
            ),
            is_termination_msg=lambda x: x.get("content", "")
            and x.get("content", "").startswith(self.ANSWER_CONFIRMED_PREFIX),
            llm_config=llm_config,
            human_input_mode="NEVER",
        )

        self.critic_agent = ConversableAgent(
            name="CriticAgent",
            system_message=(
                "You are a critic agent responsible for evaluating the answer provided by the summarizer agent.\n"
                "Your task is to assess the quality of the answer based on its coherence, relevance, and completeness.\n"
                "Provide constructive feedback on how the answer can be improved.\n"
                "If the answer is satisfactory, call the 'confirm_answer' method to end the task.\n"
            ),
            is_termination_msg=lambda x: x.get("content", "")
            and x.get("content", "").startswith(self.ANSWER_CONFIRMED_PREFIX),
            llm_config=llm_config,
            human_input_mode="NEVER",
        )

        def delegate_research_task(
            task: Annotated[str, "The task to perform a research on."],
            llm_config: Annotated[LLMConfig | dict[str, Any], Depends(on(llm_config))],
            max_web_steps: Annotated[int, Depends(on(max_web_steps))],
        ) -> str:
            """Delegate a research task to the agent.

            Args:
                task (str): The task to perform a research on.
                llm_config (LLMConfig, dict[str, Any]): The LLM configuration.
                max_web_steps (int): The maximum number of web steps.

            Returns:
                str: The answer to the research task.
            """

            @self.summarizer_agent.register_for_execution()
            @self.critic_agent.register_for_llm(description="Call this method to confirm the final answer.")
            def confirm_summary(answer: str, reasoning: str) -> str:
                return f"{self.ANSWER_CONFIRMED_PREFIX}" + answer + "\nReasoning: " + reasoning

            split_question_and_answer_subquestions = DeepResearchTool._get_split_question_and_answer_subquestions(
                llm_config=llm_config,
                max_web_steps=max_web_steps,
            )

            self.summarizer_agent.register_for_llm(description="Split the question into subquestions and get answers.")(
                split_question_and_answer_subquestions
            )
            self.critic_agent.register_for_execution()(split_question_and_answer_subquestions)

            result = self.critic_agent.initiate_chat(
                self.summarizer_agent,
                message="Please answer the following question: " + task,
                # This outer chat should preserve the history of the conversation
                clear_history=False,
            )

            return result.summary

        super().__init__(
            name=delegate_research_task.__name__,
            description="Delegate a research task to the deep research agent.",
            func_or_tool=delegate_research_task,
        )

    SUBQUESTIONS_ANSWER_PREFIX = "Subquestions answered:"

    @staticmethod
    def _get_split_question_and_answer_subquestions(
        llm_config: LLMConfig | dict[str, Any], max_web_steps: int
    ) -> Callable[..., Any]:
        def split_question_and_answer_subquestions(
            question: Annotated[str, "The question to split and answer."],
            llm_config: Annotated[LLMConfig | dict[str, Any], Depends(on(llm_config))],
            max_web_steps: Annotated[int, Depends(on(max_web_steps))],
        ) -> str:
            decomposition_agent = ConversableAgent(
                name="DecompositionAgent",
                system_message=(
                    "You are an expert at breaking down complex questions into smaller, focused subquestions.\n"
                    "Your task is to take any question provided and divide it into clear, actionable subquestions that can be individually answered.\n"
                    "Ensure the subquestions are logical, non-redundant, and cover all key aspects of the original question.\n"
                    "Avoid providing answers or interpretationsfocus solely on decomposition.\n"
                    "Do not include banal, general knowledge questions\n"
                    "Do not include questions that go into unnecessary detail that is not relevant to the original question\n"
                    "Do not include question that require knowledge of the original or other subquestions to answer\n"
                    "Some rule of thumb is to have only one subquestion for easy questions, 3 for medium questions, and 5 for hard questions.\n"
                ),
                llm_config=llm_config,
                is_termination_msg=lambda x: x.get("content", "")
                and x.get("content", "").startswith(DeepResearchTool.SUBQUESTIONS_ANSWER_PREFIX),
                human_input_mode="NEVER",
            )

            example_task = Task(
                question="What is the capital of France?",
                subquestions=[Subquestion(question="What is the capital of France?")],
            )
            decomposition_critic = ConversableAgent(
                name="DecompositionCritic",
                system_message=(
                    "You are a critic agent responsible for evaluating the subquestions provided by the initial analysis agent.\n"
                    "You need to confirm whether the subquestions are clear, actionable, and cover all key aspects of the original question.\n"
                    "Do not accept redundant or unnecessary subquestions, focus solely on the minimal viable subset of subqestions necessary to answer the original question. \n"
                    "Do not accept banal, general knowledge questions\n"
                    "Do not accept questions that go into unnecessary detail that is not relevant to the original question\n"
                    "Remove questions that can be answered with combining knowledge from other questions\n"
                    "After you are satisfied with the subquestions, call the 'generate_subquestions' method to answer each subquestion.\n"
                    "This is an example of an argument that can be passed to the 'generate_subquestions' method:\n"
                    f"{{'task': {example_task.model_dump()}}}\n"
                    "Some rule of thumb is to have only one subquestion for easy questions, 3 for medium questions, and 5 for hard questions.\n"
                ),
                llm_config=llm_config,
                is_termination_msg=lambda x: x.get("content", "")
                and x.get("content", "").startswith(DeepResearchTool.SUBQUESTIONS_ANSWER_PREFIX),
                human_input_mode="NEVER",
            )

            generate_subquestions = DeepResearchTool._get_generate_subquestions(
                llm_config=llm_config, max_web_steps=max_web_steps
            )
            decomposition_agent.register_for_execution()(generate_subquestions)
            decomposition_critic.register_for_llm(description="Generate subquestions for a task.")(
                generate_subquestions
            )

            result = decomposition_critic.initiate_chat(
                decomposition_agent,
                message="Analyse and gather subqestions for the following question: " + question,
            )

            return result.summary

        return split_question_and_answer_subquestions

    @staticmethod
    def _get_generate_subquestions(
        llm_config: LLMConfig | dict[str, Any],
        max_web_steps: int,
    ) -> Callable[..., str]:
        """Get the generate_subquestions method.

        Args:
            llm_config (Union[LLMConfig, dict[str, Any]]): The LLM configuration.
            max_web_steps (int): The maximum number of web steps.

        Returns:
            Callable[..., str]: The generate_subquestions method.
        """

        def generate_subquestions(
            task: Task,
            llm_config: Annotated[LLMConfig | dict[str, Any], Depends(on(llm_config))],
            max_web_steps: Annotated[int, Depends(on(max_web_steps))],
        ) -> str:
            if not task.subquestions:
                task.subquestions = [Subquestion(question=task.question)]

            subquestions_answers: list[SubquestionAnswer] = []
            for subquestion in task.subquestions:
                answer = DeepResearchTool._answer_question(
                    subquestion.question, llm_config=llm_config, max_web_steps=max_web_steps
                )
                subquestions_answers.append(SubquestionAnswer(question=subquestion.question, answer=answer))

            completed_task = CompletedTask(question=task.question, subquestions=subquestions_answers)

            return f"{DeepResearchTool.SUBQUESTIONS_ANSWER_PREFIX} \n" + completed_task.format()

        return generate_subquestions

    @staticmethod
    def _answer_question(
        question: str,
        llm_config: LLMConfig | dict[str, Any],
        max_web_steps: int,
    ) -> str:
        from ....agents.experimental.websurfer import WebSurferAgent

        websurfer_config = copy.deepcopy(llm_config)

        websurfer_config["config_list"][0]["response_format"] = GatheredInformation

        def is_termination_msg(x: dict[str, Any]) -> bool:
            content = x.get("content", "")
            return (content is not None) and content.startswith(DeepResearchTool.ANSWER_CONFIRMED_PREFIX)

        websurfer_agent = WebSurferAgent(
            llm_config=llm_config,
            web_tool_llm_config=websurfer_config,
            name="WebSurferAgent",
            system_message=(
                "You are a web surfer agent responsible for gathering information from the web to provide information for answering a question\n"
                "You will be asked to find information related to the question and provide a summary of the information gathered.\n"
                "The summary should include the URL, title, summary, and relevant information for each piece of information gathered.\n"
            ),
            is_termination_msg=is_termination_msg,
            human_input_mode="NEVER",
            web_tool_kwargs={
                "agent_kwargs": {"max_steps": max_web_steps},
            },
        )

        websurfer_critic = ConversableAgent(
            name="WebSurferCritic",
            system_message=(
                "You are a critic agent responsible for evaluating the answer provided by the web surfer agent.\n"
                "You need to confirm whether the information provided by the websurfer is correct and sufficient to answer the question.\n"
                "You can ask the web surfer to provide more information or provide and confirm the answer.\n"
            ),
            llm_config=llm_config,
            is_termination_msg=is_termination_msg,
            human_input_mode="NEVER",
        )

        @websurfer_agent.register_for_execution()
        @websurfer_critic.register_for_llm(
            description="Call this method when you agree that the original question can be answered with the gathered information and provide the answer."
        )
        def confirm_answer(answer: str) -> str:
            return f"{DeepResearchTool.ANSWER_CONFIRMED_PREFIX} " + answer

        websurfer_critic.register_for_execution()(websurfer_agent.tool)

        result = websurfer_critic.initiate_chat(
            websurfer_agent,
            message="Please find the answer to this question: " + question,
        )

        return result.summary
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .tavily_search import TavilySearchTool

__all__ = ["TavilySearchTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import os
from typing import Annotated, Any

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ....llm_config import LLMConfig
from ... import Depends, Tool
from ...dependency_injection import on

with optional_import_block():
    from tavily import TavilyClient


@require_optional_import(
    [
        "tavily",
    ],
    "tavily",
)
def _execute_tavily_query(
    query: str,
    tavily_api_key: str,
    search_depth: str = "basic",
    topic: str = "general",
    include_answer: str = "basic",
    include_raw_content: bool = False,
    include_domains: list[str] = [],
    num_results: int = 5,
) -> Any:
    """Execute a search query using the Tavily API.

    Args:
        query (str): The search query string.
        tavily_api_key (str): The API key for Tavily.
        search_depth (str, optional): The depth of the search ('basic' or 'advanced'). Defaults to "basic".
        topic (str, optional): The topic of the search. Defaults to "general".
        include_answer (str, optional): Whether to include an AI-generated answer ('basic' or 'advanced'). Defaults to "basic".
        include_raw_content (bool, optional): Whether to include raw content in the results. Defaults to False.
        include_domains (list[str], optional): A list of domains to include in the search. Defaults to [].
        num_results (int, optional): The maximum number of results to return. Defaults to 5.

    Returns:
        Any: The raw response object from the Tavily API client.
    """
    tavily_client = TavilyClient(api_key=tavily_api_key)
    return tavily_client.search(
        query=query,
        search_depth=search_depth,
        topic=topic,
        include_answer=include_answer,
        include_raw_content=include_raw_content,
        include_domains=include_domains,
        max_results=num_results,
    )


def _tavily_search(
    query: str,
    tavily_api_key: str,
    search_depth: str = "basic",
    topic: str = "general",
    include_answer: str = "basic",
    include_raw_content: bool = False,
    include_domains: list[str] = [],
    num_results: int = 5,
) -> list[dict[str, Any]]:
    """Perform a Tavily search and format the results.

    This function takes search parameters, executes the query using `_execute_tavily_query`,
    and formats the results into a list of dictionaries containing title, link, and snippet.

    Args:
        query (str): The search query string.
        tavily_api_key (str): The API key for Tavily.
        search_depth (str, optional): The depth of the search ('basic' or 'advanced'). Defaults to "basic".
        topic (str, optional): The topic of the search. Defaults to "general".
        include_answer (str, optional): Whether to include an AI-generated answer ('basic' or 'advanced'). Defaults to "basic".
        include_raw_content (bool, optional): Whether to include raw content in the results. Defaults to False.
        include_domains (list[str], optional): A list of domains to include in the search. Defaults to [].
        num_results (int, optional): The maximum number of results to return. Defaults to 5.

    Returns:
        list[dict[str, Any]]: A list of dictionaries, where each dictionary represents a search result
            with keys 'title', 'link', and 'snippet'. Returns an empty list if no results are found.
    """
    res = _execute_tavily_query(
        query=query,
        tavily_api_key=tavily_api_key,
        search_depth=search_depth,
        topic=topic,
        include_answer=include_answer,
        include_raw_content=include_raw_content,
        include_domains=include_domains,
        num_results=num_results,
    )

    return [
        {"title": item.get("title", ""), "link": item.get("url", ""), "snippet": item.get("content", "")}
        for item in res.get("results", [])
    ]


@export_module("autogen.tools.experimental")
class TavilySearchTool(Tool):
    """TavilySearchTool is a tool that uses the Tavily Search API to perform a search.

    This tool allows agents to leverage the Tavily search engine for information retrieval.
    It requires a Tavily API key, which can be provided during initialization or set as
    an environment variable `TAVILY_API_KEY`.

    Attributes:
        tavily_api_key (str): The API key used for authenticating with the Tavily API.
    """

    def __init__(self, *, llm_config: LLMConfig | dict[str, Any] | None = None, tavily_api_key: str | None = None):
        """Initializes the TavilySearchTool.

        Args:
            llm_config (Optional[Union[LLMConfig, dict[str, Any]]]): LLM configuration. (Currently unused but kept for potential future integration).
            tavily_api_key (Optional[str]): The API key for the Tavily Search API. If not provided,
                it attempts to read from the `TAVILY_API_KEY` environment variable.

        Raises:
            ValueError: If `tavily_api_key` is not provided either directly or via the environment variable.
        """
        self.tavily_api_key = tavily_api_key or os.getenv("TAVILY_API_KEY")

        if self.tavily_api_key is None:
            raise ValueError("tavily_api_key must be provided either as an argument or via TAVILY_API_KEY env var")

        def tavily_search(
            query: Annotated[str, "The search query."],
            tavily_api_key: Annotated[str | None, Depends(on(self.tavily_api_key))],
            search_depth: Annotated[str | None, "Either 'advanced' or 'basic'"] = "basic",
            include_answer: Annotated[str | None, "Either 'advanced' or 'basic'"] = "basic",
            include_raw_content: Annotated[bool | None, "Include the raw contents"] = False,
            include_domains: Annotated[list[str] | None, "Specific web domains to search"] = [],
            num_results: Annotated[int, "The number of results to return."] = 5,
        ) -> list[dict[str, Any]]:
            """Performs a search using the Tavily API and returns formatted results.

            Args:
                query: The search query string.
                tavily_api_key: The API key for Tavily (injected dependency).
                search_depth: The depth of the search ('basic' or 'advanced'). Defaults to "basic".
                include_answer: Whether to include an AI-generated answer ('basic' or 'advanced'). Defaults to "basic".
                include_raw_content: Whether to include raw content in the results. Defaults to False.
                include_domains: A list of domains to include in the search. Defaults to [].
                num_results: The maximum number of results to return. Defaults to 5.

            Returns:
                A list of dictionaries, each containing 'title', 'link', and 'snippet' of a search result.

            Raises:
                ValueError: If the Tavily API key is not available.
            """
            if tavily_api_key is None:
                raise ValueError("Tavily API key is missing.")
            return _tavily_search(
                query=query,
                tavily_api_key=tavily_api_key,
                search_depth=search_depth or "basic",
                include_answer=include_answer or "basic",
                include_raw_content=include_raw_content or False,
                include_domains=include_domains or [],
                num_results=num_results,
            )

        super().__init__(
            name="tavily_search",
            description="Use the Tavily Search API to perform a search.",
            func_or_tool=tavily_search,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .python_code_execution import PythonCodeExecutionTool

__all__ = ["PythonCodeExecutionTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import os
import tempfile
from typing import Annotated, Any

from pydantic import BaseModel, Field

from ....doc_utils import export_module
from ....environments import WorkingDirectory
from ....environments.python_environment import PythonEnvironment
from ... import Tool

__all__ = ["PythonCodeExecutionTool"]


@export_module("autogen.tools.experimental")
class PythonCodeExecutionTool(Tool):
    """Executes Python code in a given environment and returns the result."""

    def __init__(
        self,
        *,
        timeout: int = 30,
        working_directory: WorkingDirectory | None = None,
        python_environment: PythonEnvironment | None = None,
    ) -> None:
        """Initialize the PythonCodeExecutionTool.

        **CAUTION**: If provided a local environment, this tool will execute code in your local environment, which can be dangerous if the code is untrusted.

        Args:
            timeout: Maximum execution time allowed in seconds, will raise a TimeoutError exception if exceeded.
            working_directory: Optional WorkingDirectory context manager to use.
            python_environment: Optional PythonEnvironment to use. If None, will auto-detect or create based on other parameters.
        """
        # Store configuration parameters
        self.timeout = timeout
        self.working_directory = WorkingDirectory.get_current_working_directory(working_directory)
        tool_python_environment = PythonEnvironment.get_current_python_environment(python_environment)

        assert self.working_directory, "No Working directory found"
        assert tool_python_environment, "No Python environment found"

        self.python_environment = tool_python_environment

        # Pydantic model to contain the code and list of libraries to execute
        class CodeExecutionRequest(BaseModel):
            code: Annotated[str, Field(description="Python code to execute")]
            libraries: Annotated[list[str], Field(description="List of libraries to install before execution")]

        # The tool function, this is what goes to the LLM
        async def execute_python_code(
            code_execution_request: Annotated[CodeExecutionRequest, "Python code and the libraries required"],
        ) -> dict[str, Any]:
            """Executes Python code in the attached environment and returns the result.

            Args:
                code_execution_request (CodeExecutionRequest): The Python code and libraries to execute
            """
            code = code_execution_request.code

            # NOTE: Libraries are not installed (something to consider for future versions)

            # Prepare a script file path
            script_dir = self._get_script_directory()
            script_path = os.path.join(script_dir, "script.py")

            # Execute the code
            return await self.python_environment.execute_code(code=code, script_path=script_path, timeout=self.timeout)

        super().__init__(
            name="python_execute_code",
            description="Executes Python code and returns the result.",
            func_or_tool=execute_python_code,
        )

    def _get_script_directory(self) -> str:
        """Get the directory to use for scripts."""
        if self.working_directory and hasattr(self.working_directory, "path") and self.working_directory.path:
            path = self.working_directory.path
            os.makedirs(path, exist_ok=True)
            return path
        return tempfile.mkdtemp(prefix="ag2_script_dir_")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING

from ..doc_utils import export_module
from .tool import Tool

if TYPE_CHECKING:
    from ..agentchat.conversable_agent import ConversableAgent

__all__ = ["Toolkit"]


@export_module("autogen.tools")
class Toolkit:
    """A class representing a set of tools that can be used by an agent for various tasks."""

    def __init__(self, tools: list[Tool]) -> None:
        """Create a new Toolkit object.

        Args:
            tools (list[Tool]): The list of tools in the
        """
        self.toolkit = {tool.name: tool for tool in tools}

    @property
    def tools(self) -> list[Tool]:
        """Get the list of tools in the set."""
        return list(self.toolkit.values())

    def register_for_llm(self, agent: "ConversableAgent") -> None:
        """Register the tools in the set with an LLM agent.

        Args:
            agent (ConversableAgent): The LLM agent to register the tools with.
        """
        for tool in self.toolkit.values():
            tool.register_for_llm(agent)

    def register_for_execution(self, agent: "ConversableAgent") -> None:
        """Register the tools in the set with an agent for

        Args:
            agent (ConversableAgent): The agent to register the tools with.
        """
        for tool in self.toolkit.values():
            tool.register_for_execution(agent)

    def get_tool(self, tool_name: str) -> Tool:
        """Get a tool from the set by name.

        Args:
            tool_name (str): The name of the tool to get.

        Returns:
            Tool: The tool with the given name.
        """
        if tool_name in self.toolkit:
            return self.toolkit[tool_name]

        raise ValueError(f"Tool '{tool_name}' not found in Toolkit.")

    def set_tool(self, tool: Tool) -> None:
        """Set a tool in the set.

        Args:
            tool (Tool): The tool to set.
        """
        self.toolkit[tool.name] = tool

    def remove_tool(self, tool_name: str) -> None:
        """Remove a tool from the set by name.

        Args:
            tool_name (str): The name of the tool to remove.
        """
        if tool_name in self.toolkit:
            del self.toolkit[tool_name]
        else:
            raise ValueError(f"Tool '{tool_name}' not found in Toolkit.")

    def __len__(self) -> int:
        """Get the number of tools in the map."""
        return len(self.toolkit)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .dependency_injection import BaseContext, ChatContext, Depends
from .function_utils import get_function_schema, load_basemodels_if_needed, serialize_to_str
from .tool import Tool, tool
from .toolkit import Toolkit

__all__ = [
    "BaseContext",
    "ChatContext",
    "Depends",
    "Tool",
    "Toolkit",
    "get_function_schema",
    "load_basemodels_if_needed",
    "serialize_to_str",
    "tool",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .time import TimeTool

__all__ = [
    "TimeTool",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from datetime import datetime
from typing import Annotated

from autogen.tools import Tool

from ....doc_utils import export_module

__all__ = ["TimeTool"]


@export_module("autogen.tools.contrib")  # API Reference: autogen > tools > contrib > TimeAgent
class TimeTool(Tool):
    """Outputs the current date and time of the computer."""

    def __init__(
        self,
        *,
        date_time_format: str = "%Y-%m-%d %H:%M:%S",  # This is a parameter that is unique to this tool
    ):
        """Get the date and time of the computer.

        Args:
            date_time_format (str, optional): The format of the date and time. Defaults to "%Y-%m-%d %H:%M:%S".
        """
        self._date_time_format = date_time_format

        async def get_date_and_time(
            date_time_format: Annotated[str, "date/time Python format"] = self._date_time_format,
        ) -> str:
            return datetime.now().strftime(date_time_format)

        super().__init__(
            name="date_time",
            description="Get the current computer's date and time.",
            func_or_tool=get_date_and_time,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .time import TimeTool

__all__ = ["TimeTool"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import functools
import inspect
import json
from collections.abc import Callable
from logging import getLogger
from typing import Annotated, Any, ForwardRef, Literal, TypeVar, get_args, get_origin

from packaging.version import parse
from pydantic import BaseModel, Field, TypeAdapter
from pydantic import __version__ as pydantic_version
from pydantic.json_schema import JsonSchemaValue

from ..doc_utils import export_module
from ..fast_depends.utils import is_coroutine_callable
from .dependency_injection import Field as AG2Field

if parse(pydantic_version) < parse("2.10.2"):
    from pydantic._internal._typing_extra import eval_type_lenient as try_eval_type
else:
    from pydantic._internal._typing_extra import try_eval_type


__all__ = ["get_function_schema", "load_basemodels_if_needed", "serialize_to_str"]

logger = getLogger(__name__)

T = TypeVar("T")


def get_typed_annotation(annotation: Any, globalns: dict[str, Any]) -> Any:
    """Get the type annotation of a parameter.

    Args:
        annotation: The annotation of the parameter
        globalns: The global namespace of the function

    Returns:
        The type annotation of the parameter
    """
    if isinstance(annotation, AG2Field):
        annotation = annotation.description
    if isinstance(annotation, str):
        annotation = ForwardRef(annotation)
        annotation, _ = try_eval_type(annotation, globalns, globalns)
    return annotation


def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:
    """Get the signature of a function with type annotations.

    Args:
        call: The function to get the signature for

    Returns:
        The signature of the function with type annotations
    """
    signature = inspect.signature(call)
    globalns = getattr(call, "__globals__", {})
    typed_params = [
        inspect.Parameter(
            name=param.name,
            kind=param.kind,
            default=param.default,
            annotation=get_typed_annotation(param.annotation, globalns),
        )
        for param in signature.parameters.values()
    ]
    typed_signature = inspect.Signature(typed_params)
    return typed_signature


def get_typed_return_annotation(call: Callable[..., Any]) -> Any:
    """Get the return annotation of a function.

    Args:
        call: The function to get the return annotation for

    Returns:
        The return annotation of the function
    """
    signature = inspect.signature(call)
    annotation = signature.return_annotation

    if annotation is inspect.Signature.empty:
        return None

    globalns = getattr(call, "__globals__", {})
    return get_typed_annotation(annotation, globalns)


def get_param_annotations(typed_signature: inspect.Signature) -> dict[str, Annotated[type[Any], str] | type[Any]]:
    """Get the type annotations of the parameters of a function

    Args:
        typed_signature: The signature of the function with type annotations

    Returns:
        A dictionary of the type annotations of the parameters of the function
    """
    return {
        k: v.annotation for k, v in typed_signature.parameters.items() if v.annotation is not inspect.Signature.empty
    }


class Parameters(BaseModel):
    """Parameters of a function as defined by the OpenAI API"""

    type: Literal["object"] = "object"
    properties: dict[str, JsonSchemaValue]
    required: list[str]


class Function(BaseModel):
    """A function as defined by the OpenAI API"""

    description: Annotated[str, Field(description="Description of the function")]
    name: Annotated[str, Field(description="Name of the function")]
    parameters: Annotated[Parameters, Field(description="Parameters of the function")]


class ToolFunction(BaseModel):
    """A function under tool as defined by the OpenAI API."""

    type: Literal["function"] = "function"
    function: Annotated[Function, Field(description="Function under tool")]


def get_parameter_json_schema(k: str, v: Any, default_values: dict[str, Any]) -> JsonSchemaValue:
    """Get a JSON schema for a parameter as defined by the OpenAI API

    Args:
        k: The name of the parameter
        v: The type of the parameter
        default_values: The default values of the parameters of the function

    Returns:
        A Pydanitc model for the parameter
    """

    def type2description(k: str, v: Annotated[type[Any], str] | type[Any]) -> str:
        if not hasattr(v, "__metadata__"):
            return k

        # handles Annotated
        retval = v.__metadata__[0]
        if isinstance(retval, AG2Field):
            return retval.description  # type: ignore[return-value]
        else:
            raise ValueError(f"Invalid {retval} for parameter {k}, should be a DescriptionField, got {type(retval)}")

    schema = TypeAdapter(v).json_schema()
    if k in default_values:
        dv = default_values[k]
        schema["default"] = dv

    schema["description"] = type2description(k, v)

    return schema


def get_required_params(typed_signature: inspect.Signature) -> list[str]:
    """Get the required parameters of a function

    Args:
        typed_signature: The signature of the function as returned by inspect.signature

    Returns:
        A list of the required parameters of the function
    """
    return [k for k, v in typed_signature.parameters.items() if v.default == inspect.Signature.empty]


def get_default_values(typed_signature: inspect.Signature) -> dict[str, Any]:
    """Get default values of parameters of a function

    Args:
        typed_signature: The signature of the function as returned by inspect.signature

    Returns:
        A dictionary of the default values of the parameters of the function
    """
    return {k: v.default for k, v in typed_signature.parameters.items() if v.default != inspect.Signature.empty}


def get_parameters(
    required: list[str],
    param_annotations: dict[str, Annotated[type[Any], str] | type[Any]],
    default_values: dict[str, Any],
) -> Parameters:
    """Get the parameters of a function as defined by the OpenAI API

    Args:
        required: The required parameters of the function
        param_annotations: The type annotations of the parameters of the function
        default_values: The default values of the parameters of the function

    Returns:
        A Pydantic model for the parameters of the function
    """
    return Parameters(
        properties={
            k: get_parameter_json_schema(k, v, default_values)
            for k, v in param_annotations.items()
            if v is not inspect.Signature.empty
        },
        required=required,
    )


def get_missing_annotations(typed_signature: inspect.Signature, required: list[str]) -> tuple[set[str], set[str]]:
    """Get the missing annotations of a function

    Ignores the parameters with default values as they are not required to be annotated, but logs a warning.

    Args:
        typed_signature: The signature of the function with type annotations
        required: The required parameters of the function

    Returns:
        A set of the missing annotations of the function
    """
    all_missing = {k for k, v in typed_signature.parameters.items() if v.annotation is inspect.Signature.empty}
    missing = all_missing.intersection(set(required))
    unannotated_with_default = all_missing.difference(missing)
    return missing, unannotated_with_default


@export_module("autogen.tools")
def get_function_schema(f: Callable[..., Any], *, name: str | None = None, description: str) -> dict[str, Any]:
    """Get a JSON schema for a function as defined by the OpenAI API

    Args:
        f: The function to get the JSON schema for
        name: The name of the function
        description: The description of the function

    Returns:
        A JSON schema for the function

    Raises:
        TypeError: If the function is not annotated

    Examples:
    ```python
    def f(a: Annotated[str, "Parameter a"], b: int = 2, c: Annotated[float, "Parameter c"] = 0.1) -> None:
        pass


    get_function_schema(f, description="function f")

    #   {'type': 'function',
    #    'function': {'description': 'function f',
    #        'name': 'f',
    #        'parameters': {'type': 'object',
    #           'properties': {'a': {'type': 'str', 'description': 'Parameter a'},
    #               'b': {'type': 'int', 'description': 'b'},
    #               'c': {'type': 'float', 'description': 'Parameter c'}},
    #           'required': ['a']}}}
    ```

    """
    typed_signature = get_typed_signature(f)
    required = get_required_params(typed_signature)
    default_values = get_default_values(typed_signature)
    param_annotations = get_param_annotations(typed_signature)
    return_annotation = get_typed_return_annotation(f)
    missing, unannotated_with_default = get_missing_annotations(typed_signature, required)

    if return_annotation is None:
        logger.warning(
            f"The return type of the function '{f.__name__}' is not annotated. Although annotating it is "
            + "optional, the function should return either a string, a subclass of 'pydantic.BaseModel'."
        )

    if unannotated_with_default != set():
        unannotated_with_default_s = [f"'{k}'" for k in sorted(unannotated_with_default)]
        logger.warning(
            f"The following parameters of the function '{f.__name__}' with default values are not annotated: "
            + f"{', '.join(unannotated_with_default_s)}."
        )

    if missing != set():
        missing_s = [f"'{k}'" for k in sorted(missing)]
        raise TypeError(
            f"All parameters of the function '{f.__name__}' without default values must be annotated. "
            + f"The annotations are missing for the following parameters: {', '.join(missing_s)}"
        )

    fname = name if name else f.__name__

    parameters = get_parameters(required, param_annotations, default_values=default_values)

    function = ToolFunction(
        function=Function(
            description=description,
            name=fname,
            parameters=parameters,
        )
    )

    return function.model_dump()


def get_load_param_if_needed_function(t: Any) -> Callable[[dict[str, Any], type[BaseModel]], BaseModel] | None:
    """Get a function to load a parameter if it is a Pydantic model

    Args:
        t: The type annotation of the parameter

    Returns:
        A function to load the parameter if it is a Pydantic model, otherwise None

    """
    origin = get_origin(t)

    if origin is Annotated:
        args = get_args(t)
        if args:
            return get_load_param_if_needed_function(args[0])
        else:
            # Invalid Annotated usage
            return None

    # Handle generic types (list[str], dict[str,Any], Union[...], etc.) or where t is not a type at all
    # This means it's not a BaseModel subclass
    if origin is not None or not isinstance(t, type):
        return None

    def load_base_model(v: dict[str, Any], model_type: type[BaseModel]) -> BaseModel:
        return model_type(**v)

    # Check if it's a class and a subclass of BaseModel
    if issubclass(t, BaseModel):
        return load_base_model
    else:
        return None


@export_module("autogen.tools")
def load_basemodels_if_needed(func: Callable[..., Any]) -> Callable[..., Any]:
    """A decorator to load the parameters of a function if they are Pydantic models

    Args:
        func: The function with annotated parameters

    Returns:
        A function that loads the parameters before calling the original function

    """
    # get the type annotations of the parameters
    typed_signature = get_typed_signature(func)
    param_annotations = get_param_annotations(typed_signature)

    # get functions for loading BaseModels when needed based on the type annotations
    kwargs_mapping_with_nones = {k: get_load_param_if_needed_function(t) for k, t in param_annotations.items()}

    # remove the None values
    kwargs_mapping = {k: f for k, f in kwargs_mapping_with_nones.items() if f is not None}

    # a function that loads the parameters before calling the original function
    @functools.wraps(func)
    def _load_parameters_if_needed(*args: Any, **kwargs: Any) -> Any:
        # load the BaseModels if needed
        for k, f in kwargs_mapping.items():
            kwargs[k] = f(kwargs[k], param_annotations[k])

        # call the original function
        return func(*args, **kwargs)

    @functools.wraps(func)
    async def _a_load_parameters_if_needed(*args: Any, **kwargs: Any) -> Any:
        # load the BaseModels if needed
        for k, f in kwargs_mapping.items():
            kwargs[k] = f(kwargs[k], param_annotations[k])

        # call the original function
        return await func(*args, **kwargs)

    if is_coroutine_callable(func):
        return _a_load_parameters_if_needed
    else:
        return _load_parameters_if_needed


class _SerializableResult(BaseModel):
    result: Any


@export_module("autogen.tools")
def serialize_to_str(x: Any) -> str:
    if isinstance(x, str):
        return x
    if isinstance(x, BaseModel):
        return x.model_dump_json()

    retval_model = _SerializableResult(result=x)
    try:
        return str(retval_model.model_dump()["result"])
    except Exception:
        pass

    # try json.dumps() and then just return str(x) if that fails too
    try:
        return json.dumps(x, ensure_ascii=False)
    except Exception:
        return str(x)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import inspect
from collections.abc import Callable
from typing import TYPE_CHECKING, Any, Union

from ..doc_utils import export_module
from ..tools.function_utils import get_function_schema
from .dependency_injection import ChatContext, get_context_params, inject_params

if TYPE_CHECKING:
    from ..agentchat.conversable_agent import ConversableAgent

__all__ = ["Tool", "tool"]


@export_module("autogen.tools")
class Tool:
    """A class representing a Tool that can be used by an agent for various tasks.

    This class encapsulates a tool with a name, description, and an executable function.
    The tool can be registered with a ConversableAgent for use either with an LLM or for direct execution.

    Attributes:
        name (str): The name of the tool.
        description (str): The description of the tool.
        func_or_tool (Union[Tool, Callable[..., Any]]): The function or Tool instance to create a Tool from.
        parameters_json_schema (Optional[ict[str, Any]]): A schema describing the parameters that the function accepts. If None, the schema will be generated from the function signature.
    """

    def __init__(
        self,
        *,
        name: str | None = None,
        description: str | None = None,
        func_or_tool: Union["Tool", Callable[..., Any]],
        parameters_json_schema: dict[str, Any] | None = None,
    ) -> None:
        """Create a new Tool object.

        Args:
            name (str): The name of the tool.
            description (str): The description of the tool.
            func_or_tool (Union[Tool, Callable[..., Any]]): The function or Tool instance to create a Tool from.
            parameters_json_schema (Optional[dict[str, Any]]): A schema describing the parameters that the function accepts. If None, the schema will be generated from the function signature.
        """
        if isinstance(func_or_tool, Tool):
            self._name: str = name or func_or_tool.name
            self._description: str = description or func_or_tool.description
            self._func: Callable[..., Any] = func_or_tool.func
            self._chat_context_param_names: list[str] = func_or_tool._chat_context_param_names
        elif inspect.isfunction(func_or_tool) or inspect.ismethod(func_or_tool):
            self._chat_context_param_names = get_context_params(func_or_tool, subclass=ChatContext)
            self._func = inject_params(func_or_tool)
            self._name = name or func_or_tool.__name__
            self._description = description or func_or_tool.__doc__ or ""
        else:
            raise ValueError(
                f"Parameter 'func_or_tool' must be a function, method or a Tool instance, it is '{type(func_or_tool)}' instead."
            )

        self._func_schema = (
            {
                "type": "function",
                "function": {
                    "name": name,
                    "description": description,
                    "parameters": parameters_json_schema,
                },
            }
            if parameters_json_schema
            else None
        )

    @property
    def name(self) -> str:
        return self._name

    @property
    def description(self) -> str:
        return self._description

    @property
    def func(self) -> Callable[..., Any]:
        return self._func

    def register_for_llm(self, agent: "ConversableAgent") -> None:
        """Registers the tool for use with a ConversableAgent's language model (LLM).

        This method registers the tool so that it can be invoked by the agent during
        interactions with the language model.

        Args:
            agent (ConversableAgent): The agent to which the tool will be registered.
        """
        if self._func_schema:
            agent.update_tool_signature(self._func_schema, is_remove=False)
        else:
            agent.register_for_llm()(self)

    def register_for_execution(self, agent: "ConversableAgent") -> None:
        """Registers the tool for direct execution by a ConversableAgent.

        This method registers the tool so that it can be executed by the agent,
        typically outside of the context of an LLM interaction.

        Args:
            agent (ConversableAgent): The agent to which the tool will be registered.
        """
        agent.register_for_execution()(self)

    def register_tool(self, agent: "ConversableAgent") -> None:
        """Register a tool to be both proposed and executed by an agent.

        Equivalent to calling both `register_for_llm` and `register_for_execution` with the same agent.

        Note: This will not make the agent recommend and execute the call in the one step. If the agent
        recommends the tool, it will need to be the next agent to speak in order to execute the tool.

        Args:
            agent (ConversableAgent): The agent to which the tool will be registered.
        """
        self.register_for_llm(agent)
        self.register_for_execution(agent)

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        """Execute the tool by calling its underlying function with the provided arguments.

        Args:
            *args: Positional arguments to pass to the tool
            **kwargs: Keyword arguments to pass to the tool

        Returns:
            The result of executing the tool's function.
        """
        return self._func(*args, **kwargs)

    @property
    def tool_schema(self) -> dict[str, Any]:
        """Get the schema for the tool.

        This is the preferred way of handling function calls with OpeaAI and compatible frameworks.

        """
        return get_function_schema(self.func, name=self.name, description=self.description)

    @property
    def function_schema(self) -> dict[str, Any]:
        """Get the schema for the function.

        This is the old way of handling function calls with OpenAI and compatible frameworks.
        It is provided for backward compatibility.

        """
        schema = get_function_schema(self.func, name=self.name, description=self.description)
        return schema["function"]  # type: ignore[no-any-return]

    @property
    def realtime_tool_schema(self) -> dict[str, Any]:
        """Get the schema for the tool.

        This is the preferred way of handling function calls with OpeaAI and compatible frameworks.

        """
        schema = get_function_schema(self.func, name=self.name, description=self.description)
        schema = {"type": schema["type"], **schema["function"]}

        return schema


@export_module("autogen.tools")
def tool(name: str | None = None, description: str | None = None) -> Callable[[Callable[..., Any]], Tool]:
    """Decorator to create a Tool from a function.

    Args:
        name (str): The name of the tool.
        description (str): The description of the tool.

    Returns:
        Callable[[Callable[..., Any]], Tool]: A decorator that creates a Tool from a function.
    """

    def decorator(func: Callable[..., Any]) -> Tool:
        return Tool(name=name, description=description, func_or_tool=func)

    return decorator
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Callable
from typing import Any, Literal
from uuid import UUID

from pydantic import BaseModel

from ..events import deprecated_by
from ..events.client_events import StreamEvent, UsageSummaryEvent
from .base_message import BaseMessage, wrap_message

__all__ = ["UsageSummaryMessage"]


class ModelUsageSummary(BaseModel):
    """Model usage summary."""

    model: str
    """Model name."""
    completion_tokens: int
    """Number of tokens used for completion."""
    cost: float
    """Cost of the completion."""
    prompt_tokens: int
    """Number of tokens used for prompt."""
    total_tokens: int
    """Total number of tokens used."""


class ActualUsageSummary(BaseModel):
    """Actual usage summary."""

    usages: list[ModelUsageSummary] | None = None
    """List of model usage summaries."""
    total_cost: float | None = None
    """Total cost."""


class TotalUsageSummary(BaseModel):
    """Total usage summary."""

    usages: list[ModelUsageSummary] | None = None
    """List of model usage summaries."""
    total_cost: float | None = None
    """Total cost."""


Mode = Literal["both", "total", "actual"]


def _change_usage_summary_format(
    actual_usage_summary: dict[str, Any] | None = None, total_usage_summary: dict[str, Any] | None = None
) -> dict[str, dict[str, Any]]:
    summary: dict[str, Any] = {}

    for usage_type, usage_summary in {"actual": actual_usage_summary, "total": total_usage_summary}.items():
        if usage_summary is None:
            summary[usage_type] = {"usages": None, "total_cost": None}
            continue

        usage_summary_altered_format: dict[str, list[dict[str, Any]]] = {"usages": []}
        for k, v in usage_summary.items():
            if isinstance(k, str) and isinstance(v, dict):
                current_usage = dict(v.items())
                current_usage["model"] = k
                usage_summary_altered_format["usages"].append(current_usage)
            else:
                usage_summary_altered_format[k] = v
        summary[usage_type] = usage_summary_altered_format

    return summary


@deprecated_by(UsageSummaryEvent)
@wrap_message
class UsageSummaryMessage(BaseMessage):
    """Usage summary message."""

    actual: ActualUsageSummary
    """Actual usage summary."""
    total: TotalUsageSummary
    """Total usage summary."""
    mode: Mode
    """Mode to display the usage summary."""

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        actual_usage_summary: dict[str, Any] | None = None,
        total_usage_summary: dict[str, Any] | None = None,
        mode: Mode = "both",
    ):
        # print(f"{actual_usage_summary=}")
        # print(f"{total_usage_summary=}")

        summary_dict = _change_usage_summary_format(actual_usage_summary, total_usage_summary)

        super().__init__(uuid=uuid, **summary_dict, mode=mode)

    def _print_usage(
        self,
        usage_summary: ActualUsageSummary | TotalUsageSummary,
        usage_type: str = "total",
        f: Callable[..., Any] | None = None,
    ) -> None:
        f = f or print
        word_from_type = "including" if usage_type == "total" else "excluding"
        if usage_summary.usages is None or len(usage_summary.usages) == 0:
            f("No actual cost incurred (all completions are using cache).", flush=True)
            return

        f(f"Usage summary {word_from_type} cached usage: ", flush=True)
        f(f"Total cost: {round(usage_summary.total_cost, 5)}", flush=True)  # type: ignore [arg-type]

        for usage in usage_summary.usages:
            f(
                f"* Model '{usage.model}': cost: {round(usage.cost, 5)}, prompt_tokens: {usage.prompt_tokens}, completion_tokens: {usage.completion_tokens}, total_tokens: {usage.total_tokens}",
                flush=True,
            )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        if self.total.usages is None:
            f('No usage summary. Please call "create" first.', flush=True)
            return

        f("-" * 100, flush=True)
        if self.mode == "both":
            self._print_usage(self.actual, "actual", f)
            f()
            if self.total.model_dump_json() != self.actual.model_dump_json():
                self._print_usage(self.total, "total", f)
            else:
                f(
                    "All completions are non-cached: the total cost with cached completions is the same as actual cost.",
                    flush=True,
                )
        elif self.mode == "total":
            self._print_usage(self.total, "total", f)
        elif self.mode == "actual":
            self._print_usage(self.actual, "actual", f)
        else:
            raise ValueError(f'Invalid mode: {self.mode}, choose from "actual", "total", ["actual", "total"]')
        f("-" * 100, flush=True)


@deprecated_by(StreamEvent)
@wrap_message
class StreamMessage(BaseMessage):
    """Stream message."""

    content: str
    """Content of the message."""

    def __init__(self, *, uuid: UUID | None = None, content: str) -> None:
        super().__init__(uuid=uuid, content=content)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        # Set the terminal text color to green
        f("\033[32m", end="")

        f(self.content, end="", flush=True)

        # Reset the terminal text color
        f("\033[0m\n")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .base_message import BaseMessage, get_annotated_type_for_message_classes, wrap_message

__all__ = ["BaseMessage", "get_annotated_type_for_message_classes", "wrap_message"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from abc import ABC
from collections.abc import Callable
from copy import deepcopy
from typing import TYPE_CHECKING, Any, Literal, Optional, Union
from uuid import UUID

from pydantic import BaseModel, field_validator
from termcolor import colored

from ..agentchat.agent import LLMMessageType
from ..code_utils import content_str
from ..events import deprecated_by
from ..events.agent_events import (
    ClearAgentsHistoryEvent,
    ClearConversableAgentHistoryEvent,
    ClearConversableAgentHistoryWarningEvent,
    ConversableAgentUsageSummaryEvent,
    ConversableAgentUsageSummaryNoCostIncurredEvent,
    ExecuteCodeBlockEvent,
    ExecuteFunctionEvent,
    ExecutedFunctionEvent,
    FunctionCallEvent,
    FunctionResponseEvent,
    GenerateCodeExecutionReplyEvent,
    GroupChatResumeEvent,
    GroupChatRunChatEvent,
    PostCarryoverProcessingEvent,
    SelectSpeakerEvent,
    SelectSpeakerInvalidInputEvent,
    SelectSpeakerTryCountExceededEvent,
    SpeakerAttemptFailedMultipleAgentsEvent,
    SpeakerAttemptFailedNoAgentsEvent,
    SpeakerAttemptSuccessfulEvent,
    TerminationAndHumanReplyNoInputEvent,
    TerminationEvent,
    TextEvent,
    ToolCallEvent,
    ToolResponseEvent,
    UsingAutoReplyEvent,
)
from ..import_utils import optional_import_block, require_optional_import
from ..oai.client import OpenAIWrapper
from .base_message import BaseMessage, wrap_message

with optional_import_block() as result:
    from PIL.Image import Image

IS_PIL_AVAILABLE = result.is_successful

if TYPE_CHECKING:
    from ..agentchat.agent import Agent
    from ..coding.base import CodeBlock


__all__ = [
    "ClearAgentsHistoryMessage",
    "ClearConversableAgentHistoryMessage",
    "ConversableAgentUsageSummaryMessage",
    "ConversableAgentUsageSummaryNoCostIncurredMessage",
    "ExecuteCodeBlockMessage",
    "ExecuteFunctionMessage",
    "FunctionCallMessage",
    "FunctionResponseMessage",
    "GenerateCodeExecutionReplyMessage",
    "GroupChatResumeMessage",
    "GroupChatRunChatMessage",
    "PostCarryoverProcessingMessage",
    "SelectSpeakerMessage",
    "SpeakerAttemptFailedMultipleAgentsMessage",
    "SpeakerAttemptFailedNoAgentsMessage",
    "SpeakerAttemptSuccessfulMessage",
    "TerminationAndHumanReplyNoInputMessage",
    "TerminationMessage",
    "TextMessage",
    "ToolCallMessage",
    "ToolResponseMessage",
]

MessageRole = Literal["assistant", "function", "tool"]


class BasePrintReceivedMessage(BaseMessage, ABC):
    content: str | int | float | bool
    sender_name: str
    recipient_name: str

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        f(f"{colored(self.sender_name, 'yellow')} (to {self.recipient_name}):\n", flush=True)


@deprecated_by(FunctionResponseEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"})
@wrap_message
class FunctionResponseMessage(BasePrintReceivedMessage):
    name: str | None = None
    role: MessageRole = "function"
    content: str | int | float | bool

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        id = self.name or "No id found"
        func_print = f"***** Response from calling {self.role} ({id}) *****"
        f(colored(func_print, "green"), flush=True)
        f(self.content, flush=True)
        f(colored("*" * len(func_print), "green"), flush=True)

        f("\n", "-" * 80, flush=True, sep="")


class ToolResponse(BaseModel):
    tool_call_id: str | None = None
    role: MessageRole = "tool"
    content: str | int | float | bool

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        id = self.tool_call_id or "No id found"
        tool_print = f"***** Response from calling {self.role} ({id}) *****"
        f(colored(tool_print, "green"), flush=True)
        f(self.content, flush=True)
        f(colored("*" * len(tool_print), "green"), flush=True)


@deprecated_by(ToolResponseEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"})
@wrap_message
class ToolResponseMessage(BasePrintReceivedMessage):
    role: MessageRole = "tool"
    tool_responses: list[ToolResponse]
    content: str | int | float | bool

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        for tool_response in self.tool_responses:
            tool_response.print(f)
            f("\n", "-" * 80, flush=True, sep="")


class FunctionCall(BaseModel):
    name: str | None = None
    arguments: str | None = None

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        name = self.name or "(No function name found)"
        arguments = self.arguments or "(No arguments found)"

        func_print = f"***** Suggested function call: {name} *****"
        f(colored(func_print, "green"), flush=True)
        f(
            "Arguments: \n",
            arguments,
            flush=True,
            sep="",
        )
        f(colored("*" * len(func_print), "green"), flush=True)


@deprecated_by(FunctionCallEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"})
@wrap_message
class FunctionCallMessage(BasePrintReceivedMessage):
    content: str | int | float | bool | None = None  # type: ignore [assignment]
    function_call: FunctionCall

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        if self.content is not None:
            f(self.content, flush=True)

        self.function_call.print(f)

        f("\n", "-" * 80, flush=True, sep="")


class ToolCall(BaseModel):
    id: str | None = None
    function: FunctionCall
    type: str

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        id = self.id or "No tool call id found"

        name = self.function.name or "(No function name found)"
        arguments = self.function.arguments or "(No arguments found)"

        func_print = f"***** Suggested tool call ({id}): {name} *****"
        f(colored(func_print, "green"), flush=True)
        f(
            "Arguments: \n",
            arguments,
            flush=True,
            sep="",
        )
        f(colored("*" * len(func_print), "green"), flush=True)


@deprecated_by(ToolCallEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"})
@wrap_message
class ToolCallMessage(BasePrintReceivedMessage):
    content: str | int | float | bool | None = None  # type: ignore [assignment]
    refusal: str | None = None
    role: MessageRole | None = None
    audio: str | None = None
    function_call: FunctionCall | None = None
    tool_calls: list[ToolCall]

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        if self.content is not None:
            f(self.content, flush=True)

        for tool_call in self.tool_calls:
            tool_call.print(f)

        f("\n", "-" * 80, flush=True, sep="")


@deprecated_by(TextEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"})
@wrap_message
class TextMessage(BasePrintReceivedMessage):
    content: str | int | float | bool | list[dict[str, str | dict[str, Any]]] | None = None  # type: ignore [assignment]

    @classmethod
    @require_optional_import("PIL", "unknown")
    def _replace_pil_image_with_placeholder(cls, image_url: dict[str, Any]) -> None:
        if "url" in image_url and isinstance(image_url["url"], Image):
            image_url["url"] = "<image>"

    @field_validator("content", mode="before")
    @classmethod
    def validate_and_encode_content(
        cls, content: str | int | float | bool | list[dict[str, str | dict[str, Any]]] | None
    ) -> str | int | float | bool | list[dict[str, str | dict[str, Any]]] | None:
        if not IS_PIL_AVAILABLE:
            return content

        if not isinstance(content, list):
            return content

        for item in content:
            if isinstance(item, dict) and "image_url" in item and isinstance(item["image_url"], dict):
                cls._replace_pil_image_with_placeholder(item["image_url"])

        return content

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        if self.content is not None:
            f(content_str(self.content), flush=True)  # type: ignore [arg-type]

        f("\n", "-" * 80, flush=True, sep="")


def create_received_message_model(
    *, uuid: UUID | None = None, message: dict[str, Any], sender: "Agent", recipient: "Agent"
) -> Union[FunctionResponseMessage, ToolResponseMessage, FunctionCallMessage, ToolCallMessage, TextMessage]:  # noqa: UP007
    role = message.get("role")
    if role == "function":
        return FunctionResponseMessage(**message, sender_name=sender.name, recipient_name=recipient.name, uuid=uuid)
    if role == "tool":
        return ToolResponseMessage(**message, sender_name=sender.name, recipient_name=recipient.name, uuid=uuid)

    # Role is neither function nor tool

    if message.get("function_call"):
        return FunctionCallMessage(
            **message,
            sender_name=sender.name,
            recipient_name=recipient.name,
            uuid=uuid,
        )

    if message.get("tool_calls"):
        return ToolCallMessage(
            **message,
            sender_name=sender.name,
            recipient_name=recipient.name,
            uuid=uuid,
        )

    # Now message is a simple content message
    content = message.get("content")
    allow_format_str_template = (
        recipient.llm_config.get("allow_format_str_template", False) if recipient.llm_config else False  # type: ignore [attr-defined]
    )
    if content is not None and "context" in message:
        content = OpenAIWrapper.instantiate(
            content,  # type: ignore [arg-type]
            message["context"],
            allow_format_str_template,
        )

    return TextMessage(
        content=content,
        sender_name=sender.name,
        recipient_name=recipient.name,
        uuid=uuid,
    )


@deprecated_by(PostCarryoverProcessingEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"})
@wrap_message
class PostCarryoverProcessingMessage(BaseMessage):
    carryover: str | list[str | dict[str, Any] | Any]
    message: str
    verbose: bool = False

    sender_name: str
    recipient_name: str
    summary_method: str
    summary_args: dict[str, Any] | None = None
    max_turns: int | None = None

    def __init__(self, *, uuid: UUID | None = None, chat_info: dict[str, Any]):
        carryover = chat_info.get("carryover", "")
        message = chat_info.get("message")
        verbose = chat_info.get("verbose", False)

        sender_name = chat_info["sender"].name
        recipient_name = chat_info["recipient"].name
        summary_args = chat_info.get("summary_args")
        max_turns = chat_info.get("max_turns")

        # Fix Callable in chat_info
        summary_method = chat_info.get("summary_method", "")
        if callable(summary_method):
            summary_method = summary_method.__name__

        print_message = ""
        if isinstance(message, str):
            print_message = message
        elif callable(message):
            print_message = "Callable: " + message.__name__
        elif isinstance(message, dict):
            print_message = "Dict: " + str(message)
        elif message is None:
            print_message = "None"

        super().__init__(
            uuid=uuid,
            carryover=carryover,
            message=print_message,
            verbose=verbose,
            summary_method=summary_method,
            summary_args=summary_args,
            max_turns=max_turns,
            sender_name=sender_name,
            recipient_name=recipient_name,
        )

    def _process_carryover(self) -> str:
        if not isinstance(self.carryover, list):
            return self.carryover

        print_carryover = []
        for carryover_item in self.carryover:
            if isinstance(carryover_item, str):
                print_carryover.append(carryover_item)
            elif isinstance(carryover_item, dict) and "content" in carryover_item:
                print_carryover.append(str(carryover_item["content"]))
            else:
                print_carryover.append(str(carryover_item))

        return ("\n").join(print_carryover)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        print_carryover = self._process_carryover()

        f(colored("\n" + "*" * 80, "blue"), flush=True, sep="")
        f(
            colored(
                "Starting a new chat....",
                "blue",
            ),
            flush=True,
        )
        if self.verbose:
            f(colored("Message:\n" + self.message, "blue"), flush=True)
            f(colored("Carryover:\n" + print_carryover, "blue"), flush=True)
        f(colored("\n" + "*" * 80, "blue"), flush=True, sep="")


@deprecated_by(ClearAgentsHistoryEvent, param_mapping={"nr_messages_to_preserve": "nr_events_to_preserve"})
@wrap_message
class ClearAgentsHistoryMessage(BaseMessage):
    agent_name: str | None = None
    nr_messages_to_preserve: int | None = None

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        agent: Optional["Agent"] = None,
        nr_messages_to_preserve: int | None = None,
    ):
        return super().__init__(
            uuid=uuid, agent_name=agent.name if agent else None, nr_messages_to_preserve=nr_messages_to_preserve
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        if self.agent_name:
            if self.nr_messages_to_preserve:
                f(f"Clearing history for {self.agent_name} except last {self.nr_messages_to_preserve} messages.")
            else:
                f(f"Clearing history for {self.agent_name}.")
        else:
            if self.nr_messages_to_preserve:
                f(f"Clearing history for all agents except last {self.nr_messages_to_preserve} messages.")
            else:
                f("Clearing history for all agents.")


# todo: break into multiple messages
@deprecated_by(SpeakerAttemptSuccessfulEvent)
@wrap_message
class SpeakerAttemptSuccessfulMessage(BaseMessage):
    mentions: dict[str, int]
    attempt: int
    attempts_left: int
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        mentions: dict[str, int],
        attempt: int,
        attempts_left: int,
        select_speaker_auto_verbose: bool | None = False,
    ):
        super().__init__(
            uuid=uuid,
            mentions=deepcopy(mentions),
            attempt=attempt,
            attempts_left=attempts_left,
            verbose=select_speaker_auto_verbose,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        selected_agent_name = next(iter(self.mentions))
        f(
            colored(
                f">>>>>>>> Select speaker attempt {self.attempt} of {self.attempt + self.attempts_left} successfully selected: {selected_agent_name}",
                "green",
            ),
            flush=True,
        )


@deprecated_by(SpeakerAttemptFailedMultipleAgentsEvent)
@wrap_message
class SpeakerAttemptFailedMultipleAgentsMessage(BaseMessage):
    mentions: dict[str, int]
    attempt: int
    attempts_left: int
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        mentions: dict[str, int],
        attempt: int,
        attempts_left: int,
        select_speaker_auto_verbose: bool | None = False,
    ):
        super().__init__(
            uuid=uuid,
            mentions=deepcopy(mentions),
            attempt=attempt,
            attempts_left=attempts_left,
            verbose=select_speaker_auto_verbose,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f">>>>>>>> Select speaker attempt {self.attempt} of {self.attempt + self.attempts_left} failed as it included multiple agent names.",
                "red",
            ),
            flush=True,
        )


@deprecated_by(SpeakerAttemptFailedNoAgentsEvent)
@wrap_message
class SpeakerAttemptFailedNoAgentsMessage(BaseMessage):
    mentions: dict[str, int]
    attempt: int
    attempts_left: int
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        mentions: dict[str, int],
        attempt: int,
        attempts_left: int,
        select_speaker_auto_verbose: bool | None = False,
    ):
        super().__init__(
            uuid=uuid,
            mentions=deepcopy(mentions),
            attempt=attempt,
            attempts_left=attempts_left,
            verbose=select_speaker_auto_verbose,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f">>>>>>>> Select speaker attempt #{self.attempt} failed as it did not include any agent names.",
                "red",
            ),
            flush=True,
        )


@deprecated_by(GroupChatResumeEvent, param_mapping={"messages": "events"})
@wrap_message
class GroupChatResumeMessage(BaseMessage):
    last_speaker_name: str
    messages: list[LLMMessageType]
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        last_speaker_name: str,
        messages: list["LLMMessageType"],
        silent: bool | None = False,
    ):
        super().__init__(uuid=uuid, last_speaker_name=last_speaker_name, messages=messages, verbose=not silent)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            f"Prepared group chat with {len(self.messages)} messages, the last speaker is",
            colored(self.last_speaker_name, "yellow"),
            flush=True,
        )


@deprecated_by(GroupChatRunChatEvent)
@wrap_message
class GroupChatRunChatMessage(BaseMessage):
    speaker_name: str
    verbose: bool | None = False

    def __init__(self, *, uuid: UUID | None = None, speaker: "Agent", silent: bool | None = False):
        super().__init__(uuid=uuid, speaker_name=speaker.name, verbose=not silent)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored(f"\nNext speaker: {self.speaker_name}\n", "green"), flush=True)


@deprecated_by(
    TerminationAndHumanReplyNoInputEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"}
)
@wrap_message
class TerminationAndHumanReplyNoInputMessage(BaseMessage):
    """When the human-in-the-loop is prompted but provides no input."""

    no_human_input_msg: str
    sender_name: str
    recipient_name: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        no_human_input_msg: str,
        sender: Optional["Agent"] = None,
        recipient: "Agent",
    ):
        super().__init__(
            uuid=uuid,
            no_human_input_msg=no_human_input_msg,
            sender_name=sender.name if sender else "No sender",
            recipient_name=recipient.name,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored(f"\n>>>>>>>> {self.no_human_input_msg}", "red"), flush=True)


@deprecated_by(UsingAutoReplyEvent, param_mapping={"sender_name": "sender", "recipient_name": "recipient"})
@wrap_message
class UsingAutoReplyMessage(BaseMessage):
    human_input_mode: str
    sender_name: str
    recipient_name: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        human_input_mode: str,
        sender: Optional["Agent"] = None,
        recipient: "Agent",
    ):
        super().__init__(
            uuid=uuid,
            human_input_mode=human_input_mode,
            sender_name=sender.name if sender else "No sender",
            recipient_name=recipient.name,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored("\n>>>>>>>> USING AUTO REPLY...", "red"), flush=True)


@deprecated_by(TerminationEvent, default_params={"sender": "system"})
@wrap_message
class TerminationMessage(BaseMessage):
    """When a workflow termination condition is met"""

    termination_reason: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        termination_reason: str,
    ):
        super().__init__(
            uuid=uuid,
            termination_reason=termination_reason,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored(f"\n>>>>>>>> TERMINATING RUN ({str(self.uuid)}): {self.termination_reason}", "red"), flush=True)


@deprecated_by(ExecuteCodeBlockEvent, param_mapping={"recipient_name": "recipient"})
@wrap_message
class ExecuteCodeBlockMessage(BaseMessage):
    code: str
    language: str
    code_block_count: int
    recipient_name: str

    def __init__(
        self, *, uuid: UUID | None = None, code: str, language: str, code_block_count: int, recipient: "Agent"
    ):
        super().__init__(
            uuid=uuid, code=code, language=language, code_block_count=code_block_count, recipient_name=recipient.name
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f"\n>>>>>>>> EXECUTING CODE BLOCK {self.code_block_count} (inferred language is {self.language})...",
                "red",
            ),
            flush=True,
        )


@deprecated_by(ExecuteFunctionEvent, param_mapping={"recipient_name": "recipient"})
@wrap_message
class ExecuteFunctionMessage(BaseMessage):
    func_name: str
    call_id: str | None = None
    arguments: dict[str, Any]
    recipient_name: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        func_name: str,
        call_id: str | None = None,
        arguments: dict[str, Any],
        recipient: "Agent",
    ):
        super().__init__(
            uuid=uuid, func_name=func_name, call_id=call_id, arguments=arguments, recipient_name=recipient.name
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f"\n>>>>>>>> EXECUTING FUNCTION {self.func_name}...\nCall ID: {self.call_id}\nInput arguments: {self.arguments}",
                "magenta",
            ),
            flush=True,
        )


@deprecated_by(ExecutedFunctionEvent, param_mapping={"recipient_name": "recipient"})
@wrap_message
class ExecutedFunctionMessage(BaseMessage):
    func_name: str
    call_id: str | None = None
    arguments: dict[str, Any]
    content: str
    recipient_name: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        func_name: str,
        call_id: str | None = None,
        arguments: dict[str, Any],
        content: str,
        recipient: "Agent",
    ):
        super().__init__(
            uuid=uuid,
            func_name=func_name,
            call_id=call_id,
            arguments=arguments,
            content=content,
            recipient_name=recipient.name,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f"\n>>>>>>>> EXECUTED FUNCTION {self.func_name}...\nCall ID: {self.call_id}\nInput arguments: {self.arguments}\nOutput:\n{self.content}",
                "magenta",
            ),
            flush=True,
        )


@deprecated_by(SelectSpeakerEvent)
@wrap_message
class SelectSpeakerMessage(BaseMessage):
    agent_names: list[str] | None = None

    def __init__(self, *, uuid: UUID | None = None, agents: list["Agent"] | None = None):
        agent_names = [agent.name for agent in agents] if agents else None
        super().__init__(uuid=uuid, agent_names=agent_names)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f("Please select the next speaker from the following list:")
        agent_names = self.agent_names or []
        for i, agent_name in enumerate(agent_names):
            f(f"{i + 1}: {agent_name}")


@deprecated_by(SelectSpeakerTryCountExceededEvent)
@wrap_message
class SelectSpeakerTryCountExceededMessage(BaseMessage):
    try_count: int
    agent_names: list[str] | None = None

    def __init__(self, *, uuid: UUID | None = None, try_count: int, agents: list["Agent"] | None = None):
        agent_names = [agent.name for agent in agents] if agents else None
        super().__init__(uuid=uuid, try_count=try_count, agent_names=agent_names)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"You have tried {self.try_count} times. The next speaker will be selected automatically.")


@deprecated_by(SelectSpeakerInvalidInputEvent)
@wrap_message
class SelectSpeakerInvalidInputMessage(BaseMessage):
    agent_names: list[str] | None = None

    def __init__(self, *, uuid: UUID | None = None, agents: list["Agent"] | None = None):
        agent_names = [agent.name for agent in agents] if agents else None
        super().__init__(uuid=uuid, agent_names=agent_names)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"Invalid input. Please enter a number between 1 and {len(self.agent_names or [])}.")


@deprecated_by(
    ClearConversableAgentHistoryEvent,
    param_mapping={"no_messages_preserved": "no_events_preserved", "recipient_name": "recipient"},
)
@wrap_message
class ClearConversableAgentHistoryMessage(BaseMessage):
    agent_name: str
    recipient_name: str
    no_messages_preserved: int

    def __init__(self, *, uuid: UUID | None = None, agent: "Agent", no_messages_preserved: int | None = None):
        super().__init__(
            uuid=uuid,
            agent_name=agent.name,
            recipient_name=agent.name,
            no_messages_preserved=no_messages_preserved,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        for _ in range(self.no_messages_preserved):
            f(
                f"Preserving one more message for {self.agent_name} to not divide history between tool call and "
                f"tool response."
            )


@deprecated_by(ClearConversableAgentHistoryWarningEvent, param_mapping={"recipient_name": "recipient"})
@wrap_message
class ClearConversableAgentHistoryWarningMessage(BaseMessage):
    recipient_name: str

    def __init__(self, *, uuid: UUID | None = None, recipient: "Agent"):
        super().__init__(
            uuid=uuid,
            recipient_name=recipient.name,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                "WARNING: `nr_preserved_messages` is ignored when clearing chat history with a specific agent.",
                "yellow",
            ),
            flush=True,
        )


@deprecated_by(
    GenerateCodeExecutionReplyEvent,
    param_mapping={"sender_name": "sender", "recipient_name": "recipient", "code_block_languages": "code_blocks"},
)
@wrap_message
class GenerateCodeExecutionReplyMessage(BaseMessage):
    code_block_languages: list[str]
    sender_name: str | None = None
    recipient_name: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        code_blocks: list["CodeBlock"],
        sender: Optional["Agent"] = None,
        recipient: "Agent",
    ):
        code_block_languages = [code_block.language for code_block in code_blocks]

        super().__init__(
            uuid=uuid,
            code_block_languages=code_block_languages,
            sender_name=sender.name if sender else None,
            recipient_name=recipient.name,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        num_code_blocks = len(self.code_block_languages)
        if num_code_blocks == 1:
            f(
                colored(
                    f"\n>>>>>>>> EXECUTING CODE BLOCK (inferred language is {self.code_block_languages[0]})...",
                    "red",
                ),
                flush=True,
            )
        else:
            f(
                colored(
                    f"\n>>>>>>>> EXECUTING {num_code_blocks} CODE BLOCKS (inferred languages are [{', '.join(list(self.code_block_languages))}])...",
                    "red",
                ),
                flush=True,
            )


@deprecated_by(ConversableAgentUsageSummaryNoCostIncurredEvent, param_mapping={"recipient_name": "recipient"})
@wrap_message
class ConversableAgentUsageSummaryNoCostIncurredMessage(BaseMessage):
    recipient_name: str

    def __init__(self, *, uuid: UUID | None = None, recipient: "Agent"):
        super().__init__(uuid=uuid, recipient_name=recipient.name)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"No cost incurred from agent '{self.recipient_name}'.")


@deprecated_by(ConversableAgentUsageSummaryEvent, param_mapping={"recipient_name": "recipient"})
@wrap_message
class ConversableAgentUsageSummaryMessage(BaseMessage):
    recipient_name: str

    def __init__(self, *, uuid: UUID | None = None, recipient: "Agent"):
        super().__init__(uuid=uuid, recipient_name=recipient.name)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"Agent '{self.recipient_name}':")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import json
from collections.abc import Callable
from typing import Any
from uuid import UUID

from ..events import deprecated_by
from ..events.print_event import PrintEvent
from .base_message import BaseMessage, wrap_message


@deprecated_by(PrintEvent)
@wrap_message
class PrintMessage(BaseMessage):
    """Print message"""

    objects: list[str]
    """List of objects to print"""
    sep: str
    """Separator between objects"""
    end: str
    """End of the print"""

    def __init__(self, *objects: Any, sep: str = " ", end: str = "\n", flush: bool = False, uuid: UUID | None = None):
        objects_as_string = [self._to_json(x) for x in objects]

        super().__init__(uuid=uuid, objects=objects_as_string, sep=sep, end=end)

    def _to_json(self, obj: Any) -> str:
        if isinstance(obj, str):
            return obj

        if hasattr(obj, "model_dump_json"):
            return obj.model_dump_json()  # type: ignore [no-any-return]
        try:
            return json.dumps(obj)
        except Exception:
            return str(obj)
            # return repr(obj)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(*self.objects, sep=self.sep, end=self.end, flush=True)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from abc import ABC
from collections.abc import Callable
from typing import Annotated, Any, Literal, TypeVar, Union
from uuid import UUID, uuid4

from pydantic import BaseModel, Field, create_model

from ..doc_utils import export_module

PetType = TypeVar("PetType", bound=Literal["cat", "dog"])

__all__ = ["BaseMessage", "get_annotated_type_for_message_classes", "wrap_message"]


@export_module("autogen.messages")
class BaseMessage(BaseModel, ABC):
    uuid: UUID

    def __init__(self, uuid: UUID | None = None, **kwargs: Any) -> None:
        """Base message class

        Args:
            uuid (Optional[UUID], optional): Unique identifier for the message. Defaults to None.
            **kwargs (Any): Additional keyword arguments
        """
        uuid = uuid or uuid4()
        super().__init__(uuid=uuid, **kwargs)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        """Print message

        Args:
            f (Optional[Callable[..., Any]], optional): Print function. If none, python's default print will be used.
        """
        ...


def camel2snake(name: str) -> str:
    return "".join(["_" + i.lower() if i.isupper() else i for i in name]).lstrip("_")


_message_classes: dict[str, type[BaseModel]] = {}


@export_module("autogen.messages")
def wrap_message(message_cls: type[BaseMessage]) -> type[BaseModel]:
    """Wrap a message class with a type field to be used in a union type

    This is needed for proper serialization and deserialization of messages in a union type.

    Args:
        message_cls (type[BaseMessage]): Message class to wrap
    """
    global _message_classes

    if not message_cls.__name__.endswith("Message"):
        raise ValueError("Message class name must end with 'Message'")

    type_name = camel2snake(message_cls.__name__)
    type_name = type_name[: -len("_message")]

    class WrapperBase(BaseModel):
        # these types are generated dynamically so we need to disable the type checker
        type: Literal[type_name] = type_name  # type: ignore[valid-type]
        content: message_cls  # type: ignore[valid-type]

        def __init__(self, *args: Any, **data: Any):
            if set(data.keys()) == {"type", "content"} and "content" in data:
                super().__init__(*args, **data)
            else:
                if "content" in data:
                    content = data.pop("content")
                    super().__init__(*args, content=message_cls(*args, **data, content=content), **data)
                else:
                    super().__init__(content=message_cls(*args, **data), **data)

        def print(self, f: Callable[..., Any] | None = None) -> None:
            self.content.print(f)  # type: ignore[attr-defined]

    wrapper_cls = create_model(message_cls.__name__, __base__=WrapperBase)

    # Preserve the original class's docstring and other attributes
    wrapper_cls.__doc__ = message_cls.__doc__
    wrapper_cls.__module__ = message_cls.__module__

    # Copy any other relevant attributes/metadata from the original class
    if hasattr(message_cls, "__annotations__"):
        wrapper_cls.__annotations__ = message_cls.__annotations__

    _message_classes[type_name] = wrapper_cls

    return wrapper_cls


@export_module("autogen.messages")
def get_annotated_type_for_message_classes() -> type[Any]:
    # this is a dynamic type so we need to disable the type checker
    union_type = Union[tuple(_message_classes.values())]  # type: ignore[valid-type]  # noqa: UP007
    return Annotated[union_type, Field(discriminator="type")]  # type: ignore[return-value]


def get_message_classes() -> dict[str, type[BaseModel]]:
    return _message_classes
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT


def remove_boxed(string: str) -> str | None:
    """Source: https://github.com/hendrycks/math
    Extract the text within a \\boxed`{...}` environment.

    Example:
    ```python
    > remove_boxed("\\boxed{\\frac{2}{3}}")

    \\frac{2}{3}
    ```
    """
    left = "\\boxed{"
    try:
        if not all((string[: len(left)] == left, string[-1] == "}")):
            raise AssertionError

        return string[len(left) : -1]
    except Exception:
        return None


def last_boxed_only_string(string: str) -> str | None:
    """Source: https://github.com/hendrycks/math
    Extract the last \\boxed`{...}` or \\fbox`{...}` element from a string.
    """
    idx = string.rfind("\\boxed")
    if idx < 0:
        idx = string.rfind("\\fbox")
        if idx < 0:
            return None

    i = idx
    right_brace_idx = None
    num_left_braces_open = 0
    while i < len(string):
        if string[i] == "{":
            num_left_braces_open += 1
        if string[i] == "}":
            num_left_braces_open -= 1
            if num_left_braces_open == 0:
                right_brace_idx = i
                break
        i += 1

    retval = None if right_brace_idx is None else string[idx : right_brace_idx + 1]

    return retval


def _fix_fracs(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Reformat fractions.

    Examples:
    ```
    >>> _fix_fracs("\\frac1b")
    \frac{1}{b}
    >>> _fix_fracs("\\frac12")
    \frac{1}{2}
    >>> _fix_fracs("\\frac1{72}")
    \frac{1}{72}
    ```
    """
    substrs = string.split("\\frac")
    new_str = substrs[0]
    if len(substrs) > 1:
        substrs = substrs[1:]
        for substr in substrs:
            new_str += "\\frac"
            if substr[0] == "{":
                new_str += substr
            else:
                try:
                    if not len(substr) >= 2:
                        raise AssertionError
                except Exception:
                    return string
                a = substr[0]
                b = substr[1]
                if b != "{":
                    if len(substr) > 2:
                        post_substr = substr[2:]
                        new_str += "{" + a + "}{" + b + "}" + post_substr
                    else:
                        new_str += "{" + a + "}{" + b + "}"
                else:
                    if len(substr) > 2:
                        post_substr = substr[2:]
                        new_str += "{" + a + "}" + b + post_substr
                    else:
                        new_str += "{" + a + "}" + b
    string = new_str
    return string


def _fix_a_slash_b(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Reformat fractions formatted as a/b to \\`frac{a}{b}`.

    Example:
    ```
    >>> _fix_a_slash_b("2/3")
    \frac{2}{3}
    ```
    """
    if len(string.split("/")) != 2:
        return string
    a_str = string.split("/")[0]
    b_str = string.split("/")[1]
    try:
        a = int(a_str)
        b = int(b_str)
        if not string == f"{a}/{b}":
            raise AssertionError
        new_string = "\\frac{" + str(a) + "}{" + str(b) + "}"
        return new_string
    except Exception:
        return string


def _remove_right_units(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Remove units (on the right).
    "\\text{ " only ever occurs (at least in the val set) when describing units.
    """
    if "\\text{ " in string:
        splits = string.split("\\text{ ")
        if not len(splits) == 2:
            raise AssertionError
        return splits[0]
    else:
        return string


def _fix_sqrt(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Reformat square roots.

    Example:
    ```
    >>> _fix_sqrt("\\sqrt3")
    \\sqrt{3}
    ```
    """
    if "\\sqrt" not in string:
        return string
    splits = string.split("\\sqrt")
    new_string = splits[0]
    for split in splits[1:]:
        if split[0] != "{":
            a = split[0]
            new_substr = "\\sqrt{" + a + "}" + split[1:]
        else:
            new_substr = "\\sqrt" + split
        new_string += new_substr
    return new_string


def _strip_string(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Apply the reformatting helper functions above.
    """
    # linebreaks
    string = string.replace("\n", "")
    # print(string)

    # remove inverse spaces
    string = string.replace("\\!", "")
    # print(string)

    # replace \\ with \
    string = string.replace("\\\\", "\\")
    # print(string)

    # replace tfrac and dfrac with frac
    string = string.replace("tfrac", "frac")
    string = string.replace("dfrac", "frac")
    # print(string)

    # remove \left and \right
    string = string.replace("\\left", "")
    string = string.replace("\\right", "")
    # print(string)

    # Remove circ (degrees)
    string = string.replace("^{\\circ}", "")
    string = string.replace("^\\circ", "")

    # remove dollar signs
    string = string.replace("\\$", "")

    # remove units (on the right)
    string = _remove_right_units(string)

    # remove percentage
    string = string.replace("\\%", "")
    string = string.replace("%", "")

    # " 0." equivalent to " ." and "{0." equivalent to "{." Alternatively, add "0" if "." is the start of the string
    string = string.replace(" .", " 0.")
    string = string.replace("{.", "{0.")
    # if empty, return empty string
    if len(string) == 0:
        return string
    if string[0] == ".":
        string = "0" + string

    # to consider: get rid of e.g. "k = " or "q = " at beginning
    if len(string.split("=")) == 2 and len(string.split("=")[0]) <= 2:
        string = string.split("=")[1]

    # fix sqrt3 --> sqrt{3}
    string = _fix_sqrt(string)

    # remove spaces
    string = string.replace(" ", "")

    # \frac1b or \frac12 --> \frac{1}{b} and \frac{1}{2}, etc.
    # Even works with \frac1{72} (but not \frac{72}1).
    # Also does a/b --> \\frac{a}{b}
    string = _fix_fracs(string)

    # manually change 0.5 --> \frac{1}{2}
    if string == "0.5":
        string = "\\frac{1}{2}"

    # NOTE: X/Y changed to \frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y
    string = _fix_a_slash_b(string)

    return string


def get_answer(solution: str | None) -> str | None:
    if solution is None:
        return None
    last_boxed = last_boxed_only_string(solution)
    if last_boxed is None:
        return None
    answer = remove_boxed(last_boxed)
    if answer is None:
        return None
    return answer


def is_equiv(str1: str | None, str2: str | None) -> float:
    """Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in
    - units
    - fractions
    - square roots
    - superfluous LaTeX.
    Source: https://github.com/hendrycks/math
    """
    if str1 is None and str2 is None:
        print("WARNING: Both None")
        return 1.0
    if str1 is None or str2 is None:
        return 0.0

    try:
        ss1 = _strip_string(str1)
        ss2 = _strip_string(str2)
        return float(ss1 == ss2)
    except Exception:
        return float(str1 == str2)


def is_equiv_chain_of_thought(str1: str, str2: str) -> float:
    """Strips the solution first before calling `is_equiv`."""
    ans1 = get_answer(str1)
    ans2 = get_answer(str2)

    return is_equiv(ans1, ans2)


def voting_counts(responses):
    answers = {}
    for i in range(len(responses)):
        equiv = i
        if get_answer(responses[i]) is None:
            # ignore None answers
            continue
        for j in answers:
            if is_equiv_chain_of_thought(responses[i], responses[j]):
                equiv = j
                break
        if equiv in answers:
            answers[equiv] += 1
        else:
            answers[equiv] = 1
    return answers


def eval_math_responses(responses, solution=None):
    """Select a response for a math problem using voting, and check if the response is correct if the solution is provided.

    Args:
        responses: The list of responses.
        solution: The canonical solution.

    Returns:
        dict: The success metrics.
    """
    n = len(responses)
    if not n:
        return {
            "expected_success": 0,
            "success": False,
            "success_vote": 0,
            "voted_answer": None,
            "votes": 0,
        }
    success_list = []
    if solution is not None:
        for i in range(n):
            response = responses[i]
            succeed = is_equiv_chain_of_thought(response, solution)
            success_list.append(succeed)
    # voting
    answers = voting_counts(responses)
    # find the answer with highest votes in answers
    answer, votes = max(answers.items(), key=lambda x: x[1], default=(0, 0))
    # check if the answer is correct
    success_vote = is_equiv_chain_of_thought(responses[answer], solution)
    return {
        "expected_success": 1 - pow(1 - sum(success_list) / n, n),
        "success": any(s for s in success_list),
        "success_vote": success_vote,
        "voted_answer": responses[answer],
        "votes": votes,
    }
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

__all__ = ["__version__"]

__version__ = "0.9.10"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any, Literal

from ..doc_utils import export_module
from .base_logger import BaseLogger
from .file_logger import FileLogger
from .sqlite_logger import SqliteLogger

__all__ = ("LoggerFactory",)


@export_module("autogen.logger")
class LoggerFactory:
    """Factory class to create logger objects."""

    @staticmethod
    def get_logger(
        logger_type: Literal["sqlite", "file"] = "sqlite", config: dict[str, Any] | None = None
    ) -> BaseLogger:
        """Factory method to create logger objects.

        Args:
            logger_type (Literal["sqlite", "file"], optional): Type of logger. Defaults to "sqlite".
            config (Optional[dict[str, Any]], optional): Configuration for logger. Defaults to None.

        Returns:
            BaseLogger: Logger object
        """
        if config is None:
            config = {}

        if logger_type == "sqlite":
            return SqliteLogger(config)
        elif logger_type == "file":
            return FileLogger(config)
        else:
            raise ValueError(f"[logger_factory] Unknown logger type: {logger_type}")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import json
import logging
import os
import sqlite3
import threading
import uuid
from collections.abc import Callable
from typing import TYPE_CHECKING, Any, TypeVar

from ..doc_utils import export_module
from .base_logger import BaseLogger, LLMConfig
from .logger_utils import get_current_ts, to_dict

if TYPE_CHECKING:
    from openai import AzureOpenAI, OpenAI
    from openai.types.chat import ChatCompletion

    from .. import Agent, ConversableAgent, OpenAIWrapper
    from ..oai.anthropic import AnthropicClient
    from ..oai.bedrock import BedrockClient
    from ..oai.cerebras import CerebrasClient
    from ..oai.cohere import CohereClient
    from ..oai.gemini import GeminiClient
    from ..oai.groq import GroqClient
    from ..oai.mistral import MistralAIClient
    from ..oai.ollama import OllamaClient
    from ..oai.together import TogetherClient

logger = logging.getLogger(__name__)
lock = threading.Lock()

__all__ = ("SqliteLogger",)

F = TypeVar("F", bound=Callable[..., Any])


def safe_serialize(obj: Any) -> str:
    """Safely serialize an object to JSON.

    Args:
        obj (Any): Object to serialize.

    Returns:
        str: Serialized object.
    """

    def default(o: Any) -> str:
        if hasattr(o, "to_json"):
            return str(o.to_json())
        else:
            return f"<<non-serializable: {type(o).__qualname__}>>"

    return json.dumps(obj, default=default)


@export_module("autogen.logger")
class SqliteLogger(BaseLogger):
    """Sqlite logger class."""

    schema_version = 1

    def __init__(self, config: dict[str, Any]):
        """Initialize the SqliteLogger.

        Args:
            config (dict[str, Any]): Configuration for the logger.
        """
        self.config = config

        try:
            self.dbname = self.config.get("dbname", "logs.db")
            self.con = sqlite3.connect(self.dbname, check_same_thread=False)
            self.cur = self.con.cursor()
            self.session_id = str(uuid.uuid4())
        except sqlite3.Error as e:
            logger.error(f"[SqliteLogger] Failed to connect to database {self.dbname}: {e}")

    def start(self) -> str:
        try:
            query = """
                CREATE TABLE IF NOT EXISTS chat_completions(
                    id INTEGER PRIMARY KEY,
                    invocation_id TEXT,
                    client_id INTEGER,
                    wrapper_id INTEGER,
                    session_id TEXT,
                    source_name TEXT,
                    request TEXT,
                    response TEXT,
                    is_cached INEGER,
                    cost REAL,
                    start_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                    end_time DATETIME DEFAULT CURRENT_TIMESTAMP)
            """
            self._run_query(query=query)

            query = """
                CREATE TABLE IF NOT EXISTS agents (
                    id INTEGER PRIMARY KEY,                             -- Key assigned by the database
                    agent_id INTEGER,                                   -- result of python id(agent)
                    wrapper_id INTEGER,                                 -- result of python id(agent.client)
                    session_id TEXT,
                    name TEXT,                                          -- agent.name
                    class TEXT,                                         -- type or class name of agent
                    init_args TEXT,                                     -- JSON serialization of constructor
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(agent_id, session_id))
            """
            self._run_query(query=query)

            query = """
                CREATE TABLE IF NOT EXISTS oai_wrappers (
                    id INTEGER PRIMARY KEY,                             -- Key assigned by the database
                    wrapper_id INTEGER,                                 -- result of python id(wrapper)
                    session_id TEXT,
                    init_args TEXT,                                     -- JSON serialization of constructor
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(wrapper_id, session_id))
            """
            self._run_query(query=query)

            query = """
                CREATE TABLE IF NOT EXISTS oai_clients (
                    id INTEGER PRIMARY KEY,                             -- Key assigned by the database
                    client_id INTEGER,                                  -- result of python id(client)
                    wrapper_id INTEGER,                                 -- result of python id(wrapper)
                    session_id TEXT,
                    class TEXT,                                         -- type or class name of client
                    init_args TEXT,                                     -- JSON serialization of constructor
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(client_id, session_id))
            """
            self._run_query(query=query)

            query = """
            CREATE TABLE IF NOT EXISTS version (
                id INTEGER PRIMARY KEY CHECK (id = 1),                  -- id of the logging database
                version_number INTEGER NOT NULL                         -- version of the logging database
            );
            """
            self._run_query(query=query)

            query = """
            CREATE TABLE IF NOT EXISTS events (
                event_name TEXT,
                source_id INTEGER,
                source_name TEXT,
                agent_module TEXT DEFAULT NULL,
                agent_class_name TEXT DEFAULT NULL,
                id INTEGER PRIMARY KEY,
                json_state TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            );
            """
            self._run_query(query=query)

            query = """
                        CREATE TABLE IF NOT EXISTS function_calls (
                            source_id INTEGER,
                            source_name TEXT,
                            function_name TEXT,
                            args TEXT DEFAULT NULL,
                            returns TEXT DEFAULT NULL,
                            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                        );
                        """
            self._run_query(query=query)

            current_version = self._get_current_db_version()
            if current_version is None:
                self._run_query(
                    query="INSERT INTO version (id, version_number) VALUES (1, ?);", args=(SqliteLogger.schema_version,)
                )
            self._apply_migration()

        except sqlite3.Error as e:
            logger.error(f"[SqliteLogger] start logging error: {e}")
        finally:
            return self.session_id

    def _get_current_db_version(self) -> None | int:
        self.cur.execute("SELECT version_number FROM version ORDER BY id DESC LIMIT 1")
        result = self.cur.fetchone()
        return result[0] if result is not None else None

    # Example migration script name format: 002_update_agents_table.sql
    def _apply_migration(self, migrations_dir: str = "./migrations") -> None:
        current_version = self._get_current_db_version()
        current_version = SqliteLogger.schema_version if current_version is None else current_version

        if os.path.isdir(migrations_dir):
            migrations = sorted(os.listdir(migrations_dir))
        else:
            logger.info("no migration scripts, skip...")
            return

        migrations_to_apply = [m for m in migrations if int(m.split("_")[0]) > current_version]

        for script in migrations_to_apply:
            with open(script) as f:
                migration_sql = f.read()
                self._run_query_script(script=migration_sql)

                latest_version = int(script.split("_")[0])
                query = "UPDATE version SET version_number = ? WHERE id = 1"
                args = (latest_version,)
                self._run_query(query=query, args=args)

    def _run_query(self, query: str, args: tuple[Any, ...] = ()) -> None:
        """Executes a given SQL query.

        Args:
            query (str):        The SQL query to execute.
            args (Tuple):       The arguments to pass to the SQL query.
        """
        try:
            with lock:
                self.cur.execute(query, args)
                self.con.commit()
        except Exception as e:
            logger.error("[sqlite logger]Error running query with query %s and args %s: %s", query, args, e)

    def _run_query_script(self, script: str) -> None:
        """Executes SQL script.

        Args:
            script (str):       SQL script to execute.
        """
        try:
            with lock:
                self.cur.executescript(script)
                self.con.commit()
        except Exception as e:
            logger.error("[sqlite logger]Error running query script %s: %s", script, e)

    def log_chat_completion(
        self,
        invocation_id: uuid.UUID,
        client_id: int,
        wrapper_id: int,
        source: str | Agent,
        request: dict[str, float | str | list[dict[str, str]]],
        response: str | ChatCompletion,
        is_cached: int,
        cost: float,
        start_time: str,
    ) -> None:
        """Log chat completion.

        Args:
            invocation_id (uuid.UUID): Invocation ID.
            client_id (int): Client ID.
            wrapper_id (int): Wrapper ID.
            source (str | Agent): Source of the chat completion.
            request (dict[str, float | str | list[dict[str, str]]]): Request for the chat completion.
            response (str | ChatCompletion): Response for the chat completion.
            is_cached (int): Whether the response is cached.
            cost (float): Cost of the chat completion.
            start_time (str): Start time of the chat completion.
        """
        if self.con is None:
            return

        end_time = get_current_ts()

        if response is None or isinstance(response, str):
            response_messages = json.dumps({"response": response})
        else:
            response_messages = json.dumps(to_dict(response), indent=4)

        source_name = (
            source
            if isinstance(source, str)
            else source.name
            if hasattr(source, "name") and source.name is not None
            else ""
        )

        query = """
            INSERT INTO chat_completions (
                invocation_id, client_id, wrapper_id, session_id, request, response, is_cached, cost, start_time, end_time, source_name
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        args = (
            invocation_id,
            client_id,
            wrapper_id,
            self.session_id,
            json.dumps(to_dict(request)),
            response_messages,
            is_cached,
            cost,
            start_time,
            end_time,
            source_name,
        )

        self._run_query(query=query, args=args)

    def log_new_agent(self, agent: ConversableAgent, init_args: dict[str, Any]) -> None:
        """Log new agent.

        Args:
            agent (ConversableAgent): Agent to log.
            init_args (dict[str, Any]): Initialization arguments of the agent
        """
        from .. import Agent

        if self.con is None:
            return

        args = to_dict(
            init_args,
            exclude=(
                "self",
                "__class__",
                "api_key",
                "organization",
                "base_url",
                "azure_endpoint",
                "azure_ad_token",
                "azure_ad_token_provider",
            ),
            no_recursive=(Agent,),
        )

        # We do an upsert since both the superclass and subclass may call this method (in that order)
        query = """
        INSERT INTO agents (agent_id, wrapper_id, session_id, name, class, init_args, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?)
        ON CONFLICT (agent_id, session_id) DO UPDATE SET
            wrapper_id = excluded.wrapper_id,
            name = excluded.name,
            class = excluded.class,
            init_args = excluded.init_args,
            timestamp = excluded.timestamp
        """
        args = (
            id(agent),
            agent.client.wrapper_id if hasattr(agent, "client") and agent.client is not None else "",
            self.session_id,
            agent.name if hasattr(agent, "name") and agent.name is not None else "",
            type(agent).__name__,
            json.dumps(args),
            get_current_ts(),
        )
        self._run_query(query=query, args=args)

    def log_event(self, source: str | Agent, name: str, **kwargs: dict[str, Any]) -> None:
        """Log event.

        Args:
            source (str | Agent): Source of the event.
            name (str): Name of the event.
            **kwargs (dict[str, Any]): Additional arguments for the event.
        """
        from autogen import Agent

        if self.con is None:
            return

        json_args = json.dumps(kwargs, default=lambda o: f"<<non-serializable: {type(o).__qualname__}>>")

        if isinstance(source, Agent):
            query = """
            INSERT INTO events (source_id, source_name, event_name, agent_module, agent_class_name, json_state, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?)
            """
            args = (
                id(source),
                source.name if hasattr(source, "name") else source,
                name,
                source.__module__,
                source.__class__.__name__,
                json_args,
                get_current_ts(),
            )
            self._run_query(query=query, args=args)
        else:
            query = """
            INSERT INTO events (source_id, source_name, event_name, json_state, timestamp) VALUES (?, ?, ?, ?, ?)
            """
            args_str_based = (
                id(source),
                source.name if hasattr(source, "name") else source,
                name,
                json_args,
                get_current_ts(),
            )
            self._run_query(query=query, args=args_str_based)

    def log_new_wrapper(self, wrapper: OpenAIWrapper, init_args: dict[str, LLMConfig | list[LLMConfig]]) -> None:
        """Log new wrapper.

        Args:
            wrapper (OpenAIWrapper): Wrapper to log.
            init_args (dict[str, LLMConfig | list[LLMConfig]]): Initialization arguments of the wrapper
        """
        if self.con is None:
            return

        args = to_dict(
            init_args,
            exclude=(
                "self",
                "__class__",
                "api_key",
                "organization",
                "base_url",
                "azure_endpoint",
                "azure_ad_token",
                "azure_ad_token_provider",
            ),
        )

        query = """
        INSERT INTO oai_wrappers (wrapper_id, session_id, init_args, timestamp) VALUES (?, ?, ?, ?)
        ON CONFLICT (wrapper_id, session_id) DO NOTHING;
        """
        args = (
            id(wrapper),
            self.session_id,
            json.dumps(args),
            get_current_ts(),
        )
        self._run_query(query=query, args=args)

    def log_function_use(self, source: str | Agent, function: F, args: dict[str, Any], returns: Any) -> None:
        """Log function use.

        Args:
            source (str | Agent): Source of the function use.
            function (F): Function to log.
            args (dict[str, Any]): Arguments of the function.
            returns (Any): Returns of the function.
        """
        if self.con is None:
            return

        query = """
        INSERT INTO function_calls (source_id, source_name, function_name, args, returns, timestamp) VALUES (?, ?, ?, ?, ?, ?)
        """
        query_args: tuple[Any, ...] = (
            id(source),
            source.name if hasattr(source, "name") else source,
            function.__name__,
            safe_serialize(args),
            safe_serialize(returns),
            get_current_ts(),
        )
        self._run_query(query=query, args=query_args)

    def log_new_client(
        self,
        client: (
            AzureOpenAI
            | OpenAI
            | CerebrasClient
            | GeminiClient
            | AnthropicClient
            | MistralAIClient
            | TogetherClient
            | GroqClient
            | CohereClient
            | OllamaClient
            | BedrockClient
        ),
        wrapper: OpenAIWrapper,
        init_args: dict[str, Any],
    ) -> None:
        """Log new client.

        Args:
            client (AzureOpenAI | OpenAI | CerebrasClient | GeminiClient | AnthropicClient | MistralAIClient | TogetherClient | GroqClient | CohereClient | OllamaClient | BedrockClient): Client to log.
            wrapper (OpenAIWrapper): Wrapper of the client.
            init_args (dict[str, Any]): Initialization arguments of the client.
        """
        if self.con is None:
            return

        args = to_dict(
            init_args,
            exclude=(
                "self",
                "__class__",
                "api_key",
                "organization",
                "base_url",
                "azure_endpoint",
                "azure_ad_token",
                "azure_ad_token_provider",
            ),
        )

        query = """
        INSERT INTO oai_clients (client_id, wrapper_id, session_id, class, init_args, timestamp) VALUES (?, ?, ?, ?, ?, ?)
        ON CONFLICT (client_id, session_id) DO NOTHING;
        """
        args = (
            id(client),
            id(wrapper),
            self.session_id,
            type(client).__name__,
            json.dumps(args),
            get_current_ts(),
        )
        self._run_query(query=query, args=args)

    def stop(self) -> None:
        """Stop the logger"""
        if self.con:
            self.con.close()

    def get_connection(self) -> None | sqlite3.Connection:
        """Get connection."""
        if self.con:
            return self.con
        return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from .file_logger import FileLogger
from .logger_factory import LoggerFactory
from .sqlite_logger import SqliteLogger

__all__ = ("FileLogger", "LoggerFactory", "SqliteLogger")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import sqlite3
import uuid
from abc import ABC, abstractmethod
from collections.abc import Callable
from typing import TYPE_CHECKING, Any, TypeVar

if TYPE_CHECKING:
    from openai import AzureOpenAI, OpenAI
    from openai.types.chat import ChatCompletion

    from .. import Agent, ConversableAgent, OpenAIWrapper

F = TypeVar("F", bound=Callable[..., Any])
ConfigItem = dict[str, str | list[str]]
LLMConfig = dict[str, None | float | int | ConfigItem | list[ConfigItem]]


class BaseLogger(ABC):
    @abstractmethod
    def start(self) -> str:
        """Open a connection to the logging database, and start recording.

        Returns:
            session_id (str):     a unique id for the logging session
        """
        ...

    @abstractmethod
    def log_chat_completion(
        self,
        invocation_id: uuid.UUID,
        client_id: int,
        wrapper_id: int,
        source: str | Agent,
        request: dict[str, float | str | list[dict[str, str]]],
        response: str | ChatCompletion,
        is_cached: int,
        cost: float,
        start_time: str,
    ) -> None:
        """Log a chat completion to database.

        In AG2, chat completions are somewhat complicated because they are handled by the `autogen.oai.OpenAIWrapper` class.
        One invocation to `create` can lead to multiple underlying OpenAI calls, depending on the llm_config list used, and
        any errors or retries.

        Args:
            invocation_id (uuid):               A unique identifier for the invocation to the OpenAIWrapper.create method call
            client_id (int):                    A unique identifier for the underlying OpenAI client instance
            wrapper_id (int):                   A unique identifier for the OpenAIWrapper instance
            source (str or Agent):              The source/creator of the event as a string name or an Agent instance
            request (dict):                     A dictionary representing the request or call to the OpenAI client endpoint
            response (str or ChatCompletion):   The response from OpenAI
            is_cached (int):                    1 if the response was a cache hit, 0 otherwise
            cost(float):                        The cost for OpenAI response
            start_time (str):                   A string representing the moment the request was initiated
        """
        ...

    @abstractmethod
    def log_new_agent(self, agent: ConversableAgent, init_args: dict[str, Any]) -> None:
        """Log the birth of a new agent.

        Args:
            agent (ConversableAgent):   The agent to log.
            init_args (dict):           The arguments passed to the construct the conversable agent
        """
        ...

    @abstractmethod
    def log_event(self, source: str | Agent, name: str, **kwargs: dict[str, Any]) -> None:
        """Log an event for an agent.

        Args:
            source (str or Agent):      The source/creator of the event as a string name or an Agent instance
            name (str):                 The name of the event
            kwargs (dict):              The event information to log
        """
        ...

    @abstractmethod
    def log_new_wrapper(self, wrapper: OpenAIWrapper, init_args: dict[str, LLMConfig | list[LLMConfig]]) -> None:
        """Log the birth of a new OpenAIWrapper.

        Args:
            wrapper (OpenAIWrapper):    The wrapper to log.
            init_args (dict):           The arguments passed to the construct the wrapper
        """
        ...

    @abstractmethod
    def log_new_client(self, client: AzureOpenAI | OpenAI, wrapper: OpenAIWrapper, init_args: dict[str, Any]) -> None:
        """Log the birth of a new OpenAIWrapper.

        Args:
            client: The client to log.
            wrapper: The wrapper that created the client.
            init_args: The arguments passed to the construct the client.
        """
        ...

    @abstractmethod
    def log_function_use(self, source: str | Agent, function: F, args: dict[str, Any], returns: Any) -> None:
        """Log the use of a registered function (could be a tool)

        Args:
            source (str or Agent):      The source/creator of the event as a string name or an Agent instance
            function (F):               The function information
            args (dict):                The function args to log
            returns (any):              The return
        """

    @abstractmethod
    def stop(self) -> None:
        """Close the connection to the logging database, and stop logging."""
        ...

    @abstractmethod
    def get_connection(self) -> None | sqlite3.Connection:
        """Return a connection to the logging database."""
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import inspect
from datetime import datetime, timezone
from pathlib import Path, PurePath
from typing import Any

__all__ = ("get_current_ts", "to_dict")


def get_current_ts() -> str:
    """Get current timestamp in UTC timezone.

    Returns:
        str: Current timestamp in UTC timezone
    """
    return datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S.%f")


def to_dict(
    obj: int | float | str | bool | dict[Any, Any] | list[Any] | tuple[Any, ...] | Any,
    exclude: tuple[str, ...] = (),
    no_recursive: tuple[Any, ...] = (),
) -> Any:
    """Convert object to dictionary.

    Args:
        obj (Union[int, float, str, bool, dict[Any, Any], list[Any], tuple[Any, ...], Any]): Object to convert
        exclude (tuple[str, ...], optional): Keys to exclude. Defaults to ().
        no_recursive (tuple[Any, ...], optional): Types to exclude from recursive conversion. Defaults to ().
    """
    if isinstance(obj, (int, float, str, bool)):
        return obj
    elif isinstance(obj, (Path, PurePath)):
        return str(obj)
    elif callable(obj):
        return inspect.getsource(obj).strip()
    elif isinstance(obj, dict):
        return {
            str(k): to_dict(str(v)) if isinstance(v, no_recursive) else to_dict(v, exclude, no_recursive)
            for k, v in obj.items()
            if k not in exclude
        }
    elif isinstance(obj, (list, tuple)):
        return [to_dict(str(v)) if isinstance(v, no_recursive) else to_dict(v, exclude, no_recursive) for v in obj]
    elif hasattr(obj, "__dict__"):
        return {
            str(k): to_dict(str(v)) if isinstance(v, no_recursive) else to_dict(v, exclude, no_recursive)
            for k, v in vars(obj).items()
            if k not in exclude
        }
    else:
        return obj
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import json
import logging
import os
import threading
import uuid
from collections.abc import Callable
from typing import TYPE_CHECKING, Any, TypeVar

from ..doc_utils import export_module
from .base_logger import BaseLogger, LLMConfig
from .logger_utils import get_current_ts, to_dict

if TYPE_CHECKING:
    from openai import AzureOpenAI, OpenAI
    from openai.types.chat import ChatCompletion

    from .. import Agent, ConversableAgent, OpenAIWrapper
    from ..oai.anthropic import AnthropicClient
    from ..oai.bedrock import BedrockClient
    from ..oai.cerebras import CerebrasClient
    from ..oai.cohere import CohereClient
    from ..oai.gemini import GeminiClient
    from ..oai.groq import GroqClient
    from ..oai.mistral import MistralAIClient
    from ..oai.ollama import OllamaClient
    from ..oai.together import TogetherClient

logger = logging.getLogger(__name__)

F = TypeVar("F", bound=Callable[..., Any])

__all__ = ("FileLogger",)


def safe_serialize(obj: Any) -> str:
    def default(o: Any) -> str:
        if hasattr(o, "to_json"):
            return str(o.to_json())
        else:
            return f"<<non-serializable: {type(o).__qualname__}>>"

    return json.dumps(obj, default=default)


@export_module("autogen.logger")
class FileLogger(BaseLogger):
    def __init__(self, config: dict[str, Any]):
        self.config = config
        self.session_id = str(uuid.uuid4())

        curr_dir = os.getcwd()
        self.log_dir = os.path.join(curr_dir, "autogen_logs")
        os.makedirs(self.log_dir, exist_ok=True)

        self.log_file = os.path.join(self.log_dir, self.config.get("filename", "runtime.log"))
        try:
            with open(self.log_file, "a"):
                pass
        except Exception as e:
            logger.error(f"[file_logger] Failed to create logging file: {e}")

        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        file_handler = logging.FileHandler(self.log_file)
        self.logger.addHandler(file_handler)

    def start(self) -> str:
        """Start the logger and return the session_id."""
        try:
            self.logger.info(f"Started new session with Session ID: {self.session_id}")
        except Exception as e:
            logger.error(f"[file_logger] Failed to create logging file: {e}")
        finally:
            return self.session_id

    def log_chat_completion(
        self,
        invocation_id: uuid.UUID,
        client_id: int,
        wrapper_id: int,
        source: str | Agent,
        request: dict[str, float | str | list[dict[str, str]]],
        response: str | ChatCompletion,
        is_cached: int,
        cost: float,
        start_time: str,
    ) -> None:
        """Log a chat completion."""
        thread_id = threading.get_ident()
        source_name = (
            source
            if isinstance(source, str)
            else source.name
            if hasattr(source, "name") and source.name is not None
            else ""
        )
        try:
            log_data = json.dumps({
                "invocation_id": str(invocation_id),
                "client_id": client_id,
                "wrapper_id": wrapper_id,
                "request": to_dict(request),
                "response": str(response),
                "is_cached": is_cached,
                "cost": cost,
                "start_time": start_time,
                "end_time": get_current_ts(),
                "thread_id": thread_id,
                "source_name": source_name,
            })

            self.logger.info(log_data)
        except Exception as e:
            self.logger.error(f"[file_logger] Failed to log chat completion: {e}")

    def log_new_agent(self, agent: ConversableAgent, init_args: dict[str, Any] = {}) -> None:
        """Log a new agent instance."""
        thread_id = threading.get_ident()

        try:
            log_data = json.dumps({
                "id": id(agent),
                "agent_name": agent.name if hasattr(agent, "name") and agent.name is not None else "",
                "wrapper_id": to_dict(
                    agent.client.wrapper_id if hasattr(agent, "client") and agent.client is not None else ""
                ),
                "session_id": self.session_id,
                "current_time": get_current_ts(),
                "agent_type": type(agent).__name__,
                "args": to_dict(init_args),
                "thread_id": thread_id,
            })
            self.logger.info(log_data)
        except Exception as e:
            self.logger.error(f"[file_logger] Failed to log new agent: {e}")

    def log_event(self, source: str | Agent, name: str, **kwargs: dict[str, Any]) -> None:
        """Log an event from an agent or a string source."""
        from .. import Agent

        # This takes an object o as input and returns a string. If the object o cannot be serialized, instead of raising an error,
        # it returns a string indicating that the object is non-serializable, along with its type's qualified name obtained using __qualname__.
        json_args = json.dumps(kwargs, default=lambda o: f"<<non-serializable: {type(o).__qualname__}>>")
        thread_id = threading.get_ident()

        if isinstance(source, Agent):
            try:
                log_data = json.dumps({
                    "source_id": id(source),
                    "source_name": str(source.name) if hasattr(source, "name") else source,
                    "event_name": name,
                    "agent_module": source.__module__,
                    "agent_class": source.__class__.__name__,
                    "json_state": json_args,
                    "timestamp": get_current_ts(),
                    "thread_id": thread_id,
                })
                self.logger.info(log_data)
            except Exception as e:
                self.logger.error(f"[file_logger] Failed to log event {e}")
        else:
            try:
                log_data = json.dumps({
                    "source_id": id(source),
                    "source_name": str(source.name) if hasattr(source, "name") else source,
                    "event_name": name,
                    "json_state": json_args,
                    "timestamp": get_current_ts(),
                    "thread_id": thread_id,
                })
                self.logger.info(log_data)
            except Exception as e:
                self.logger.error(f"[file_logger] Failed to log event {e}")

    def log_new_wrapper(self, wrapper: OpenAIWrapper, init_args: dict[str, LLMConfig | list[LLMConfig]] = {}) -> None:
        """Log a new wrapper instance."""
        thread_id = threading.get_ident()

        try:
            log_data = json.dumps({
                "wrapper_id": id(wrapper),
                "session_id": self.session_id,
                "json_state": json.dumps(init_args),
                "timestamp": get_current_ts(),
                "thread_id": thread_id,
            })
            self.logger.info(log_data)
        except Exception as e:
            self.logger.error(f"[file_logger] Failed to log event {e}")

    def log_new_client(
        self,
        client: (
            AzureOpenAI
            | OpenAI
            | CerebrasClient
            | GeminiClient
            | AnthropicClient
            | MistralAIClient
            | TogetherClient
            | GroqClient
            | CohereClient
            | OllamaClient
            | BedrockClient
        ),
        wrapper: OpenAIWrapper,
        init_args: dict[str, Any],
    ) -> None:
        """Log a new client instance."""
        thread_id = threading.get_ident()

        try:
            log_data = json.dumps({
                "client_id": id(client),
                "wrapper_id": id(wrapper),
                "session_id": self.session_id,
                "class": type(client).__name__,
                "json_state": json.dumps(init_args),
                "timestamp": get_current_ts(),
                "thread_id": thread_id,
            })
            self.logger.info(log_data)
        except Exception as e:
            self.logger.error(f"[file_logger] Failed to log event {e}")

    def log_function_use(self, source: str | Agent, function: F, args: dict[str, Any], returns: Any) -> None:
        """Log a registered function(can be a tool) use from an agent or a string source."""
        thread_id = threading.get_ident()

        try:
            log_data = json.dumps({
                "source_id": id(source),
                "source_name": str(source.name) if hasattr(source, "name") else source,
                "agent_module": source.__module__,
                "agent_class": source.__class__.__name__,
                "timestamp": get_current_ts(),
                "thread_id": thread_id,
                "input_args": safe_serialize(args),
                "returns": safe_serialize(returns),
            })
            self.logger.info(log_data)
        except Exception as e:
            self.logger.error(f"[file_logger] Failed to log event {e}")

    def get_connection(self) -> None:
        """Method is intentionally left blank because there is no specific connection needed for the FileLogger."""
        pass

    def stop(self) -> None:
        """Close the file handler and remove it from the logger."""
        for handler in self.logger.handlers:
            if isinstance(handler, logging.FileHandler):
                handler.close()
                self.logger.removeHandler(handler)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from types import TracebackType
from typing import Any

import diskcache
from typing_extensions import Self

from .abstract_cache_base import AbstractCache


class DiskCache(AbstractCache):
    """Implementation of AbstractCache using the DiskCache library.

    This class provides a concrete implementation of the AbstractCache
    interface using the diskcache library for caching data on disk.

    Attributes:
        cache (diskcache.Cache): The DiskCache instance used for caching.

    Methods:
        __init__(self, seed): Initializes the DiskCache with the given seed.
        get(self, key, default=None): Retrieves an item from the cache.
        set(self, key, value): Sets an item in the cache.
        close(self): Closes the cache.
        __enter__(self): Context management entry.
        __exit__(self, exc_type, exc_value, traceback): Context management exit.
    """

    def __init__(self, seed: str | int):
        """Initialize the DiskCache instance.

        Args:
            seed (Union[str, int]): A seed or namespace for the cache. This is used to create
                        a unique storage location for the cache data.

        """
        self.cache = diskcache.Cache(seed)

    def get(self, key: str, default: Any | None = None) -> Any | None:
        """Retrieve an item from the cache.

        Args:
            key (str): The key identifying the item in the cache.
            default (optional): The default value to return if the key is not found.
                                Defaults to None.

        Returns:
            The value associated with the key if found, else the default value.
        """
        return self.cache.get(key, default)

    def set(self, key: str, value: Any) -> None:
        """Set an item in the cache.

        Args:
            key (str): The key under which the item is to be stored.
            value: The value to be stored in the cache.
        """
        self.cache.set(key, value)

    def close(self) -> None:
        """Close the cache.

        Perform any necessary cleanup, such as closing file handles or
        releasing resources.
        """
        self.cache.close()

    def __enter__(self) -> Self:
        """Enter the runtime context related to the object.

        Returns:
            self: The instance itself.
        """
        return self

    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        traceback: TracebackType | None,
    ) -> None:
        """Exit the runtime context related to the object.

        Perform cleanup actions such as closing the cache.

        Args:
            exc_type: The exception type if an exception was raised in the context.
            exc_value: The exception value if an exception was raised in the context.
            traceback: The traceback if an exception was raised in the context.
        """
        self.close()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

from contextvars import ContextVar
from types import TracebackType
from typing import Any

from ..doc_utils import export_module
from .abstract_cache_base import AbstractCache
from .cache_factory import CacheFactory


@export_module("autogen")
class Cache(AbstractCache):
    """A wrapper class for managing cache configuration and instances.

    This class provides a unified interface for creating and interacting with
    different types of cache (e.g., Redis, Disk). It abstracts the underlying
    cache implementation details, providing methods for cache operations.

    Attributes:
        config (Dict[str, Any]): A dictionary containing cache configuration.
        cache: The cache instance created based on the provided configuration.
    """

    _current_cache: ContextVar[Cache] = ContextVar("current_cache", default=None)

    ALLOWED_CONFIG_KEYS = [
        "cache_seed",
        "redis_url",
        "cache_path_root",
        "cosmos_db_config",
    ]

    @staticmethod
    def redis(cache_seed: str | int = 42, redis_url: str = "redis://localhost:6379/0") -> Cache:
        """Create a Redis cache instance.

        Args:
            cache_seed (Union[str, int], optional): A seed for the cache. Defaults to 42.
            redis_url (str, optional): The URL for the Redis server. Defaults to "redis://localhost:6379/0".

        Returns:
            Cache: A Cache instance configured for Redis.
        """
        return Cache({"cache_seed": cache_seed, "redis_url": redis_url})

    @staticmethod
    def disk(cache_seed: str | int = 42, cache_path_root: str = ".cache") -> Cache:
        """Create a Disk cache instance.

        Args:
            cache_seed (Union[str, int], optional): A seed for the cache. Defaults to 42.
            cache_path_root (str, optional): The root path for the disk cache. Defaults to ".cache".

        Returns:
            Cache: A Cache instance configured for Disk caching.
        """
        return Cache({"cache_seed": cache_seed, "cache_path_root": cache_path_root})

    @staticmethod
    def cosmos_db(
        connection_string: str | None = None,
        container_id: str | None = None,
        cache_seed: str | int = 42,
        client: Any | None = None,
    ) -> Cache:
        """Create a Cosmos DB cache instance with 'autogen_cache' as database ID.

        Args:
            connection_string (str, optional): Connection string to the Cosmos DB account.
            container_id (str, optional): The container ID for the Cosmos DB account.
            cache_seed (Union[str, int], optional): A seed for the cache.
            client: Optional[CosmosClient]: Pass an existing Cosmos DB client.

        Returns:
            Cache: A Cache instance configured for Cosmos DB.
        """
        cosmos_db_config = {
            "connection_string": connection_string,
            "database_id": "autogen_cache",
            "container_id": container_id,
            "client": client,
        }
        return Cache({"cache_seed": str(cache_seed), "cosmos_db_config": cosmos_db_config})

    def __init__(self, config: dict[str, Any]):
        """Initialize the Cache with the given configuration.

        Validates the configuration keys and creates the cache instance.

        Args:
            config (Dict[str, Any]): A dictionary containing the cache configuration.

        Raises:
            ValueError: If an invalid configuration key is provided.
        """
        self.config = config
        # Ensure that the seed is always treated as a string before being passed to any cache factory or stored.
        self.config["cache_seed"] = str(self.config.get("cache_seed", 42))

        # validate config
        for key in self.config:
            if key not in self.ALLOWED_CONFIG_KEYS:
                raise ValueError(f"Invalid config key: {key}")
        # create cache instance
        self.cache = CacheFactory.cache_factory(
            seed=self.config["cache_seed"],
            redis_url=self.config.get("redis_url"),
            cache_path_root=self.config.get("cache_path_root"),
            cosmosdb_config=self.config.get("cosmos_db_config"),
        )

    def __enter__(self) -> Cache:
        """Enter the runtime context related to the cache object.

        Returns:
            The cache instance for use within a context block.
        """
        # Store the previous cache so we can restore it
        self._previous_cache = self.__class__._current_cache.get(None)
        # Set the current cache to this instance
        self._token = self.__class__._current_cache.set(self)
        # Call the underlying cache's __enter__ method
        return self.cache.__enter__()

    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        traceback: TracebackType | None,
    ) -> None:
        """Exit the runtime context related to the cache object.

        Cleans up the cache instance and handles any exceptions that occurred
        within the context.

        Args:
            exc_type: The exception type if an exception was raised in the context.
            exc_value: The exception value if an exception was raised in the context.
            traceback: The traceback if an exception was raised in the context.
        """
        # First exit the underlying cache context
        result = self.cache.__exit__(exc_type, exc_value, traceback)

        try:
            # Then reset the context variable to previous value
            self.__class__._current_cache.reset(self._token)
        except RuntimeError:
            # Token might have been reset by a nested context manager
            # In this case, we just set it back to the previous value
            if self._previous_cache is not None:
                self.__class__._current_cache.set(self._previous_cache)

        return result

    def get(self, key: str, default: Any | None = None) -> Any | None:
        """Retrieve an item from the cache.

        Args:
            key (str): The key identifying the item in the cache.
            default (optional): The default value to return if the key is not found.
                                Defaults to None.

        Returns:
            The value associated with the key if found, else the default value.
        """
        return self.cache.get(key, default)

    def set(self, key: str, value: Any) -> None:
        """Set an item in the cache.

        Args:
            key (str): The key under which the item is to be stored.
            value: The value to be stored in the cache.
        """
        self.cache.set(key, value)

    def close(self) -> None:
        """Close the cache.

        Perform any necessary cleanup, such as closing connections or releasing resources.
        """
        self.cache.close()

    @classmethod
    def get_current_cache(cls, cache: Cache | None = None) -> Cache | None:
        """Get the current cache instance.

        Returns:
            Cache: The current cache instance.
        """
        if cache is not None:
            return cache
        try:
            return cls._current_cache.get()
        except LookupError:
            return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from .abstract_cache_base import AbstractCache
from .cache import Cache

__all__ = ["AbstractCache", "Cache"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from types import TracebackType
from typing import Any, Protocol

from typing_extensions import Self

from ..doc_utils import export_module


@export_module("autogen.cache")
class AbstractCache(Protocol):
    """This protocol defines the basic interface for cache operations.
    Implementing classes should provide concrete implementations for
    these methods to handle caching mechanisms.
    """

    def get(self, key: str, default: Any | None = None) -> Any | None:
        """Retrieve an item from the cache.

        Args:
            key (str): The key identifying the item in the cache.
            default (optional): The default value to return if the key is not found.
                                Defaults to None.

        Returns:
            The value associated with the key if found, else the default value.
        """
        ...

    def set(self, key: str, value: Any) -> None:
        """Set an item in the cache.

        Args:
            key (str): The key under which the item is to be stored.
            value: The value to be stored in the cache.
        """
        ...

    def close(self) -> None:
        """Close the cache. Perform any necessary cleanup, such as closing network connections or
        releasing resources.
        """
        ...

    def __enter__(self) -> Self:
        """Enter the runtime context related to this object.

        The with statement will bind this method's return value to the target(s)
        specified in the as clause of the statement, if any.
        """
        ...

    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        traceback: TracebackType | None,
    ) -> None:
        """Exit the runtime context and close the cache.

        Args:
            exc_type: The exception type if an exception was raised in the context.
            exc_value: The exception value if an exception was raised in the context.
            traceback: The traceback if an exception was raised in the context.
        """
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import logging
import os
from typing import Any

from ..import_utils import optional_import_block
from .abstract_cache_base import AbstractCache
from .disk_cache import DiskCache


class CacheFactory:
    @staticmethod
    def cache_factory(
        seed: str | int,
        redis_url: str | None = None,
        cache_path_root: str = ".cache",
        cosmosdb_config: dict[str, Any] | None = None,
    ) -> AbstractCache:
        """Factory function for creating cache instances.

        This function decides whether to create a RedisCache, DiskCache, or CosmosDBCache instance
        based on the provided parameters. If RedisCache is available and a redis_url is provided,
        a RedisCache instance is created. If connection_string, database_id, and container_id
        are provided, a CosmosDBCache is created. Otherwise, a DiskCache instance is used.

        Args:
            seed (Union[str, int]): Used as a seed or namespace for the cache.
            redis_url (Optional[str]): URL for the Redis server.
            cache_path_root (str): Root path for the disk cache.
            cosmosdb_config (Optional[Dict[str, str]]): Dictionary containing 'connection_string',
                                                       'database_id', and 'container_id' for Cosmos DB cache.

        Returns:
            An instance of RedisCache, DiskCache, or CosmosDBCache.

        Examples:
        Creating a Redis cache

        ```python
        redis_cache = cache_factory("myseed", "redis://localhost:6379/0")
        ```
        Creating a Disk cache

        ```python
        disk_cache = cache_factory("myseed", None)
        ```

        Creating a Cosmos DB cache:
        ```python
        cosmos_cache = cache_factory(
            "myseed",
            cosmosdb_config={
                "connection_string": "your_connection_string",
                "database_id": "your_database_id",
                "container_id": "your_container_id",
            },
        )
        ```

        """
        if redis_url:
            with optional_import_block() as result:
                from .redis_cache import RedisCache

            if result.is_successful:
                return RedisCache(seed, redis_url)
            else:
                logging.warning(
                    "RedisCache is not available. Checking other cache options. The last fallback is DiskCache."
                )

        if cosmosdb_config:
            with optional_import_block() as result:
                from .cosmos_db_cache import CosmosDBCache

            if result.is_successful:
                return CosmosDBCache.create_cache(seed, cosmosdb_config)
            else:
                logging.warning("CosmosDBCache is not available. Fallback to DiskCache.")

        # Default to DiskCache if neither Redis nor Cosmos DB configurations are provided
        path = os.path.join(cache_path_root, str(seed))
        return DiskCache(os.path.join(".", path))
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
# Install Azure Cosmos DB SDK if not already

import pickle
from typing import Any, Optional, TypedDict

from ..import_utils import optional_import_block, require_optional_import
from .abstract_cache_base import AbstractCache

with optional_import_block():
    from azure.cosmos import CosmosClient, PartitionKey
    from azure.cosmos.exceptions import CosmosResourceNotFoundError


@require_optional_import("azure", "cosmosdb")
class CosmosDBConfig(TypedDict, total=False):
    connection_string: str
    database_id: str
    container_id: str
    cache_seed: str | int | None
    client: Optional["CosmosClient"]


@require_optional_import("azure", "cosmosdb")
class CosmosDBCache(AbstractCache):
    """Synchronous implementation of AbstractCache using Azure Cosmos DB NoSQL API.

    This class provides a concrete implementation of the AbstractCache
    interface using Azure Cosmos DB for caching data, with synchronous operations.

    Attributes:
        seed (Union[str, int]): A seed or namespace used as a partition key.
        client (CosmosClient): The Cosmos DB client used for caching.
        container: The container instance used for caching.
    """

    def __init__(self, seed: str | int, cosmosdb_config: CosmosDBConfig):
        """Initialize the CosmosDBCache instance.

        Args:
            seed: A seed or namespace for the cache, used as a partition key.
            cosmosdb_config: The configuration for the Cosmos DB cache.
        """
        self.seed = str(seed)
        self.client = cosmosdb_config.get("client") or CosmosClient.from_connection_string(
            cosmosdb_config["connection_string"]
        )
        database_id = cosmosdb_config.get("database_id", "autogen_cache")
        self.database = self.client.get_database_client(database_id)
        container_id = cosmosdb_config.get("container_id")
        self.container = self.database.create_container_if_not_exists(
            id=container_id, partition_key=PartitionKey(path="/partitionKey")
        )

    @classmethod
    def create_cache(cls, seed: str | int, cosmosdb_config: CosmosDBConfig):
        """Factory method to create a CosmosDBCache instance based on the provided configuration.
        This method decides whether to use an existing CosmosClient or create a new one.
        """
        if "client" in cosmosdb_config and isinstance(cosmosdb_config["client"], CosmosClient):
            return cls.from_existing_client(seed, **cosmosdb_config)
        else:
            return cls.from_config(seed, cosmosdb_config)

    @classmethod
    def from_config(cls, seed: str | int, cosmosdb_config: CosmosDBConfig):
        return cls(str(seed), cosmosdb_config)

    @classmethod
    def from_connection_string(cls, seed: str | int, connection_string: str, database_id: str, container_id: str):
        config = {"connection_string": connection_string, "database_id": database_id, "container_id": container_id}
        return cls(str(seed), config)

    @classmethod
    def from_existing_client(cls, seed: str | int, client: "CosmosClient", database_id: str, container_id: str):
        config = {"client": client, "database_id": database_id, "container_id": container_id}
        return cls(str(seed), config)

    def get(self, key: str, default: Any | None = None) -> Any | None:
        """Retrieve an item from the Cosmos DB cache.

        Args:
            key (str): The key identifying the item in the cache.
            default (optional): The default value to return if the key is not found.

        Returns:
            The deserialized value associated with the key if found, else the default value.
        """
        try:
            response = self.container.read_item(item=key, partition_key=str(self.seed))
            return pickle.loads(response["data"])
        except CosmosResourceNotFoundError:
            return default
        except Exception as e:
            # Log the exception or rethrow after logging if needed
            # Consider logging or handling the error appropriately here
            raise e

    def set(self, key: str, value: Any) -> None:
        """Set an item in the Cosmos DB cache.

        Args:
            key (str): The key under which the item is to be stored.
            value: The value to be stored in the cache.

        Notes:
            The value is serialized using pickle before being stored.
        """
        try:
            serialized_value = pickle.dumps(value)
            item = {"id": key, "partitionKey": str(self.seed), "data": serialized_value}
            self.container.upsert_item(item)
        except Exception as e:
            # Log or handle exception
            raise e

    def close(self) -> None:
        """Close the Cosmos DB client.

        Perform any necessary cleanup, such as closing network connections.
        """
        # CosmosClient doesn"t require explicit close in the current SDK
        # If you created the client inside this class, you should close it if necessary
        pass

    def __enter__(self):
        """Context management entry.

        Returns:
            self: The instance itself.
        """
        return self

    def __exit__(self, exc_type: type | None, exc_value: Exception | None, traceback: Any | None) -> None:
        """Context management exit.

        Perform cleanup actions such as closing the Cosmos DB client.
        """
        self.close()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from types import TracebackType
from typing import Any

from typing_extensions import Self

from .abstract_cache_base import AbstractCache


class InMemoryCache(AbstractCache):
    def __init__(self, seed: str | int = ""):
        self._seed = str(seed)
        self._cache: dict[str, Any] = {}

    def _prefixed_key(self, key: str) -> str:
        separator = "_" if self._seed else ""
        return f"{self._seed}{separator}{key}"

    def get(self, key: str, default: Any | None = None) -> Any | None:
        result = self._cache.get(self._prefixed_key(key))
        if result is None:
            return default
        return result

    def set(self, key: str, value: Any) -> None:
        self._cache[self._prefixed_key(key)] = value

    def close(self) -> None:
        pass

    def __enter__(self) -> Self:
        """Enter the runtime context related to the object.

        Returns:
            self: The instance itself.
        """
        return self

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None
    ) -> None:
        """Exit the runtime context related to the object.

        Args:
            exc_type: The exception type if an exception was raised in the context.
            exc_val: The exception value if an exception was raised in the context.
            exc_tb: The traceback if an exception was raised in the context.
        """
        self.close()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import pickle
from types import TracebackType
from typing import Any

from typing_extensions import Self

from ..import_utils import optional_import_block, require_optional_import
from .abstract_cache_base import AbstractCache

with optional_import_block():
    import redis


@require_optional_import("redis", "redis")
class RedisCache(AbstractCache):
    """Implementation of AbstractCache using the Redis database.

    This class provides a concrete implementation of the AbstractCache
    interface using the Redis database for caching data.

    Attributes:
        seed (Union[str, int]): A seed or namespace used as a prefix for cache keys.
        cache (redis.Redis): The Redis client used for caching.

    Methods:
        __init__(self, seed, redis_url): Initializes the RedisCache with the given seed and Redis URL.
        _prefixed_key(self, key): Internal method to get a namespaced cache key.
        get(self, key, default=None): Retrieves an item from the cache.
        set(self, key, value): Sets an item in the cache.
        close(self): Closes the Redis client.
        __enter__(self): Context management entry.
        __exit__(self, exc_type, exc_value, traceback): Context management exit.
    """

    def __init__(self, seed: str | int, redis_url: str):
        """Initialize the RedisCache instance.

        Args:
            seed (Union[str, int]): A seed or namespace for the cache. This is used as a prefix for all cache keys.
            redis_url (str): The URL for the Redis server.

        """
        self.seed = seed
        self.cache = redis.Redis.from_url(redis_url)

    def _prefixed_key(self, key: str) -> str:
        """Get a namespaced key for the cache.

        Args:
            key (str): The original key.

        Returns:
            str: The namespaced key.
        """
        return f"autogen:{self.seed}:{key}"

    def get(self, key: str, default: Any | None = None) -> Any | None:
        """Retrieve an item from the Redis cache.

        Args:
            key (str): The key identifying the item in the cache.
            default (optional): The default value to return if the key is not found.
                                Defaults to None.

        Returns:
            The deserialized value associated with the key if found, else the default value.
        """
        result = self.cache.get(self._prefixed_key(key))
        if result is None:
            return default
        return pickle.loads(result)

    def set(self, key: str, value: Any) -> None:
        """Set an item in the Redis cache.

        Args:
            key (str): The key under which the item is to be stored.
            value: The value to be stored in the cache.

        Notes:
            The value is serialized using pickle before being stored in Redis.
        """
        serialized_value = pickle.dumps(value)
        self.cache.set(self._prefixed_key(key), serialized_value)

    def close(self) -> None:
        """Close the Redis client.

        Perform any necessary cleanup, such as closing network connections.
        """
        self.cache.close()

    def __enter__(self) -> Self:
        """Enter the runtime context related to the object.

        Returns:
            self: The instance itself.
        """
        return self

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None
    ) -> None:
        """Exit the runtime context related to the object.

        Perform cleanup actions such as closing the Redis client.

        Args:
            exc_type: The exception type if an exception was raised in the context.
            exc_val: The exception value if an exception was raised in the context.
            exc_tb: The traceback if an exception was raised in the context.
        """
        self.close()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import io
import mimetypes
import os
import re
import uuid
from contextlib import suppress
from typing import Any
from urllib.parse import urljoin, urlparse

from .import_utils import optional_import_block, require_optional_import

with optional_import_block():
    import markdownify
    import requests
    from bs4 import BeautifulSoup

# Optional PDF support
with optional_import_block() as result:
    import pdfminer
    import pdfminer.high_level

IS_PDF_CAPABLE = result.is_successful

# Other optional dependencies
with optional_import_block():
    import pathvalidate


@require_optional_import(["markdownify", "requests", "bs4", "pdfminer", "pathvalidate"], "websurfer")
class SimpleTextBrowser:
    """(In preview) An extremely simple text-based web browser comparable to Lynx. Suitable for Agentic use."""

    def __init__(
        self,
        start_page: str | None = None,
        viewport_size: int | None = 1024 * 8,
        downloads_folder: str | None | None = None,
        bing_base_url: str = "https://api.bing.microsoft.com/v7.0/search",
        bing_api_key: str | None | None = None,
        request_kwargs: dict[str, Any] | None | None = None,
    ):
        """Initialize the browser with the given parameters.

        Args:
            start_page (Optional[str], optional): The initial page to load. Defaults to None.
            viewport_size (Optional[int], optional): The number of characters to display per page. Defaults to 1024 * 8.
            downloads_folder (Optional[Union[str, None]], optional): The folder to save downloads to. Defaults to None.
            bing_base_url (str, optional): The base URL for Bing searches. Defaults to "https://api.bing.microsoft.com/v7.0/search".
            bing_api_key (Optional[Union[str, None]], optional): The API key for Bing searches. Defaults to None.
            request_kwargs (Optional[Union[dict[str, Any], None]], optional): Additional keyword arguments to pass to the requests library. Defaults to None.
        """
        self.start_page: str = start_page if start_page else "about:blank"
        self.viewport_size = viewport_size  # Applies only to the standard uri types
        self.downloads_folder = downloads_folder
        self.history: list[str] = []
        self.page_title: str | None = None
        self.viewport_current_page = 0
        self.viewport_pages: list[tuple[int, int]] = []
        self.set_address(self.start_page)
        self.bing_base_url = bing_base_url
        self.bing_api_key = bing_api_key
        self.request_kwargs = request_kwargs

        self._page_content = ""

    @property
    def address(self) -> str:
        """Return the address of the current page."""
        return self.history[-1]

    def set_address(self, uri_or_path: str) -> None:
        """Set the address of the current page.

        Args:
            uri_or_path (str): The URI or path to set as the current page.
        """
        self.history.append(uri_or_path)

        # Handle special URIs
        if uri_or_path == "about:blank":
            self._set_page_content("")
        elif uri_or_path.startswith("bing:"):
            self._bing_search(uri_or_path[len("bing:") :].strip())
        else:
            if not uri_or_path.startswith("http:") and not uri_or_path.startswith("https:"):
                uri_or_path = urljoin(self.address, uri_or_path)
                self.history[-1] = uri_or_path  # Update the address with the fully-qualified path
            self._fetch_page(uri_or_path)

        self.viewport_current_page = 0

    @property
    def viewport(self) -> str:
        """Return the content of the current viewport."""
        bounds = self.viewport_pages[self.viewport_current_page]
        return self.page_content[bounds[0] : bounds[1]]

    @property
    def page_content(self) -> str:
        """Return the full contents of the current page."""
        return self._page_content

    def _set_page_content(self, content: str) -> None:
        """Sets the text content of the current page."""
        self._page_content = content
        self._split_pages()
        if self.viewport_current_page >= len(self.viewport_pages):
            self.viewport_current_page = len(self.viewport_pages) - 1

    def page_down(self) -> None:
        """Move the viewport down by one page."""
        self.viewport_current_page = min(self.viewport_current_page + 1, len(self.viewport_pages) - 1)

    def page_up(self) -> None:
        """Move the viewport up by one page."""
        self.viewport_current_page = max(self.viewport_current_page - 1, 0)

    def visit_page(self, path_or_uri: str) -> str:
        """Update the address, visit the page, and return the content of the viewport.

        Args:
            path_or_uri (str): The URI or path to visit.
        """
        self.set_address(path_or_uri)
        return self.viewport

    def _split_pages(self) -> None:
        # Split only regular pages
        if not self.address.startswith("http:") and not self.address.startswith("https:"):
            self.viewport_pages = [(0, len(self._page_content))]
            return

        # Handle empty pages
        if len(self._page_content) == 0:
            self.viewport_pages = [(0, 0)]
            return

        # Break the viewport into pages
        self.viewport_pages = []
        start_idx = 0
        while start_idx < len(self._page_content):
            end_idx = min(start_idx + self.viewport_size, len(self._page_content))  # type: ignore[operator]
            # Adjust to end on a space
            while end_idx < len(self._page_content) and self._page_content[end_idx - 1] not in [" ", "\t", "\r", "\n"]:
                end_idx += 1
            self.viewport_pages.append((start_idx, end_idx))
            start_idx = end_idx

    def _bing_api_call(self, query: str) -> dict[str, dict[str, list[dict[str, str | dict[str, str]]]]]:
        # Make sure the key was set
        if self.bing_api_key is None:
            raise ValueError("Missing Bing API key.")

        # Prepare the request parameters
        request_kwargs = self.request_kwargs.copy() if self.request_kwargs is not None else {}

        if "headers" not in request_kwargs:
            request_kwargs["headers"] = {}
        request_kwargs["headers"]["Ocp-Apim-Subscription-Key"] = self.bing_api_key

        if "params" not in request_kwargs:
            request_kwargs["params"] = {}
        request_kwargs["params"]["q"] = query
        request_kwargs["params"]["textDecorations"] = False
        request_kwargs["params"]["textFormat"] = "raw"

        request_kwargs["stream"] = False

        # Make the request
        response = requests.get(self.bing_base_url, **request_kwargs)
        response.raise_for_status()
        results = response.json()

        return results  # type: ignore[no-any-return]

    def _bing_search(self, query: str) -> None:
        results = self._bing_api_call(query)

        web_snippets: list[str] = []
        idx = 0
        for page in results["webPages"]["value"]:
            idx += 1
            web_snippets.append(f"{idx}. [{page['name']}]({page['url']})\n{page['snippet']}")
            if "deepLinks" in page:
                for dl in page["deepLinks"]:
                    idx += 1
                    web_snippets.append(
                        f"{idx}. [{dl['name']}]({dl['url']})\n{dl.get('snippet', '')}"  # type: ignore[index]
                    )

        news_snippets = []
        if "news" in results:
            for page in results["news"]["value"]:
                idx += 1
                news_snippets.append(f"{idx}. [{page['name']}]({page['url']})\n{page['description']}")

        self.page_title = f"{query} - Search"

        content = (
            f"A Bing search for '{query}' found {len(web_snippets) + len(news_snippets)} results:\n\n## Web Results\n"
            + "\n\n".join(web_snippets)
        )
        if len(news_snippets) > 0:
            content += "\n\n## News Results:\n" + "\n\n".join(news_snippets)
        self._set_page_content(content)

    def _fetch_page(self, url: str) -> None:
        try:
            # Prepare the request parameters
            request_kwargs = self.request_kwargs.copy() if self.request_kwargs is not None else {}
            request_kwargs["stream"] = True

            # Send a HTTP request to the URL
            response = requests.get(url, **request_kwargs)
            response.raise_for_status()

            # If the HTTP request returns a status code 200, proceed
            if response.status_code == 200:
                content_type = response.headers.get("content-type", "")
                for ct in ["text/html", "text/plain", "application/pdf"]:
                    if ct in content_type.lower():
                        content_type = ct
                        break

                if content_type == "text/html":
                    # Get the content of the response
                    html = ""
                    for chunk in response.iter_content(chunk_size=512, decode_unicode=True):
                        html += chunk

                    soup = BeautifulSoup(html, "html.parser")

                    # Remove javascript and style blocks
                    for script in soup(["script", "style"]):
                        script.extract()

                    # Convert to markdown -- Wikipedia gets special attention to get a clean version of the page
                    if url.startswith("https://en.wikipedia.org/"):
                        body_elm = soup.find("div", {"id": "mw-content-text"})
                        title_elm = soup.find("span", {"class": "mw-page-title-main"})

                        if body_elm:
                            # What's the title
                            main_title = soup.title.string
                            if title_elm and len(title_elm) > 0:
                                main_title = title_elm.string
                            webpage_text = (
                                "# " + main_title + "\n\n" + markdownify.MarkdownConverter().convert_soup(body_elm)
                            )
                        else:
                            webpage_text = markdownify.MarkdownConverter().convert_soup(soup)
                    else:
                        webpage_text = markdownify.MarkdownConverter().convert_soup(soup)

                    # Convert newlines
                    webpage_text = re.sub(r"\r\n", "\n", webpage_text)

                    # Remove excessive blank lines
                    self.page_title = soup.title.string
                    self._set_page_content(re.sub(r"\n{2,}", "\n\n", webpage_text).strip())
                elif content_type == "text/plain":
                    # Get the content of the response
                    plain_text = ""
                    for chunk in response.iter_content(chunk_size=512, decode_unicode=True):
                        plain_text += chunk

                    self.page_title = None
                    self._set_page_content(plain_text)
                elif IS_PDF_CAPABLE and content_type == "application/pdf":
                    pdf_data = io.BytesIO(response.raw.read())
                    self.page_title = None
                    self._set_page_content(pdfminer.high_level.extract_text(pdf_data))
                elif self.downloads_folder is not None:
                    # Try producing a safe filename
                    fname = None
                    with suppress(NameError):
                        fname = pathvalidate.sanitize_filename(os.path.basename(urlparse(url).path)).strip()

                    # No suitable name, so make one
                    if fname is None:
                        extension = mimetypes.guess_extension(content_type)
                        if extension is None:
                            extension = ".download"
                        fname = str(uuid.uuid4()) + extension

                    # Open a file for writing
                    download_path = os.path.abspath(os.path.join(self.downloads_folder, fname))
                    with open(download_path, "wb") as fh:
                        for chunk in response.iter_content(chunk_size=512):
                            fh.write(chunk)

                    # Return a page describing what just happened
                    self.page_title = "Download complete."
                    self._set_page_content(f"Downloaded '{url}' to '{download_path}'.")
                else:
                    self.page_title = f"Error - Unsupported Content-Type '{content_type}'"
                    self._set_page_content(self.page_title)
            else:
                self.page_title = "Error"
                self._set_page_content("Failed to retrieve " + url)
        except requests.exceptions.RequestException as e:
            self.page_title = "Error"
            self._set_page_content(str(e))
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
import subprocess
import sys
import tempfile
from typing import Any

from anyio import to_thread

from .python_environment import PythonEnvironment

__all__ = ["VenvPythonEnvironment"]


class VenvPythonEnvironment(PythonEnvironment):
    """A Python environment using a virtual environment (venv)."""

    def __init__(
        self,
        python_version: str | None = None,
        python_path: str | None = None,
        venv_path: str | None = None,
    ):
        """Initialize a virtual environment for Python execution.

        If you pass in a venv_path the path will be checked for a valid venv. If the venv doesn't exist it will be created using the python_version or python_path provided.

        If the python_version or python_path is provided and the venv_path is not, a temporary directory will be created for venv and it will be setup with the provided python version.

        If python_path is provided, it will take precedence over python_version.

        The python version will not be installed if it doesn't exist and a RuntimeError will be raised.

        Args:
            python_version: The Python version to use (e.g., "3.11"), otherwise defaults to the current executing Python version. Ignored if venv_path is provided and has a valid environment already.
            python_path: Optional direct path to a Python executable to use (must include the executable). Takes precedence over python_version if both are provided.
            venv_path: Optional path for the virtual environment, will create it if it doesn't exist. If None, creates a temp directory.
        """
        self.python_version = python_version
        self.python_path = python_path
        self.venv_path = venv_path
        self.created_venv = False
        self._executable = None
        super().__init__()

    def _setup_environment(self) -> None:
        """Set up the virtual environment."""
        # Create a venv directory if not provided
        if self.venv_path is None:
            self.venv_path = tempfile.mkdtemp(prefix="ag2_python_env_")
            self.created_venv = True

            # Determine the python version, getting it from the venv if it already has one
            base_python = self._get_python_executable_for_version()
            needs_creation = True
        else:
            # If venv_path is provided, check if it's already a valid venv
            if os.name == "nt":  # Windows
                venv_python = os.path.join(self.venv_path, "Scripts", "python.exe")
            else:  # Unix-like (Mac/Linux)
                venv_python = os.path.join(self.venv_path, "bin", "python")

            if os.path.exists(venv_python) and os.access(venv_python, os.X_OK):
                # Valid venv already exists, just use it
                self._executable = venv_python
                logging.info(f"Using existing virtual environment at {self.venv_path}")
                needs_creation = False
            else:
                # Path exists but not a valid venv, or doesn't exist
                if not os.path.exists(self.venv_path):
                    os.makedirs(self.venv_path, exist_ok=True)
                self.created_venv = True
                base_python = sys.executable
                needs_creation = True

        # Only create the venv if needed
        if needs_creation:
            logging.info(f"Creating virtual environment at {self.venv_path} using {base_python}")

            try:
                # Create the virtual environment
                _ = subprocess.run(
                    [base_python, "-m", "venv", "--system-site-packages", self.venv_path],
                    check=True,
                    capture_output=True,
                    text=True,
                )

                # Determine the Python executable path
                if os.name == "nt":  # Windows
                    self._executable = os.path.join(self.venv_path, "Scripts", "python.exe")
                else:  # Unix-like (Mac/Linux)
                    self._executable = os.path.join(self.venv_path, "bin", "python")

                # Verify the executable exists
                if not os.path.exists(self._executable):
                    raise RuntimeError(
                        f"Virtual environment created but Python executable not found at {self._executable}"
                    )

            except subprocess.CalledProcessError as e:
                raise RuntimeError(f"Failed to create virtual environment: {e.stderr}") from e

    def _cleanup_environment(self) -> None:
        """Clean up the virtual environment."""
        # Note: We intentionally don't clean up the venv here to allow
        # tools to continue using it after the context exits.
        pass

    def get_executable(self) -> str:
        """Get the path to the Python executable in the virtual environment."""
        if not self._executable or not os.path.exists(self._executable):
            raise RuntimeError("Virtual environment Python executable not found")
        return self._executable

    async def execute_code(self, code: str, script_path: str, timeout: int = 30) -> dict[str, Any]:
        """Execute code in the virtual environment."""
        try:
            # Get the Python executable
            python_executable = self.get_executable()

            # Verify the executable exists
            if not os.path.exists(python_executable):
                return {"success": False, "error": f"Python executable not found at {python_executable}"}

            # Ensure the directory for the script exists
            script_dir = os.path.dirname(script_path)
            if script_dir:
                os.makedirs(script_dir, exist_ok=True)

            # Write the code to the script file using anyio.to_thread.run_sync (from base class)
            await to_thread.run_sync(self._write_to_file, script_path, code)

            logging.info(f"Wrote code to {script_path}")

            try:
                # Execute directly with subprocess using anyio.to_thread.run_sync for better reliability
                result = await to_thread.run_sync(self._run_subprocess, [python_executable, script_path], timeout)

                # Main execution result
                return {
                    "success": result.returncode == 0,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "returncode": result.returncode,
                }
            except subprocess.TimeoutExpired:
                return {"success": False, "error": f"Execution timed out after {timeout} seconds"}

        except Exception as e:
            return {"success": False, "error": f"Execution error: {str(e)}"}

    def _get_python_executable_for_version(self) -> str:
        """Get the Python executable for the specified version and verify it can create a venv."""
        # If a specific path is provided, use it directly
        if self.python_path:
            if not os.path.exists(self.python_path) or not os.access(self.python_path, os.X_OK):
                raise RuntimeError(f"Python executable not found at {self.python_path}")
            return self.python_path

        # If no specific version is requested, use the current Python
        if not self.python_version:
            return sys.executable

        potential_executables = []

        # Try to find a specific Python version using pyenv if available
        try:
            pyenv_result = subprocess.run(
                ["pyenv", "which", f"python{self.python_version}"],
                check=True,
                capture_output=True,
                text=True,
            )
            potential_executables.append(pyenv_result.stdout.strip())
        except (subprocess.SubprocessError, FileNotFoundError):
            pass

        # Try common system paths based on platform
        if os.name == "nt":  # Windows
            potential_executables.extend([
                f"C:\\Python{self.python_version.replace('.', '')}\\python.exe",
                f"C:\\Program Files\\Python{self.python_version.replace('.', '')}\\python.exe",
                f"C:\\Program Files (x86)\\Python{self.python_version.replace('.', '')}\\python.exe",
            ])
        else:  # Unix-like (Mac and Linux)
            # Add more paths that might exist on macOS
            potential_executables.extend([
                f"/usr/bin/python{self.python_version}",
                f"/usr/local/bin/python{self.python_version}",
                f"/opt/homebrew/bin/python{self.python_version}",  # Homebrew on Apple Silicon
                f"/opt/python/bin/python{self.python_version}",
            ])

        # Try each potential path and verify it can create a venv
        for path in potential_executables:
            if os.path.exists(path) and os.access(path, os.X_OK):
                # Verify this Python can create a venv
                try:
                    test_result = subprocess.run(
                        [path, "-m", "venv", "--help"],
                        check=False,  # Don't raise exception
                        capture_output=True,
                        text=True,
                        timeout=5,  # Add timeout for safety
                    )
                    if test_result.returncode == 0:
                        # Successfully found a valid Python executable
                        return path
                except (subprocess.SubprocessError, FileNotFoundError):
                    continue

        # If we couldn't find the specified version, raise an exception
        raise RuntimeError(
            f"Python {self.python_version} not found or cannot create virtual environments. Provide a python_path to use a specific Python executable."
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .docker_python_environment import DockerPythonEnvironment
from .system_python_environment import SystemPythonEnvironment
from .venv_python_environment import VenvPythonEnvironment
from .working_directory import WorkingDirectory

__all__ = ["DockerPythonEnvironment", "SystemPythonEnvironment", "VenvPythonEnvironment", "WorkingDirectory"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
import shutil
import subprocess
import tempfile
import uuid
from typing import Any

from anyio import to_thread

from .python_environment import PythonEnvironment

__all__ = ["DockerPythonEnvironment"]


class DockerPythonEnvironment(PythonEnvironment):
    """A Python environment using Docker containers for isolated execution."""

    def __init__(
        self,
        image: str = "python:3.11-slim",
        container_name_prefix: str = "ag2_docker_env_",
        volumes: dict[str, str] | None = None,
        environment: dict[str, str] | None = None,
        network: str | None = None,
        pip_packages: list[str] | None = None,
        requirements_file: str | None = None,
        dockerfile: str | None = None,
        build_args: dict[str, str] | None = None,
        cleanup_container: bool = True,
        keep_container_running: bool = False,
        container_startup_timeout: int = 30,
    ):
        """Initialize a Docker Python environment.

        Args:
            image: Docker image to use (ignored if dockerfile is provided)
            container_name_prefix: Prefix for container names
            volumes: Dictionary mapping host paths to container paths for mounting
            environment: Dictionary of environment variables to set in the container
            network: Docker network to attach the container to
            pip_packages: List of pip packages to install in the container
            requirements_file: Path to requirements.txt file to install in the container
            dockerfile: Optional path to a Dockerfile to build and use instead of pulling an image
            build_args: Optional build arguments for the Dockerfile
            cleanup_container: Whether to remove the container after use
            keep_container_running: Whether to keep the container running after execution
            container_startup_timeout: Timeout in seconds for container startup
        """
        self.image = image
        self.container_name_prefix = container_name_prefix
        self.volumes = volumes or {}
        self.environment = environment or {}
        self.network = network
        self.pip_packages = pip_packages or []
        self.requirements_file = requirements_file
        self.dockerfile = dockerfile
        self.build_args = build_args or {}
        self.cleanup_container = cleanup_container
        self.keep_container_running = keep_container_running
        self.container_startup_timeout = container_startup_timeout

        # Internal state
        self._container_id = None
        self._container_name = None
        self._custom_image_name = None
        self._temp_dir = None

        super().__init__()

    def _setup_environment(self) -> None:
        """Set up the Docker environment."""
        # Verify Docker is installed and accessible
        try:
            result = subprocess.run(["docker", "--version"], capture_output=True, text=True, check=True)
            logging.info(f"Docker version: {result.stdout.strip()}")
        except (subprocess.SubprocessError, FileNotFoundError) as e:
            raise RuntimeError(
                "Docker not found or not accessible. Please ensure Docker is installed and running."
            ) from e

        # Create a temporary directory for file operations
        self._temp_dir = tempfile.mkdtemp(prefix="ag2_docker_")

        # Generate a unique container name
        self._container_name = f"{self.container_name_prefix}{uuid.uuid4().hex[:8]}"

        # Build custom image if Dockerfile is provided
        if self.dockerfile:
            self._build_custom_image()
        else:
            # Pull the specified image
            try:
                subprocess.run(
                    ["docker", "pull", self.image],
                    check=True,
                    capture_output=True,
                    text=True,
                )
                logging.info(f"Pulled Docker image: {self.image}")
            except subprocess.CalledProcessError as e:
                raise RuntimeError(f"Failed to pull Docker image: {e.stderr}") from e

        # Start the container
        self._start_container()

    def _build_custom_image(self) -> None:
        """Build a custom Docker image from the provided Dockerfile."""
        if not os.path.exists(self.dockerfile):
            raise RuntimeError(f"Dockerfile not found at: {self.dockerfile}")

        # Create a unique image name
        self._custom_image_name = f"ag2-custom-python-{uuid.uuid4().hex[:8]}"

        # Build command
        build_cmd = ["docker", "build", "-t", self._custom_image_name]

        # Add build args
        for arg_name, arg_value in self.build_args.items():
            build_cmd.extend(["--build-arg", f"{arg_name}={arg_value}"])

        # Add Dockerfile path
        build_cmd.extend(["-f", self.dockerfile, os.path.dirname(self.dockerfile)])

        try:
            logging.info(f"Building custom Docker image: {self._custom_image_name}")
            _ = subprocess.run(
                build_cmd,
                check=True,
                capture_output=True,
                text=True,
            )
            logging.info(f"Built custom Docker image: {self._custom_image_name}")
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Failed to build Docker image: {e.stderr}") from e

        # Use the custom image
        self.image = self._custom_image_name

    def _start_container(self) -> None:
        """Start the Docker container."""
        # Basic container run command
        run_cmd = ["docker", "run", "--name", self._container_name]

        # Add detached mode flag to run container in background
        run_cmd.append("-d")

        # Add network if specified
        if self.network:
            run_cmd.extend(["--network", self.network])

        # Add environment variables
        for env_name, env_value in self.environment.items():
            run_cmd.extend(["-e", f"{env_name}={env_value}"])

        # Add volume mounts including temp directory
        work_dir_mount = f"{self._temp_dir}:/workspace"
        run_cmd.extend(["-v", work_dir_mount])

        for host_path, container_path in self.volumes.items():
            run_cmd.extend(["-v", f"{host_path}:{container_path}"])

        # Set workspace as working directory
        run_cmd.extend(["-w", "/workspace"])

        # Add tty to keep container running
        run_cmd.append("-t")

        # Add image name
        run_cmd.append(self.image)

        # Initial command to keep container running
        run_cmd.extend(["tail", "-f", "/dev/null"])

        try:
            # Start the container
            logging.info(f"Starting Docker container: {self._container_name}")
            result = subprocess.run(
                run_cmd,
                check=True,
                capture_output=True,
                text=True,
            )

            # Get container ID
            self._container_id = result.stdout.strip()
            logging.info(f"Started Docker container: {self._container_name} ({self._container_id})")

            # Install pip packages if specified
            if self.pip_packages or self.requirements_file:
                self._install_packages()

        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Failed to start Docker container: {e.stderr}") from e

    def _install_packages(self) -> None:
        """Install Python packages in the running container."""
        # Install pip packages
        if self.pip_packages:
            packages_str = " ".join(self.pip_packages)
            try:
                logging.info(f"Installing pip packages: {packages_str}")
                _ = subprocess.run(
                    ["docker", "exec", self._container_name, "pip", "install", "--no-cache-dir"] + self.pip_packages,
                    check=True,
                    capture_output=True,
                    text=True,
                )
                logging.info("Successfully installed pip packages")
            except subprocess.CalledProcessError as e:
                logging.warning(f"Failed to install pip packages: {e.stderr}")

        # Install from requirements file
        if self.requirements_file:
            if os.path.exists(self.requirements_file):
                # Copy requirements file to temp directory
                req_filename = os.path.basename(self.requirements_file)
                temp_req_path = os.path.join(self._temp_dir, req_filename)
                shutil.copy(self.requirements_file, temp_req_path)

                try:
                    logging.info(f"Installing requirements from: {req_filename}")
                    _ = subprocess.run(
                        [
                            "docker",
                            "exec",
                            self._container_name,
                            "pip",
                            "install",
                            "--no-cache-dir",
                            "-r",
                            f"/workspace/{req_filename}",
                        ],
                        check=True,
                        capture_output=True,
                        text=True,
                    )
                    logging.info("Successfully installed requirements")
                except subprocess.CalledProcessError as e:
                    logging.warning(f"Failed to install requirements: {e.stderr}")
            else:
                logging.warning(f"Requirements file not found: {self.requirements_file}")

    def _cleanup_environment(self) -> None:
        """Clean up the Docker environment."""
        if self._container_id:
            # Stop the container if it's running and we want to clean it up
            if not self.keep_container_running:
                try:
                    logging.info(f"Stopping Docker container: {self._container_name}")
                    subprocess.run(
                        ["docker", "stop", self._container_name],
                        check=True,
                        capture_output=True,
                        text=True,
                    )
                except subprocess.CalledProcessError:
                    logging.warning(f"Failed to stop Docker container: {self._container_name}")

            # Remove the container if cleanup is enabled
            if self.cleanup_container and not self.keep_container_running:
                try:
                    logging.info(f"Removing Docker container: {self._container_name}")
                    subprocess.run(
                        ["docker", "rm", "-f", self._container_name],
                        check=True,
                        capture_output=True,
                        text=True,
                    )
                except subprocess.CalledProcessError:
                    logging.warning(f"Failed to remove Docker container: {self._container_name}")

            # Remove the custom image if it was created
            if self._custom_image_name and self.cleanup_container:
                try:
                    logging.info(f"Removing custom Docker image: {self._custom_image_name}")
                    subprocess.run(
                        ["docker", "rmi", self._custom_image_name],
                        check=True,
                        capture_output=True,
                        text=True,
                    )
                except subprocess.CalledProcessError:
                    logging.warning(f"Failed to remove custom Docker image: {self._custom_image_name}")

        # Clean up the temporary directory
        if self._temp_dir and os.path.exists(self._temp_dir):
            try:
                shutil.rmtree(self._temp_dir)
            except Exception as e:
                logging.warning(f"Failed to remove temporary directory: {e}")

    def get_executable(self) -> str:
        """Get the path to the Python executable in the Docker container."""
        # This is a virtual path in the container
        return "python"

    async def execute_code(self, code: str, script_path: str, timeout: int = 30) -> dict[str, Any]:
        """Execute code in the Docker container."""
        # Ensure the container is running
        if not self._container_id:
            return {"success": False, "error": "Docker container not started"}

        try:
            # Calculate the relative path within the temp directory
            if os.path.isabs(script_path):
                rel_path = os.path.basename(script_path)
                host_script_path = os.path.join(self._temp_dir, rel_path)
            else:
                rel_path = script_path
                host_script_path = os.path.join(self._temp_dir, rel_path)

            # Ensure the directory for the script exists
            script_dir = os.path.dirname(host_script_path)
            if script_dir:
                os.makedirs(script_dir, exist_ok=True)

            # Write the code to the script file on the host
            await to_thread.run_sync(self._write_to_file, host_script_path, code)

            # Path to the script in the container
            container_script_path = f"/workspace/{rel_path}"

            # Execute the script in the container
            exec_cmd = ["docker", "exec", self._container_name, "python", container_script_path]

            # Run the command with a timeout
            result = await to_thread.run_sync(self._run_subprocess_with_timeout, exec_cmd, timeout)

            return {
                "success": result[0],
                "stdout": result[1],
                "stderr": result[2],
                "returncode": result[3] if result[0] else 1,
            }

        except Exception as e:
            return {"success": False, "error": f"Execution error: {str(e)}"}

    def _run_subprocess_with_timeout(self, cmd: list[str], timeout: int) -> tuple[bool, str, str, int]:
        """Run a subprocess with timeout and return status, stdout, stderr, and return code.

        Args:
            cmd: Command to run as a list of strings
            timeout: Maximum execution time in seconds

        Returns:
            Tuple of (success, stdout, stderr, return_code)
        """
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
            )
            return (result.returncode == 0, result.stdout, result.stderr, result.returncode)
        except subprocess.TimeoutExpired:
            return (False, "", f"Execution timed out after {timeout} seconds", -1)
        except Exception as e:
            return (False, "", str(e), -1)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import subprocess
from abc import ABC, abstractmethod
from contextvars import ContextVar
from typing import Any, Optional

__all__ = ["PythonEnvironment"]


class PythonEnvironment(ABC):
    """Python execution environments base class"""

    # Shared context variable for tracking the current environment
    _current_python_environment: ContextVar["PythonEnvironment"] = ContextVar("_current_python_environment")

    def __init__(self):
        """Initialize the Python environment."""
        self._token = None

        # Set up the environment
        self._setup_environment()

    def __enter__(self):
        """Enter the environment context.
        Sets this environment as the current one.
        """
        # Set this as the current Python environment in the context
        self._token = PythonEnvironment._current_python_environment.set(self)

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the environment context.
        Resets the current environment and performs cleanup.
        """
        # Reset the context variable if this was the active environment
        if self._token is not None:
            PythonEnvironment._current_python_environment.reset(self._token)
            self._token = None

        # Clean up resources
        self._cleanup_environment()

    @abstractmethod
    def _setup_environment(self) -> None:
        """Set up the Python environment. Called by __enter__."""
        pass

    @abstractmethod
    def _cleanup_environment(self) -> None:
        """Clean up the Python environment. Called by __exit__."""
        pass

    @abstractmethod
    def get_executable(self) -> str:
        """Get the path to the Python executable in this environment.

        Returns:
            The full path to the Python executable.
        """
        pass

    @abstractmethod
    async def execute_code(self, code: str, script_path: str, timeout: int = 30) -> dict[str, Any]:
        """Execute the given code in this environment.

        Args:
            code: The Python code to execute.
            script_path: Path where the code should be saved before execution.
            timeout: Maximum execution time in seconds.

        Returns:
            dict with execution results including stdout, stderr, and success status.
        """
        pass

    # Utility method for subclasses to wrap (for async support)
    def _write_to_file(self, script_path: str, content: str) -> None:
        """Write content to a file (blocking operation).

        This is a helper method for use with asyncify in async contexts.

        Args:
            script_path: Path to the file to write.
            content: Content to write to the file.
        """
        with open(script_path, "w") as f:
            f.write(content)

    # Utility method for subclasses to wrap (for async support)
    def _run_subprocess(self, cmd: list[str], timeout: int) -> subprocess.CompletedProcess:
        """Run a subprocess (blocking operation).

        This is a helper method for use with asyncify in async contexts.

        Args:
            cmd: Command to run as a list of strings.
            timeout: Maximum execution time in seconds.

        Returns:
            CompletedProcess instance with results of the subprocess.
        """
        return subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)

    @classmethod
    def get_current_python_environment(
        cls, python_environment: Optional["PythonEnvironment"] = None
    ) -> Optional["PythonEnvironment"]:
        """Get the current Python environment or the specified one if provided.

        Args:
            python_environment: Optional environment to return if specified.

        Returns:
            The current Python environment or None if none is active.
        """
        if python_environment is not None:
            return python_environment
        try:
            return cls._current_python_environment.get()
        except LookupError:
            return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import contextlib
import os
import shutil
import tempfile
from contextvars import ContextVar
from typing import Optional

__all__ = ["WorkingDirectory"]


class WorkingDirectory:
    """Context manager for changing the current working directory."""

    _current_working_directory: ContextVar["WorkingDirectory"] = ContextVar("_current_working_directory")

    def __init__(self, path: str):
        """Initialize with a directory path.

        Args:
            path: The directory path to change to.
        """
        self.path = path
        self.original_path = None
        self.created_tmp = False
        self._token = None

    def __enter__(self):
        """Change to the specified directory and return self."""
        self.original_path = os.getcwd()
        if self.path:
            os.makedirs(self.path, exist_ok=True)
            os.chdir(self.path)

        # Set this as the current working directory in the context
        self._token = WorkingDirectory._current_working_directory.set(self)

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Change back to the original directory and clean up if necessary."""
        # Reset the context variable if this was the active working directory
        if self._token is not None:
            WorkingDirectory._current_working_directory.reset(self._token)
            self._token = None

        if self.original_path:
            os.chdir(self.original_path)
        if self.created_tmp and self.path and os.path.exists(self.path):
            with contextlib.suppress(Exception):
                shutil.rmtree(self.path)

    @classmethod
    def create_tmp(cls):
        """Create a temporary directory and return a WorkingDirectory instance for it."""
        tmp_dir = tempfile.mkdtemp(prefix="ag2_work_dir_")
        instance = cls(tmp_dir)
        instance.created_tmp = True
        return instance

    @classmethod
    def get_current_working_directory(
        cls, working_directory: Optional["WorkingDirectory"] = None
    ) -> Optional["WorkingDirectory"]:
        """Get the current working directory or the specified one if provided."""
        if working_directory is not None:
            return working_directory
        try:
            return cls._current_working_directory.get()
        except LookupError:
            return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
import subprocess
import sys
from typing import Any

from anyio import to_thread

from .python_environment import PythonEnvironment

__all__ = ["SystemPythonEnvironment"]


class SystemPythonEnvironment(PythonEnvironment):
    """A Python environment using the system's Python installation."""

    def __init__(
        self,
        executable: str | None = None,
    ):
        """Initialize a system Python environment.

        Args:
            executable: Optional path to a specific Python executable. If None, uses the current Python executable.
        """
        self._executable = executable or sys.executable
        super().__init__()

    def _setup_environment(self) -> None:
        """Set up the system Python environment."""
        # Verify the Python executable exists
        if not os.path.exists(self._executable):
            raise RuntimeError(f"Python executable not found at: {self._executable}")

        logging.info(f"Using system Python at: {self._executable}")

    def _cleanup_environment(self) -> None:
        """Clean up the system Python environment."""
        # No cleanup needed for system Python
        pass

    def get_executable(self) -> str:
        """Get the path to the Python executable."""
        return self._executable

    async def execute_code(self, code: str, script_path: str, timeout: int = 30) -> dict[str, Any]:
        """Execute code using the system Python."""
        try:
            # Get the Python executable
            python_executable = self.get_executable()

            # Verify the executable exists
            if not os.path.exists(python_executable):
                return {"success": False, "error": f"Python executable not found at {python_executable}"}

            # Ensure the directory for the script exists
            script_dir = os.path.dirname(script_path)
            if script_dir:
                os.makedirs(script_dir, exist_ok=True)

            # Write the code to the script file using anyio.to_thread.run_sync (from base class)
            await to_thread.run_sync(self._write_to_file, script_path, code)

            logging.info(f"Wrote code to {script_path}")

            try:
                # Execute directly with subprocess using anyio.to_thread.run_sync for better reliability
                result = await to_thread.run_sync(self._run_subprocess, [python_executable, script_path], timeout)

                # Main execution result
                return {
                    "success": result.returncode == 0,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "returncode": result.returncode,
                }
            except subprocess.TimeoutExpired:
                return {"success": False, "error": f"Execution timed out after {timeout} seconds"}

        except Exception as e:
            return {"success": False, "error": f"Execution error: {str(e)}"}
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import asyncio
import copy
import functools
import inspect
import json
import logging
import re
import threading
import warnings
from collections import defaultdict
from collections.abc import Callable, Generator, Iterable
from contextlib import contextmanager
from dataclasses import dataclass
from inspect import signature
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
    Optional,
    TypeVar,
    Union,
)

from ..cache.cache import AbstractCache, Cache
from ..code_utils import (
    PYTHON_VARIANTS,
    UNKNOWN,
    check_can_use_docker_or_throw,
    content_str,
    decide_use_docker,
    execute_code,
    extract_code,
    infer_lang,
)
from ..coding.base import CodeExecutor
from ..coding.factory import CodeExecutorFactory
from ..doc_utils import export_module
from ..events.agent_events import (
    ClearConversableAgentHistoryEvent,
    ClearConversableAgentHistoryWarningEvent,
    ConversableAgentUsageSummaryEvent,
    ConversableAgentUsageSummaryNoCostIncurredEvent,
    ErrorEvent,
    ExecuteCodeBlockEvent,
    ExecuteFunctionEvent,
    ExecutedFunctionEvent,
    GenerateCodeExecutionReplyEvent,
    PostCarryoverProcessingEvent,
    RunCompletionEvent,
    TerminationAndHumanReplyNoInputEvent,
    TerminationEvent,
    UsingAutoReplyEvent,
    create_received_event_model,
)
from ..exception_utils import InvalidCarryOverTypeError, SenderRequiredError
from ..fast_depends.utils import is_coroutine_callable
from ..io.base import IOStream
from ..io.run_response import AsyncRunResponse, AsyncRunResponseProtocol, RunResponse, RunResponseProtocol
from ..io.thread_io_stream import AsyncThreadIOStream, ThreadIOStream
from ..llm_config import LLMConfig
from ..llm_config.client import ModelClient
from ..oai.client import OpenAIWrapper
from ..runtime_logging import log_event, log_function_use, log_new_agent, logging_enabled
from ..tools import ChatContext, Tool, load_basemodels_if_needed, serialize_to_str
from .agent import Agent, LLMAgent
from .chat import (
    ChatResult,
    _post_process_carryover_item,
    _validate_recipients,
    a_initiate_chats,
    initiate_chats,
)
from .group.context_variables import ContextVariables
from .group.guardrails import Guardrail
from .group.handoffs import Handoffs
from .utils import consolidate_chat_info, gather_usage_summary

if TYPE_CHECKING:
    from .group.on_condition import OnCondition
    from .group.on_context_condition import OnContextCondition
__all__ = ("ConversableAgent",)

logger = logging.getLogger(__name__)

F = TypeVar("F", bound=Callable[..., Any])


@dataclass
@export_module("autogen")
class UpdateSystemMessage:
    """Update the agent's system message before they reply

    Args:
        content_updater: The format string or function to update the agent's system message. Can be a format string or a Callable.
            If a string, it will be used as a template and substitute the context variables.
            If a Callable, it should have the signature:
                def my_content_updater(agent: ConversableAgent, messages: List[Dict[str, Any]]) -> str
    """

    content_updater: Callable | str

    def __post_init__(self):
        if isinstance(self.content_updater, str):
            # find all {var} in the string
            vars = re.findall(r"\{(\w+)\}", self.content_updater)
            if len(vars) == 0:
                warnings.warn("Update function string contains no variables. This is probably unintended.")

        elif isinstance(self.content_updater, Callable):
            sig = signature(self.content_updater)
            if len(sig.parameters) != 2:
                raise ValueError(
                    "The update function must accept two parameters of type ConversableAgent and List[Dict[str, Any]], respectively"
                )
            if sig.return_annotation != str:
                raise ValueError("The update function must return a string")
        else:
            raise ValueError("The update function must be either a string or a callable")


@export_module("autogen")
class ConversableAgent(LLMAgent):
    """(In preview) A class for generic conversable agents which can be configured as assistant or user proxy.

    After receiving each message, the agent will send a reply to the sender unless the msg is a termination msg.
    For example, AssistantAgent and UserProxyAgent are subclasses of this class,
    configured with different default settings.

    To modify auto reply, override `generate_reply` method. \n
    To disable/enable human response in every turn, set `human_input_mode` to "NEVER" or "ALWAYS". \n
    To modify the way to get human input, override `get_human_input` method. \n
    To modify the way to execute code blocks, single code block, or function call, override `execute_code_blocks`, \n
    `run_code`, and `execute_function` methods respectively. \n
    """

    DEFAULT_CONFIG = False  # False or dict, the default config for llm inference
    MAX_CONSECUTIVE_AUTO_REPLY = 100  # maximum number of consecutive auto replies (subject to future change)

    DEFAULT_SUMMARY_PROMPT = "Summarize the takeaway from the conversation. Do not add any introductory phrases."
    DEFAULT_SUMMARY_METHOD = "last_msg"
    llm_config: LLMConfig | Literal[False]

    def __init__(
        self,
        name: str,
        system_message: str | list | None = "You are a helpful AI Assistant.",
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        max_consecutive_auto_reply: int | None = None,
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "TERMINATE",
        function_map: dict[str, Callable[..., Any]] | None = None,
        code_execution_config: dict[str, Any] | Literal[False] = False,
        llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None,
        default_auto_reply: str | dict[str, Any] = "",
        description: str | None = None,
        chat_messages: dict[Agent, list[dict[str, Any]]] | None = None,
        silent: bool | None = None,
        context_variables: Optional["ContextVariables"] = None,
        functions: list[Callable[..., Any]] | Callable[..., Any] = None,
        update_agent_state_before_reply: list[Callable | UpdateSystemMessage]
        | Callable
        | UpdateSystemMessage
        | None = None,
        handoffs: Handoffs | None = None,
    ):
        """Args:\n
        1) name (str): name of the agent.\n
        2) system_message (str or list): system message for the ChatCompletion inference.\n
        3) is_termination_msg (function): a function that takes a message in the form of a dictionary
            and returns a boolean value indicating if this received message is a termination message.
            The dict can contain the following keys: "content", "role", "name", "function_call".\n
        4) max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.
            default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).
            When set to 0, no auto reply will be generated.\n
        5) human_input_mode (str): whether to ask for human inputs every time a message is received.\n
            Possible values are "ALWAYS", "TERMINATE", "NEVER".\n
            (1) When "ALWAYS", the agent prompts for human input every time a message is received.
                Under this mode, the conversation stops when the human input is "exit",
                or when is_termination_msg is True and there is no human input.\n
            (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or
                the number of auto reply reaches the max_consecutive_auto_reply.\n
            (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops
                when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True. \n
        6) function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions, also used for tool calls. \n
        7) code_execution_config (dict or False): config for the code execution.\n
            To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n
            - work_dir (Optional, str): The working directory for the code execution.\n
                If None, a default working directory will be used.\n
                The default working directory is the "extensions" directory under
                "path_to_autogen".\n
            - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n
                Default is True, which means the code will be executed in a docker container. A default list of images will be used.\n
                If a list or a str of image name(s) is provided, the code will be executed in a docker container\n
                with the first image successfully pulled.\n
                If False, the code will be executed in the current environment.\n
                We strongly recommend using docker for code execution.\n
            - timeout (Optional, int): The maximum execution time in seconds.\n
            - last_n_messages (Experimental, int or str): The number of messages to look back for code execution.
                If set to 'auto', it will scan backwards through all messages arriving since the agent last spoke, which is typically the last time execution was attempted. (Default: auto)\n
        8) llm_config (LLMConfig or dict or False or None): llm inference configuration.\n
            Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create)\n
            for available options.\n
            When using OpenAI or Azure OpenAI endpoints, please specify a non-empty 'model' either in `llm_config` or in each config of 'config_list' in `llm_config`.\n
            To disable llm-based auto reply, set to False.\n
            When set to None, will use self.DEFAULT_CONFIG, which defaults to False.\n
        9) default_auto_reply (str or dict): default auto reply when no code execution or llm-based reply is generated.\n
        10) description (str): a short description of the agent. This description is used by other agents
            (e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)\n
        11) chat_messages (dict or None): the previous chat messages that this agent had in the past with other agents.
            Can be used to give the agent a memory by providing the chat history. This will allow the agent to
            resume previous had conversations. Defaults to an empty chat history.\n
        12) silent (bool or None): (Experimental) whether to print the message sent. If None, will use the value of
            silent in each function.\n
        13) context_variables (ContextVariables or None): Context variables that provide a persistent context for the agent.
            Note: This will be a reference to a shared context for multi-agent chats.\n
            Behaves like a dictionary with keys and values (akin to dict[str, Any]).\n
        14) functions (List[Callable[..., Any]]): A list of functions to register with the agent, these will be wrapped up as tools and registered for LLM (not execution).\n
        15) update_agent_state_before_reply (List[Callable[..., Any]]): A list of functions, including UpdateSystemMessage's, called to update the agent before it replies.\n
        16) handoffs (Handoffs): Handoffs object containing all handoff transition conditions.\n
        """
        self.handoffs = handoffs if handoffs is not None else Handoffs()
        self.input_guardrails: list[Guardrail] = []
        self.output_guardrails: list[Guardrail] = []

        # we change code_execution_config below and we have to make sure we don't change the input
        # in case of UserProxyAgent, without this we could even change the default value {}
        code_execution_config = (
            code_execution_config.copy() if hasattr(code_execution_config, "copy") else code_execution_config
        )

        # a dictionary of conversations, default value is list
        if chat_messages is None:
            self._oai_messages = defaultdict(list)
        else:
            self._oai_messages = chat_messages

        self._oai_system_message = [{"content": system_message, "role": "system"}]
        self._description = description if description is not None else system_message
        self._is_termination_msg = (
            is_termination_msg
            if is_termination_msg is not None
            else (lambda x: content_str(x.get("content")) == "TERMINATE")
        )
        self.silent = silent
        self.run_executor: ConversableAgent | None = None

        # Take a copy to avoid modifying the given dict
        if isinstance(llm_config, dict):
            try:
                llm_config = copy.deepcopy(llm_config)
            except TypeError as e:
                raise TypeError(
                    "Please implement __deepcopy__ method for each value class in llm_config to support deepcopy."
                    " Refer to the docs for more details: https://docs.ag2.ai/docs/user-guide/advanced-concepts/llm-configuration-deep-dive/#adding-http-client-in-llm_config-for-proxy"
                ) from e

        self.llm_config = self._validate_llm_config(llm_config)
        self.client = self._create_client(self.llm_config)
        self._validate_name(name)
        self._name = name

        if logging_enabled():
            log_new_agent(self, locals())

        # Initialize standalone client cache object.
        self.client_cache = None

        # To track UI tools
        self._ui_tools: list[Tool] = []

        self.human_input_mode = human_input_mode
        self._max_consecutive_auto_reply = (
            max_consecutive_auto_reply if max_consecutive_auto_reply is not None else self.MAX_CONSECUTIVE_AUTO_REPLY
        )
        self._consecutive_auto_reply_counter = defaultdict(int)
        self._max_consecutive_auto_reply_dict = defaultdict(self.max_consecutive_auto_reply)
        self._function_map = (
            {}
            if function_map is None
            else {name: callable for name, callable in function_map.items() if self._assert_valid_name(name)}
        )
        self._default_auto_reply = default_auto_reply
        self._reply_func_list = []
        self._human_input = []
        self.reply_at_receive = defaultdict(bool)
        self.register_reply([Agent, None], ConversableAgent.generate_oai_reply)
        self.register_reply([Agent, None], ConversableAgent.a_generate_oai_reply, ignore_async_in_sync_chat=True)

        self.context_variables = context_variables if context_variables is not None else ContextVariables()

        self._tools: list[Tool] = []

        # Register functions to the agent
        if isinstance(functions, list):
            if not all(isinstance(func, Callable) for func in functions):
                raise TypeError("All elements in the functions list must be callable")
            self._add_functions(functions)
        elif isinstance(functions, Callable):
            self._add_single_function(functions)
        elif functions is not None:
            raise TypeError("Functions must be a callable or a list of callables")

        # Setting up code execution.
        # Do not register code execution reply if code execution is disabled.
        if code_execution_config is not False:
            # If code_execution_config is None, set it to an empty dict.
            if code_execution_config is None:
                warnings.warn(
                    "Using None to signal a default code_execution_config is deprecated. "
                    "Use {} to use default or False to disable code execution.",
                    stacklevel=2,
                )
                code_execution_config = {}
            if not isinstance(code_execution_config, dict):
                raise ValueError("code_execution_config must be a dict or False.")

            # We have got a valid code_execution_config.
            self._code_execution_config: dict[str, Any] | Literal[False] = code_execution_config

            if self._code_execution_config.get("executor") is not None:
                if "use_docker" in self._code_execution_config:
                    raise ValueError(
                        "'use_docker' in code_execution_config is not valid when 'executor' is set. Use the appropriate arg in the chosen executor instead."
                    )

                if "work_dir" in self._code_execution_config:
                    raise ValueError(
                        "'work_dir' in code_execution_config is not valid when 'executor' is set. Use the appropriate arg in the chosen executor instead."
                    )

                if "timeout" in self._code_execution_config:
                    raise ValueError(
                        "'timeout' in code_execution_config is not valid when 'executor' is set. Use the appropriate arg in the chosen executor instead."
                    )

                # Use the new code executor.
                self._code_executor = CodeExecutorFactory.create(self._code_execution_config)
                self.register_reply([Agent, None], ConversableAgent._generate_code_execution_reply_using_executor)
            else:
                # Legacy code execution using code_utils.
                use_docker = self._code_execution_config.get("use_docker", None)
                use_docker = decide_use_docker(use_docker)
                check_can_use_docker_or_throw(use_docker)
                self._code_execution_config["use_docker"] = use_docker
                self.register_reply([Agent, None], ConversableAgent.generate_code_execution_reply)
        else:
            # Code execution is disabled.
            self._code_execution_config = False

        self.register_reply([Agent, None], ConversableAgent.generate_tool_calls_reply)
        self.register_reply([Agent, None], ConversableAgent.a_generate_tool_calls_reply, ignore_async_in_sync_chat=True)
        self.register_reply([Agent, None], ConversableAgent.generate_function_call_reply)
        self.register_reply(
            [Agent, None], ConversableAgent.a_generate_function_call_reply, ignore_async_in_sync_chat=True
        )
        self.register_reply([Agent, None], ConversableAgent.check_termination_and_human_reply)
        self.register_reply(
            [Agent, None], ConversableAgent.a_check_termination_and_human_reply, ignore_async_in_sync_chat=True
        )

        # Registered hooks are kept in lists, indexed by hookable method, to be called in their order of registration.
        # New hookable methods should be added to this list as required to support new agent capabilities.
        self.hook_lists: dict[str, list[Callable[..., Any]]] = {
            "process_last_received_message": [],
            "process_all_messages_before_reply": [],
            "process_message_before_send": [],
            "update_agent_state": [],
            # Safeguard hooks for monitoring agent interactions
            "safeguard_tool_inputs": [],  # Hook for processing tool inputs before execution
            "safeguard_tool_outputs": [],  # Hook for processing tool outputs after execution
            "safeguard_llm_inputs": [],  # Hook for processing LLM inputs before sending
            "safeguard_llm_outputs": [],  # Hook for processing LLM outputs after receiving
            "safeguard_human_inputs": [],  # Hook for processing human inputs
        }

        # Associate agent update state hooks
        self._register_update_agent_state_before_reply(update_agent_state_before_reply)

    def _validate_name(self, name: str) -> None:
        if not self.llm_config:
            return

        if any(entry for entry in self.llm_config.config_list if entry.api_type == "openai" and re.search(r"\s", name)):
            raise ValueError(f"The name of the agent cannot contain any whitespace. The name provided is: '{name}'")

    def _get_display_name(self):
        """Get the string representation of the agent.

        If you would like to change the standard string representation for an
        instance of ConversableAgent, you can point it to another function.
        In this example a function called _group_agent_str that returns a string:
        agent._get_display_name = MethodType(_group_agent_str, agent)
        """
        return self.name

    def __str__(self):
        return self._get_display_name()

    def _add_functions(self, func_list: list[Callable[..., Any]]):
        """Add (Register) a list of functions to the agent

        Args:
            func_list (list[Callable[..., Any]]): A list of functions to register with the agent.
        """
        for func in func_list:
            self._add_single_function(func)

    def _add_single_function(self, func: Callable, name: str | None = None, description: str | None = ""):
        """Add a single function to the agent

        Args:
            func (Callable): The function to register.
            name (str): The name of the function. If not provided, the function's name will be used.
            description (str): The description of the function, used by the LLM. If not provided, the function's docstring will be used.
        """
        if name:
            func._name = name
        elif not hasattr(func, "_name"):
            func._name = func.__name__

        if hasattr(func, "_description") and func._description and not description:
            # If the function already has a description, use it
            description = func._description
        else:
            if description:
                func._description = description
            else:
                # Use function's docstring, strip whitespace, fall back to empty string
                description = (func.__doc__ or "").strip()
                func._description = description

        # Register the function
        self.register_for_llm(name=name, description=description, silent_override=True)(func)

    def _register_update_agent_state_before_reply(
        self, functions: list[Callable[..., Any]] | Callable[..., Any] | None
    ):
        """Register functions that will be called when the agent is selected and before it speaks.
        You can add your own validation or precondition functions here.

        Args:
            functions (List[Callable[[], None]]): A list of functions to be registered. Each function
                is called when the agent is selected and before it speaks.
        """
        if functions is None:
            return
        if not isinstance(functions, list) and type(functions) not in [UpdateSystemMessage, Callable[..., Any]]:
            raise ValueError("functions must be a list of callables")

        if not isinstance(functions, list):
            functions = [functions]

        for func in functions:
            if isinstance(func, UpdateSystemMessage):
                # Wrapper function that allows this to be used in the update_agent_state hook
                # Its primary purpose, however, is just to update the agent's system message
                # Outer function to create a closure with the update function
                def create_wrapper(update_func: UpdateSystemMessage):
                    def update_system_message_wrapper(
                        agent: ConversableAgent, messages: list[dict[str, Any]]
                    ) -> list[dict[str, Any]]:
                        if isinstance(update_func.content_updater, str):
                            # Templates like "My context variable passport is {passport}" will
                            # use the context_variables for substitution
                            sys_message = OpenAIWrapper.instantiate(
                                template=update_func.content_updater,
                                context=agent.context_variables.to_dict(),
                                allow_format_str_template=True,
                            )
                        else:
                            sys_message = update_func.content_updater(agent, messages)

                        agent.update_system_message(sys_message)
                        return messages

                    return update_system_message_wrapper

                self.register_hook(hookable_method="update_agent_state", hook=create_wrapper(func))

            else:
                self.register_hook(hookable_method="update_agent_state", hook=func)

    @classmethod
    def _validate_llm_config(
        cls, llm_config: LLMConfig | dict[str, Any] | Literal[False] | None
    ) -> LLMConfig | Literal[False]:
        if llm_config is None:
            llm_config = LLMConfig.get_current_llm_config()
            if llm_config is None:
                return cls.DEFAULT_CONFIG

        elif llm_config is False:
            return False

        return LLMConfig.ensure_config(llm_config)

    @classmethod
    def _create_client(cls, llm_config: LLMConfig | Literal[False]) -> OpenAIWrapper | None:
        return None if llm_config is False else OpenAIWrapper(**llm_config)

    @staticmethod
    def _is_silent(agent: Agent, silent: bool | None = False) -> bool:
        return agent.silent if agent.silent is not None else silent

    @property
    def name(self) -> str:
        """Get the name of the agent."""
        return self._name

    @property
    def description(self) -> str:
        """Get the description of the agent."""
        return self._description

    @description.setter
    def description(self, description: str):
        """Set the description of the agent."""
        self._description = description

    @property
    def code_executor(self) -> CodeExecutor | None:
        """The code executor used by this agent. Returns None if code execution is disabled."""
        if not hasattr(self, "_code_executor"):
            return None
        return self._code_executor

    def register_reply(
        self,
        trigger: type[Agent] | str | Agent | Callable[[Agent], bool] | list,
        reply_func: Callable,
        position: int = 0,
        config: Any | None = None,
        reset_config: Callable[..., Any] | None = None,
        *,
        ignore_async_in_sync_chat: bool = False,
        remove_other_reply_funcs: bool = False,
    ):
        """Register a reply function.

        The reply function will be called when the trigger matches the sender.
        The function registered later will be checked earlier by default.
        To change the order, set the position to a positive integer.

        Both sync and async reply functions can be registered. The sync reply function will be triggered
        from both sync and async chats. However, an async reply function will only be triggered from async
        chats (initiated with `ConversableAgent.a_initiate_chat`). If an `async` reply function is registered
        and a chat is initialized with a sync function, `ignore_async_in_sync_chat` determines the behaviour as follows:
            if `ignore_async_in_sync_chat` is set to `False` (default value), an exception will be raised, and
            if `ignore_async_in_sync_chat` is set to `True`, the reply function will be ignored.

        Args:
            trigger (Agent class, str, Agent instance, callable, or list): the trigger.
                If a class is provided, the reply function will be called when the sender is an instance of the class.
                If a string is provided, the reply function will be called when the sender's name matches the string.
                If an agent instance is provided, the reply function will be called when the sender is the agent instance.
                If a callable is provided, the reply function will be called when the callable returns True.
                If a list is provided, the reply function will be called when any of the triggers in the list is activated.
                If None is provided, the reply function will be called only when the sender is None.
                Note: Be sure to register `None` as a trigger if you would like to trigger an auto-reply function with non-empty messages and `sender=None`.
            reply_func (Callable): the reply function.
                The function takes a recipient agent, a list of messages, a sender agent and a config as input and returns a reply message.

                ```python
                def reply_func(
                    recipient: ConversableAgent,
                    messages: Optional[List[Dict]] = None,
                    sender: Optional[Agent] = None,
                    config: Optional[Any] = None,
                ) -> Tuple[bool, Union[str, Dict, None]]:
                ```
            position (int): the position of the reply function in the reply function list.
                The function registered later will be checked earlier by default.
                To change the order, set the position to a positive integer.
            config (Any): the config to be passed to the reply function.
                When an agent is reset, the config will be reset to the original value.
            reset_config (Callable): the function to reset the config.
                The function returns None. Signature: ```def reset_config(config: Any)```
            ignore_async_in_sync_chat (bool): whether to ignore the async reply function in sync chats. If `False`, an exception
                will be raised if an async reply function is registered and a chat is initialized with a sync
                function.
            remove_other_reply_funcs (bool): whether to remove other reply functions when registering this reply function.
        """
        if not isinstance(trigger, (type, str, Agent, Callable, list)):
            raise ValueError("trigger must be a class, a string, an agent, a callable or a list.")
        if remove_other_reply_funcs:
            self._reply_func_list.clear()
        self._reply_func_list.insert(
            position,
            {
                "trigger": trigger,
                "reply_func": reply_func,
                "config": copy.copy(config),
                "init_config": config,
                "reset_config": reset_config,
                "ignore_async_in_sync_chat": ignore_async_in_sync_chat and inspect.iscoroutinefunction(reply_func),
            },
        )

    def replace_reply_func(self, old_reply_func: Callable, new_reply_func: Callable):
        """Replace a registered reply function with a new one.

        Args:
            old_reply_func (Callable): the old reply function to be replaced.
            new_reply_func (Callable): the new reply function to replace the old one.
        """
        for f in self._reply_func_list:
            if f["reply_func"] == old_reply_func:
                f["reply_func"] = new_reply_func

    @staticmethod
    def _get_chats_to_run(
        chat_queue: list[dict[str, Any]],
        recipient: Agent,
        messages: list[dict[str, Any]] | None,
        sender: Agent,
        config: Any,
    ) -> list[dict[str, Any]]:
        """A simple chat reply function.
        This function initiate one or a sequence of chats between the "recipient" and the agents in the
        chat_queue.

        It extracts and returns a summary from the nested chat based on the "summary_method" in each chat in chat_queue.

        Returns:
            Tuple[bool, str]: A tuple where the first element indicates the completion of the chat, and the second element contains the summary of the last chat if any chats were initiated.
        """
        last_msg = messages[-1].get("content")
        chat_to_run = []
        for i, c in enumerate(chat_queue):
            current_c = c.copy()
            if current_c.get("sender") is None:
                current_c["sender"] = recipient
            message = current_c.get("message")
            # If message is not provided in chat_queue, we by default use the last message from the original chat history as the first message in this nested chat (for the first chat in the chat queue).
            # NOTE: This setting is prone to change.
            if message is None and i == 0:
                message = last_msg
            if callable(message):
                message = message(recipient, messages, sender, config)
            # We only run chat that has a valid message. NOTE: This is prone to change depending on applications.
            if message:
                current_c["message"] = message
                chat_to_run.append(current_c)
        return chat_to_run

    @staticmethod
    def _process_nested_chat_carryover(
        chat: dict[str, Any],
        recipient: Agent,
        messages: list[dict[str, Any]],
        sender: Agent,
        config: Any,
        trim_n_messages: int = 0,
    ) -> None:
        """Process carryover messages for a nested chat (typically for the first chat of a group chat)

        The carryover_config key is a dictionary containing:
            "summary_method": The method to use to summarise the messages, can be "all", "last_msg", "reflection_with_llm" or a Callable
            "summary_args": Optional arguments for the summary method

        Supported carryover 'summary_methods' are:
            "all" - all messages will be incorporated
            "last_msg" - the last message will be incorporated
            "reflection_with_llm" - an llm will summarise all the messages and the summary will be incorporated as a single message
            Callable - a callable with the signature: my_method(agent: ConversableAgent, messages: List[Dict[str, Any]]) -> str

        Args:
            chat: The chat dictionary containing the carryover configuration
            recipient: The recipient agent
            messages: The messages from the parent chat
            sender: The sender agent
            config: The LLM configuration
            trim_n_messages: The number of latest messages to trim from the messages list
        """

        def concat_carryover(chat_message: str, carryover_message: str | list[dict[str, Any]]) -> str:
            """Concatenate the carryover message to the chat message."""
            prefix = f"{chat_message}\n" if chat_message else ""

            if isinstance(carryover_message, str):
                content = carryover_message
            elif isinstance(carryover_message, list):
                content = "\n".join(
                    msg["content"] for msg in carryover_message if "content" in msg and msg["content"] is not None
                )
            else:
                raise ValueError("Carryover message must be a string or a list of dictionaries")

            return f"{prefix}Context:\n{content}"

        carryover_config = chat["carryover_config"]

        if "summary_method" not in carryover_config:
            raise ValueError("Carryover configuration must contain a 'summary_method' key")

        carryover_summary_method = carryover_config["summary_method"]
        carryover_summary_args = carryover_config.get("summary_args") or {}

        chat_message = ""
        message = chat.get("message")

        # If the message is a callable, run it and get the result
        if message:
            chat_message = message(recipient, messages, sender, config) if callable(message) else message

        # deep copy and trim the latest messages
        content_messages = copy.deepcopy(messages)
        content_messages = content_messages[:-trim_n_messages]

        if carryover_summary_method == "all":
            # Put a string concatenated value of all parent messages into the first message
            # (e.g. message = <first nested chat message>\nContext: \n<chat message 1>\n<chat message 2>\n...)
            carry_over_message = concat_carryover(chat_message, content_messages)

        elif carryover_summary_method == "last_msg":
            # (e.g. message = <first nested chat message>\nContext: \n<last chat message>)
            carry_over_message = concat_carryover(chat_message, content_messages[-1]["content"])

        elif carryover_summary_method == "reflection_with_llm":
            # (e.g. message = <first nested chat message>\nContext: \n<llm summary>)

            # Add the messages to the nested chat agent for reflection (we'll clear after reflection)
            chat["recipient"]._oai_messages[sender] = content_messages

            carry_over_message_llm = ConversableAgent._reflection_with_llm_as_summary(
                sender=sender,
                recipient=chat["recipient"],  # Chat recipient LLM config will be used for the reflection
                summary_args=carryover_summary_args,
            )

            recipient._oai_messages[sender] = []

            carry_over_message = concat_carryover(chat_message, carry_over_message_llm)

        elif isinstance(carryover_summary_method, Callable):
            # (e.g. message = <first nested chat message>\nContext: \n<function's return string>)
            carry_over_message_result = carryover_summary_method(recipient, content_messages, carryover_summary_args)

            carry_over_message = concat_carryover(chat_message, carry_over_message_result)

        chat["message"] = carry_over_message

    @staticmethod
    def _process_chat_queue_carryover(
        chat_queue: list[dict[str, Any]],
        recipient: Agent,
        messages: str | Callable[..., Any],
        sender: Agent,
        config: Any,
        trim_messages: int = 2,
    ) -> tuple[bool, str | None]:
        """Process carryover configuration for the first chat in the queue.

        Args:
            chat_queue: List of chat configurations
            recipient: Receiving agent
            messages: Chat messages
            sender: Sending agent
            config: LLM configuration
            trim_messages: Number of messages to trim for nested chat carryover (default 2 for nested chat in group chats)

        Returns:
            Tuple containing:
                - restore_flag: Whether the original message needs to be restored
                - original_message: The original message to restore (if any)
        """
        restore_chat_queue_message = False
        original_chat_queue_message = None

        # Carryover configuration allowed on the first chat in the queue only, trim the last two messages specifically for group chat nested chat carryover as these are the messages for the transition to the nested chat agent
        if len(chat_queue) > 0 and "carryover_config" in chat_queue[0]:
            if "message" in chat_queue[0]:
                # As we're updating the message in the nested chat queue, we need to restore it after finishing this nested chat.
                restore_chat_queue_message = True
                original_chat_queue_message = chat_queue[0]["message"]

            # TODO Check the trimming required if not a group chat, it may not be 2 because other chats don't have the group transition messages. We may need to add as a carryover_config parameter.
            ConversableAgent._process_nested_chat_carryover(
                chat=chat_queue[0],
                recipient=recipient,
                messages=messages,
                sender=sender,
                config=config,
                trim_n_messages=trim_messages,
            )

        return restore_chat_queue_message, original_chat_queue_message

    @staticmethod
    def _summary_from_nested_chats(
        chat_queue: list[dict[str, Any]],
        recipient: Agent,
        messages: list[dict[str, Any]] | None,
        sender: Agent,
        config: Any,
    ) -> tuple[bool, str | None]:
        """A simple chat reply function.
        This function initiate one or a sequence of chats between the "recipient" and the agents in the
        chat_queue.

        It extracts and returns a summary from the nested chat based on the "summary_method" in each chat in chat_queue.

        The first chat in the queue can contain a 'carryover_config' which is a dictionary that denotes how to carryover messages from the parent chat into the first chat of the nested chats). Only applies to the first chat.
            e.g.: carryover_summarize_chat_config = {"summary_method": "reflection_with_llm", "summary_args": None}
            summary_method can be "last_msg", "all", "reflection_with_llm", Callable
            The Callable signature: my_method(agent: ConversableAgent, messages: List[Dict[str, Any]]) -> str
            The summary will be concatenated to the message of the first chat in the queue.

        Returns:
            Tuple[bool, str]: A tuple where the first element indicates the completion of the chat, and the second element contains the summary of the last chat if any chats were initiated.
        """
        # Process carryover configuration
        restore_chat_queue_message, original_chat_queue_message = ConversableAgent._process_chat_queue_carryover(
            chat_queue, recipient, messages, sender, config
        )

        chat_to_run = ConversableAgent._get_chats_to_run(chat_queue, recipient, messages, sender, config)
        if not chat_to_run:
            return True, None
        res = initiate_chats(chat_to_run)

        # We need to restore the chat queue message if it has been modified so that it will be the original message for subsequent uses
        if restore_chat_queue_message:
            chat_queue[0]["message"] = original_chat_queue_message

        return True, res[-1].summary

    @staticmethod
    async def _a_summary_from_nested_chats(
        chat_queue: list[dict[str, Any]],
        recipient: Agent,
        messages: list[dict[str, Any]] | None,
        sender: Agent,
        config: Any,
    ) -> tuple[bool, str | None]:
        """A simple chat reply function.
        This function initiate one or a sequence of chats between the "recipient" and the agents in the
        chat_queue.

        It extracts and returns a summary from the nested chat based on the "summary_method" in each chat in chat_queue.

        The first chat in the queue can contain a 'carryover_config' which is a dictionary that denotes how to carryover messages from the parent chat into the first chat of the nested chats). Only applies to the first chat.
            e.g.: carryover_summarize_chat_config = {"summary_method": "reflection_with_llm", "summary_args": None}
            summary_method can be "last_msg", "all", "reflection_with_llm", Callable
            The Callable signature: my_method(agent: ConversableAgent, messages: List[Dict[str, Any]]) -> str
            The summary will be concatenated to the message of the first chat in the queue.

        Returns:
            Tuple[bool, str]: A tuple where the first element indicates the completion of the chat, and the second element contains the summary of the last chat if any chats were initiated.
        """
        # Process carryover configuration
        restore_chat_queue_message, original_chat_queue_message = ConversableAgent._process_chat_queue_carryover(
            chat_queue, recipient, messages, sender, config
        )

        chat_to_run = ConversableAgent._get_chats_to_run(chat_queue, recipient, messages, sender, config)
        if not chat_to_run:
            return True, None
        res = await a_initiate_chats(chat_to_run)
        index_of_last_chat = chat_to_run[-1]["chat_id"]

        # We need to restore the chat queue message if it has been modified so that it will be the original message for subsequent uses
        if restore_chat_queue_message:
            chat_queue[0]["message"] = original_chat_queue_message

        return True, res[index_of_last_chat].summary

    def register_nested_chats(
        self,
        chat_queue: list[dict[str, Any]],
        trigger: type[Agent] | str | Agent | Callable[[Agent], bool] | list,
        reply_func_from_nested_chats: str | Callable[..., Any] = "summary_from_nested_chats",
        position: int = 2,
        use_async: bool | None = None,
        **kwargs: Any,
    ) -> None:
        """Register a nested chat reply function.

        Args:
            chat_queue (list): a list of chat objects to be initiated. If use_async is used, then all messages in chat_queue must have a chat-id associated with them.
            trigger (Agent class, str, Agent instance, callable, or list): refer to `register_reply` for details.
            reply_func_from_nested_chats (Callable, str): the reply function for the nested chat.
                The function takes a chat_queue for nested chat, recipient agent, a list of messages, a sender agent and a config as input and returns a reply message.
                Default to "summary_from_nested_chats", which corresponds to a built-in reply function that get summary from the nested chat_queue.
                ```python
                def reply_func_from_nested_chats(
                    chat_queue: List[Dict],
                    recipient: ConversableAgent,
                    messages: Optional[List[Dict]] = None,
                    sender: Optional[Agent] = None,
                    config: Optional[Any] = None,
                ) -> Tuple[bool, Union[str, Dict, None]]:
                ```
            position (int): Ref to `register_reply` for details. Default to 2. It means we first check the termination and human reply, then check the registered nested chat reply.
            use_async: Uses a_initiate_chats internally to start nested chats. If the original chat is initiated with a_initiate_chats, you may set this to true so nested chats do not run in sync.
            kwargs: Ref to `register_reply` for details.
        """
        if use_async:
            for chat in chat_queue:
                if chat.get("chat_id") is None:
                    raise ValueError("chat_id is required for async nested chats")

        if use_async:
            if reply_func_from_nested_chats == "summary_from_nested_chats":
                reply_func_from_nested_chats = self._a_summary_from_nested_chats
            if not callable(reply_func_from_nested_chats) or not inspect.iscoroutinefunction(
                reply_func_from_nested_chats
            ):
                raise ValueError("reply_func_from_nested_chats must be a callable and a coroutine")

            async def wrapped_reply_func(recipient, messages=None, sender=None, config=None):
                return await reply_func_from_nested_chats(chat_queue, recipient, messages, sender, config)

        else:
            if reply_func_from_nested_chats == "summary_from_nested_chats":
                reply_func_from_nested_chats = self._summary_from_nested_chats
            if not callable(reply_func_from_nested_chats):
                raise ValueError("reply_func_from_nested_chats must be a callable")

            def wrapped_reply_func(recipient, messages=None, sender=None, config=None):
                return reply_func_from_nested_chats(chat_queue, recipient, messages, sender, config)

        functools.update_wrapper(wrapped_reply_func, reply_func_from_nested_chats)

        self.register_reply(
            trigger,
            wrapped_reply_func,
            position,
            kwargs.get("config"),
            kwargs.get("reset_config"),
            ignore_async_in_sync_chat=(
                not use_async if use_async is not None else kwargs.get("ignore_async_in_sync_chat")
            ),
        )

    @property
    def system_message(self) -> str:
        """Return the system message."""
        return self._oai_system_message[0]["content"]

    def update_system_message(self, system_message: str) -> None:
        """Update the system message.

        Args:
            system_message (str): system message for the ChatCompletion inference.
        """
        self._oai_system_message[0]["content"] = system_message

    def update_max_consecutive_auto_reply(self, value: int, sender: Agent | None = None):
        """Update the maximum number of consecutive auto replies.

        Args:
            value (int): the maximum number of consecutive auto replies.
            sender (Agent): when the sender is provided, only update the max_consecutive_auto_reply for that sender.
        """
        if sender is None:
            self._max_consecutive_auto_reply = value
            for k in self._max_consecutive_auto_reply_dict:
                self._max_consecutive_auto_reply_dict[k] = value
        else:
            self._max_consecutive_auto_reply_dict[sender] = value

    def max_consecutive_auto_reply(self, sender: Agent | None = None) -> int:
        """The maximum number of consecutive auto replies."""
        return self._max_consecutive_auto_reply if sender is None else self._max_consecutive_auto_reply_dict[sender]

    @property
    def chat_messages(self) -> dict[Agent, list[dict[str, Any]]]:
        """A dictionary of conversations from agent to list of messages."""
        return self._oai_messages

    def chat_messages_for_summary(self, agent: Agent) -> list[dict[str, Any]]:
        """A list of messages as a conversation to summarize."""
        return self._oai_messages[agent]

    def last_message(self, agent: Agent | None = None) -> dict[str, Any] | None:
        """The last message exchanged with the agent.

        Args:
            agent (Agent): The agent in the conversation.
                If None and more than one agent's conversations are found, an error will be raised.
                If None and only one conversation is found, the last message of the only conversation will be returned.

        Returns:
            The last message exchanged with the agent.
        """
        if agent is None:
            n_conversations = len(self._oai_messages)
            if n_conversations == 0:
                return None
            if n_conversations == 1:
                for conversation in self._oai_messages.values():
                    return conversation[-1]
            raise ValueError("More than one conversation is found. Please specify the sender to get the last message.")
        if agent not in self._oai_messages:
            raise KeyError(
                f"The agent '{agent.name}' is not present in any conversation. No history available for this agent."
            )
        return self._oai_messages[agent][-1]

    @property
    def use_docker(self) -> bool | str | None:
        """Bool value of whether to use docker to execute the code,
        or str value of the docker image name to use, or None when code execution is disabled.
        """
        return None if self._code_execution_config is False else self._code_execution_config.get("use_docker")

    @staticmethod
    def _message_to_dict(message: dict[str, Any] | str) -> dict:
        """Convert a message to a dictionary.

        The message can be a string or a dictionary. The string will be put in the "content" field of the new dictionary.
        """
        if isinstance(message, str):
            return {"content": message}
        elif isinstance(message, dict):
            return message
        else:
            return dict(message)

    @staticmethod
    def _normalize_name(name):
        """LLMs sometimes ask functions while ignoring their own format requirements, this function should be used to replace invalid characters with "_".

        Prefer _assert_valid_name for validating user configuration or input
        """
        return re.sub(r"[^a-zA-Z0-9_-]", "_", name)[:64]

    @staticmethod
    def _assert_valid_name(name):
        """Ensure that configured names are valid, raises ValueError if not.

        For munging LLM responses use _normalize_name to ensure LLM specified names don't break the API.
        """
        if not re.match(r"^[a-zA-Z0-9_-]+$", name):
            raise ValueError(f"Invalid name: {name}. Only letters, numbers, '_' and '-' are allowed.")
        if len(name) > 64:
            raise ValueError(f"Invalid name: {name}. Name must be less than 64 characters.")
        return name

    def _append_oai_message(
        self, message: dict[str, Any] | str, role, conversation_id: Agent, is_sending: bool
    ) -> bool:
        """Append a message to the ChatCompletion conversation.

        If the message received is a string, it will be put in the "content" field of the new dictionary.
        If the message received is a dictionary but does not have any of the three fields "content", "function_call", or "tool_calls",
            this message is not a valid ChatCompletion message.
        If only "function_call" or "tool_calls" is provided, "content" will be set to None if not provided, and the role of the message will be forced "assistant".

        Args:
            message (dict or str): message to be appended to the ChatCompletion conversation.
            role (str): role of the message, can be "assistant" or "function".
            conversation_id (Agent): id of the conversation, should be the recipient or sender.
            is_sending (bool): If the agent (aka self) is sending to the conversation_id agent, otherwise receiving.

        Returns:
            bool: whether the message is appended to the ChatCompletion conversation.
        """
        message = self._message_to_dict(message)
        # create oai message to be appended to the oai conversation that can be passed to oai directly.
        oai_message = {
            k: message[k]
            for k in ("content", "function_call", "tool_calls", "tool_responses", "tool_call_id", "name", "context")
            if k in message and message[k] is not None
        }
        if "content" not in oai_message:
            if "function_call" in oai_message or "tool_calls" in oai_message:
                oai_message["content"] = None  # if only function_call is provided, content will be set to None.
            else:
                return False

        if message.get("role") in ["function", "tool"]:
            oai_message["role"] = message.get("role")
            if "tool_responses" in oai_message:
                for tool_response in oai_message["tool_responses"]:
                    tool_response["content"] = str(tool_response["content"])
        elif "override_role" in message:
            # If we have a direction to override the role then set the
            # role accordingly. Used to customise the role for the
            # select speaker prompt.
            oai_message["role"] = message.get("override_role")
        else:
            oai_message["role"] = role

        if oai_message.get("function_call", False) or oai_message.get("tool_calls", False):
            oai_message["role"] = "assistant"  # only messages with role 'assistant' can have a function call.
        elif "name" not in oai_message:
            # If we don't have a name field, append it
            if is_sending:
                oai_message["name"] = self.name
            else:
                oai_message["name"] = conversation_id.name

        self._oai_messages[conversation_id].append(oai_message)

        return True

    def _process_message_before_send(
        self, message: dict[str, Any] | str, recipient: Agent, silent: bool
    ) -> dict[str, Any] | str:
        """Process the message before sending it to the recipient."""
        hook_list = self.hook_lists["process_message_before_send"]
        for hook in hook_list:
            message = hook(
                sender=self, message=message, recipient=recipient, silent=ConversableAgent._is_silent(self, silent)
            )
        return message

    def send(
        self,
        message: dict[str, Any] | str,
        recipient: Agent,
        request_reply: bool | None = None,
        silent: bool | None = False,
    ):
        """Send a message to another agent.

        Args:
            message (dict or str): message to be sent.
                The message could contain the following fields:
                - content (str or List): Required, the content of the message. (Can be None)
                - function_call (str): the name of the function to be called.
                - name (str): the name of the function to be called.
                - role (str): the role of the message, any role that is not "function"
                    will be modified to "assistant".
                - context (dict): the context of the message, which will be passed to
                    [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create).
                    For example, one agent can send a message A as:
        ```python
        {
            "content": lambda context: context["use_tool_msg"],
            "context": {"use_tool_msg": "Use tool X if they are relevant."},
        }
        ```
                    Next time, one agent can send a message B with a different "use_tool_msg".
                    Then the content of message A will be refreshed to the new "use_tool_msg".
                    So effectively, this provides a way for an agent to send a "link" and modify
                    the content of the "link" later.
            recipient (Agent): the recipient of the message.
            request_reply (bool or None): whether to request a reply from the recipient.
            silent (bool or None): (Experimental) whether to print the message sent.

        Raises:
            ValueError: if the message can't be converted into a valid ChatCompletion message.
        """
        message = self._process_message_before_send(message, recipient, ConversableAgent._is_silent(self, silent))
        # When the agent composes and sends the message, the role of the message is "assistant"
        # unless it's "function".
        valid = self._append_oai_message(message, "assistant", recipient, is_sending=True)
        if valid:
            recipient.receive(message, self, request_reply, silent)
        else:
            raise ValueError(
                "Message can't be converted into a valid ChatCompletion message. Either content or function_call must be provided."
            )

    async def a_send(
        self,
        message: dict[str, Any] | str,
        recipient: Agent,
        request_reply: bool | None = None,
        silent: bool | None = False,
    ):
        """(async) Send a message to another agent.

        Args:
            message (dict or str): message to be sent.
                The message could contain the following fields:
                - content (str or List): Required, the content of the message. (Can be None)
                - function_call (str): the name of the function to be called.
                - name (str): the name of the function to be called.
                - role (str): the role of the message, any role that is not "function"
                    will be modified to "assistant".
                - context (dict): the context of the message, which will be passed to
                    [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create).
                    For example, one agent can send a message A as:
        ```python
        {
            "content": lambda context: context["use_tool_msg"],
            "context": {"use_tool_msg": "Use tool X if they are relevant."},
        }
        ```
                    Next time, one agent can send a message B with a different "use_tool_msg".
                    Then the content of message A will be refreshed to the new "use_tool_msg".
                    So effectively, this provides a way for an agent to send a "link" and modify
                    the content of the "link" later.
            recipient (Agent): the recipient of the message.
            request_reply (bool or None): whether to request a reply from the recipient.
            silent (bool or None): (Experimental) whether to print the message sent.

        Raises:
            ValueError: if the message can't be converted into a valid ChatCompletion message.
        """
        message = self._process_message_before_send(message, recipient, ConversableAgent._is_silent(self, silent))
        # When the agent composes and sends the message, the role of the message is "assistant"
        # unless it's "function".
        valid = self._append_oai_message(message, "assistant", recipient, is_sending=True)
        if valid:
            await recipient.a_receive(message, self, request_reply, silent)
        else:
            raise ValueError(
                "Message can't be converted into a valid ChatCompletion message. Either content or function_call must be provided."
            )

    def _print_received_message(self, message: dict[str, Any] | str, sender: Agent, skip_head: bool = False):
        message = self._message_to_dict(message)
        message_model = create_received_event_model(event=message, sender=sender, recipient=self)
        iostream = IOStream.get_default()
        # message_model.print(iostream.print)
        iostream.send(message_model)

    def _process_received_message(self, message: dict[str, Any] | str, sender: Agent, silent: bool):
        # When the agent receives a message, the role of the message is "user". (If 'role' exists and is 'function', it will remain unchanged.)
        valid = self._append_oai_message(message, "user", sender, is_sending=False)
        if logging_enabled():
            log_event(self, "received_message", message=message, sender=sender.name, valid=valid)

        if not valid:
            raise ValueError(
                "Received message can't be converted into a valid ChatCompletion message. Either content or function_call must be provided."
            )

        if not ConversableAgent._is_silent(sender, silent):
            self._print_received_message(message, sender)

    def receive(
        self,
        message: dict[str, Any] | str,
        sender: Agent,
        request_reply: bool | None = None,
        silent: bool | None = False,
    ):
        """Receive a message from another agent.

        Once a message is received, this function sends a reply to the sender or stop.
        The reply can be generated automatically or entered manually by a human.

        Args:
            message (dict or str): message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).
                1. "content": content of the message, can be None.
                2. "function_call": a dictionary containing the function name and arguments. (deprecated in favor of "tool_calls")
                3. "tool_calls": a list of dictionaries containing the function name and arguments.
                4. "role": role of the message, can be "assistant", "user", "function", "tool".
                    This field is only needed to distinguish between "function" or "assistant"/"user".
                5. "name": In most cases, this field is not needed. When the role is "function", this field is needed to indicate the function name.
                6. "context" (dict): the context of the message, which will be passed to
                    [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create).
            sender: sender of an Agent instance.
            request_reply (bool or None): whether a reply is requested from the sender.
                If None, the value is determined by `self.reply_at_receive[sender]`.
            silent (bool or None): (Experimental) whether to print the message received.

        Raises:
            ValueError: if the message can't be converted into a valid ChatCompletion message.
        """
        self._process_received_message(message, sender, silent)
        if request_reply is False or (request_reply is None and self.reply_at_receive[sender] is False):
            return
        reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)
        if reply is not None:
            self.send(reply, sender, silent=silent)

    async def a_receive(
        self,
        message: dict[str, Any] | str,
        sender: Agent,
        request_reply: bool | None = None,
        silent: bool | None = False,
    ):
        """(async) Receive a message from another agent.

        Once a message is received, this function sends a reply to the sender or stop.
        The reply can be generated automatically or entered manually by a human.

        Args:
            message (dict or str): message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).
                1. "content": content of the message, can be None.
                2. "function_call": a dictionary containing the function name and arguments. (deprecated in favor of "tool_calls")
                3. "tool_calls": a list of dictionaries containing the function name and arguments.
                4. "role": role of the message, can be "assistant", "user", "function".
                    This field is only needed to distinguish between "function" or "assistant"/"user".
                5. "name": In most cases, this field is not needed. When the role is "function", this field is needed to indicate the function name.
                6. "context" (dict): the context of the message, which will be passed to
                    [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create).
            sender: sender of an Agent instance.
            request_reply (bool or None): whether a reply is requested from the sender.
                If None, the value is determined by `self.reply_at_receive[sender]`.
            silent (bool or None): (Experimental) whether to print the message received.

        Raises:
            ValueError: if the message can't be converted into a valid ChatCompletion message.
        """
        self._process_received_message(message, sender, silent)
        if request_reply is False or (request_reply is None and self.reply_at_receive[sender] is False):
            return
        reply = await self.a_generate_reply(messages=self.chat_messages[sender], sender=sender)
        if reply is not None:
            await self.a_send(reply, sender, silent=silent)

    def _prepare_chat(
        self,
        recipient: "ConversableAgent",
        clear_history: bool,
        prepare_recipient: bool = True,
        reply_at_receive: bool = True,
    ) -> None:
        self.reset_consecutive_auto_reply_counter(recipient)
        self.reply_at_receive[recipient] = reply_at_receive
        if clear_history:
            self.clear_history(recipient)
            self._human_input = []
        if prepare_recipient:
            recipient._prepare_chat(self, clear_history, False, reply_at_receive)

    def _raise_exception_on_async_reply_functions(self) -> None:
        """Raise an exception if any async reply functions are registered.

        Raises:
            RuntimeError: if any async reply functions are registered.
        """
        reply_functions = {
            f["reply_func"] for f in self._reply_func_list if not f.get("ignore_async_in_sync_chat", False)
        }

        async_reply_functions = [f for f in reply_functions if inspect.iscoroutinefunction(f)]
        if async_reply_functions:
            msg = (
                "Async reply functions can only be used with ConversableAgent.a_initiate_chat(). The following async reply functions are found: "
                + ", ".join([f.__name__ for f in async_reply_functions])
            )

            raise RuntimeError(msg)

    def _should_terminate_chat(self, recipient: "ConversableAgent", message: dict[str, Any]) -> bool:
        """
        Determines whether the chat should be terminated based on the message content
        and the recipient's termination condition.

        Args:
            recipient (ConversableAgent): The agent to check for termination condition.
            message (dict[str, Any]): The message dictionary to evaluate for termination.

        Returns:
            bool: True if the chat should be terminated, False otherwise.
        """
        return (
            isinstance(recipient, ConversableAgent)
            and isinstance(message.get("content"), str)
            and hasattr(recipient, "_is_termination_msg")
            and recipient._is_termination_msg(message)
        )

    def initiate_chat(
        self,
        recipient: "ConversableAgent",
        clear_history: bool = True,
        silent: bool | None = False,
        cache: AbstractCache | None = None,
        max_turns: int | None = None,
        summary_method: str | Callable[..., Any] | None = DEFAULT_SUMMARY_METHOD,
        summary_args: dict[str, Any] | None = {},
        message: dict[str, Any] | str | Callable[..., Any] | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Initiate a chat with the recipient agent.

        Reset the consecutive auto reply counter.
        If `clear_history` is True, the chat history with the recipient agent will be cleared.


        Args:
            recipient: the recipient agent.
            clear_history (bool): whether to clear the chat history with the agent. Default is True.
            silent (bool or None): (Experimental) whether to print the messages for this conversation. Default is False.
            cache (AbstractCache or None): the cache client to be used for this conversation. Default is None.
            max_turns (int or None): the maximum number of turns for the chat between the two agents. One turn means one conversation round trip. Note that this is different from
                `max_consecutive_auto_reply` which is the maximum number of consecutive auto replies; and it is also different from `max_rounds` in GroupChat which is the maximum number of rounds in a group chat session.
                If max_turns is set to None, the chat will continue until a termination condition is met. Default is None.
            summary_method (str or callable): a method to get a summary from the chat. Default is DEFAULT_SUMMARY_METHOD, i.e., "last_msg".
                Supported strings are "last_msg" and "reflection_with_llm":
                    - when set to "last_msg", it returns the last message of the dialog as the summary.
                    - when set to "reflection_with_llm", it returns a summary extracted using an llm client.
                        `llm_config` must be set in either the recipient or sender.

                A callable summary_method should take the recipient and sender agent in a chat as input and return a string of summary. E.g.,

                ```python
                def my_summary_method(
                    sender: ConversableAgent,
                    recipient: ConversableAgent,
                    summary_args: dict,
                ):
                    return recipient.last_message(sender)["content"]
                ```
            summary_args (dict): a dictionary of arguments to be passed to the summary_method.
                One example key is "summary_prompt", and value is a string of text used to prompt a LLM-based agent (the sender or recipient agent) to reflect
                on the conversation and extract a summary when summary_method is "reflection_with_llm".
                The default summary_prompt is DEFAULT_SUMMARY_PROMPT, i.e., "Summarize takeaway from the conversation. Do not add any introductory phrases. If the intended request is NOT properly addressed, please point it out."
                Another available key is "summary_role", which is the role of the message sent to the agent in charge of summarizing. Default is "system".
            message (str, dict or Callable): the initial message to be sent to the recipient. Needs to be provided. Otherwise, input() will be called to get the initial message.
                - If a string or a dict is provided, it will be used as the initial message.        `generate_init_message` is called to generate the initial message for the agent based on this string and the context.
                    If dict, it may contain the following reserved fields (either content or tool_calls need to be provided).

                        1. "content": content of the message, can be None.
                        2. "function_call": a dictionary containing the function name and arguments. (deprecated in favor of "tool_calls")
                        3. "tool_calls": a list of dictionaries containing the function name and arguments.
                        4. "role": role of the message, can be "assistant", "user", "function".
                            This field is only needed to distinguish between "function" or "assistant"/"user".
                        5. "name": In most cases, this field is not needed. When the role is "function", this field is needed to indicate the function name.
                        6. "context" (dict): the context of the message, which will be passed to
                            `OpenAIWrapper.create`.

                - If a callable is provided, it will be called to get the initial message in the form of a string or a dict.
                    If the returned type is dict, it may contain the reserved fields mentioned above.

                    Example of a callable message (returning a string):

                    ```python
                    def my_message(
                        sender: ConversableAgent, recipient: ConversableAgent, context: dict
                    ) -> Union[str, Dict]:
                        carryover = context.get("carryover", "")
                        if isinstance(message, list):
                            carryover = carryover[-1]
                        final_msg = "Write a blogpost." + "\\nContext: \\n" + carryover
                        return final_msg
                    ```

                    Example of a callable message (returning a dict):

                    ```python
                    def my_message(
                        sender: ConversableAgent, recipient: ConversableAgent, context: dict
                    ) -> Union[str, Dict]:
                        final_msg = {}
                        carryover = context.get("carryover", "")
                        if isinstance(message, list):
                            carryover = carryover[-1]
                        final_msg["content"] = "Write a blogpost." + "\\nContext: \\n" + carryover
                        final_msg["context"] = {"prefix": "Today I feel"}
                        return final_msg
                    ```
            **kwargs: any additional information. It has the following reserved fields:
                - "carryover": a string or a list of string to specify the carryover information to be passed to this chat.
                    If provided, we will combine this carryover (by attaching a "context: " string and the carryover content after the message content) with the "message" content when generating the initial chat
                    message in `generate_init_message`.
                - "verbose": a boolean to specify whether to print the message and carryover in a chat. Default is False.

        Raises:
            RuntimeError: if any async reply functions are registered and not ignored in sync chat.

        Returns:
            ChatResult: an ChatResult object.
        """
        iostream = IOStream.get_default()

        cache = Cache.get_current_cache(cache)
        _chat_info = locals().copy()
        _chat_info["sender"] = self
        consolidate_chat_info(_chat_info, uniform_sender=self)
        for agent in [self, recipient]:
            agent._raise_exception_on_async_reply_functions()
            agent.previous_cache = agent.client_cache
            agent.client_cache = cache
        if isinstance(max_turns, int):
            self._prepare_chat(recipient, clear_history, reply_at_receive=False)
            is_termination = False
            for i in range(max_turns):
                # check recipient max consecutive auto reply limit
                if self._consecutive_auto_reply_counter[recipient] >= recipient._max_consecutive_auto_reply:
                    break
                if i == 0:
                    if isinstance(message, Callable):
                        msg2send = message(_chat_info["sender"], _chat_info["recipient"], kwargs)
                    else:
                        msg2send = self.generate_init_message(message, **kwargs)
                else:
                    last_message = self.chat_messages[recipient][-1]
                    if self._should_terminate_chat(recipient, last_message):
                        break
                    msg2send = self.generate_reply(messages=self.chat_messages[recipient], sender=recipient)
                if msg2send is None:
                    break
                self.send(msg2send, recipient, request_reply=True, silent=silent)
            else:  # No breaks in the for loop, so we have reached max turns
                iostream.send(
                    TerminationEvent(
                        termination_reason=f"Maximum turns ({max_turns}) reached", sender=self, recipient=recipient
                    )
                )
        else:
            self._prepare_chat(recipient, clear_history)
            if isinstance(message, Callable):
                msg2send = message(_chat_info["sender"], _chat_info["recipient"], kwargs)
            else:
                msg2send = self.generate_init_message(message, **kwargs)
            self.send(msg2send, recipient, silent=silent)
        summary = self._summarize_chat(
            summary_method,
            summary_args,
            recipient,
            cache=cache,
        )
        for agent in [self, recipient]:
            agent.client_cache = agent.previous_cache
            agent.previous_cache = None
        chat_result = ChatResult(
            chat_history=self.chat_messages[recipient],
            summary=summary,
            cost=gather_usage_summary([self, recipient]),
            human_input=self._human_input,
        )
        return chat_result

    def run(
        self,
        recipient: Optional["ConversableAgent"] = None,
        clear_history: bool = True,
        silent: bool | None = False,
        cache: AbstractCache | None = None,
        max_turns: int | None = None,
        summary_method: str | Callable[..., Any] | None = DEFAULT_SUMMARY_METHOD,
        summary_args: dict[str, Any] | None = {},
        message: dict[str, Any] | str | Callable[..., Any] | None = None,
        executor_kwargs: dict[str, Any] | None = None,
        tools: Tool | Iterable[Tool] | None = None,
        user_input: bool | None = False,
        msg_to: str | None = "agent",
        **kwargs: Any,
    ) -> RunResponseProtocol:
        iostream = ThreadIOStream()
        agents = [self, recipient] if recipient else [self]
        response = RunResponse(iostream, agents=agents)

        if recipient is None:

            def initiate_chat(
                self=self,
                iostream: ThreadIOStream = iostream,
                response: RunResponse = response,
            ) -> None:
                with (
                    IOStream.set_default(iostream),
                    self._create_or_get_executor(
                        executor_kwargs=executor_kwargs,
                        tools=tools,
                        agent_name="user",
                        agent_human_input_mode="ALWAYS" if user_input else "NEVER",
                    ) as executor,
                ):
                    try:
                        if msg_to == "agent":
                            chat_result = executor.initiate_chat(
                                self,
                                message=message,
                                clear_history=clear_history,
                                max_turns=max_turns,
                                summary_method=summary_method,
                            )
                        else:
                            chat_result = self.initiate_chat(
                                executor,
                                message=message,
                                clear_history=clear_history,
                                max_turns=max_turns,
                                summary_method=summary_method,
                            )

                        IOStream.get_default().send(
                            RunCompletionEvent(
                                history=chat_result.chat_history,
                                summary=chat_result.summary,
                                cost=chat_result.cost,
                                last_speaker=self.name,
                            )
                        )
                    except Exception as e:
                        response.iostream.send(ErrorEvent(error=e))

        else:

            def initiate_chat(
                self=self,
                iostream: ThreadIOStream = iostream,
                response: RunResponse = response,
            ) -> None:
                with IOStream.set_default(iostream):  # type: ignore[arg-type]
                    try:
                        chat_result = self.initiate_chat(
                            recipient,
                            clear_history=clear_history,
                            silent=silent,
                            cache=cache,
                            max_turns=max_turns,
                            summary_method=summary_method,
                            summary_args=summary_args,
                            message=message,
                            **kwargs,
                        )

                        response._summary = chat_result.summary
                        response._messages = chat_result.chat_history

                        _last_speaker = recipient if chat_result.chat_history[-1]["name"] == recipient.name else self
                        if hasattr(recipient, "last_speaker"):
                            _last_speaker = recipient.last_speaker

                        IOStream.get_default().send(
                            RunCompletionEvent(
                                history=chat_result.chat_history,
                                summary=chat_result.summary,
                                cost=chat_result.cost,
                                last_speaker=_last_speaker.name,
                            )
                        )
                    except Exception as e:
                        response.iostream.send(ErrorEvent(error=e))

        threading.Thread(
            target=initiate_chat,
        ).start()

        return response

    async def a_initiate_chat(
        self,
        recipient: "ConversableAgent",
        clear_history: bool = True,
        silent: bool | None = False,
        cache: AbstractCache | None = None,
        max_turns: int | None = None,
        summary_method: str | Callable[..., Any] | None = DEFAULT_SUMMARY_METHOD,
        summary_args: dict[str, Any] | None = {},
        message: str | Callable[..., Any] | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """(async) Initiate a chat with the recipient agent.

        Reset the consecutive auto reply counter.
        If `clear_history` is True, the chat history with the recipient agent will be cleared.
        `a_generate_init_message` is called to generate the initial message for the agent.

        Args: Please refer to `initiate_chat`.

        Returns:
            ChatResult: an ChatResult object.
        """
        iostream = IOStream.get_default()

        _chat_info = locals().copy()
        _chat_info["sender"] = self
        consolidate_chat_info(_chat_info, uniform_sender=self)
        for agent in [self, recipient]:
            agent.previous_cache = agent.client_cache
            agent.client_cache = cache
        if isinstance(max_turns, int):
            self._prepare_chat(recipient, clear_history, reply_at_receive=False)
            is_termination = False
            for _ in range(max_turns):
                if _ == 0:
                    if isinstance(message, Callable):
                        msg2send = message(_chat_info["sender"], _chat_info["recipient"], kwargs)
                    else:
                        msg2send = await self.a_generate_init_message(message, **kwargs)
                else:
                    last_message = self.chat_messages[recipient][-1]
                    if self._should_terminate_chat(recipient, last_message):
                        break
                    msg2send = await self.a_generate_reply(messages=self.chat_messages[recipient], sender=recipient)
                    if msg2send is None:
                        break
                await self.a_send(msg2send, recipient, request_reply=True, silent=silent)
            else:  # No breaks in the for loop, so we have reached max turns
                iostream.send(
                    TerminationEvent(
                        termination_reason=f"Maximum turns ({max_turns}) reached", sender=self, recipient=recipient
                    )
                )
        else:
            self._prepare_chat(recipient, clear_history)
            if isinstance(message, Callable):
                msg2send = message(_chat_info["sender"], _chat_info["recipient"], kwargs)
            else:
                msg2send = await self.a_generate_init_message(message, **kwargs)
            await self.a_send(msg2send, recipient, silent=silent)
        summary = self._summarize_chat(
            summary_method,
            summary_args,
            recipient,
            cache=cache,
        )
        for agent in [self, recipient]:
            agent.client_cache = agent.previous_cache
            agent.previous_cache = None
        chat_result = ChatResult(
            chat_history=self.chat_messages[recipient],
            summary=summary,
            cost=gather_usage_summary([self, recipient]),
            human_input=self._human_input,
        )
        return chat_result

    async def a_run(
        self,
        recipient: Optional["ConversableAgent"] = None,
        clear_history: bool = True,
        silent: bool | None = False,
        cache: AbstractCache | None = None,
        max_turns: int | None = None,
        summary_method: str | Callable[..., Any] | None = DEFAULT_SUMMARY_METHOD,
        summary_args: dict[str, Any] | None = {},
        message: dict[str, Any] | str | Callable[..., Any] | None = None,
        executor_kwargs: dict[str, Any] | None = None,
        tools: Tool | Iterable[Tool] | None = None,
        user_input: bool | None = False,
        msg_to: str | None = "agent",
        **kwargs: Any,
    ) -> AsyncRunResponseProtocol:
        iostream = AsyncThreadIOStream()
        agents = [self, recipient] if recipient else [self]
        response = AsyncRunResponse(iostream, agents=agents)

        if recipient is None:

            async def initiate_chat(
                self=self,
                iostream: AsyncThreadIOStream = iostream,
                response: AsyncRunResponse = response,
            ) -> None:
                with (
                    IOStream.set_default(iostream),
                    self._create_or_get_executor(
                        executor_kwargs=executor_kwargs,
                        tools=tools,
                        agent_name="user",
                        agent_human_input_mode="ALWAYS" if user_input else "NEVER",
                    ) as executor,
                ):
                    try:
                        if msg_to == "agent":
                            chat_result = await executor.a_initiate_chat(
                                self,
                                message=message,
                                clear_history=clear_history,
                                max_turns=max_turns,
                                summary_method=summary_method,
                            )
                        else:
                            chat_result = await self.a_initiate_chat(
                                executor,
                                message=message,
                                clear_history=clear_history,
                                max_turns=max_turns,
                                summary_method=summary_method,
                            )

                        IOStream.get_default().send(
                            RunCompletionEvent(
                                history=chat_result.chat_history,
                                summary=chat_result.summary,
                                cost=chat_result.cost,
                                last_speaker=self.name,
                            )
                        )
                    except Exception as e:
                        response.iostream.send(ErrorEvent(error=e))

        else:

            async def initiate_chat(
                self=self,
                iostream: AsyncThreadIOStream = iostream,
                response: AsyncRunResponse = response,
            ) -> None:
                with IOStream.set_default(iostream):  # type: ignore[arg-type]
                    try:
                        chat_result = await self.a_initiate_chat(
                            recipient,
                            clear_history=clear_history,
                            silent=silent,
                            cache=cache,
                            max_turns=max_turns,
                            summary_method=summary_method,
                            summary_args=summary_args,
                            message=message,
                            **kwargs,
                        )

                        last_speaker = recipient if chat_result.chat_history[-1]["name"] == recipient.name else self
                        if hasattr(recipient, "last_speaker"):
                            last_speaker = recipient.last_speaker

                        IOStream.get_default().send(
                            RunCompletionEvent(
                                history=chat_result.chat_history,
                                summary=chat_result.summary,
                                cost=chat_result.cost,
                                last_speaker=last_speaker.name,
                            )
                        )

                    except Exception as e:
                        response.iostream.send(ErrorEvent(error=e))

        asyncio.create_task(initiate_chat())

        return response

    def _summarize_chat(
        self,
        summary_method,
        summary_args,
        recipient: Agent | None = None,
        cache: AbstractCache | None = None,
    ) -> str:
        """Get a chat summary from an agent participating in a chat.

        Args:
            summary_method (str or callable): the summary_method to get the summary.
                The callable summary_method should take the recipient and sender agent in a chat as input and return a string of summary. E.g,
                ```python
                def my_summary_method(
                    sender: ConversableAgent,
                    recipient: ConversableAgent,
                    summary_args: dict,
                ):
                    return recipient.last_message(sender)["content"]
                ```
            summary_args (dict): a dictionary of arguments to be passed to the summary_method.
            recipient: the recipient agent in a chat.
            cache: the cache client to be used for this conversation. When provided,
                the cache will be used to store and retrieve LLM responses when generating
                summaries, which can improve performance and reduce API costs for
                repetitive summary requests. The cache is passed to the summary_method
                via summary_args['cache'].

        Returns:
            str: a chat summary from the agent.
        """
        summary = ""
        if summary_method is None:
            return summary
        if "cache" not in summary_args:
            summary_args["cache"] = cache
        if summary_method == "reflection_with_llm":
            summary_method = self._reflection_with_llm_as_summary
        elif summary_method == "last_msg":
            summary_method = self._last_msg_as_summary

        if isinstance(summary_method, Callable):
            summary = summary_method(self, recipient, summary_args)
        else:
            raise ValueError(
                "If not None, the summary_method must be a string from [`reflection_with_llm`, `last_msg`] or a callable."
            )
        if isinstance(summary, dict):
            summary = str(summary.get("content", ""))
        return summary

    @staticmethod
    def _last_msg_as_summary(sender, recipient, summary_args) -> str:
        """Get a chat summary from the last message of the recipient."""
        summary = ""
        try:
            content = recipient.last_message(sender)["content"]
            if isinstance(content, str):
                summary = content.replace("TERMINATE", "")
            elif isinstance(content, list):
                # Remove the `TERMINATE` word in the content list.
                summary = "\n".join(
                    x["text"].replace("TERMINATE", "") for x in content if isinstance(x, dict) and "text" in x
                )
        except (IndexError, AttributeError) as e:
            warnings.warn(f"Cannot extract summary using last_msg: {e}. Using an empty str as summary.", UserWarning)
        return summary

    @staticmethod
    def _reflection_with_llm_as_summary(sender, recipient, summary_args):
        prompt = summary_args.get("summary_prompt")
        prompt = ConversableAgent.DEFAULT_SUMMARY_PROMPT if prompt is None else prompt
        if not isinstance(prompt, str):
            raise ValueError("The summary_prompt must be a string.")
        msg_list = recipient.chat_messages_for_summary(sender)
        agent = sender if recipient is None else recipient
        role = summary_args.get("summary_role", None)
        if role and not isinstance(role, str):
            raise ValueError("The summary_role in summary_arg must be a string.")
        try:
            summary = sender._reflection_with_llm(
                prompt, msg_list, llm_agent=agent, cache=summary_args.get("cache"), role=role
            )
        except Exception as e:
            warnings.warn(
                f"Cannot extract summary using reflection_with_llm: {e}. Using an empty str as summary.", UserWarning
            )
            summary = ""
        return summary

    def _reflection_with_llm(
        self,
        prompt,
        messages,
        llm_agent: Agent | None = None,
        cache: AbstractCache | None = None,
        role: str | None = None,
    ) -> str:
        """Get a chat summary using reflection with an llm client based on the conversation history.

        Args:
            prompt (str): The prompt (in this method it is used as system prompt) used to get the summary.
            messages (list): The messages generated as part of a chat conversation.
            llm_agent: the agent with an llm client.
            cache (AbstractCache or None): the cache client to be used for this conversation.
            role (str): the role of the message, usually "system" or "user". Default is "system".
        """
        if not role:
            role = "system"

        system_msg = [
            {
                "role": role,
                "content": prompt,
            }
        ]

        messages = messages + system_msg
        if llm_agent and llm_agent.client is not None:
            llm_client = llm_agent.client
        elif self.client is not None:
            llm_client = self.client
        else:
            raise ValueError("No OpenAIWrapper client is found.")
        response = self._generate_oai_reply_from_client(llm_client=llm_client, messages=messages, cache=cache)
        return response

    def _check_chat_queue_for_sender(self, chat_queue: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Check the chat queue and add the "sender" key if it's missing.

        Args:
            chat_queue (List[Dict[str, Any]]): A list of dictionaries containing chat information.

        Returns:
            List[Dict[str, Any]]: A new list of dictionaries with the "sender" key added if it was missing.
        """
        chat_queue_with_sender = []
        for chat_info in chat_queue:
            if chat_info.get("sender") is None:
                chat_info["sender"] = self
            chat_queue_with_sender.append(chat_info)
        return chat_queue_with_sender

    def initiate_chats(self, chat_queue: list[dict[str, Any]]) -> list[ChatResult]:
        """(Experimental) Initiate chats with multiple agents.

        Args:
            chat_queue (List[Dict]): a list of dictionaries containing the information of the chats.
                Each dictionary should contain the input arguments for [`initiate_chat`](#initiate-chat)

        Returns: a list of ChatResult objects corresponding to the finished chats in the chat_queue.
        """
        _chat_queue = self._check_chat_queue_for_sender(chat_queue)
        self._finished_chats = initiate_chats(_chat_queue)

        return self._finished_chats

    def sequential_run(
        self,
        chat_queue: list[dict[str, Any]],
    ) -> list[RunResponseProtocol]:
        """(Experimental) Initiate chats with multiple agents sequentially.

        Args:
            chat_queue (List[Dict]): a list of dictionaries containing the information of the chats.
                Each dictionary should contain the input arguments for [`initiate_chat`](#initiate-chat)

        Returns: a list of ChatResult objects corresponding to the finished chats in the chat_queue.
        """
        iostreams = [ThreadIOStream() for _ in range(len(chat_queue))]
        # todo: add agents
        responses = [RunResponse(iostream, agents=[]) for iostream in iostreams]

        def _initiate_chats(
            iostreams: list[ThreadIOStream] = iostreams,
            responses: list[RunResponseProtocol] = responses,
        ) -> None:
            response = responses[0]
            try:
                _chat_queue = self._check_chat_queue_for_sender(chat_queue)

                consolidate_chat_info(_chat_queue)
                _validate_recipients(_chat_queue)
                finished_chats = []
                for chat_info, response, iostream in zip(_chat_queue, responses, iostreams):
                    with IOStream.set_default(iostream):
                        _chat_carryover = chat_info.get("carryover", [])
                        finished_chat_indexes_to_exclude_from_carryover = chat_info.get(
                            "finished_chat_indexes_to_exclude_from_carryover", []
                        )

                        if isinstance(_chat_carryover, str):
                            _chat_carryover = [_chat_carryover]
                        chat_info["carryover"] = _chat_carryover + [
                            r.summary
                            for i, r in enumerate(finished_chats)
                            if i not in finished_chat_indexes_to_exclude_from_carryover
                        ]

                        if not chat_info.get("silent", False):
                            IOStream.get_default().send(PostCarryoverProcessingEvent(chat_info=chat_info))

                        sender = chat_info["sender"]
                        chat_res = sender.initiate_chat(**chat_info)

                        IOStream.get_default().send(
                            RunCompletionEvent(
                                history=chat_res.chat_history,
                                summary=chat_res.summary,
                                cost=chat_res.cost,
                                last_speaker=(self if chat_res.chat_history[-1]["name"] == self.name else sender).name,
                            )
                        )

                        finished_chats.append(chat_res)
            except Exception as e:
                response.iostream.send(ErrorEvent(error=e))

        threading.Thread(target=_initiate_chats).start()

        return responses

    async def a_initiate_chats(self, chat_queue: list[dict[str, Any]]) -> dict[int, ChatResult]:
        _chat_queue = self._check_chat_queue_for_sender(chat_queue)
        self._finished_chats = await a_initiate_chats(_chat_queue)
        return self._finished_chats

    async def a_sequential_run(
        self,
        chat_queue: list[dict[str, Any]],
    ) -> list[AsyncRunResponseProtocol]:
        """(Experimental) Initiate chats with multiple agents sequentially.

        Args:
            chat_queue (List[Dict]): a list of dictionaries containing the information of the chats.
                Each dictionary should contain the input arguments for [`initiate_chat`](#initiate-chat)

        Returns: a list of ChatResult objects corresponding to the finished chats in the chat_queue.
        """
        iostreams = [AsyncThreadIOStream() for _ in range(len(chat_queue))]
        # todo: add agents
        responses = [AsyncRunResponse(iostream, agents=[]) for iostream in iostreams]

        async def _a_initiate_chats(
            iostreams: list[AsyncThreadIOStream] = iostreams,
            responses: list[AsyncRunResponseProtocol] = responses,
        ) -> None:
            response = responses[0]
            try:
                _chat_queue = self._check_chat_queue_for_sender(chat_queue)

                consolidate_chat_info(_chat_queue)
                _validate_recipients(_chat_queue)
                finished_chats = []
                for chat_info, response, iostream in zip(_chat_queue, responses, iostreams):
                    with IOStream.set_default(iostream):
                        _chat_carryover = chat_info.get("carryover", [])
                        finished_chat_indexes_to_exclude_from_carryover = chat_info.get(
                            "finished_chat_indexes_to_exclude_from_carryover", []
                        )

                        if isinstance(_chat_carryover, str):
                            _chat_carryover = [_chat_carryover]
                        chat_info["carryover"] = _chat_carryover + [
                            r.summary
                            for i, r in enumerate(finished_chats)
                            if i not in finished_chat_indexes_to_exclude_from_carryover
                        ]

                        if not chat_info.get("silent", False):
                            IOStream.get_default().send(PostCarryoverProcessingEvent(chat_info=chat_info))

                        sender = chat_info["sender"]
                        chat_res = await sender.a_initiate_chat(**chat_info)

                        IOStream.get_default().send(
                            RunCompletionEvent(
                                history=chat_res.chat_history,
                                summary=chat_res.summary,
                                cost=chat_res.cost,
                                last_speaker=(self if chat_res.chat_history[-1]["name"] == self.name else sender).name,
                            )
                        )

                        finished_chats.append(chat_res)

            except Exception as e:
                response.iostream.send(ErrorEvent(error=e))

        asyncio.create_task(_a_initiate_chats())

        return responses

    def get_chat_results(self, chat_index: int | None = None) -> list[ChatResult] | ChatResult:
        """A summary from the finished chats of particular agents."""
        if chat_index is not None:
            return self._finished_chats[chat_index]
        else:
            return self._finished_chats

    def reset(self) -> None:
        """Reset the agent."""
        self.clear_history()
        self.reset_consecutive_auto_reply_counter()
        self.stop_reply_at_receive()
        if self.client is not None:
            self.client.clear_usage_summary()
        for reply_func_tuple in self._reply_func_list:
            if reply_func_tuple["reset_config"] is not None:
                reply_func_tuple["reset_config"](reply_func_tuple["config"])
            else:
                reply_func_tuple["config"] = copy.copy(reply_func_tuple["init_config"])

    def stop_reply_at_receive(self, sender: Agent | None = None):
        """Reset the reply_at_receive of the sender."""
        if sender is None:
            self.reply_at_receive.clear()
        else:
            self.reply_at_receive[sender] = False

    def reset_consecutive_auto_reply_counter(self, sender: Agent | None = None):
        """Reset the consecutive_auto_reply_counter of the sender."""
        if sender is None:
            self._consecutive_auto_reply_counter.clear()
        else:
            self._consecutive_auto_reply_counter[sender] = 0

    def clear_history(self, recipient: Agent | None = None, nr_messages_to_preserve: int | None = None):
        """Clear the chat history of the agent.

        Args:
            recipient: the agent with whom the chat history to clear. If None, clear the chat history with all agents.
            nr_messages_to_preserve: the number of newest messages to preserve in the chat history.
        """
        iostream = IOStream.get_default()
        if recipient is None:
            no_messages_preserved = 0
            if nr_messages_to_preserve:
                for key in self._oai_messages:
                    nr_messages_to_preserve_internal = nr_messages_to_preserve
                    # if breaking history between function call and function response, save function call message
                    # additionally, otherwise openai will return error
                    first_msg_to_save = self._oai_messages[key][-nr_messages_to_preserve_internal]
                    if "tool_responses" in first_msg_to_save:
                        nr_messages_to_preserve_internal += 1
                        # clear_conversable_agent_history.print_preserving_message(iostream.print)
                        no_messages_preserved += 1
                    # Remove messages from history except last `nr_messages_to_preserve` messages.
                    self._oai_messages[key] = self._oai_messages[key][-nr_messages_to_preserve_internal:]
                iostream.send(ClearConversableAgentHistoryEvent(agent=self, no_events_preserved=no_messages_preserved))
            else:
                self._oai_messages.clear()
        else:
            self._oai_messages[recipient].clear()
            # clear_conversable_agent_history.print_warning(iostream.print)
            if nr_messages_to_preserve:
                iostream.send(ClearConversableAgentHistoryWarningEvent(recipient=self))

    def generate_oai_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Generate a reply using autogen.oai."""
        client = self.client if config is None else config
        if client is None:
            return False, None
        if messages is None:
            messages = self._oai_messages[sender]

        # Process messages before sending to LLM, hook point for llm input monitoring
        processed_messages = self._process_llm_input(self._oai_system_message + messages)
        if processed_messages is None:
            return True, {"content": "LLM call blocked by safeguard", "role": "assistant"}

        extracted_response = self._generate_oai_reply_from_client(client, processed_messages, self.client_cache)

        # Process LLM response
        if extracted_response is not None:
            processed_extracted_response = self._process_llm_output(extracted_response)
            if processed_extracted_response is None:
                raise ValueError("safeguard_llm_outputs hook returned None")

        return (False, None) if extracted_response is None else (True, extracted_response)

    def _generate_oai_reply_from_client(self, llm_client, messages, cache) -> str | dict[str, Any] | None:
        # unroll tool_responses
        all_messages = []
        for message in messages:
            tool_responses = message.get("tool_responses", [])
            if tool_responses:
                all_messages += tool_responses
                # tool role on the parent message means the content is just concatenation of all of the tool_responses
                if message.get("role") != "tool":
                    all_messages.append({key: message[key] for key in message if key != "tool_responses"})
            else:
                all_messages.append(message)

        # TODO: #1143 handle token limit exceeded error
        response = llm_client.create(
            context=messages[-1].pop("context", None),
            messages=all_messages,
            cache=cache,
            agent=self,
        )
        extracted_response = llm_client.extract_text_or_completion_object(response)[0]

        if extracted_response is None:
            warnings.warn(f"Extracted_response from {response} is None.", UserWarning)
            return None
        # ensure function and tool calls will be accepted when sent back to the LLM
        if not isinstance(extracted_response, str) and hasattr(extracted_response, "model_dump"):
            extracted_response = extracted_response.model_dump()
        if isinstance(extracted_response, dict):
            if extracted_response.get("function_call"):
                extracted_response["function_call"]["name"] = self._normalize_name(
                    extracted_response["function_call"]["name"]
                )
            for tool_call in extracted_response.get("tool_calls") or []:
                tool_call["function"]["name"] = self._normalize_name(tool_call["function"]["name"])
                # Remove id and type if they are not present.
                # This is to make the tool call object compatible with Mistral API.
                if tool_call.get("id") is None:
                    tool_call.pop("id")
                if tool_call.get("type") is None:
                    tool_call.pop("type")
        return extracted_response

    async def a_generate_oai_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Generate a reply using autogen.oai asynchronously."""
        iostream = IOStream.get_default()

        def _generate_oai_reply(
            self, iostream: IOStream, *args: Any, **kwargs: Any
        ) -> tuple[bool, str | dict[str, Any] | None]:
            with IOStream.set_default(iostream):
                return self.generate_oai_reply(*args, **kwargs)

        return await asyncio.get_event_loop().run_in_executor(
            None,
            functools.partial(
                _generate_oai_reply, self=self, iostream=iostream, messages=messages, sender=sender, config=config
            ),
        )

    def _generate_code_execution_reply_using_executor(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: dict[str, Any] | Literal[False] | None = None,
    ):
        """Generate a reply using code executor."""
        iostream = IOStream.get_default()

        if config is not None:
            raise ValueError("config is not supported for _generate_code_execution_reply_using_executor.")
        if self._code_execution_config is False:
            return False, None
        if messages is None:
            messages = self._oai_messages[sender]
        last_n_messages = self._code_execution_config.get("last_n_messages", "auto")

        if not (isinstance(last_n_messages, (int, float)) and last_n_messages >= 0) and last_n_messages != "auto":
            raise ValueError("last_n_messages must be either a non-negative integer, or the string 'auto'.")

        num_messages_to_scan = last_n_messages
        if last_n_messages == "auto":
            # Find when the agent last spoke
            num_messages_to_scan = 0
            for message in reversed(messages):
                if "role" not in message or message["role"] != "user":
                    break
                else:
                    num_messages_to_scan += 1
        num_messages_to_scan = min(len(messages), num_messages_to_scan)
        messages_to_scan = messages[-num_messages_to_scan:]

        # iterate through the last n messages in reverse
        # if code blocks are found, execute the code blocks and return the output
        # if no code blocks are found, continue
        for message in reversed(messages_to_scan):
            if not message["content"]:
                continue
            code_blocks = self._code_executor.code_extractor.extract_code_blocks(message["content"])
            if len(code_blocks) == 0:
                continue

            iostream.send(GenerateCodeExecutionReplyEvent(code_blocks=code_blocks, sender=sender, recipient=self))

            # found code blocks, execute code.
            code_result = self._code_executor.execute_code_blocks(code_blocks)
            exitcode2str = "execution succeeded" if code_result.exit_code == 0 else "execution failed"
            return True, f"exitcode: {code_result.exit_code} ({exitcode2str})\nCode output: {code_result.output}"

        return False, None

    def generate_code_execution_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: dict[str, Any] | Literal[False] | None = None,
    ):
        """Generate a reply using code execution."""
        code_execution_config = config if config is not None else self._code_execution_config
        if code_execution_config is False:
            return False, None
        if messages is None:
            messages = self._oai_messages[sender]
        last_n_messages = code_execution_config.pop("last_n_messages", "auto")

        if not (isinstance(last_n_messages, (int, float)) and last_n_messages >= 0) and last_n_messages != "auto":
            raise ValueError("last_n_messages must be either a non-negative integer, or the string 'auto'.")

        messages_to_scan = last_n_messages
        if last_n_messages == "auto":
            # Find when the agent last spoke
            messages_to_scan = 0
            for i in range(len(messages)):
                message = messages[-(i + 1)]
                if "role" not in message or message["role"] != "user":
                    break
                else:
                    messages_to_scan += 1

        # iterate through the last n messages in reverse
        # if code blocks are found, execute the code blocks and return the output
        # if no code blocks are found, continue
        for i in range(min(len(messages), messages_to_scan)):
            message = messages[-(i + 1)]
            if not message["content"]:
                continue
            code_blocks = extract_code(message["content"])
            if len(code_blocks) == 1 and code_blocks[0][0] == UNKNOWN:
                continue

            # found code blocks, execute code and push "last_n_messages" back
            exitcode, logs = self.execute_code_blocks(code_blocks)
            code_execution_config["last_n_messages"] = last_n_messages
            exitcode2str = "execution succeeded" if exitcode == 0 else "execution failed"
            return True, f"exitcode: {exitcode} ({exitcode2str})\nCode output: {logs}"

        # no code blocks are found, push last_n_messages back and return.
        code_execution_config["last_n_messages"] = last_n_messages

        return False, None

    def _run_async_in_thread(self, coro):
        """Run an async coroutine in a separate thread with its own event loop."""
        result = {}

        def runner():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result["value"] = loop.run_until_complete(coro)
            loop.close()

        t = threading.Thread(target=runner)
        t.start()
        t.join()
        return result["value"]

    def generate_function_call_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, dict[str, Any] | None]:
        """Generate a reply using function call.

        "function_call" replaced by "tool_calls" as of [OpenAI API v1.1.0](https://github.com/openai/openai-python/releases/tag/v1.1.0)
        See https://platform.openai.com/docs/api-reference/chat/create#chat-create-functions
        """
        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        if message.get("function_call"):
            call_id = message.get("id", None)
            func_call = message["function_call"]
            func = self._function_map.get(func_call.get("name", None), None)
            if inspect.iscoroutinefunction(func):
                coro = self.a_execute_function(func_call, call_id=call_id)
                _, func_return = self._run_async_in_thread(coro)
            else:
                _, func_return = self.execute_function(message["function_call"], call_id=call_id)
            return True, func_return
        return False, None

    async def a_generate_function_call_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, dict[str, Any] | None]:
        """Generate a reply using async function call.

        "function_call" replaced by "tool_calls" as of [OpenAI API v1.1.0](https://github.com/openai/openai-python/releases/tag/v1.1.0)
        See https://platform.openai.com/docs/api-reference/chat/create#chat-create-functions
        """
        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        if "function_call" in message:
            call_id = message.get("id", None)
            func_call = message["function_call"]
            func_name = func_call.get("name", "")
            func = self._function_map.get(func_name, None)
            if func and inspect.iscoroutinefunction(func):
                _, func_return = await self.a_execute_function(func_call, call_id=call_id)
            else:
                _, func_return = self.execute_function(func_call, call_id=call_id)
            return True, func_return

        return False, None

    def _str_for_tool_response(self, tool_response):
        return str(tool_response.get("content", ""))

    def generate_tool_calls_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, dict[str, Any] | None]:
        """Generate a reply using tool call."""
        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        tool_returns = []
        for tool_call in message.get("tool_calls", []):
            function_call = tool_call.get("function", {})

            # Hook: Process tool input before execution
            processed_call = self._process_tool_input(function_call)
            if processed_call is None:
                raise ValueError("safeguard_tool_inputs hook returned None")

            tool_call_id = tool_call.get("id", None)
            func = self._function_map.get(processed_call.get("name", None), None)
            if is_coroutine_callable(func):
                coro = self.a_execute_function(processed_call, call_id=tool_call_id)
                _, func_return = self._run_async_in_thread(coro)
            else:
                _, func_return = self.execute_function(processed_call, call_id=tool_call_id)

            # Hook: Process tool output before returning
            processed_return = self._process_tool_output(func_return)
            if processed_return is None:
                raise ValueError("safeguard_tool_outputs hook returned None")

            content = processed_return.get("content", "")
            if content is None:
                content = ""

            if tool_call_id is not None:
                tool_call_response = {
                    "tool_call_id": tool_call_id,
                    "role": "tool",
                    "content": content,
                }
            else:
                # Do not include tool_call_id if it is not present.
                # This is to make the tool call object compatible with Mistral API.
                tool_call_response = {
                    "role": "tool",
                    "content": content,
                }
            tool_returns.append(tool_call_response)
        if tool_returns:
            return True, {
                "role": "tool",
                "tool_responses": tool_returns,
                "content": "\n\n".join([self._str_for_tool_response(tool_return) for tool_return in tool_returns]),
            }
        return False, None

    async def _a_execute_tool_call(self, tool_call):
        tool_call_id = tool_call["id"]
        function_call = tool_call.get("function", {})
        _, func_return = await self.a_execute_function(function_call, call_id=tool_call_id)
        return {
            "tool_call_id": tool_call_id,
            "role": "tool",
            "content": func_return.get("content", ""),
        }

    async def a_generate_tool_calls_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, dict[str, Any] | None]:
        """Generate a reply using async function call."""
        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        async_tool_calls = []
        for tool_call in message.get("tool_calls", []):
            async_tool_calls.append(self._a_execute_tool_call(tool_call))
        if async_tool_calls:
            tool_returns = await asyncio.gather(*async_tool_calls)
            return True, {
                "role": "tool",
                "tool_responses": tool_returns,
                "content": "\n\n".join([self._str_for_tool_response(tool_return) for tool_return in tool_returns]),
            }

        return False, None

    def check_termination_and_human_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | None]:
        """Check if the conversation should be terminated, and if human reply is provided.

        This method checks for conditions that require the conversation to be terminated, such as reaching
        a maximum number of consecutive auto-replies or encountering a termination message. Additionally,
        it prompts for and processes human input based on the configured human input mode, which can be
        'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter
        for the conversation and prints relevant messages based on the human input received.

        Args:
            messages: A list of message dictionaries, representing the conversation history.
            sender: The agent object representing the sender of the message.
            config: Configuration object, defaults to the current instance if not provided.

        Returns:
            A tuple containing a boolean indicating if the conversation
            should be terminated, and a human reply which can be a string, a dictionary, or None.
        """
        iostream = IOStream.get_default()

        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender] if sender else []

        termination_reason = None

        # if there are no messages, continue the conversation
        if not messages:
            return False, None
        message = messages[-1]

        reply = ""
        no_human_input_msg = ""
        sender_name = "the sender" if sender is None else sender.name
        if self.human_input_mode == "ALWAYS":
            reply = self.get_human_input(
                f"Replying as {self.name}. Provide feedback to {sender_name}. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: "
            )
            no_human_input_msg = "NO HUMAN INPUT RECEIVED." if not reply else ""
            # if the human input is empty, and the message is a termination message, then we will terminate the conversation
            if not reply and self._is_termination_msg(message):
                termination_reason = f"Termination message condition on agent '{self.name}' met"
            elif reply == "exit":
                termination_reason = "User requested to end the conversation"

            reply = reply if reply or not self._is_termination_msg(message) else "exit"
        else:
            if self._consecutive_auto_reply_counter[sender] >= self._max_consecutive_auto_reply_dict[sender]:
                if self.human_input_mode == "NEVER":
                    termination_reason = "Maximum number of consecutive auto-replies reached"
                    reply = "exit"
                else:
                    # self.human_input_mode == "TERMINATE":
                    terminate = self._is_termination_msg(message)
                    reply = self.get_human_input(
                        f"Please give feedback to {sender_name}. Press enter or type 'exit' to stop the conversation: "
                        if terminate
                        else f"Please give feedback to {sender_name}. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: "
                    )
                    no_human_input_msg = "NO HUMAN INPUT RECEIVED." if not reply else ""
                    # if the human input is empty, and the message is a termination message, then we will terminate the conversation
                    if reply != "exit" and terminate:
                        termination_reason = (
                            f"Termination message condition on agent '{self.name}' met and no human input provided"
                        )
                    elif reply == "exit":
                        termination_reason = "User requested to end the conversation"

                    reply = reply if reply or not terminate else "exit"
            elif self._is_termination_msg(message):
                if self.human_input_mode == "NEVER":
                    termination_reason = f"Termination message condition on agent '{self.name}' met"
                    reply = "exit"
                else:
                    # self.human_input_mode == "TERMINATE":
                    reply = self.get_human_input(
                        f"Please give feedback to {sender_name}. Press enter or type 'exit' to stop the conversation: "
                    )
                    no_human_input_msg = "NO HUMAN INPUT RECEIVED." if not reply else ""

                    # if the human input is empty, and the message is a termination message, then we will terminate the conversation
                    if not reply or reply == "exit":
                        termination_reason = (
                            f"Termination message condition on agent '{self.name}' met and no human input provided"
                        )

                    reply = reply or "exit"

        # print the no_human_input_msg
        if no_human_input_msg:
            iostream.send(
                TerminationAndHumanReplyNoInputEvent(
                    no_human_input_msg=no_human_input_msg, sender=sender, recipient=self
                )
            )

        # stop the conversation
        if reply == "exit":
            # reset the consecutive_auto_reply_counter
            self._consecutive_auto_reply_counter[sender] = 0

            if termination_reason:
                iostream.send(TerminationEvent(termination_reason=termination_reason, sender=self, recipient=sender))

            return True, None

        # send the human reply
        if reply or self._max_consecutive_auto_reply_dict[sender] == 0:
            # reset the consecutive_auto_reply_counter
            self._consecutive_auto_reply_counter[sender] = 0
            # User provided a custom response, return function and tool failures indicating user interruption
            tool_returns = []
            if message.get("function_call", False):
                tool_returns.append({
                    "role": "function",
                    "name": message["function_call"].get("name", ""),
                    "content": "USER INTERRUPTED",
                })

            if message.get("tool_calls", False):
                tool_returns.extend([
                    {"role": "tool", "tool_call_id": tool_call.get("id", ""), "content": "USER INTERRUPTED"}
                    for tool_call in message["tool_calls"]
                ])

            response = {"role": "user", "content": reply}
            if tool_returns:
                response["tool_responses"] = tool_returns

            return True, response

        # increment the consecutive_auto_reply_counter
        self._consecutive_auto_reply_counter[sender] += 1
        if self.human_input_mode != "NEVER":
            iostream.send(UsingAutoReplyEvent(human_input_mode=self.human_input_mode, sender=sender, recipient=self))

        return False, None

    async def a_check_termination_and_human_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | None]:
        """(async) Check if the conversation should be terminated, and if human reply is provided.

        This method checks for conditions that require the conversation to be terminated, such as reaching
        a maximum number of consecutive auto-replies or encountering a termination message. Additionally,
        it prompts for and processes human input based on the configured human input mode, which can be
        'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter
        for the conversation and prints relevant messages based on the human input received.

        Args:
            messages (Optional[List[Dict]]): A list of message dictionaries, representing the conversation history.
            sender (Optional[Agent]): The agent object representing the sender of the message.
            config (Optional[Any]): Configuration object, defaults to the current instance if not provided.

        Returns:
            Tuple[bool, Union[str, Dict, None]]: A tuple containing a boolean indicating if the conversation
            should be terminated, and a human reply which can be a string, a dictionary, or None.
        """
        iostream = IOStream.get_default()

        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender] if sender else []

        termination_reason = None

        message = messages[-1] if messages else {}
        reply = ""
        no_human_input_msg = ""
        sender_name = "the sender" if sender is None else sender.name
        if self.human_input_mode == "ALWAYS":
            reply = await self.a_get_human_input(
                f"Replying as {self.name}. Provide feedback to {sender_name}. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: "
            )
            no_human_input_msg = "NO HUMAN INPUT RECEIVED." if not reply else ""
            # if the human input is empty, and the message is a termination message, then we will terminate the conversation
            if not reply and self._is_termination_msg(message):
                termination_reason = f"Termination message condition on agent '{self.name}' met"
            elif reply == "exit":
                termination_reason = "User requested to end the conversation"

            reply = reply if reply or not self._is_termination_msg(message) else "exit"
        else:
            if self._consecutive_auto_reply_counter[sender] >= self._max_consecutive_auto_reply_dict[sender]:
                if self.human_input_mode == "NEVER":
                    termination_reason = "Maximum number of consecutive auto-replies reached"
                    reply = "exit"
                else:
                    # self.human_input_mode == "TERMINATE":
                    terminate = self._is_termination_msg(message)
                    reply = await self.a_get_human_input(
                        f"Please give feedback to {sender_name}. Press enter or type 'exit' to stop the conversation: "
                        if terminate
                        else f"Please give feedback to {sender_name}. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: "
                    )
                    no_human_input_msg = "NO HUMAN INPUT RECEIVED." if not reply else ""
                    # if the human input is empty, and the message is a termination message, then we will terminate the conversation
                    if reply != "exit" and terminate:
                        termination_reason = (
                            f"Termination message condition on agent '{self.name}' met and no human input provided"
                        )
                    elif reply == "exit":
                        termination_reason = "User requested to end the conversation"

                    reply = reply if reply or not terminate else "exit"
            elif self._is_termination_msg(message):
                if self.human_input_mode == "NEVER":
                    termination_reason = f"Termination message condition on agent '{self.name}' met"
                    reply = "exit"
                else:
                    # self.human_input_mode == "TERMINATE":
                    reply = await self.a_get_human_input(
                        f"Please give feedback to {sender_name}. Press enter or type 'exit' to stop the conversation: "
                    )
                    no_human_input_msg = "NO HUMAN INPUT RECEIVED." if not reply else ""

                    # if the human input is empty, and the message is a termination message, then we will terminate the conversation
                    if not reply or reply == "exit":
                        termination_reason = (
                            f"Termination message condition on agent '{self.name}' met and no human input provided"
                        )

                    reply = reply or "exit"

        # print the no_human_input_msg
        if no_human_input_msg:
            iostream.send(
                TerminationAndHumanReplyNoInputEvent(
                    no_human_input_msg=no_human_input_msg, sender=sender, recipient=self
                )
            )

        # stop the conversation
        if reply == "exit":
            # reset the consecutive_auto_reply_counter
            self._consecutive_auto_reply_counter[sender] = 0

            if termination_reason:
                iostream.send(TerminationEvent(termination_reason=termination_reason, sender=self, recipient=sender))

            return True, None

        # send the human reply
        if reply or self._max_consecutive_auto_reply_dict[sender] == 0:
            # User provided a custom response, return function and tool results indicating user interruption
            # reset the consecutive_auto_reply_counter
            self._consecutive_auto_reply_counter[sender] = 0
            tool_returns = []
            if message.get("function_call", False):
                tool_returns.append({
                    "role": "function",
                    "name": message["function_call"].get("name", ""),
                    "content": "USER INTERRUPTED",
                })

            if message.get("tool_calls", False):
                tool_returns.extend([
                    {"role": "tool", "tool_call_id": tool_call.get("id", ""), "content": "USER INTERRUPTED"}
                    for tool_call in message["tool_calls"]
                ])

            response = {"role": "user", "content": reply}
            if tool_returns:
                response["tool_responses"] = tool_returns

            return True, response

        # increment the consecutive_auto_reply_counter
        self._consecutive_auto_reply_counter[sender] += 1
        if self.human_input_mode != "NEVER":
            iostream.send(UsingAutoReplyEvent(human_input_mode=self.human_input_mode, sender=sender, recipient=self))

        return False, None

    def generate_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Optional["Agent"] = None,
        **kwargs: Any,
    ) -> str | dict[str, Any] | None:
        """Reply based on the conversation history and the sender.

        Either messages or sender must be provided.
        Register a reply_func with `None` as one trigger for it to be activated when `messages` is non-empty and `sender` is `None`.
        Use registered auto reply functions to generate replies.
        By default, the following functions are checked in order:
        1. check_termination_and_human_reply
        2. generate_function_call_reply (deprecated in favor of tool_calls)
        3. generate_tool_calls_reply
        4. generate_code_execution_reply
        5. generate_oai_reply
        Every function returns a tuple (final, reply).
        When a function returns final=False, the next function will be checked.
        So by default, termination and human reply will be checked first.
        If not terminating and human reply is skipped, execute function or code and return the result.
        AI replies are generated only when no code execution is performed.

        Args:
            messages: a list of messages in the conversation history.
            sender: sender of an Agent instance.
            **kwargs (Any): Additional arguments to customize reply generation. Supported kwargs:
                - exclude (List[Callable[..., Any]]): A list of reply functions to exclude from
                the reply generation process. Functions in this list will be skipped even if
                they would normally be triggered.

        Returns:
            str or dict or None: reply. None if no reply is generated.
        """
        if all((messages is None, sender is None)):
            error_msg = f"Either {messages=} or {sender=} must be provided."
            logger.error(error_msg)
            raise AssertionError(error_msg)

        if messages is None:
            messages = self._oai_messages[sender]

        # Call the hookable method that gives registered hooks a chance to update agent state, used for their context variables.
        self.update_agent_state_before_reply(messages)

        # Call the hookable method that gives registered hooks a chance to process the last message.
        # Message modifications do not affect the incoming messages or self._oai_messages.
        messages = self.process_last_received_message(messages)

        # Call the hookable method that gives registered hooks a chance to process all messages.
        # Message modifications do not affect the incoming messages or self._oai_messages.
        messages = self.process_all_messages_before_reply(messages)

        for reply_func_tuple in self._reply_func_list:
            reply_func = reply_func_tuple["reply_func"]
            if "exclude" in kwargs and reply_func in kwargs["exclude"]:
                continue
            if inspect.iscoroutinefunction(reply_func):
                continue
            if self._match_trigger(reply_func_tuple["trigger"], sender):
                final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple["config"])
                if logging_enabled():
                    log_event(
                        self,
                        "reply_func_executed",
                        reply_func_module=reply_func.__module__,
                        reply_func_name=reply_func.__name__,
                        final=final,
                        reply=reply,
                    )
                if final:
                    return reply
        return self._default_auto_reply

    async def a_generate_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Optional["Agent"] = None,
        **kwargs: Any,
    ) -> str | dict[str, Any] | None:
        """(async) Reply based on the conversation history and the sender.

        Either messages or sender must be provided.
        Register a reply_func with `None` as one trigger for it to be activated when `messages` is non-empty and `sender` is `None`.
        Use registered auto reply functions to generate replies.
        By default, the following functions are checked in order:
        1. check_termination_and_human_reply
        2. generate_function_call_reply
        3. generate_tool_calls_reply
        4. generate_code_execution_reply
        5. generate_oai_reply
        Every function returns a tuple (final, reply).
        When a function returns final=False, the next function will be checked.
        So by default, termination and human reply will be checked first.
        If not terminating and human reply is skipped, execute function or code and return the result.
        AI replies are generated only when no code execution is performed.

        Args:
            messages: a list of messages in the conversation history.
            sender: sender of an Agent instance.
            **kwargs (Any): Additional arguments to customize reply generation. Supported kwargs:
                - exclude (List[Callable[..., Any]]): A list of reply functions to exclude from
                the reply generation process. Functions in this list will be skipped even if
                they would normally be triggered.

        Returns:
            str or dict or None: reply. None if no reply is generated.
        """
        if all((messages is None, sender is None)):
            error_msg = f"Either {messages=} or {sender=} must be provided."
            logger.error(error_msg)
            raise AssertionError(error_msg)

        if messages is None:
            messages = self._oai_messages[sender]

        # Call the hookable method that gives registered hooks a chance to update agent state, used for their context variables.
        self.update_agent_state_before_reply(messages)

        # Call the hookable method that gives registered hooks a chance to process the last message.
        # Message modifications do not affect the incoming messages or self._oai_messages.
        messages = self.process_last_received_message(messages)

        # Call the hookable method that gives registered hooks a chance to process all messages.
        # Message modifications do not affect the incoming messages or self._oai_messages.
        messages = self.process_all_messages_before_reply(messages)

        for reply_func_tuple in self._reply_func_list:
            reply_func = reply_func_tuple["reply_func"]
            if "exclude" in kwargs and reply_func in kwargs["exclude"]:
                continue

            if self._match_trigger(reply_func_tuple["trigger"], sender):
                if inspect.iscoroutinefunction(reply_func):
                    final, reply = await reply_func(
                        self, messages=messages, sender=sender, config=reply_func_tuple["config"]
                    )
                else:
                    final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple["config"])
                if final:
                    return reply
        return self._default_auto_reply

    def _match_trigger(self, trigger: None | str | type | Agent | Callable | list, sender: Agent | None) -> bool:
        """Check if the sender matches the trigger.

        Args:
            trigger (Union[None, str, type, Agent, Callable, List]): The condition to match against the sender.
            Can be `None`, string, type, `Agent` instance, callable, or a list of these.
            sender (Agent): The sender object or type to be matched against the trigger.

        Returns:
            `True` if the sender matches the trigger, otherwise `False`.

        Raises:
            ValueError: If the trigger type is unsupported.
        """
        if trigger is None:
            return sender is None
        elif isinstance(trigger, str):
            if sender is None:
                raise SenderRequiredError()
            return trigger == sender.name
        elif isinstance(trigger, type):
            return isinstance(sender, trigger)
        elif isinstance(trigger, Agent):
            # return True if the sender is the same type (class) as the trigger
            return trigger == sender
        elif isinstance(trigger, Callable):
            rst = trigger(sender)
            assert isinstance(rst, bool), f"trigger {trigger} must return a boolean value."
            return rst
        elif isinstance(trigger, list):
            return any(self._match_trigger(t, sender) for t in trigger)
        else:
            raise ValueError(f"Unsupported trigger type: {type(trigger)}")

    def get_human_input(self, prompt: str) -> str:
        """Get human input.

        Override this method to customize the way to get human input.

        Args:
            prompt (str): prompt for the human input.

        Returns:
            str: human input.
        """
        iostream = IOStream.get_default()

        reply = iostream.input(prompt)

        # Process the human input through hooks
        processed_reply = self._process_human_input(reply)
        if processed_reply is None:
            raise ValueError("safeguard_human_inputs hook returned None")

        self._human_input.append(processed_reply)
        return processed_reply

    async def a_get_human_input(self, prompt: str) -> str:
        """(Async) Get human input.

        Override this method to customize the way to get human input.

        Args:
            prompt (str): prompt for the human input.

        Returns:
            str: human input.
        """

        iostream = IOStream.get_default()
        input_func = iostream.input

        if is_coroutine_callable(input_func):
            reply = await input_func(prompt)
        else:
            reply = await asyncio.to_thread(input_func, prompt)
        self._human_input.append(reply)
        return reply

    def run_code(self, code: str, **kwargs: Any) -> tuple[int, str, str | None]:
        """Run the code and return the result.

        Override this function to modify the way to run the code.

        Args:
            code (str): the code to be executed.
            **kwargs: other keyword arguments.

        Returns:
            A tuple of (exitcode, logs, image).
            exitcode (int): the exit code of the code execution.
            logs (str): the logs of the code execution.
            image (str or None): the docker image used for the code execution.
        """
        return execute_code(code, **kwargs)

    def execute_code_blocks(self, code_blocks):
        """Execute the code blocks and return the result."""
        iostream = IOStream.get_default()

        logs_all = ""
        for i, code_block in enumerate(code_blocks):
            lang, code = code_block
            if not lang:
                lang = infer_lang(code)

            iostream.send(ExecuteCodeBlockEvent(code=code, language=lang, code_block_count=i, recipient=self))

            if lang in ["bash", "shell", "sh"]:
                exitcode, logs, image = self.run_code(code, lang=lang, **self._code_execution_config)
            elif lang in PYTHON_VARIANTS:
                filename = code[11 : code.find("\n")].strip() if code.startswith("# filename: ") else None
                exitcode, logs, image = self.run_code(
                    code,
                    lang="python",
                    filename=filename,
                    **self._code_execution_config,
                )
            else:
                # In case the language is not supported, we return an error message.
                exitcode, logs, image = (
                    1,
                    f"unknown language {lang}",
                    None,
                )
                # raise NotImplementedError
            if image is not None:
                self._code_execution_config["use_docker"] = image
            logs_all += "\n" + logs
            if exitcode != 0:
                return exitcode, logs_all
        return exitcode, logs_all

    @staticmethod
    def _format_json_str(jstr):
        """Remove newlines outside of quotes, and handle JSON escape sequences.

        1. this function removes the newline in the query outside of quotes otherwise json.loads(s) will fail.
            Ex 1:
            "{\n"tool": "python",\n"query": "print('hello')\nprint('world')"\n}" -> "{"tool": "python","query": "print('hello')\nprint('world')"}"
            Ex 2:
            "{\n  \"location\": \"Boston, MA\"\n}" -> "{"location": "Boston, MA"}"

        2. this function also handles JSON escape sequences inside quotes.
            Ex 1:
            '{"args": "a\na\na\ta"}' -> '{"args": "a\\na\\na\\ta"}'
        """
        result = []
        inside_quotes = False
        last_char = " "
        for char in jstr:
            if last_char != "\\" and char == '"':
                inside_quotes = not inside_quotes
            last_char = char
            if not inside_quotes and char == "\n":
                continue
            if inside_quotes and char == "\n":
                char = "\\n"
            if inside_quotes and char == "\t":
                char = "\\t"
            result.append(char)
        return "".join(result)

    def execute_function(
        self, func_call: dict[str, Any], call_id: str | None = None, verbose: bool = False
    ) -> tuple[bool, dict[str, Any]]:
        """Execute a function call and return the result.

        Override this function to modify the way to execute function and tool calls.

        Args:
            func_call: a dictionary extracted from openai message at "function_call" or "tool_calls" with keys "name" and "arguments".
            call_id: a string to identify the tool call.
            verbose (bool): Whether to send messages about the execution details to the
                output stream. When True, both the function call arguments and the execution
                result will be displayed. Defaults to False.


        Returns:
            A tuple of (is_exec_success, result_dict).
            is_exec_success (boolean): whether the execution is successful.
            result_dict: a dictionary with keys "name", "role", and "content". Value of "role" is "function".

        "function_call" deprecated as of [OpenAI API v1.1.0](https://github.com/openai/openai-python/releases/tag/v1.1.0)
        See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call
        """
        iostream = IOStream.get_default()

        func_name = func_call.get("name", "")
        func = self._function_map.get(func_name, None)

        is_exec_success = False
        if func is not None:
            # Extract arguments from a json-like string and put it into a dict.
            input_string = self._format_json_str(func_call.get("arguments", "{}"))
            try:
                arguments = json.loads(input_string)
            except json.JSONDecodeError as e:
                arguments = None
                content = f"Error: {e}\n The argument must be in JSON format."

            # Try to execute the function
            if arguments is not None:
                iostream.send(
                    ExecuteFunctionEvent(func_name=func_name, call_id=call_id, arguments=arguments, recipient=self)
                )
                try:
                    content = func(**arguments)
                    is_exec_success = True
                except Exception as e:
                    content = f"Error: {e}"
        else:
            arguments = {}
            content = f"Error: Function {func_name} not found."

        iostream.send(
            ExecutedFunctionEvent(
                func_name=func_name,
                call_id=call_id,
                arguments=arguments,
                content=content,
                recipient=self,
                is_exec_success=is_exec_success,
            )
        )

        return is_exec_success, {
            "name": func_name,
            "role": "function",
            "content": content,
        }

    async def a_execute_function(
        self, func_call: dict[str, Any], call_id: str | None = None, verbose: bool = False
    ) -> tuple[bool, dict[str, Any]]:
        """Execute an async function call and return the result.

        Override this function to modify the way async functions and tools are executed.

        Args:
            func_call: a dictionary extracted from openai message at key "function_call" or "tool_calls" with keys "name" and "arguments".
            call_id: a string to identify the tool call.
            verbose (bool): Whether to send messages about the execution details to the
                output stream. When True, both the function call arguments and the execution
                result will be displayed. Defaults to False.

        Returns:
            A tuple of (is_exec_success, result_dict).
            is_exec_success (boolean): whether the execution is successful.
            result_dict: a dictionary with keys "name", "role", and "content". Value of "role" is "function".

        "function_call" deprecated as of [OpenAI API v1.1.0](https://github.com/openai/openai-python/releases/tag/v1.1.0)
        See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call
        """
        iostream = IOStream.get_default()

        func_name = func_call.get("name", "")
        func = self._function_map.get(func_name, None)

        is_exec_success = False
        if func is not None:
            # Extract arguments from a json-like string and put it into a dict.
            input_string = self._format_json_str(func_call.get("arguments", "{}"))
            try:
                arguments = json.loads(input_string)
            except json.JSONDecodeError as e:
                arguments = None
                content = f"Error: {e}\n The argument must be in JSON format."

            # Try to execute the function
            if arguments is not None:
                iostream.send(
                    ExecuteFunctionEvent(func_name=func_name, call_id=call_id, arguments=arguments, recipient=self)
                )
                try:
                    if inspect.iscoroutinefunction(func):
                        content = await func(**arguments)
                    else:
                        # Fallback to sync function if the function is not async
                        content = func(**arguments)
                    is_exec_success = True
                except Exception as e:
                    content = f"Error: {e}"
        else:
            arguments = {}
            content = f"Error: Function {func_name} not found."

        iostream.send(
            ExecutedFunctionEvent(
                func_name=func_name,
                call_id=call_id,
                arguments=arguments,
                content=content,
                recipient=self,
                is_exec_success=is_exec_success,
            )
        )

        return is_exec_success, {
            "name": func_name,
            "role": "function",
            "content": content,
        }

    def generate_init_message(self, message: dict[str, Any] | str | None, **kwargs: Any) -> str | dict[str, Any]:
        """Generate the initial message for the agent.
        If message is None, input() will be called to get the initial message.

        Args:
            message (str or None): the message to be processed.
            **kwargs: any additional information. It has the following reserved fields:
                "carryover": a string or a list of string to specify the carryover information to be passed to this chat. It can be a string or a list of string.
                    If provided, we will combine this carryover with the "message" content when generating the initial chat
                    message.

        Returns:
            str or dict: the processed message.
        """
        if message is None:
            message = self.get_human_input(">")

        return self._handle_carryover(message, kwargs)

    def _handle_carryover(self, message: str | dict[str, Any], kwargs: dict) -> str | dict[str, Any]:
        if not kwargs.get("carryover"):
            return message

        if isinstance(message, str):
            return self._process_carryover(message, kwargs)

        elif isinstance(message, dict):
            if isinstance(message.get("content"), str):
                # Makes sure the original message is not mutated
                message = message.copy()
                message["content"] = self._process_carryover(message["content"], kwargs)
            elif isinstance(message.get("content"), list):
                # Makes sure the original message is not mutated
                message = message.copy()
                message["content"] = self._process_multimodal_carryover(message["content"], kwargs)
        else:
            raise InvalidCarryOverTypeError("Carryover should be a string or a list of strings.")

        return message

    def _process_carryover(self, content: str, kwargs: dict) -> str:
        # Makes sure there's a carryover
        if not kwargs.get("carryover"):
            return content

        # if carryover is string
        if isinstance(kwargs["carryover"], str):
            content += "\nContext: \n" + kwargs["carryover"]
        elif isinstance(kwargs["carryover"], list):
            content += "\nContext: \n" + ("\n").join([_post_process_carryover_item(t) for t in kwargs["carryover"]])
        else:
            raise InvalidCarryOverTypeError(
                "Carryover should be a string or a list of strings. Not adding carryover to the message."
            )
        return content

    def _process_multimodal_carryover(self, content: list[dict[str, Any]], kwargs: dict) -> list[dict[str, Any]]:
        """Prepends the context to a multimodal message."""
        # Makes sure there's a carryover
        if not kwargs.get("carryover"):
            return content

        return [{"type": "text", "text": self._process_carryover("", kwargs)}] + content

    async def a_generate_init_message(
        self, message: dict[str, Any] | str | None, **kwargs: Any
    ) -> str | dict[str, Any]:
        """Generate the initial message for the agent.
        If message is None, input() will be called to get the initial message.

        Args:
            message (str or None): the message to be processed.
            **kwargs: any additional information. It has the following reserved fields:
                "carryover": a string or a list of string to specify the carryover information to be passed to this chat. It can be a string or a list of string.
                    If provided, we will combine this carryover with the "message" content when generating the initial chat
                    message.

        Returns:
            str or dict: the processed message.
        """
        if message is None:
            message = await self.a_get_human_input(">")

        return self._handle_carryover(message, kwargs)

    @property
    def tools(self) -> list[Tool]:
        """Get the agent's tools (registered for LLM)

        Note this is a copy of the tools list, use add_tool and remove_tool to modify the tools list.
        """
        return self._tools.copy()

    def remove_tool_for_llm(self, tool: Tool) -> None:
        """Remove a tool (register for LLM tool)"""
        try:
            self._register_for_llm(tool=tool, api_style="tool", is_remove=True)
            self._tools.remove(tool)
        except ValueError:
            raise ValueError(f"Tool {tool} not found in collection")

    def register_function(self, function_map: dict[str, Callable[..., Any]], silent_override: bool = False):
        """Register functions to the agent.

        Args:
            function_map: a dictionary mapping function names to functions. if function_map[name] is None, the function will be removed from the function_map.
            silent_override: whether to print warnings when overriding functions.
        """
        for name, func in function_map.items():
            self._assert_valid_name(name)
            if func is None and name not in self._function_map:
                warnings.warn(f"The function {name} to remove doesn't exist", name)
            if not silent_override and name in self._function_map:
                warnings.warn(f"Function '{name}' is being overridden.", UserWarning)
        self._function_map.update(function_map)
        self._function_map = {k: v for k, v in self._function_map.items() if v is not None}

    def update_function_signature(
        self, func_sig: str | dict[str, Any], is_remove: bool = False, silent_override: bool = False
    ):
        """Update a function_signature in the LLM configuration for function_call.

        Args:
            func_sig (str or dict): description/name of the function to update/remove to the model. See: https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions
            is_remove: whether removing the function from llm_config with name 'func_sig'
            silent_override: whether to print warnings when overriding functions.

        Deprecated as of [OpenAI API v1.1.0](https://github.com/openai/openai-python/releases/tag/v1.1.0)
        See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call
        """
        if not isinstance(self.llm_config, (dict, LLMConfig)):
            error_msg = "To update a function signature, agent must have an llm_config"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        if is_remove:
            if "functions" not in self.llm_config or len(self.llm_config["functions"]) == 0:
                error_msg = f"The agent config doesn't have function {func_sig}."
                logger.error(error_msg)
                raise AssertionError(error_msg)
            else:
                self.llm_config["functions"] = [
                    func for func in self.llm_config["functions"] if func["name"] != func_sig
                ]
        else:
            if not isinstance(func_sig, dict):
                raise ValueError(
                    f"The function signature must be of the type dict. Received function signature type {type(func_sig)}"
                )
            if "name" not in func_sig:
                raise ValueError(f"The function signature must have a 'name' key. Received: {func_sig}")
            self._assert_valid_name(func_sig["name"]), func_sig
            if "functions" in self.llm_config:
                if not silent_override and any(
                    func["name"] == func_sig["name"] for func in self.llm_config["functions"]
                ):
                    warnings.warn(f"Function '{func_sig['name']}' is being overridden.", UserWarning)

                self.llm_config["functions"] = [
                    func for func in self.llm_config["functions"] if func.get("name") != func_sig["name"]
                ] + [func_sig]
            else:
                self.llm_config["functions"] = [func_sig]

        # Do this only if llm_config is a dict. If llm_config is LLMConfig, LLMConfig will handle this.
        if len(self.llm_config["functions"]) == 0 and isinstance(self.llm_config, dict):
            del self.llm_config["functions"]

        self.client = OpenAIWrapper(**self.llm_config)

    def update_tool_signature(self, tool_sig: str | dict[str, Any], is_remove: bool, silent_override: bool = False):
        """Update a tool_signature in the LLM configuration for tool_call.

        Args:
            tool_sig (str or dict): description/name of the tool to update/remove to the model. See: https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools
            is_remove: whether removing the tool from llm_config with name 'tool_sig'
            silent_override: whether to print warnings when overriding functions.
        """
        if not self.llm_config:
            error_msg = "To update a tool signature, agent must have an llm_config"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        if is_remove:
            if "tools" not in self.llm_config or len(self.llm_config["tools"]) == 0:
                error_msg = f"The agent config doesn't have tool {tool_sig}."
                logger.error(error_msg)
                raise AssertionError(error_msg)
            else:
                current_tools = self.llm_config["tools"]
                filtered_tools = []

                # Loop through and rebuild tools list without the tool to remove
                for tool in current_tools:
                    tool_name = tool["function"]["name"]

                    # Match by tool name, or by tool signature
                    is_different = tool_name != tool_sig if isinstance(tool_sig, str) else tool != tool_sig

                    if is_different:
                        filtered_tools.append(tool)

                self.llm_config["tools"] = filtered_tools
        else:
            if not isinstance(tool_sig, dict):
                raise ValueError(
                    f"The tool signature must be of the type dict. Received tool signature type {type(tool_sig)}"
                )
            self._assert_valid_name(tool_sig["function"]["name"])
            if "tools" in self.llm_config and len(self.llm_config["tools"]) > 0:
                if not silent_override and any(
                    tool["function"]["name"] == tool_sig["function"]["name"] for tool in self.llm_config["tools"]
                ):
                    warnings.warn(f"Function '{tool_sig['function']['name']}' is being overridden.", UserWarning)
                self.llm_config["tools"] = [
                    tool
                    for tool in self.llm_config["tools"]
                    if tool.get("function", {}).get("name") != tool_sig["function"]["name"]
                ] + [tool_sig]
            else:
                self.llm_config["tools"] = [tool_sig]

        # Do this only if llm_config is a dict. If llm_config is LLMConfig, LLMConfig will handle this.
        if len(self.llm_config["tools"]) == 0 and isinstance(self.llm_config, dict):
            del self.llm_config["tools"]

        self.client = OpenAIWrapper(**self.llm_config)

    def can_execute_function(self, name: list[str] | str) -> bool:
        """Whether the agent can execute the function."""
        names = name if isinstance(name, list) else [name]
        return all(n in self._function_map for n in names)

    @property
    def function_map(self) -> dict[str, Callable[..., Any]]:
        """Return the function map."""
        return self._function_map

    def _wrap_function(self, func: F, inject_params: dict[str, Any] = {}, *, serialize: bool = True) -> F:
        """Wrap the function inject chat context parameters and to dump the return value to json.

        Handles both sync and async functions.

        Args:
            func: the function to be wrapped.
            inject_params: the chat context parameters which will be passed to the function.
            serialize: whether to serialize the return value

        Returns:
            The wrapped function.
        """

        @load_basemodels_if_needed
        @functools.wraps(func)
        def _wrapped_func(*args, **kwargs):
            retval = func(*args, **kwargs, **inject_params)
            if logging_enabled():
                log_function_use(self, func, kwargs, retval)
            return serialize_to_str(retval) if serialize else retval

        @load_basemodels_if_needed
        @functools.wraps(func)
        async def _a_wrapped_func(*args, **kwargs):
            retval = await func(*args, **kwargs, **inject_params)
            if logging_enabled():
                log_function_use(self, func, kwargs, retval)
            return serialize_to_str(retval) if serialize else retval

        wrapped_func = _a_wrapped_func if inspect.iscoroutinefunction(func) else _wrapped_func

        # needed for testing
        wrapped_func._origin = func

        return wrapped_func

    @staticmethod
    def _create_tool_if_needed(
        func_or_tool: F | Tool,
        name: str | None,
        description: str | None,
    ) -> Tool:
        if isinstance(func_or_tool, Tool):
            tool: Tool = func_or_tool
            # create new tool object if name or description is not None
            if name or description:
                tool = Tool(func_or_tool=tool, name=name, description=description)
        elif inspect.isfunction(func_or_tool):
            function: Callable[..., Any] = func_or_tool
            tool = Tool(func_or_tool=function, name=name, description=description)
        else:
            raise TypeError(f"'func_or_tool' must be a function or a Tool object, got '{type(func_or_tool)}' instead.")
        return tool

    def register_for_llm(
        self,
        *,
        name: str | None = None,
        description: str | None = None,
        api_style: Literal["function", "tool"] = "tool",
        silent_override: bool = False,
    ) -> Callable[[F | Tool], Tool]:
        """Decorator factory for registering a function to be used by an agent.

        It's return value is used to decorate a function to be registered to the agent. The function uses type hints to
        specify the arguments and return type. The function name is used as the default name for the function,
        but a custom name can be provided. The function description is used to describe the function in the
        agent's configuration.

        Args:
            name (optional(str)): name of the function. If None, the function name will be used (default: None).
            description (optional(str)): description of the function (default: None). It is mandatory
                for the initial decorator, but the following ones can omit it.
            api_style: (literal): the API style for function call.
                For Azure OpenAI API, use version 2023-12-01-preview or later.
                `"function"` style will be deprecated. For earlier version use
                `"function"` if `"tool"` doesn't work.
                See [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling?tabs=python) for details.
            silent_override (bool): whether to suppress any override warning messages.

        Returns:
            The decorator for registering a function to be used by an agent.

        Examples:
            ```
            @user_proxy.register_for_execution()
            @agent2.register_for_llm()
            @agent1.register_for_llm(description="This is a very useful function")
            def my_function(a: Annotated[str, "description of a parameter"] = "a", b: int, c=3.14) -> str:
                 return a + str(b * c)
            ```

            For Azure OpenAI versions prior to 2023-12-01-preview, set `api_style`
            to `"function"` if `"tool"` doesn't work:
            ```
            @agent2.register_for_llm(api_style="function")
            def my_function(a: Annotated[str, "description of a parameter"] = "a", b: int, c=3.14) -> str:
                 return a + str(b * c)
            ```

        """

        def _decorator(func_or_tool: F | Tool, name: str | None = name, description: str | None = description) -> Tool:
            """Decorator for registering a function to be used by an agent.

            Args:
                func_or_tool: The function or the tool to be registered.
                name: The name of the function or the tool.
                description: The description of the function or the tool.

            Returns:
                The function to be registered, with the _description attribute set to the function description.

            Raises:
                ValueError: if the function description is not provided and not propagated by a previous decorator.
                RuntimeError: if the LLM config is not set up before registering a function.

            """
            tool = self._create_tool_if_needed(func_or_tool, name, description)

            self._register_for_llm(tool, api_style, silent_override=silent_override)
            if tool not in self._tools:
                self._tools.append(tool)

            return tool

        return _decorator

    def _register_for_llm(
        self, tool: Tool, api_style: Literal["tool", "function"], is_remove: bool = False, silent_override: bool = False
    ) -> None:
        """Register a tool for LLM.

        Args:
            tool: the tool to be registered.
            api_style: the API style for function call ("tool" or "function").
            is_remove: whether to remove the function or tool.
            silent_override: whether to suppress any override warning messages.

        Returns:
            None
        """
        # register the function to the agent if there is LLM config, raise an exception otherwise
        if self.llm_config is None:
            raise RuntimeError("LLM config must be setup before registering a function for LLM.")

        if api_style == "function":
            self.update_function_signature(tool.function_schema, is_remove=is_remove, silent_override=silent_override)
        elif api_style == "tool":
            self.update_tool_signature(tool.tool_schema, is_remove=is_remove, silent_override=silent_override)
        else:
            raise ValueError(f"Unsupported API style: {api_style}")

    def set_ui_tools(self, tools: list[Tool]) -> None:
        """Set the UI tools for the agent.

        Args:
            tools: a list of tools to be set.
        """
        # Unset the previous UI tools
        self._unset_previous_ui_tools()

        # Set the new UI tools
        for tool in tools:
            # Register the tool for LLM
            self._register_for_llm(tool, api_style="tool", silent_override=True)
            if tool not in self._tools:
                self._tools.append(tool)

            # Register for execution
            self.register_for_execution(serialize=False, silent_override=True)(tool)

        # Set the current UI tools
        self._ui_tools = tools

    def unset_ui_tools(self, tools: list[Tool]) -> None:
        """Unset the UI tools for the agent.

        Args:
            tools: a list of tools to be unset.
        """
        for tool in tools:
            self.remove_tool_for_llm(tool)

    def _unset_previous_ui_tools(self) -> None:
        """Unset the previous UI tools for the agent.

        This is used to remove UI tools that were previously registered for LLM.
        """
        self.unset_ui_tools(self._ui_tools)
        for tool in self._ui_tools:
            if tool in self._tools:
                self._tools.remove(tool)

            # Unregister the function from the function map
            if tool.name in self._function_map:
                del self._function_map[tool.name]

        self._ui_tools = []

    def register_for_execution(
        self,
        name: str | None = None,
        description: str | None = None,
        *,
        serialize: bool = True,
        silent_override: bool = False,
    ) -> Callable[[Tool | F], Tool]:
        """Decorator factory for registering a function to be executed by an agent.

        It's return value is used to decorate a function to be registered to the agent.

        Args:
            name: name of the function. If None, the function name will be used (default: None).
            description: description of the function (default: None).
            serialize: whether to serialize the return value
            silent_override: whether to suppress any override warning messages

        Returns:
            The decorator for registering a function to be used by an agent.

        Examples:
            ```
            @user_proxy.register_for_execution()
            @agent2.register_for_llm()
            @agent1.register_for_llm(description="This is a very useful function")
            def my_function(a: Annotated[str, "description of a parameter"] = "a", b: int, c=3.14):
                 return a + str(b * c)
            ```

        """

        def _decorator(func_or_tool: Tool | F, name: str | None = name, description: str | None = description) -> Tool:
            """Decorator for registering a function to be used by an agent.

            Args:
                func_or_tool: the function or the tool to be registered.
                name: the name of the function.
                description: the description of the function.

            Returns:
                The tool to be registered.

            """
            tool = self._create_tool_if_needed(func_or_tool, name, description)
            chat_context = ChatContext(self)
            chat_context_params = dict.fromkeys(tool._chat_context_param_names, chat_context)

            self.register_function(
                {tool.name: self._wrap_function(tool.func, chat_context_params, serialize=serialize)},
                silent_override=silent_override,
            )

            return tool

        return _decorator

    def register_model_client(self, model_client_cls: ModelClient, **kwargs: Any):
        """Register a model client.

        Args:
            model_client_cls: A custom client class that follows the Client interface
            **kwargs: The kwargs for the custom client class to be initialized with
        """
        self.client.register_model_client(model_client_cls, **kwargs)

    def register_hook(self, hookable_method: str, hook: Callable):
        """Registers a hook to be called by a hookable method, in order to add a capability to the agent.
        Registered hooks are kept in lists (one per hookable method), and are called in their order of registration.

        Args:
            hookable_method: A hookable method name implemented by ConversableAgent.
            hook: A method implemented by a subclass of AgentCapability.
        """
        assert hookable_method in self.hook_lists, f"{hookable_method} is not a hookable method."
        hook_list = self.hook_lists[hookable_method]
        assert hook not in hook_list, f"{hook} is already registered as a hook."
        hook_list.append(hook)

    def update_agent_state_before_reply(self, messages: list[dict[str, Any]]) -> None:
        """Calls any registered capability hooks to update the agent's state.
        Primarily used to update context variables.
        Will, potentially, modify the messages.
        """
        hook_list = self.hook_lists["update_agent_state"]

        # Call each hook (in order of registration) to process the messages.
        for hook in hook_list:
            hook(self, messages)

    def process_all_messages_before_reply(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Calls any registered capability hooks to process all messages, potentially modifying the messages."""
        hook_list = self.hook_lists["process_all_messages_before_reply"]
        # If no hooks are registered, or if there are no messages to process, return the original message list.
        if len(hook_list) == 0 or messages is None:
            return messages

        # Call each hook (in order of registration) to process the messages.
        processed_messages = messages
        for hook in hook_list:
            processed_messages = hook(processed_messages)
        return processed_messages

    def process_last_received_message(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Calls any registered capability hooks to use and potentially modify the text of the last message,
        as long as the last message is not a function call or exit command.
        """
        # If any required condition is not met, return the original message list.
        hook_list = self.hook_lists["process_last_received_message"]
        if len(hook_list) == 0:
            return messages  # No hooks registered.
        if messages is None:
            return None  # No message to process.
        if len(messages) == 0:
            return messages  # No message to process.
        last_message = messages[-1]
        if "function_call" in last_message:
            return messages  # Last message is a function call.
        if "context" in last_message:
            return messages  # Last message contains a context key.
        if "content" not in last_message:
            return messages  # Last message has no content.

        user_content = last_message["content"]
        if not isinstance(user_content, str) and not isinstance(user_content, list):
            # if the user_content is a string, it is for regular LLM
            # if the user_content is a list, it should follow the multimodal LMM format.
            return messages
        if user_content == "exit":
            return messages  # Last message is an exit command.

        # Call each hook (in order of registration) to process the user's message.
        processed_user_content = user_content
        for hook in hook_list:
            processed_user_content = hook(processed_user_content)

        if processed_user_content == user_content:
            return messages  # No hooks actually modified the user's message.

        # Replace the last user message with the expanded one.
        messages = messages.copy()
        messages[-1]["content"] = processed_user_content
        return messages

    def _process_tool_input(self, tool_input: dict[str, Any]) -> dict[str, Any] | None:
        """Process tool input through registered hooks."""
        hook_list = self.hook_lists["safeguard_tool_inputs"]

        # If no hooks are registered, allow the tool input
        if len(hook_list) == 0:
            return tool_input

        # Process through each hook
        processed_input = tool_input
        for hook in hook_list:
            processed_input = hook(processed_input)
            if processed_input is None:
                return None

        return processed_input

    def _process_tool_output(self, response: dict[str, Any]) -> dict[str, Any]:
        """Process tool output through registered hooks"""
        hook_list = self.hook_lists["safeguard_tool_outputs"]

        # If no hooks are registered, return original response
        if len(hook_list) == 0:
            return response

        # Process through each hook
        processed_response = response
        for hook in hook_list:
            processed_response = hook(processed_response)

        return processed_response

    def _process_llm_input(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]] | None:
        """Process messages before sending to LLM through registered hooks."""
        hook_list = self.hook_lists["safeguard_llm_inputs"]

        # If no hooks registered, allow the messages through
        if len(hook_list) == 0:
            return messages

        # Process through each hook
        processed_messages = messages
        for hook in hook_list:
            processed_messages = hook(processed_messages)
            if processed_messages is None:
                return None

        return processed_messages

    def _process_llm_output(self, response: str | dict[str, Any]) -> str | dict[str, Any]:
        """Process LLM response through registered hooks"""
        hook_list = self.hook_lists["safeguard_llm_outputs"]

        # If no hooks registered, return original response
        if len(hook_list) == 0:
            return response

        # Process through each hook
        processed_response = response
        for hook in hook_list:
            processed_response = hook(processed_response)

        return processed_response

    def _process_human_input(self, human_input: str) -> str | None:
        """Process human input through registered hooks."""
        hook_list = self.hook_lists["safeguard_human_inputs"]

        # If no hooks registered, allow the input through
        if len(hook_list) == 0:
            return human_input

        # Process through each hook
        processed_input = human_input
        for hook in hook_list:
            processed_input = hook(processed_input)
            if processed_input is None:
                return None

        return processed_input

    def print_usage_summary(self, mode: str | list[str] = ["actual", "total"]) -> None:
        """Print the usage summary."""
        iostream = IOStream.get_default()
        if self.client is None:
            iostream.send(ConversableAgentUsageSummaryNoCostIncurredEvent(recipient=self))
        else:
            iostream.send(ConversableAgentUsageSummaryEvent(recipient=self))

        if self.client is not None:
            self.client.print_usage_summary(mode)

    def get_actual_usage(self) -> None | dict[str, int]:
        """Get the actual usage summary."""
        if self.client is None:
            return None
        else:
            return self.client.actual_usage_summary

    def get_total_usage(self) -> None | dict[str, int]:
        """Get the total usage summary."""
        if self.client is None:
            return None
        else:
            return self.client.total_usage_summary

    @contextmanager
    def _create_or_get_executor(
        self,
        executor_kwargs: dict[str, Any] | None = None,
        tools: Tool | Iterable[Tool] | None = None,
        agent_name: str = "executor",
        agent_human_input_mode: str = "NEVER",
    ) -> Generator["ConversableAgent", None, None]:
        """Creates a user proxy / tool executor agent.

        Note: Code execution is not enabled by default. Pass the code execution config into executor_kwargs, if needed.

        Args:
            executor_kwargs: agent's arguments.
            tools: tools to register for execution with the agent.
            agent_name: agent's name, defaults to 'executor'.
            agent_human_input_mode: agent's human input mode, defaults to 'NEVER'.
        """
        if executor_kwargs is None:
            executor_kwargs = {}
        if "is_termination_msg" not in executor_kwargs:
            executor_kwargs["is_termination_msg"] = lambda x: (x["content"] is not None) and "TERMINATE" in x["content"]

        try:
            if not self.run_executor:
                self.run_executor = ConversableAgent(
                    name=agent_name,
                    human_input_mode=agent_human_input_mode,
                    **executor_kwargs,
                )

            # Combine agent's existing tools with passed tools
            agent_tools = self._tools.copy()  # Get agent's pre-registered tools
            passed_tools = [] if tools is None else tools
            passed_tools = [passed_tools] if isinstance(passed_tools, Tool) else passed_tools

            # Combine both sets of tools (avoid duplicates)
            all_tools = agent_tools.copy()
            for tool in passed_tools:
                if tool not in all_tools:
                    all_tools.append(tool)

            # Register all tools with the executor
            for tool in all_tools:
                tool.register_for_execution(self.run_executor)

            # Register only newly passed tools for LLM (agent's pre-existing tools are already registered)
            for tool in passed_tools:
                tool.register_for_llm(self)
            yield self.run_executor
        finally:
            # Clean up only newly passed tools (not agent's pre-existing tools)
            if "passed_tools" in locals():
                for tool in passed_tools:
                    self.update_tool_signature(tool_sig=tool.tool_schema["function"]["name"], is_remove=True)

    def _deprecated_run(
        self,
        message: str,
        *,
        tools: Tool | Iterable[Tool] | None = None,
        executor_kwargs: dict[str, Any] | None = None,
        max_turns: int | None = None,
        msg_to: Literal["agent", "user"] = "agent",
        clear_history: bool = False,
        user_input: bool = True,
        summary_method: str | Callable[..., Any] | None = DEFAULT_SUMMARY_METHOD,
    ) -> ChatResult:
        """Run a chat with the agent using the given message.

        A second agent will be created to represent the user, this agent will by known by the name 'user'. This agent does not have code execution enabled by default, if needed pass the code execution config in with the executor_kwargs parameter.

        The user can terminate the conversation when prompted or, if agent's reply contains 'TERMINATE', it will terminate.

        Args:
            message: the message to be processed.
            tools: the tools to be used by the agent.
            executor_kwargs: the keyword arguments for the executor.
            max_turns: maximum number of turns (a turn is equivalent to both agents having replied), defaults no None which means unlimited. The original message is included.
            msg_to: which agent is receiving the message and will be the first to reply, defaults to the agent.
            clear_history: whether to clear the chat history.
            user_input: the user will be asked for input at their turn.
            summary_method: the method to summarize the chat.
        """
        with self._create_or_get_executor(
            executor_kwargs=executor_kwargs,
            tools=tools,
            agent_name="user",
            agent_human_input_mode="ALWAYS" if user_input else "NEVER",
        ) as executor:
            if msg_to == "agent":
                return executor.initiate_chat(
                    self,
                    message=message,
                    clear_history=clear_history,
                    max_turns=max_turns,
                    summary_method=summary_method,
                )
            else:
                return self.initiate_chat(
                    executor,
                    message=message,
                    clear_history=clear_history,
                    max_turns=max_turns,
                    summary_method=summary_method,
                )

    async def _deprecated_a_run(
        self,
        message: str,
        *,
        tools: Tool | Iterable[Tool] | None = None,
        executor_kwargs: dict[str, Any] | None = None,
        max_turns: int | None = None,
        msg_to: Literal["agent", "user"] = "agent",
        clear_history: bool = False,
        user_input: bool = True,
        summary_method: str | Callable[..., Any] | None = DEFAULT_SUMMARY_METHOD,
    ) -> ChatResult:
        """Run a chat asynchronously with the agent using the given message.

        A second agent will be created to represent the user, this agent will by known by the name 'user'.

        The user can terminate the conversation when prompted or, if agent's reply contains 'TERMINATE', it will terminate.

        Args:
            message: the message to be processed.
            tools: the tools to be used by the agent.
            executor_kwargs: the keyword arguments for the executor.
            max_turns: maximum number of turns (a turn is equivalent to both agents having replied), defaults no None which means unlimited. The original message is included.
            msg_to: which agent is receiving the message and will be the first to reply, defaults to the agent.
            clear_history: whether to clear the chat history.
            user_input: the user will be asked for input at their turn.
            summary_method: the method to summarize the chat.
        """
        with self._create_or_get_executor(
            executor_kwargs=executor_kwargs,
            tools=tools,
            agent_name="user",
            agent_human_input_mode="ALWAYS" if user_input else "NEVER",
        ) as executor:
            if msg_to == "agent":
                return await executor.a_initiate_chat(
                    self,
                    message=message,
                    clear_history=clear_history,
                    max_turns=max_turns,
                    summary_method=summary_method,
                )
            else:
                return await self.a_initiate_chat(
                    executor,
                    message=message,
                    clear_history=clear_history,
                    max_turns=max_turns,
                    summary_method=summary_method,
                )

    def register_handoff(self, condition: Union["OnContextCondition", "OnCondition"]) -> None:
        """Register a single handoff condition (OnContextCondition or OnCondition).

        Args:
            condition: The condition to add (OnContextCondition, OnCondition)
        """
        self.handoffs.add(condition)

    def register_handoffs(self, conditions: list[Union["OnContextCondition", "OnCondition"]]) -> None:
        """Register multiple handoff conditions (OnContextCondition or OnCondition).

        Args:
            conditions: List of conditions to add
        """
        self.handoffs.add_many(conditions)

    def register_input_guardrail(self, guardrail: "Guardrail") -> None:
        """Register a guardrail to be used for input validation.

        Args:
            guardrail: The guardrail to register.
        """
        self.input_guardrails.append(guardrail)

    def register_input_guardrails(self, guardrails: list["Guardrail"]) -> None:
        """Register multiple guardrails to be used for input validation.

        Args:
            guardrails: List of guardrails to register.
        """
        self.input_guardrails.extend(guardrails)

    def register_output_guardrail(self, guardrail: "Guardrail") -> None:
        """Register a guardrail to be used for output validation.

        Args:
            guardrail: The guardrail to register.
        """
        self.output_guardrails.append(guardrail)

    def register_output_guardrails(self, guardrails: list["Guardrail"]) -> None:
        """Register multiple guardrails to be used for output validation.

        Args:
            guardrails: List of guardrails to register.
        """
        self.output_guardrails.extend(guardrails)


@export_module("autogen")
def register_function(
    f: Callable[..., Any],
    *,
    caller: ConversableAgent,
    executor: ConversableAgent,
    name: str | None = None,
    description: str,
) -> None:
    """Register a function to be proposed by an agent and executed for an executor.

    This function can be used instead of function decorators `@ConversationAgent.register_for_llm` and
    `@ConversationAgent.register_for_execution`.

    Args:
        f: the function to be registered.
        caller: the agent calling the function, typically an instance of ConversableAgent.
        executor: the agent executing the function, typically an instance of UserProxy.
        name: name of the function. If None, the function name will be used (default: None).
        description: description of the function. The description is used by LLM to decode whether the function
            is called. Make sure the description is properly describing what the function does or it might not be
            called by LLM when needed.

    """
    f = caller.register_for_llm(name=name, description=description)(f)
    executor.register_for_execution(name=name)(f)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from collections.abc import Callable
from typing import Any, Literal

from ..doc_utils import export_module
from ..llm_config import LLMConfig
from ..runtime_logging import log_new_agent, logging_enabled
from .conversable_agent import ConversableAgent


@export_module("autogen")
class AssistantAgent(ConversableAgent):
    """(In preview) Assistant agent, designed to solve a task with LLM.

    AssistantAgent is a subclass of ConversableAgent configured with a default system message.
    The default system message is designed to solve a task with LLM,
    including suggesting python code blocks and debugging. \n
    `human_input_mode` is default to "NEVER" \n
    and `code_execution_config` is default to False. \n
    This agent doesn't execute code by default, and expects the user to execute the code. \n
    """

    DEFAULT_SYSTEM_MESSAGE = """You are a helpful AI assistant.
Solve tasks using your coding and language skills.
In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.
    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.
    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.
Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.
When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.
If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.
If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.
Reply "TERMINATE" in the end when everything is done.
    """

    DEFAULT_DESCRIPTION = "A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills."

    def __init__(
        self,
        name: str,
        system_message: str | None = DEFAULT_SYSTEM_MESSAGE,
        llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None,
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        max_consecutive_auto_reply: int | None = None,
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "NEVER",
        description: str | None = None,
        **kwargs: Any,
    ):
        """Args:
        - name (str): agent name. \n
        - system_message (str): system message for the ChatCompletion inference. \n
            Please override this attribute if you want to reprogram the agent.
        - llm_config (dict or False or None): llm inference configuration. \n
            Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create) \n
            for available options. \n
        - is_termination_msg (function): a function that takes a message in the form of a dictionary
            and returns a boolean value indicating if this received message is a termination message.
            The dict can contain the following keys: "content", "role", "name", "function_call". \n
        - max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.
            default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).
            The limit only plays a role when human_input_mode is not "ALWAYS". \n
        - **kwargs (dict): Please refer to other kwargs in
            [ConversableAgent](https://docs.ag2.ai/latest/docs/api-reference/autogen/ConversableAgent). \n
        """
        super().__init__(
            name,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            llm_config=llm_config,
            description=description,
            **kwargs,
        )
        if logging_enabled():
            log_new_agent(self, locals())

        # Update the provided description if None, and we are using the default system_message,
        # then use the default description.
        if description is None and system_message == self.DEFAULT_SYSTEM_MESSAGE:
            self.description = self.DEFAULT_DESCRIPTION
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
import threading
from typing import TYPE_CHECKING, Any

from ...doc_utils import export_module
from ...events.agent_events import ErrorEvent, RunCompletionEvent
from ...io.base import IOStream
from ...io.run_response import AsyncRunResponse, AsyncRunResponseProtocol, RunResponse, RunResponseProtocol
from ...io.thread_io_stream import AsyncThreadIOStream, ThreadIOStream
from ..chat import ChatResult
from .context_variables import ContextVariables
from .group_utils import cleanup_temp_user_messages

if TYPE_CHECKING:
    from ..agent import Agent
    from .patterns.pattern import Pattern

__all__ = [
    "a_initiate_group_chat",
    "a_run_group_chat",
    "initiate_group_chat",
    "run_group_chat",
]


@export_module("autogen")
def initiate_group_chat(
    pattern: "Pattern",
    messages: list[dict[str, Any]] | str,
    max_rounds: int = 20,
) -> tuple[ChatResult, ContextVariables, "Agent"]:
    """Initialize and run a group chat using a pattern for configuration.

    Args:
        pattern: Pattern object that encapsulates the chat configuration.
        messages: Initial message(s).
        max_rounds: Maximum number of conversation rounds.

    Returns:
        ChatResult:         Conversations chat history.
        ContextVariables:   Updated Context variables.
        "ConversableAgent":   Last speaker.
    """
    # Let the pattern prepare the group chat and all its components
    # Only passing the necessary parameters that aren't already in the pattern
    (
        _,  # agents,
        _,  # wrapped_agents,
        _,  # user_agent,
        context_variables,
        _,  # initial_agent,
        _,  # group_after_work,
        _,  # tool_execution,
        _,  # groupchat,
        manager,
        processed_messages,
        last_agent,
        _,  # group_agent_names,
        _,  # temp_user_list,
    ) = pattern.prepare_group_chat(
        max_rounds=max_rounds,
        messages=messages,
    )

    # Start or resume the conversation
    if len(processed_messages) > 1:
        last_agent, last_message = manager.resume(messages=processed_messages)
        clear_history = False
    else:
        last_message = processed_messages[0]
        clear_history = True

    if last_agent is None:
        raise ValueError("No agent selected to start the conversation")

    chat_result = last_agent.initiate_chat(
        manager,
        message=last_message,
        clear_history=clear_history,
        summary_method=pattern.summary_method,
    )

    cleanup_temp_user_messages(chat_result)

    return chat_result, context_variables, manager.last_speaker


@export_module("autogen.agentchat")
async def a_initiate_group_chat(
    pattern: "Pattern",
    messages: list[dict[str, Any]] | str,
    max_rounds: int = 20,
) -> tuple[ChatResult, ContextVariables, "Agent"]:
    """Initialize and run a group chat using a pattern for configuration, asynchronously.

    Args:
        pattern: Pattern object that encapsulates the chat configuration.
        messages: Initial message(s).
        max_rounds: Maximum number of conversation rounds.

    Returns:
        ChatResult:         Conversations chat history.
        ContextVariables:   Updated Context variables.
        "ConversableAgent":   Last speaker.
    """
    # Let the pattern prepare the group chat and all its components
    # Only passing the necessary parameters that aren't already in the pattern
    (
        _,  # agents,
        _,  # wrapped_agents,
        _,  # user_agent,
        context_variables,
        _,  # initial_agent,
        _,  # group_after_work,
        _,  # tool_execution,
        _,  # groupchat,
        manager,
        processed_messages,
        last_agent,
        _,  # group_agent_names,
        _,  # temp_user_list,
    ) = pattern.prepare_group_chat(
        max_rounds=max_rounds,
        messages=messages,
    )

    # Start or resume the conversation
    if len(processed_messages) > 1:
        last_agent, last_message = await manager.a_resume(messages=processed_messages)
        clear_history = False
    else:
        last_message = processed_messages[0]
        clear_history = True

    if last_agent is None:
        raise ValueError("No agent selected to start the conversation")

    chat_result = await last_agent.a_initiate_chat(
        manager,
        message=last_message,  # type: ignore[arg-type]
        clear_history=clear_history,
        summary_method=pattern.summary_method,
    )

    cleanup_temp_user_messages(chat_result)

    return chat_result, context_variables, manager.last_speaker


@export_module("autogen.agentchat")
def run_group_chat(
    pattern: "Pattern",
    messages: list[dict[str, Any]] | str,
    max_rounds: int = 20,
) -> RunResponseProtocol:
    iostream = ThreadIOStream()
    # todo: add agents
    response = RunResponse(iostream, agents=[])

    def _initiate_group_chat(
        pattern: "Pattern" = pattern,
        messages: list[dict[str, Any]] | str = messages,
        max_rounds: int = max_rounds,
        iostream: ThreadIOStream = iostream,
        response: RunResponse = response,
    ) -> None:
        with IOStream.set_default(iostream):
            try:
                chat_result, context_vars, agent = initiate_group_chat(
                    pattern=pattern,
                    messages=messages,
                    max_rounds=max_rounds,
                )

                IOStream.get_default().send(
                    RunCompletionEvent(  # type: ignore[call-arg]
                        history=chat_result.chat_history,
                        summary=chat_result.summary,
                        cost=chat_result.cost,
                        last_speaker=agent.name,
                        context_variables=context_vars,
                    )
                )
            except Exception as e:
                response.iostream.send(ErrorEvent(error=e))  # type: ignore[call-arg]

    threading.Thread(
        target=_initiate_group_chat,
    ).start()

    return response


@export_module("autogen.agentchat")
async def a_run_group_chat(
    pattern: "Pattern",
    messages: list[dict[str, Any]] | str,
    max_rounds: int = 20,
) -> AsyncRunResponseProtocol:
    iostream = AsyncThreadIOStream()
    # todo: add agents
    response = AsyncRunResponse(iostream, agents=[])

    async def _initiate_group_chat(
        pattern: "Pattern" = pattern,
        messages: list[dict[str, Any]] | str = messages,
        max_rounds: int = max_rounds,
        iostream: AsyncThreadIOStream = iostream,
        response: AsyncRunResponse = response,
    ) -> None:
        with IOStream.set_default(iostream):
            try:
                chat_result, context_vars, agent = await a_initiate_group_chat(
                    pattern=pattern,
                    messages=messages,
                    max_rounds=max_rounds,
                )

                IOStream.get_default().send(
                    RunCompletionEvent(  # type: ignore[call-arg]
                        history=chat_result.chat_history,
                        summary=chat_result.summary,
                        cost=chat_result.cost,
                        last_speaker=agent.name,
                        context_variables=context_vars,
                    )
                )
            except Exception as e:
                response.iostream.send(ErrorEvent(error=e))  # type: ignore[call-arg]

    task = asyncio.create_task(_initiate_group_chat())
    # prevent the task from being garbage collected
    response._task_ref = task  # type: ignore[attr-defined]
    return response
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import copy
from collections.abc import Callable
from functools import partial
from types import MethodType
from typing import TYPE_CHECKING, Any, Optional

from ..agent import Agent
from ..groupchat import GroupChat, GroupChatManager
from .context_variables import ContextVariables
from .group_tool_executor import GroupToolExecutor
from .targets.group_manager_target import GroupManagerTarget
from .targets.transition_target import (
    AgentNameTarget,
    AgentTarget,
    TransitionTarget,
)

if TYPE_CHECKING:
    from ..conversable_agent import ConversableAgent

# Utility functions for group chat preparation and management
# These are extracted from multi_agent_chat.py to avoid circular imports


def update_conditional_functions(agent: "ConversableAgent", messages: list[dict[str, Any]]) -> None:
    """Updates the agent's functions based on the OnCondition's available condition.

    All functions are removed and then added back if they are available
    """
    for on_condition in agent.handoffs.llm_conditions:
        is_available = on_condition.available.is_available(agent, messages) if on_condition.available else True

        # Remove it from their tools
        for tool in agent.tools:
            if tool.name == on_condition.llm_function_name:
                agent.remove_tool_for_llm(tool)
                break

        # then add the function if it is available, so that the function signature is updated
        if is_available:
            agent._add_single_function(
                _create_on_condition_handoff_function(on_condition.target),
                on_condition.llm_function_name,
                on_condition.condition.get_prompt(agent, messages),
            )


def establish_group_agent(agent: "ConversableAgent") -> None:
    """Establish the group agent with the group-related attributes and hooks. Not for the tool executor.

    Args:
        agent ("ConversableAgent"): The agent to establish as a group agent.
    """

    def _group_agent_str(self: "ConversableAgent") -> str:
        """Customise the __str__ method to show the agent name for transition messages."""
        return f"Group agent --> {self.name}"

    # Register the hook to update agent state (except tool executor)
    agent.register_hook("update_agent_state", update_conditional_functions)

    # Register a reply function to run Python function-based OnContextConditions before any other reply function
    agent.register_reply(trigger=([Agent, None]), reply_func=_run_oncontextconditions, position=0)

    agent._get_display_name = MethodType(_group_agent_str, agent)  # type: ignore[method-assign]

    # Mark this agent as established as a group agent
    agent._group_is_established = True  # type: ignore[attr-defined]


def link_agents_to_group_manager(agents: list[Agent], group_chat_manager: Agent) -> None:
    """Link all agents to the GroupChatManager so they can access the underlying GroupChat and other agents.

    This is primarily used so that agents can get to the tool executor to help set the next agent.

    Does not link the Tool Executor agent.
    """
    for agent in agents:
        agent._group_manager = group_chat_manager  # type: ignore[attr-defined]


def _evaluate_after_works_conditions(
    agent: "ConversableAgent",
    groupchat: GroupChat,
    user_agent: Optional["ConversableAgent"],
) -> Agent | str | None:
    """Evaluate after_works context conditions for an agent.

    Args:
        agent: The agent to evaluate after_works conditions for
        groupchat: The current group chat
        user_agent: Optional user proxy agent

    Returns:
        The resolved speaker selection result if a condition matches, None otherwise
    """
    if not hasattr(agent, "handoffs") or not agent.handoffs.after_works:  # type: ignore[attr-defined]
        return None

    for after_work_condition in agent.handoffs.after_works:  # type: ignore[attr-defined]
        # Check if condition is available
        is_available = (
            after_work_condition.available.is_available(agent, groupchat.messages)
            if after_work_condition.available
            else True
        )

        # Evaluate the condition (None condition means always true)
        if is_available and (
            after_work_condition.condition is None or after_work_condition.condition.evaluate(agent.context_variables)
        ):
            # Condition matched, resolve and return
            return after_work_condition.target.resolve(
                groupchat,
                agent,
                user_agent,
            ).get_speaker_selection_result(groupchat)

    return None


def _run_oncontextconditions(
    agent: "ConversableAgent",
    messages: list[dict[str, Any]] | None = None,
    sender: Agent | None = None,
    config: Any | None = None,
) -> tuple[bool, str | dict[str, Any] | None]:
    """Run OnContextConditions for an agent before any other reply function."""
    for on_condition in agent.handoffs.context_conditions:  # type: ignore[attr-defined]
        is_available = (
            on_condition.available.is_available(agent, messages if messages else []) if on_condition.available else True
        )

        if is_available and (
            on_condition.condition is None or on_condition.condition.evaluate(agent.context_variables)
        ):
            on_condition.target.activate_target(agent._group_manager.groupchat)  # type: ignore[attr-defined]

            transfer_name = on_condition.target.display_name()

            return True, "[Handing off to " + transfer_name + "]"

    return False, None


def _create_on_condition_handoff_function(target: TransitionTarget) -> Callable[[], TransitionTarget]:
    """Creates a function that will be used by the tool call reply function when the condition is met.

    Args:
        target (TransitionTarget): The target to transfer to.

    Returns:
        Callable: The transfer function.
    """

    def transfer_to_target() -> TransitionTarget:
        return target

    return transfer_to_target


def create_on_condition_handoff_functions(agent: "ConversableAgent") -> None:
    """Creates the functions for the OnConditions so that the current tool handling works.

    Args:
        agent ("ConversableAgent"): The agent to create the functions for.
    """
    # Populate the function names for the handoffs
    agent.handoffs.set_llm_function_names()

    # Create a function for each OnCondition
    for on_condition in agent.handoffs.llm_conditions:
        # Create a function that will be called when the condition is met
        agent._add_single_function(
            _create_on_condition_handoff_function(on_condition.target),
            on_condition.llm_function_name,
            on_condition.condition.get_prompt(agent, []),
        )


def ensure_handoff_agents_in_group(agents: list["ConversableAgent"]) -> None:
    """Ensure the agents in handoffs are in the group chat."""
    agent_names = [agent.name for agent in agents]
    for agent in agents:
        for llm_conditions in agent.handoffs.llm_conditions:
            if (
                isinstance(llm_conditions.target, (AgentTarget, AgentNameTarget))
                and llm_conditions.target.agent_name not in agent_names
            ):
                raise ValueError("Agent in OnCondition Hand-offs must be in the agents list")
        for context_conditions in agent.handoffs.context_conditions:
            if (
                isinstance(context_conditions.target, (AgentTarget, AgentNameTarget))
                and context_conditions.target.agent_name not in agent_names
            ):
                raise ValueError("Agent in OnContextCondition Hand-offs must be in the agents list")
        # Check after_works targets
        for after_work_condition in agent.handoffs.after_works:
            if (
                isinstance(after_work_condition.target, (AgentTarget, AgentNameTarget))
                and after_work_condition.target.agent_name not in agent_names
            ):
                raise ValueError("Agent in after work target Hand-offs must be in the agents list")


def ensure_guardrail_agents_in_group(agents: list["ConversableAgent"]) -> None:
    """Ensure the agents in handoffs are in the group chat."""
    agent_names = [agent.name for agent in agents]
    for agent in agents:
        for guardrail in agent.input_guardrails + agent.output_guardrails:
            if (
                isinstance(guardrail.target, (AgentTarget, AgentNameTarget))
                and guardrail.target.agent_name not in agent_names
            ):
                raise ValueError("Agent in guardrail's target must be in the agents list")


def prepare_exclude_transit_messages(agents: list["ConversableAgent"]) -> None:
    """Preparation for excluding transit messages by getting all tool names and registering a hook on agents to remove those messages."""
    # get all transit functions names
    to_be_removed: list[str] = []
    for agent in agents:
        for on_condition in agent.handoffs.llm_conditions:
            if on_condition.llm_function_name:
                to_be_removed.append(on_condition.llm_function_name)
            else:
                raise ValueError("OnCondition must have a function name")

    remove_function = make_remove_function(to_be_removed)

    # register hook to remove transit messages for group agents
    for agent in agents:
        agent.register_hook("process_all_messages_before_reply", remove_function)


def prepare_group_agents(
    agents: list["ConversableAgent"],
    context_variables: ContextVariables,
    exclude_transit_message: bool = True,
) -> tuple[GroupToolExecutor, list["ConversableAgent"]]:
    """Validates agents, create the tool executor, wrap necessary targets in agents.

    Args:
        agents (list["ConversableAgent"]): List of all agents in the conversation.
        context_variables (ContextVariables): Context variables to assign to all agents.
        exclude_transit_message (bool): Whether to exclude transit messages from the agents.

    Returns:
        "ConversableAgent": The tool executor agent.
        list["ConversableAgent"]: List of wrapped agents.
    """
    # Initialise all agents as group agents
    for agent in agents:
        if not hasattr(agent, "_group_is_established"):
            establish_group_agent(agent)

    # Ensure all agents in hand-off after-works are in the passed in agents list
    ensure_handoff_agents_in_group(agents)

    # Ensure all agents in guardrails are in the passed in agents list
    ensure_guardrail_agents_in_group(agents)

    # Create Tool Executor for the group
    tool_execution = GroupToolExecutor()

    # Wrap handoff targets in agents that need to be wrapped
    wrapped_chat_agents: list[ConversableAgent] = []
    for agent in agents:
        wrap_agent_handoff_targets(agent, wrapped_chat_agents)

    # Create the functions for the OnConditions so that the current tool handling works
    for agent in agents:
        create_on_condition_handoff_functions(agent)

    # Register all the agents' functions with the tool executor and
    # use dependency injection for the context variables parameter
    # Update tool execution agent with all the functions from all the agents
    tool_execution.register_agents_functions(agents + wrapped_chat_agents, context_variables)

    if exclude_transit_message:
        prepare_exclude_transit_messages(agents)

    return tool_execution, wrapped_chat_agents


def wrap_agent_handoff_targets(agent: "ConversableAgent", wrapped_agent_list: list["ConversableAgent"]) -> None:
    """Wrap handoff targets in agents that need to be wrapped to be part of the group chat.

    Example is NestedChatTarget.

    Args:
        agent ("ConversableAgent"): The agent to wrap the handoff targets for.
        wrapped_agent_list (list["ConversableAgent"]): List of wrapped chat agents that will be appended to.
    """
    # Wrap OnCondition targets
    for i, handoff_oncondition_requiring_wrapping in enumerate(agent.handoffs.get_llm_conditions_requiring_wrapping()):
        # Create wrapper agent
        wrapper_agent = handoff_oncondition_requiring_wrapping.target.create_wrapper_agent(parent_agent=agent, index=i)
        wrapped_agent_list.append(wrapper_agent)

        # Change this handoff target to point to the newly created agent
        handoff_oncondition_requiring_wrapping.target = AgentTarget(wrapper_agent)

    for i, handoff_oncontextcondition_requiring_wrapping in enumerate(
        agent.handoffs.get_context_conditions_requiring_wrapping()
    ):
        # Create wrapper agent
        wrapper_agent = handoff_oncontextcondition_requiring_wrapping.target.create_wrapper_agent(
            parent_agent=agent, index=i
        )
        wrapped_agent_list.append(wrapper_agent)

        # Change this handoff target to point to the newly created agent
        handoff_oncontextcondition_requiring_wrapping.target = AgentTarget(wrapper_agent)


def process_initial_messages(
    messages: list[dict[str, Any]] | str,
    user_agent: Optional["ConversableAgent"],
    agents: list["ConversableAgent"],
    wrapped_agents: list["ConversableAgent"],
) -> tuple[list[dict[str, Any]], Optional["ConversableAgent"], list[str], list[Agent]]:
    """Process initial messages, validating agent names against messages, and determining the last agent to speak.

    Args:
        messages: Initial messages to process.
        user_agent: Optional user proxy agent passed in to a_/initiate_group_chat.
        agents: Agents in the group.
        wrapped_agents: List of wrapped agents.

    Returns:
        list[dict[str, Any]]: Processed message(s).
        Agent: Last agent to speak.
        list[str]: List of agent names.
        list[Agent]: List of temporary user proxy agents to add to GroupChat.
    """
    from ..conversable_agent import ConversableAgent  # NEED SOLUTION

    if isinstance(messages, str):
        messages = [{"role": "user", "content": messages}]

    group_agent_names = [agent.name for agent in agents + wrapped_agents]

    # If there's only one message and there's no identified group agent
    # Start with a user proxy agent, creating one if they haven't passed one in
    last_agent: ConversableAgent | None
    temp_user_proxy: ConversableAgent | None = None
    temp_user_list: list[Agent] = []
    if len(messages) == 1 and "name" not in messages[0] and not user_agent:
        temp_user_proxy = ConversableAgent(name="_User", code_execution_config=False, human_input_mode="ALWAYS")
        last_agent = temp_user_proxy
        temp_user_list.append(temp_user_proxy)
    else:
        last_message = messages[0]
        if "name" in last_message:
            if last_message["name"] in group_agent_names:
                last_agent = next(agent for agent in agents + wrapped_agents if agent.name == last_message["name"])  # type: ignore[assignment]
            elif user_agent and last_message["name"] == user_agent.name:
                last_agent = user_agent
            else:
                raise ValueError(f"Invalid group agent name in last message: {last_message['name']}")
        else:
            last_agent = user_agent if user_agent else temp_user_proxy

    return messages, last_agent, group_agent_names, temp_user_list


def setup_context_variables(
    tool_execution: "ConversableAgent",
    agents: list["ConversableAgent"],
    manager: GroupChatManager,
    user_agent: Optional["ConversableAgent"],
    context_variables: ContextVariables,
) -> None:
    """Assign a common context_variables reference to all agents in the group, including the tool executor, group chat manager, and user proxy agent.

    Args:
        tool_execution: The tool execution agent.
        agents: List of all agents in the conversation.
        manager: GroupChatManager instance.
        user_agent: Optional user proxy agent.
        context_variables: Context variables to assign to all agents.
    """
    for agent in agents + [tool_execution] + [manager] + ([user_agent] if user_agent else []):
        agent.context_variables = context_variables


def cleanup_temp_user_messages(chat_result: Any) -> None:
    """Remove temporary user proxy agent name from messages before returning.

    Args:
        chat_result: ChatResult instance.
    """
    for message in chat_result.chat_history:
        if "name" in message and message["name"] == "_User":
            del message["name"]


def get_last_agent_speaker(
    groupchat: GroupChat, group_agent_names: list[str], tool_executor: GroupToolExecutor
) -> Agent:
    """Get the last group agent from the group chat messages. Not including the tool executor."""
    last_group_speaker = None
    for message in reversed(groupchat.messages):
        if "name" in message and message["name"] in group_agent_names and message["name"] != tool_executor.name:
            agent = groupchat.agent_by_name(name=message["name"])
            if agent:
                last_group_speaker = agent
                break
    if last_group_speaker is None:
        raise ValueError("No group agent found in the message history")

    return last_group_speaker


def determine_next_agent(
    last_speaker: "ConversableAgent",
    groupchat: GroupChat,
    initial_agent: "ConversableAgent",
    use_initial_agent: bool,
    tool_executor: GroupToolExecutor,
    group_agent_names: list[str],
    user_agent: Optional["ConversableAgent"],
    group_after_work: TransitionTarget,
) -> Agent | str | None:
    """Determine the next agent in the conversation.

    Args:
        last_speaker ("ConversableAgent"): The last agent to speak.
        groupchat (GroupChat): GroupChat instance.
        initial_agent ("ConversableAgent"): The initial agent in the conversation.
        use_initial_agent (bool): Whether to use the initial agent straight away.
        tool_executor ("ConversableAgent"): The tool execution agent.
        group_agent_names (list[str]): List of agent names.
        user_agent (UserProxyAgent): Optional user proxy agent.
        group_after_work (TransitionTarget): Group-level Transition option when an agent doesn't select the next agent.

    Returns:
        Optional[Union[Agent, str]]: The next agent or speaker selection method.
    """
    # Logic for determining the next target (anything based on Transition Target: an agent, wrapped agent, TerminateTarget, StayTarget, RevertToUserTarget, GroupManagerTarget, etc.
    # 1. If it's the first response -> initial agent
    # 2. If the last message is a tool call -> tool execution agent
    # 3. If the Tool Executor has determined a next target (e.g. ReplyResult specified target) -> transition to tool reply target
    # 4. If the user last spoke -> return to the previous agent
    # NOW "AFTER WORK":
    # 5. Get the After Work condition (if the agent doesn't have one, get the group-level one)
    # 6. Resolve and return the After Work condition -> agent / wrapped agent / TerminateTarget / StayTarget / RevertToUserTarget / GroupManagerTarget / etc.

    # 1. If it's the first response, return the initial agent
    if use_initial_agent:
        return initial_agent

    # 2. If the last message is a tool call, return the tool execution agent
    if "tool_calls" in groupchat.messages[-1]:
        return tool_executor

    # 3. If the Tool Executor has determined a next target, return that
    if tool_executor.has_next_target():
        next_agent = tool_executor.get_next_target()
        tool_executor.clear_next_target()

        if next_agent.can_resolve_for_speaker_selection():
            return next_agent.resolve(groupchat, last_speaker, user_agent).get_speaker_selection_result(groupchat)
        else:
            raise ValueError(
                "Tool Executor next target must be a valid TransitionTarget that can resolve for speaker selection."
            )

    # get the last group agent
    last_agent_speaker = get_last_agent_speaker(groupchat, group_agent_names, tool_executor)

    # If we are returning from a tool execution, return to the last agent that spoke
    if groupchat.messages[-1]["role"] == "tool":
        return last_agent_speaker

    # If the user last spoke, return to the agent prior to them (if they don't have an after work, otherwise it's treated like any other agent)
    if user_agent and last_speaker == user_agent:
        if not user_agent.handoffs.after_works:
            return last_agent_speaker
        else:
            last_agent_speaker = user_agent

    # AFTER WORK:

    # First, try to evaluate after_works context conditions
    after_works_result = _evaluate_after_works_conditions(
        last_agent_speaker,  # type: ignore[arg-type]
        groupchat,
        user_agent,
    )
    if after_works_result is not None:
        return after_works_result

    # If no after_works conditions matched, use the group-level after_work
    # Resolve the next agent, termination, or speaker selection method
    resolved_speaker_selection_result = group_after_work.resolve(
        groupchat,
        last_agent_speaker,  # type: ignore[arg-type]
        user_agent,
    ).get_speaker_selection_result(groupchat)

    return resolved_speaker_selection_result


def create_group_transition(
    initial_agent: "ConversableAgent",
    tool_execution: GroupToolExecutor,
    group_agent_names: list[str],
    user_agent: Optional["ConversableAgent"],
    group_after_work: TransitionTarget,
) -> Callable[["ConversableAgent", GroupChat], Agent | str | None]:
    """Creates a transition function for group chat with enclosed state for the use_initial_agent.

    Args:
        initial_agent ("ConversableAgent"): The first agent to speak
        tool_execution (GroupToolExecutor): The tool execution agent
        group_agent_names (list[str]): List of all agent names
        user_agent (UserProxyAgent): Optional user proxy agent
        group_after_work (TransitionTarget): Group-level after work

    Returns:
        Callable[["ConversableAgent", GroupChat], Optional[Union[Agent, str]]]: The transition function
    """
    # Create enclosed state, this will be set once per creation so will only be True on the first execution
    # of group_transition
    state = {"use_initial_agent": True}

    def group_transition(last_speaker: "ConversableAgent", groupchat: GroupChat) -> Agent | str | None:
        result = determine_next_agent(
            last_speaker=last_speaker,
            groupchat=groupchat,
            initial_agent=initial_agent,
            use_initial_agent=state["use_initial_agent"],
            tool_executor=tool_execution,
            group_agent_names=group_agent_names,
            user_agent=user_agent,
            group_after_work=group_after_work,
        )
        state["use_initial_agent"] = False
        return result

    return group_transition


def create_group_manager(
    groupchat: GroupChat,
    group_manager_args: dict[str, Any] | None,
    agents: list["ConversableAgent"],
    group_after_work: TransitionTarget,
) -> GroupChatManager:
    """Create a GroupChatManager for the group chat utilising any arguments passed in and ensure an LLM Config exists if needed

    Args:
        groupchat (GroupChat): The groupchat.
        group_manager_args (dict[str, Any]): Group manager arguments to create the GroupChatManager.
        agents (list["ConversableAgent"]): List of agents in the group to check handoffs and after work.
        group_after_work (TransitionTarget): Group-level after work to check.

    Returns:
        GroupChatManager: GroupChatManager instance.
    """
    manager_args = (group_manager_args or {}).copy()
    if "groupchat" in manager_args:
        raise ValueError("'groupchat' cannot be specified in group_manager_args as it is set by initiate_group_chat")
    manager = GroupChatManager(groupchat, **manager_args)

    # Ensure that our manager has an LLM Config if we have any GroupManagerTarget targets used
    if manager.llm_config is False:
        has_group_manager_target = False

        if isinstance(group_after_work, GroupManagerTarget):
            # Check group after work
            has_group_manager_target = True
        else:
            # Check agent hand-offs and after work
            for agent in agents:
                if (
                    len(agent.handoffs.get_context_conditions_by_target_type(GroupManagerTarget)) > 0
                    or len(agent.handoffs.get_llm_conditions_by_target_type(GroupManagerTarget)) > 0
                    or any(isinstance(aw.target, GroupManagerTarget) for aw in agent.handoffs.after_works)
                ):
                    has_group_manager_target = True
                    break

        if has_group_manager_target:
            raise ValueError(
                "The group manager doesn't have an LLM Config and it is required for any targets or after works using a GroupManagerTarget. Use the 'llm_config' in the group_manager_args parameter to specify the LLM Config for the group manager."
            )

    return manager


def make_remove_function(tool_msgs_to_remove: list[str]) -> Callable[[list[dict[str, Any]]], list[dict[str, Any]]]:
    """Create a function to remove messages with tool calls from the messages list.

    The returned function can be registered as a hook to "process_all_messages_before_reply"" to remove messages with tool calls.
    """

    def remove_messages(messages: list[dict[str, Any]], tool_msgs_to_remove: list[str]) -> list[dict[str, Any]]:
        copied = copy.deepcopy(messages)
        new_messages = []
        removed_tool_ids = []
        for message in copied:
            # remove tool calls
            if message.get("tool_calls") is not None:
                filtered_tool_calls = []
                for tool_call in message["tool_calls"]:
                    if tool_call.get("function") is not None and tool_call["function"]["name"] in tool_msgs_to_remove:
                        # remove
                        removed_tool_ids.append(tool_call["id"])
                    else:
                        filtered_tool_calls.append(tool_call)
                if len(filtered_tool_calls) > 0:
                    message["tool_calls"] = filtered_tool_calls
                else:
                    del message["tool_calls"]
                    if (
                        message.get("content") is None
                        or message.get("content") == ""
                        or message.get("content") == "None"
                    ):
                        continue  # if no tool call and no content, skip this message
                    # else: keep the message with tool_calls removed
            # remove corresponding tool responses
            elif message.get("tool_responses") is not None:
                filtered_tool_responses = []
                for tool_response in message["tool_responses"]:
                    if tool_response["tool_call_id"] not in removed_tool_ids:
                        filtered_tool_responses.append(tool_response)

                if len(filtered_tool_responses) > 0:
                    message["tool_responses"] = filtered_tool_responses
                else:
                    continue

            new_messages.append(message)

        return new_messages

    return partial(remove_messages, tool_msgs_to_remove=tool_msgs_to_remove)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Generator, Iterable
from typing import Any

from pydantic import BaseModel, Field

__all__ = ["ContextVariables"]

# Parameter name for context variables
# Use the value in functions and they will be substituted with the context variables:
# e.g. def my_function(context_variables: ContextVariables, my_other_parameters: Any) -> Any:
__CONTEXT_VARIABLES_PARAM_NAME__ = "context_variables"


class ContextVariables(BaseModel):
    """Stores and manages context variables for agentic workflows.

    Utilises a dictionary-like interface for setting, getting, and removing variables.
    """

    # Internal storage for context variables
    data: dict[str, Any] = Field(default_factory=dict)

    def __init__(self, data: dict[str, Any] | None = None, **kwargs: Any) -> None:
        """Initialize with data dictionary as an optional positional parameter.

        Args:
            data: Initial dictionary of context variables (optional)
            kwargs: Additional keyword arguments for the parent class
        """
        init_data = data or {}
        super().__init__(data=init_data, **kwargs)

    def get(self, key: str, default: Any | None = None) -> Any | None:
        """Get a value from the context by key.

        Args:
            key: The key to retrieve
            default: The default value to return if key is not found

        Returns:
            The value associated with the key or default if not found
        """
        return self.data.get(key, default)

    def set(self, key: str, value: Any) -> None:
        """Set a value in the context by key.

        Args:
            key: The key to set
            value: The value to store
        """
        self.data[key] = value

    def remove(self, key: str) -> bool:
        """Remove a key from the context.

        Args:
            key: The key to remove

        Returns:
            True if the key was removed, False if it didn't exist
        """
        if key in self.data:
            del self.data[key]
            return True
        return False

    def keys(self) -> Iterable[str]:
        """Get all keys in the context.

        Returns:
            An iterable of all keys
        """
        return self.data.keys()

    def values(self) -> Iterable[Any]:
        """Get all values in the context.

        Returns:
            An iterable of all values
        """
        return self.data.values()

    def items(self) -> Iterable[tuple[str, Any]]:
        """Get all key-value pairs in the context.

        Returns:
            An iterable of all key-value pairs
        """
        return self.data.items()

    def clear(self) -> None:
        """Clear all keys and values from the context."""
        self.data.clear()

    def contains(self, key: str) -> bool:
        """Check if a key exists in the context.

        Args:
            key: The key to check

        Returns:
            True if the key exists, False otherwise
        """
        return key in self.data

    def update(self, other: dict[str, Any]) -> None:
        """Update context with key-value pairs from another dictionary.

        Args:
            other: Dictionary containing key-value pairs to add
        """
        self.data.update(other)

    def to_dict(self) -> dict[str, Any]:
        """Convert context variables to a dictionary.

        Returns:
            Dictionary representation of all context variables
        """
        return self.data.copy()

    # Dictionary-compatible interface
    def __getitem__(self, key: str) -> Any:
        """Get a value using dictionary syntax: context[key]"""
        try:
            return self.data[key]
        except KeyError:
            raise KeyError(f"Context variable '{key}' not found")

    def __setitem__(self, key: str, value: Any) -> None:
        """Set a value using dictionary syntax: context[key] = value"""
        self.data[key] = value

    def __delitem__(self, key: str) -> None:
        """Delete a key using dictionary syntax: del context[key]"""
        try:
            del self.data[key]
        except KeyError:
            raise KeyError(f"Cannot delete non-existent context variable '{key}'")

    def __contains__(self, key: str) -> bool:
        """Check if key exists using 'in' operator: key in context"""
        return key in self.data

    def __len__(self) -> int:
        """Get the number of items: len(context)"""
        return len(self.data)

    def __iter__(self) -> Generator[tuple[str, Any], None, None]:
        """Iterate over keys: for key in context"""
        for key in self.data:
            yield (key, self.data[key])

    def __str__(self) -> str:
        """String representation of context variables."""
        return f"ContextVariables({self.data})"

    def __repr__(self) -> str:
        """Detailed representation of context variables."""
        return f"ContextVariables(data={self.data!r})"

    # Utility methods
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "ContextVariables":
        """Create a new ContextVariables instance from a dictionary.

        E.g.:
        my_context = {"user_id": "12345", "settings": {"theme": "dark"}}
        context = ContextVariables.from_dict(my_context)

        Args:
            data: Dictionary of key-value pairs

        Returns:
            New ContextVariables instance
        """
        return cls(data=data)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#

from .auto import AutoPattern
from .manual import ManualPattern
from .pattern import DefaultPattern
from .random import RandomPattern
from .round_robin import RoundRobinPattern

__all__ = [
    "AutoPattern",
    "DefaultPattern",
    "ManualPattern",
    "RandomPattern",
    "RoundRobinPattern",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Patterns of agent orchestrations
# Uses the group chat or the agents' handoffs to create a pattern

from abc import ABC, abstractmethod
from collections.abc import Callable
from typing import TYPE_CHECKING, Any, Optional

from ..context_variables import ContextVariables
from ..group_utils import (
    create_group_manager,
    create_group_transition,
    link_agents_to_group_manager,
    prepare_group_agents,
    process_initial_messages,
    setup_context_variables,
)
from ..targets.transition_target import TerminateTarget, TransitionTarget

if TYPE_CHECKING:
    from ...agent import Agent
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat, GroupChatManager
    from ..group_tool_executor import GroupToolExecutor


class Pattern(ABC):
    """Base abstract class for all orchestration patterns.

    Patterns provide a reusable way to define how agents interact within a group chat.
    Each pattern encapsulates the logic for setting up agents, configuring handoffs,
    and determining the flow of conversation.

    This is an abstract base class and should not be instantiated directly.
    Use one of the concrete pattern implementations like AutoPattern,
    RoundRobinPattern, RandomPattern, or ManualPattern.
    """

    def __init__(
        self,
        initial_agent: "ConversableAgent",
        agents: list["ConversableAgent"],
        user_agent: Optional["ConversableAgent"] = None,
        group_manager_args: dict[str, Any] | None = None,
        context_variables: ContextVariables | None = None,
        group_after_work: TransitionTarget | None = None,
        exclude_transit_message: bool = True,
        summary_method: str | Callable[..., Any] | None = "last_msg",
    ):
        """Initialize the pattern with the required components.

        Args:
            initial_agent: The first agent to speak in the group chat.
            agents: List of all agents participating in the chat.
            user_agent: Optional user proxy agent.
            group_manager_args: Optional arguments for the GroupChatManager.
            context_variables: Initial context variables for the chat.
            group_after_work: Default after work transition behavior when no specific next agent is determined.
            exclude_transit_message: Whether to exclude transit messages from the conversation.
            summary_method: Method for summarizing the conversation.
        """
        self.initial_agent = initial_agent
        self.agents = agents
        self.user_agent = user_agent
        self.group_manager_args = group_manager_args or {}
        self.context_variables = context_variables or ContextVariables()
        self.group_after_work = group_after_work if group_after_work is not None else TerminateTarget()
        self.exclude_transit_message = exclude_transit_message
        self.summary_method = summary_method

    @abstractmethod
    def prepare_group_chat(
        self,
        max_rounds: int,
        messages: list[dict[str, Any]] | str,
    ) -> tuple[
        list["ConversableAgent"],
        list["ConversableAgent"],
        Optional["ConversableAgent"],
        ContextVariables,
        "ConversableAgent",
        TransitionTarget,
        "GroupToolExecutor",
        "GroupChat",
        "GroupChatManager",
        list[dict[str, Any]],
        "ConversableAgent",
        list[str],
        list["Agent"],
    ]:
        """Prepare the group chat for orchestration.

        This is the main method called by initiate_group_chat to set up the pattern.
        Subclasses must implement or extend this method to define pattern-specific behavior.

        Args:
            max_rounds: Maximum number of conversation rounds.
            messages: Initial message(s) to start the conversation.

        Returns:
            Tuple containing:
            - List of agents involved in the group chat
            - List of wrapped agents
            - User agent, if applicable
            - Context variables for the group chat
            - Initial agent for the group chat
            - Group-level after work transition for the group chat
            - Tool executor for the group chat
            - GroupChat instance
            - GroupChatManager instance
            - Processed messages
            - Last agent to speak
            - List of group agent names
            - List of temporary user agents
        """
        from ...groupchat import GroupChat

        # Prepare the agents using the existing helper function
        tool_executor, wrapped_agents = prepare_group_agents(
            self.agents, self.context_variables, self.exclude_transit_message
        )

        # Process the initial messages BEFORE creating the GroupChat
        # This will create a temporary user agent if needed
        processed_messages, last_agent, group_agent_names, temp_user_list = process_initial_messages(
            messages, self.user_agent, self.agents, wrapped_agents
        )

        # Create transition function (has enclosed state for initial agent)
        group_transition = create_group_transition(
            initial_agent=self.initial_agent,
            tool_execution=tool_executor,
            group_agent_names=group_agent_names,
            user_agent=self.user_agent,
            group_after_work=self.group_after_work,
        )

        # Create the group chat - now we use temp_user_list if no user_agent
        groupchat = GroupChat(
            agents=[tool_executor]
            + self.agents
            + wrapped_agents
            + ([self.user_agent] if self.user_agent else temp_user_list),
            messages=[],
            max_round=max_rounds,
            speaker_selection_method=group_transition,
        )

        # Create the group manager
        manager = create_group_manager(groupchat, self.group_manager_args, self.agents, self.group_after_work)

        # Point all agent's context variables to this function's context_variables
        setup_context_variables(
            tool_execution=tool_executor,
            agents=self.agents,
            manager=manager,
            user_agent=self.user_agent,
            context_variables=self.context_variables,
        )

        # Link all agents with the GroupChatManager to allow access to the group chat
        link_agents_to_group_manager(groupchat.agents, manager)

        return (
            self.agents,
            wrapped_agents,
            self.user_agent,
            self.context_variables,
            self.initial_agent,
            self.group_after_work,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        )  # type: ignore[return-value]

    @classmethod
    def create_default(
        cls,
        initial_agent: "ConversableAgent",
        agents: list["ConversableAgent"],
        user_agent: Optional["ConversableAgent"] = None,
        group_manager_args: dict[str, Any] | None = None,
        context_variables: ContextVariables | None = None,
        exclude_transit_message: bool = True,
        summary_method: str | Callable[..., Any] | None = "last_msg",
    ) -> "DefaultPattern":
        """Create a default pattern with minimal configuration.

        This replaces the need for a separate BasePattern class by providing
        a factory method that creates a simple DefaultPattern instance.

        Args:
            initial_agent: The first agent to speak in the group chat.
            agents: List of all agents participating in the chat.
            user_agent: Optional user proxy agent.
            group_manager_args: Optional arguments for the GroupChatManager.
            context_variables: Initial context variables for the chat.
            exclude_transit_message: Whether to exclude transit messages from the conversation.
            summary_method: Method for summarizing the conversation.

        Returns:
            A DefaultPattern instance with basic configuration.
        """
        return DefaultPattern(
            initial_agent=initial_agent,
            agents=agents,
            user_agent=user_agent,
            group_manager_args=group_manager_args,
            context_variables=context_variables,
            exclude_transit_message=exclude_transit_message,
            summary_method=summary_method,
        )


class DefaultPattern(Pattern):
    """DefaultPattern implements a minimal pattern for simple agent interactions.

    This replaces the previous BasePattern and provides a concrete implementation
    of the Pattern abstract base class.
    """

    def prepare_group_chat(
        self,
        max_rounds: int,
        messages: list[dict[str, Any]] | str,
    ) -> tuple[
        list["ConversableAgent"],
        list["ConversableAgent"],
        Optional["ConversableAgent"],
        ContextVariables,
        "ConversableAgent",
        TransitionTarget,
        "GroupToolExecutor",
        "GroupChat",
        "GroupChatManager",
        list[dict[str, Any]],
        Any,
        list[str],
        list[Any],
    ]:
        """Prepare the group chat with default configuration.

        This implementation calls the parent class method but ensures that
        the group_after_work in the returned tuple is the pattern's own.

        Args:
            max_rounds: Maximum number of conversation rounds.
            messages: Initial message(s) to start the conversation.

        Returns:
            Tuple containing all necessary components for the group chat.
        """
        # Use the parent class's implementation to prepare the agents and group chat
        (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            _,  # Ignore the group_after_work from parent
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        ) = super().prepare_group_chat(
            max_rounds=max_rounds,
            messages=messages,
        )

        # Return all components with our group_after_work
        return (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            self.group_after_work,  # Use our own group_after_work
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Callable
from typing import TYPE_CHECKING, Any, Optional

from ..context_variables import ContextVariables
from ..group_tool_executor import GroupToolExecutor
from ..targets.transition_target import AskUserTarget, TransitionTarget
from .pattern import Pattern

if TYPE_CHECKING:
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat, GroupChatManager


class ManualPattern(Pattern):
    """ManualPattern will ask the user to nominate the next agent to speak at each turn."""

    def __init__(
        self,
        initial_agent: "ConversableAgent",
        agents: list["ConversableAgent"],
        user_agent: Optional["ConversableAgent"] = None,
        group_manager_args: dict[str, Any] | None = None,
        context_variables: ContextVariables | None = None,
        exclude_transit_message: bool = True,
        summary_method: str | Callable[..., Any] | None = "last_msg",
    ):
        """Initialize the ManualPattern.

        The after_work is always set to ask_user, which will prompt the user for the next agent

        Args:
            initial_agent: The first agent to speak in the group chat.
            agents: List of all agents participating in the chat.
            user_agent: Optional user proxy agent.
            group_manager_args: Optional arguments for the GroupChatManager.
            context_variables: Initial context variables for the chat.
            exclude_transit_message: Whether to exclude transit messages from the conversation.
            summary_method: Method for summarizing the conversation.
        """
        # The group after work will be to ask the user
        group_after_work = AskUserTarget()

        super().__init__(
            initial_agent=initial_agent,
            agents=agents,
            user_agent=user_agent,
            group_manager_args=group_manager_args,
            context_variables=context_variables,
            group_after_work=group_after_work,
            exclude_transit_message=exclude_transit_message,
            summary_method=summary_method,
        )

    def prepare_group_chat(
        self,
        max_rounds: int,
        messages: list[dict[str, Any]] | str,
    ) -> tuple[
        list["ConversableAgent"],
        list["ConversableAgent"],
        Optional["ConversableAgent"],
        ContextVariables,
        "ConversableAgent",
        TransitionTarget,
        "GroupToolExecutor",
        "GroupChat",
        "GroupChatManager",
        list[dict[str, Any]],
        Any,
        list[str],
        list[Any],
    ]:
        """Prepare the group chat for organic agent selection.

        Ensures that:
        1. The group manager has a valid LLM config
        2. All agents have appropriate descriptions for the group manager to use

        Args:
            max_rounds: Maximum number of conversation rounds.
            messages: Initial message(s) to start the conversation.

        Returns:
            Tuple containing all necessary components for the group chat.
        """
        # Use the parent class's implementation to prepare the agents and group chat
        components = super().prepare_group_chat(
            max_rounds=max_rounds,
            messages=messages,
        )

        # Extract the group_after_work and the rest of the components
        (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            _,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        ) = components

        # Ensure we're using the group_manager after_work
        group_after_work = self.group_after_work

        # Set up the allowed speaker transitions to exclude user_agent and GroupToolExecutor
        self._setup_allowed_transitions(groupchat, user_agent, tool_executor)

        # Return all components with our group_after_work
        return (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            group_after_work,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        )

    def _setup_allowed_transitions(
        self, groupchat: "GroupChat", user_agent: Optional["ConversableAgent"], tool_executor: "GroupToolExecutor"
    ) -> None:
        """Set up the allowed speaker transitions for the group chat so that when a user selects the next agent the tool executor and user agent don't appear as options.

        Creates transitions where:
        1. Any agent can speak after any other agent, including themselves
        2. The user_agent and GroupToolExecutor are excluded from transitions

        Args:
            groupchat: The GroupChat instance to configure
            user_agent: The user agent to exclude from transitions
            tool_executor: The GroupToolExecutor to exclude from transitions
        """
        # NOTE: THIS IS NOT WORKING - THE TRANSITIONS ARE NOT BEING KEPT?!
        """
        # Get all agents in the group chat
        all_agents = groupchat.agents

        # Filter out user_agent and group tool executor
        eligible_agents = []
        for agent in all_agents:
            # Skip user_agent
            if agent == user_agent:
                continue

            # Skip GroupToolExecutor
            if isinstance(agent, GroupToolExecutor):
                continue

            eligible_agents.append(agent)

        # Create a fully connected graph among eligible agents
        # Each agent can be followed by any other eligible agent
        allowed_transitions = {}
        for agent in eligible_agents:
            # For each agent, every other eligible agent can follow
            allowed_transitions[agent] = eligible_agents

        # Set the transitions in the group chat
        groupchat.allowed_speaker_transitions_dict = allowed_transitions
        """
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Any, Optional

from ..context_variables import ContextVariables
from ..targets.transition_target import RandomAgentTarget, TransitionTarget
from .pattern import Pattern

if TYPE_CHECKING:
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat, GroupChatManager
    from ..group_tool_executor import GroupToolExecutor


class RandomPattern(Pattern):
    """RandomPattern implements a random agent selection process."""

    def _generate_handoffs(
        self,
        initial_agent: "ConversableAgent",
        agents: list["ConversableAgent"],
        user_agent: Optional["ConversableAgent"],
    ) -> None:
        """Generate handoffs between agents in a random fashion."""
        agent_list = agents + ([user_agent] if user_agent is not None else [])

        for agent in agent_list:
            # Get the list of agents except itself
            other_agents = [a for a in agent_list if a != agent]

            # Create a random after work
            agent.handoffs.set_after_work(target=RandomAgentTarget(agents=other_agents))

    def prepare_group_chat(
        self,
        max_rounds: int,
        messages: list[dict[str, Any]] | str,
    ) -> tuple[
        list["ConversableAgent"],
        list["ConversableAgent"],
        Optional["ConversableAgent"],
        ContextVariables,
        "ConversableAgent",
        TransitionTarget,
        "GroupToolExecutor",
        "GroupChat",
        "GroupChatManager",
        list[dict[str, Any]],
        Any,
        list[str],
        list[Any],
    ]:
        """Prepare the group chat for organic agent selection.

        Ensures that:
        1. The group manager has a valid LLM config
        2. All agents have appropriate descriptions for the group manager to use

        Args:
            max_rounds: Maximum number of conversation rounds.
            messages: Initial message(s) to start the conversation.

        Returns:
            Tuple containing all necessary components for the group chat.
        """
        # Use the parent class's implementation to prepare the agents and group chat
        (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            group_after_work,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        ) = super().prepare_group_chat(
            max_rounds=max_rounds,
            messages=messages,
        )

        # Create the random handoffs between agents
        self._generate_handoffs(initial_agent=initial_agent, agents=agents, user_agent=user_agent)

        # Return all components with our group_after_work
        return (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            group_after_work,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Any, Optional

from ..context_variables import ContextVariables
from ..targets.transition_target import AgentTarget, TransitionTarget
from .pattern import Pattern

if TYPE_CHECKING:
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat, GroupChatManager
    from ..group_tool_executor import GroupToolExecutor


class RoundRobinPattern(Pattern):
    """RoundRobinPattern implements a round robin with handoffs between agents."""

    def _generate_handoffs(
        self,
        initial_agent: "ConversableAgent",
        agents: list["ConversableAgent"],
        user_agent: Optional["ConversableAgent"],
    ) -> None:
        """Generate handoffs between agents in a round-robin fashion."""
        # Create a list of the agents and the user_agent but put the initial_agent first
        agent_list = [initial_agent]

        # Add the rest of the agents, excluding the initial_agent and user_agent
        for agent in agents:
            if agent != initial_agent and (user_agent is None or agent != user_agent):
                agent_list.append(agent)

        # Add the user_agent last if it exists
        if user_agent is not None:
            agent_list.append(user_agent)

        # Create handoffs in a round-robin fashion
        for i, agent in enumerate(agent_list):
            # Last agent hands off to the first agent
            # Otherwise agent hands off to the next one
            handoff_target = agent_list[0] if i == len(agent_list) - 1 else agent_list[i + 1]

            agent.handoffs.set_after_work(target=AgentTarget(agent=handoff_target))

    def prepare_group_chat(
        self,
        max_rounds: int,
        messages: list[dict[str, Any]] | str,
    ) -> tuple[
        list["ConversableAgent"],
        list["ConversableAgent"],
        Optional["ConversableAgent"],
        ContextVariables,
        "ConversableAgent",
        TransitionTarget,
        "GroupToolExecutor",
        "GroupChat",
        "GroupChatManager",
        list[dict[str, Any]],
        Any,
        list[str],
        list[Any],
    ]:
        """Prepare the group chat for organic agent selection.

        Ensures that:
        1. The group manager has a valid LLM config
        2. All agents have appropriate descriptions for the group manager to use

        Args:
            max_rounds: Maximum number of conversation rounds.
            messages: Initial message(s) to start the conversation.

        Returns:
            Tuple containing all necessary components for the group chat.
        """
        # Use the parent class's implementation to prepare the agents and group chat
        (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            group_after_work,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        ) = super().prepare_group_chat(
            max_rounds=max_rounds,
            messages=messages,
        )

        # Create the handoffs between agents
        self._generate_handoffs(initial_agent=initial_agent, agents=agents, user_agent=user_agent)

        # Return all components with our group_after_work
        return (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            group_after_work,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Callable
from typing import TYPE_CHECKING, Any, Optional

from ..context_variables import ContextVariables
from ..targets.group_manager_target import GroupManagerSelectionMessage, GroupManagerTarget
from ..targets.transition_target import TransitionTarget
from .pattern import Pattern

if TYPE_CHECKING:
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat, GroupChatManager
    from ..group_tool_executor import GroupToolExecutor


class AutoPattern(Pattern):
    """AutoPattern implements a flexible pattern where agents are selected based on their expertise.

    In this pattern, a group manager automatically selects the next agent to speak based on the context
    of the conversation and agent descriptions. The after_work is always set to "group_manager" as
    this is the defining characteristic of this pattern.
    """

    def __init__(
        self,
        initial_agent: "ConversableAgent",
        agents: list["ConversableAgent"],
        user_agent: Optional["ConversableAgent"] = None,
        group_manager_args: dict[str, Any] | None = None,
        context_variables: ContextVariables | None = None,
        selection_message: GroupManagerSelectionMessage | None = None,
        exclude_transit_message: bool = True,
        summary_method: str | Callable[..., Any] | None = "last_msg",
    ):
        """Initialize the AutoPattern.

        The after_work is always set to group_manager selection, which is the defining
        characteristic of this pattern. You can customize the selection message used
        by the group manager when selecting the next agent.

        Args:
            initial_agent: The first agent to speak in the group chat.
            agents: List of all agents participating in the chat.
            user_agent: Optional user proxy agent.
            group_manager_args: Optional arguments for the GroupChatManager.
            context_variables: Initial context variables for the chat.
            selection_message: Custom message to use when the group manager is selecting agents.
            exclude_transit_message: Whether to exclude transit messages from the conversation.
            summary_method: Method for summarizing the conversation.
        """
        # Create the group_manager after_work with the provided selection message
        group_manager_after_work = GroupManagerTarget(selection_message=selection_message)

        super().__init__(
            initial_agent=initial_agent,
            agents=agents,
            user_agent=user_agent,
            group_manager_args=group_manager_args,
            context_variables=context_variables,
            group_after_work=group_manager_after_work,
            exclude_transit_message=exclude_transit_message,
            summary_method=summary_method,
        )

        # Store the selection message for potential use
        self.selection_message = selection_message

    def prepare_group_chat(
        self,
        max_rounds: int,
        messages: list[dict[str, Any]] | str,
    ) -> tuple[
        list["ConversableAgent"],
        list["ConversableAgent"],
        Optional["ConversableAgent"],
        ContextVariables,
        "ConversableAgent",
        TransitionTarget,
        "GroupToolExecutor",
        "GroupChat",
        "GroupChatManager",
        list[dict[str, Any]],
        Any,
        list[str],
        list[Any],
    ]:
        """Prepare the group chat for organic agent selection.

        Ensures that:
        1. The group manager has a valid LLM config
        2. All agents have appropriate descriptions for the group manager to use

        Args:
            max_rounds: Maximum number of conversation rounds.
            messages: Initial message(s) to start the conversation.

        Returns:
            Tuple containing all necessary components for the group chat.
        """
        # Validate that group_manager_args has an LLM config which is required for this pattern
        if not self.group_manager_args.get("llm_config", False):
            # Check if any agent has an LLM config we can use
            has_llm_config = any(getattr(agent, "llm_config", False) for agent in self.agents)

            if not has_llm_config:
                raise ValueError(
                    "AutoPattern requires the group_manager_args to include an llm_config, "
                    "or at least one agent to have an llm_config"
                )

        # Check that all agents have descriptions for effective group manager selection
        for agent in self.agents:
            if not hasattr(agent, "description") or not agent.description:
                agent.description = f"Agent {agent.name}"

        # Use the parent class's implementation to prepare the agents and group chat
        components = super().prepare_group_chat(
            max_rounds=max_rounds,
            messages=messages,
        )

        # Extract the group_after_work and the rest of the components
        (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            _,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        ) = components

        # Ensure we're using the group_manager after_work
        group_after_work = self.group_after_work

        # Return all components with our group_after_work
        return (
            agents,
            wrapped_agents,
            user_agent,
            context_variables,
            initial_agent,
            group_after_work,
            tool_executor,
            groupchat,
            manager,
            processed_messages,
            last_agent,
            group_agent_names,
            temp_user_list,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Any, Optional

from pydantic import BaseModel, field_validator

from ....doc_utils import export_module
from ..context_str import ContextStr
from ..group_tool_executor import GroupToolExecutor
from ..speaker_selection_result import SpeakerSelectionResult
from .transition_target import TransitionTarget
from .transition_utils import __AGENT_WRAPPER_PREFIX__

if TYPE_CHECKING:
    # Avoid circular import
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat

__all__ = ["GroupManagerTarget"]


def prepare_groupchat_auto_speaker(
    groupchat: "GroupChat",
    last_group_agent: "ConversableAgent",
    group_chat_manager_selection_msg: Any | None,
) -> None:
    """Prepare the group chat for auto speaker selection, includes updating or restore the groupchat speaker selection message.

    Tool Executor and wrapped agents will be removed from the available agents list.

    Args:
        groupchat (GroupChat): GroupChat instance.
        last_group_agent ("ConversableAgent"): The last group agent for which the LLM config is used
        group_chat_manager_selection_msg (GroupManagerSelectionMessage): Optional message to use for the agent selection (in internal group chat).
    """
    from ...groupchat import SELECT_SPEAKER_PROMPT_TEMPLATE

    def substitute_agentlist(template: str) -> str:
        # Run through group chat's string substitution first for {agentlist}
        # We need to do this so that the next substitution doesn't fail with agentlist
        # and we can remove the tool executor and wrapped chats from the available agents list
        agent_list = [
            agent
            for agent in groupchat.agents
            if not isinstance(agent, GroupToolExecutor) and not agent.name.startswith(__AGENT_WRAPPER_PREFIX__)
        ]

        groupchat.select_speaker_prompt_template = template
        return groupchat.select_speaker_prompt(agent_list)

    # Use the default speaker selection prompt if one is not specified, otherwise use the specified one
    groupchat.select_speaker_prompt_template = substitute_agentlist(
        SELECT_SPEAKER_PROMPT_TEMPLATE
        if group_chat_manager_selection_msg is None
        else group_chat_manager_selection_msg.get_message(last_group_agent)
    )


# GroupManagerSelectionMessage protocol and implementations
@export_module("autogen.agentchat.group")
class GroupManagerSelectionMessage(BaseModel):
    """Base class for all GroupManager selection message types."""

    def get_message(self, agent: "ConversableAgent") -> str:
        """Get the formatted message."""
        raise NotImplementedError("Requires subclasses to implement.")


@export_module("autogen.agentchat.group")
class GroupManagerSelectionMessageString(GroupManagerSelectionMessage):
    """Selection message that uses a plain string template."""

    message: str

    def get_message(self, agent: "ConversableAgent") -> str:
        """Get the message string."""
        return self.message


@export_module("autogen.agentchat.group")
class GroupManagerSelectionMessageContextStr(GroupManagerSelectionMessage):
    """Selection message that uses a ContextStr template."""

    context_str_template: str

    # We will replace {agentlist} with another term and return it later for use with the internal group chat auto speaker selection
    # Otherwise our format will fail
    @field_validator("context_str_template", mode="before")
    def _replace_agentlist_placeholder(cls: type["GroupManagerSelectionMessageContextStr"], v: Any) -> str | Any:  # noqa: N805
        """Replace {agentlist} placeholder before validation/assignment."""
        if isinstance(v, str):
            if "{agentlist}" in v:
                return v.replace("{agentlist}", "<<agent_list>>")  # Perform the replacement
            else:
                return v  # If no replacement is needed, return the original value
        return ""

    def get_message(self, agent: "ConversableAgent") -> str:
        """Get the formatted message with context variables substituted."""
        context_str = ContextStr(template=self.context_str_template)
        format_result = context_str.format(agent.context_variables)
        if format_result is None:
            return ""

        return format_result.replace(
            "<<agent_list>>", "{agentlist}"
        )  # Restore agentlist so it can be substituted by the internal group chat auto speaker selection


class GroupManagerTarget(TransitionTarget):
    """Target that represents an agent by name."""

    selection_message: GroupManagerSelectionMessage | None = None

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to the speaker selection for the group."""
        if self.selection_message is not None:
            prepare_groupchat_auto_speaker(groupchat, current_agent, self.selection_message)

        return SpeakerSelectionResult(speaker_selection_method="auto")

    def display_name(self) -> str:
        """Get the display name for the target."""
        return "the group manager"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return self.display_name()

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return "Transfer to the group manager"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("GroupManagerTarget does not require wrapping in an agent.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import random
from typing import TYPE_CHECKING, Any, Optional

from pydantic import BaseModel

from ..speaker_selection_result import SpeakerSelectionResult
from .transition_utils import __AGENT_WRAPPER_PREFIX__

if TYPE_CHECKING:
    # Avoid circular import
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat

__all__ = [
    "AgentNameTarget",
    "AgentTarget",
    "AskUserTarget",
    "NestedChatTarget",
    "RandomAgentTarget",
    "RevertToUserTarget",
    "StayTarget",
    "TerminateTarget",
    "TransitionTarget",
]

# Common options for transitions
# terminate: Terminate the conversation
# revert_to_user: Revert to the user agent
# stay: Stay with the current agent
# group_manager: Use the group manager (auto speaker selection)
# ask_user: Use the user manager (ask the user, aka manual)
# TransitionOption = Literal["terminate", "revert_to_user", "stay", "group_manager", "ask_user"]


class TransitionTarget(BaseModel):
    """Base class for all transition targets across OnCondition, OnContextCondition, and after work."""

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve to an option for speaker selection (Agent, 'None' to end, Str for speaker selection method). In the case of a nested chat, this will return False as it should be encapsulated in an agent."""
        return False

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to a speaker selection result (Agent, None for termination, or str for speaker selection method)."""
        raise NotImplementedError("Requires subclasses to implement.")

    def display_name(self) -> str:
        """Get the display name for the target."""
        raise NotImplementedError("Requires subclasses to implement.")

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        raise NotImplementedError("Requires subclasses to implement.")

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        raise NotImplementedError("Requires subclasses to implement.")

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("Requires subclasses to implement.")

    def activate_target(self, groupchat: "GroupChat") -> None:
        """Activate the target in the groupchat, setting the next target for GroupToolExecutor.

        The Tool Executor's next target attribute will be picked up on the next iteration when _determine_next_agent is called
        """
        for agent in groupchat.agents:  # type: ignore[attr-defined]
            # get the GroupToolExecutor agent
            if type(agent).__name__ == "GroupToolExecutor":
                agent.set_next_target(self)  # type: ignore[attr-defined]
                return


class AgentTarget(TransitionTarget):
    """Target that represents a direct agent reference."""

    agent_name: str

    def __init__(self, agent: "ConversableAgent", **data: Any) -> None:  # type: ignore[no-untyped-def]
        # Store the name from the agent for serialization
        super().__init__(agent_name=agent.name, **data)

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to the actual agent object from the groupchat."""
        return SpeakerSelectionResult(agent_name=self.agent_name)

    def display_name(self) -> str:
        """Get the display name for the target."""
        return f"{self.agent_name}"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return self.display_name()

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return f"Transfer to {self.agent_name}"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("AgentTarget does not require wrapping in an agent.")


class AgentNameTarget(TransitionTarget):
    """Target that represents an agent by name."""

    agent_name: str

    def __init__(self, agent_name: str, **data: Any) -> None:
        """Initialize with agent name as a positional parameter."""
        super().__init__(agent_name=agent_name, **data)

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to the agent name string."""
        return SpeakerSelectionResult(agent_name=self.agent_name)

    def display_name(self) -> str:
        """Get the display name for the target."""
        return f"{self.agent_name}"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return self.display_name()

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return f"Transfer to {self.agent_name}"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("AgentNameTarget does not require wrapping in an agent.")


class NestedChatTarget(TransitionTarget):
    """Target that represents a nested chat configuration."""

    nested_chat_config: dict[str, Any]

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection. For NestedChatTarget the nested chat must be encapsulated into an agent."""
        return False

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to the nested chat configuration."""
        raise NotImplementedError(
            "NestedChatTarget does not support the resolve method. An agent should be used to encapsulate this nested chat and then the target changed to an AgentTarget."
        )

    def display_name(self) -> str:
        """Get the display name for the target."""
        return "a nested chat"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return "nested_chat"

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return "Transfer to nested chat"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent. NestedChatTarget must be wrapped in an agent."""
        return True

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the nested chat."""
        from ...conversable_agent import ConversableAgent  # to avoid circular import - NEED SOLUTION

        nested_chat_agent = ConversableAgent(name=f"{__AGENT_WRAPPER_PREFIX__}nested_{parent_agent.name}_{index + 1}")

        nested_chat_agent.register_nested_chats(
            self.nested_chat_config["chat_queue"],
            reply_func_from_nested_chats=self.nested_chat_config.get("reply_func_from_nested_chats")
            or "summary_from_nested_chats",
            config=self.nested_chat_config.get("config"),
            trigger=lambda sender: True,
            position=0,
            use_async=self.nested_chat_config.get("use_async", False),
        )

        # After the nested chat is complete, transfer back to the parent agent
        nested_chat_agent.handoffs.set_after_work(AgentTarget(parent_agent))

        return nested_chat_agent


class TerminateTarget(TransitionTarget):
    """Target that represents a termination of the conversation."""

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to termination."""
        return SpeakerSelectionResult(terminate=True)

    def display_name(self) -> str:
        """Get the display name for the target."""
        return "Terminate"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return "terminate"

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return "Terminate"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("TerminateTarget does not require wrapping in an agent.")


class StayTarget(TransitionTarget):
    """Target that represents staying with the current agent."""

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to staying with the current agent."""
        return SpeakerSelectionResult(agent_name=current_agent.name)

    def display_name(self) -> str:
        """Get the display name for the target."""
        return "Stay"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return "stay"

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return "Stay with agent"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("StayTarget does not require wrapping in an agent.")


class RevertToUserTarget(TransitionTarget):
    """Target that represents reverting to the user agent."""

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to reverting to the user agent."""
        if user_agent is None:
            raise ValueError("User agent must be provided to the chat for the revert_to_user option.")
        return SpeakerSelectionResult(agent_name=user_agent.name)

    def display_name(self) -> str:
        """Get the display name for the target."""
        return "Revert to User"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return "revert_to_user"

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return "Revert to User"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("RevertToUserTarget does not require wrapping in an agent.")


class AskUserTarget(TransitionTarget):
    """Target that represents asking the user for input."""

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to asking the user for input."""
        return SpeakerSelectionResult(speaker_selection_method="manual")

    def display_name(self) -> str:
        """Get the display name for the target."""
        return "Ask User"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return "ask_user"

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return "Ask User"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("AskUserTarget does not require wrapping in an agent.")


class RandomAgentTarget(TransitionTarget):
    """Target that represents a random selection from a list of agents."""

    agent_names: list[str]
    nominated_name: str = "<Not Randomly Selected Yet>"

    def __init__(self, agents: list["ConversableAgent"], **data: Any) -> None:  # type: ignore[no-untyped-def]
        # Store the name from the agent for serialization
        super().__init__(agent_names=[agent.name for agent in agents], **data)

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection."""
        return True

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to the actual agent object from the groupchat, choosing a random agent (except the current one)"""
        # Randomly select the next agent
        self.nominated_name = random.choice([name for name in self.agent_names if name != current_agent.name])

        return SpeakerSelectionResult(agent_name=self.nominated_name)

    def display_name(self) -> str:
        """Get the display name for the target."""
        return self.nominated_name

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling"""
        return self.display_name()

    def __str__(self) -> str:
        """String representation for RandomAgentTarget, can be shown as a function call message."""
        return f"Transfer to {self.nominated_name}"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent."""
        return False

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the target if needed."""
        raise NotImplementedError("RandomAgentTarget does not require wrapping in an agent.")


# TODO: Consider adding a SequentialChatTarget class
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Prefix for all wrapped agent names
__AGENT_WRAPPER_PREFIX__ = "wrapped_"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Any, Optional

from pydantic import BaseModel

from ....doc_utils import export_module
from ...agent import Agent
from ..speaker_selection_result import SpeakerSelectionResult
from .transition_target import AgentTarget, TransitionTarget
from .transition_utils import __AGENT_WRAPPER_PREFIX__

if TYPE_CHECKING:
    from ...conversable_agent import ConversableAgent
    from ...groupchat import GroupChat
    from ..patterns.pattern import Pattern


__all__ = ["GroupChatConfig", "GroupChatTarget"]


@export_module("autogen.agentchat.group")
class GroupChatConfig(BaseModel):
    """Configuration for a group chat transition target.

    Note: If context_variables are not passed in, the outer context variables will be passed in
    """

    pattern: "Pattern"
    messages: list[dict[str, Any]] | str
    max_rounds: int = 20


@export_module("autogen.agentchat.group")
class GroupChatTarget(TransitionTarget):
    """Target that represents a group chat."""

    group_chat_config: GroupChatConfig

    def can_resolve_for_speaker_selection(self) -> bool:
        """Check if the target can resolve for speaker selection. For GroupChatTarget the chat must be encapsulated into an agent."""
        return False

    def resolve(
        self,
        groupchat: "GroupChat",
        current_agent: "ConversableAgent",
        user_agent: Optional["ConversableAgent"],
    ) -> SpeakerSelectionResult:
        """Resolve to the nested chat configuration."""
        raise NotImplementedError(
            "GroupChatTarget does not support the resolve method. An agent should be used to encapsulate this nested chat and then the target changed to an AgentTarget."
        )

    def display_name(self) -> str:
        """Get the display name for the target."""
        return "a group chat"

    def normalized_name(self) -> str:
        """Get a normalized name for the target that has no spaces, used for function calling."""
        return "group_chat"

    def __str__(self) -> str:
        """String representation for AgentTarget, can be shown as a function call message."""
        return "Transfer to group chat"

    def needs_agent_wrapper(self) -> bool:
        """Check if the target needs to be wrapped in an agent. GroupChatTarget must be wrapped in an agent."""
        return True

    def create_wrapper_agent(self, parent_agent: "ConversableAgent", index: int) -> "ConversableAgent":
        """Create a wrapper agent for the group chat."""
        from autogen.agentchat import initiate_group_chat

        from ...conversable_agent import ConversableAgent  # to avoid circular import

        # Create the wrapper agent with a name that identifies it as a wrapped group chat
        group_chat_agent = ConversableAgent(
            name=f"{__AGENT_WRAPPER_PREFIX__}group_{parent_agent.name}_{index + 1}",
            # Copy LLM config from parent agent to ensure it can generate replies if needed
            llm_config=parent_agent.llm_config,
        )

        # Store the config directly on the agent
        group_chat_agent._group_chat_config = self.group_chat_config  # type: ignore[attr-defined]

        # Define the reply function that will run the group chat
        def group_chat_reply(
            agent: "ConversableAgent",
            messages: list[dict[str, Any]] | None = None,
            sender: Optional["Agent"] = None,
            config: Any | None = None,
        ) -> tuple[bool, dict[str, Any] | None]:
            """Run the inner group chat and return its results as a reply."""
            # Get the configuration stored directly on the agent
            group_config = agent._group_chat_config  # type: ignore[attr-defined]

            # Pull through the second last message from the outer chat (the last message will be the handoff message)
            # This may need work to make sure we get the right message(s) from the outer chat
            message = (
                messages[-2]["content"]
                if messages and len(messages) >= 2 and "content" in messages[-2]
                else "No message to pass through."
            )

            try:
                # Run the group chat with direct agent references from the config
                result, _, _ = initiate_group_chat(
                    pattern=group_config.pattern,
                    messages=message,
                    max_rounds=group_config.max_rounds,
                )

                # Return the summary from the chat result summary
                return True, {"content": result.summary}

            except Exception as e:
                # Handle any errors during execution
                return True, {"content": f"Error running group chat: {str(e)}"}

        # Register the reply function with the wrapper agent
        group_chat_agent.register_reply(
            trigger=[ConversableAgent, None],
            reply_func=group_chat_reply,
            remove_other_reply_funcs=True,  # Use only this reply function
        )

        # After the group chat completes, transition back to the parent agent
        group_chat_agent.handoffs.set_after_work(AgentTarget(parent_agent))

        return group_chat_agent
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Any

from pydantic import BaseModel

from .context_str import ContextStr

if TYPE_CHECKING:
    # Avoid circular import
    from ..conversable_agent import ConversableAgent

__all__ = ["ContextStrLLMCondition", "LLMCondition", "StringLLMCondition"]


class LLMCondition(BaseModel):
    """Protocol for conditions evaluated by an LLM."""

    def get_prompt(self, agent: "ConversableAgent", messages: list[dict[str, Any]]) -> str:
        """Get the prompt text for LLM evaluation.

        Args:
            agent: The agent evaluating the condition
            messages: The conversation history

        Returns:
            The prompt text to be evaluated by the LLM
        """
        raise NotImplementedError("Requires subclasses to implement.")


class StringLLMCondition(LLMCondition):
    """Simple string-based LLM condition.

    This condition provides a static string prompt to be evaluated by an LLM.
    """

    prompt: str

    def __init__(self, prompt: str, **data: Any) -> None:
        """Initialize with a prompt string as a positional parameter.

        Args:
            prompt: The static prompt string to evaluate
            data: Additional data for the parent class
        """
        super().__init__(prompt=prompt, **data)

    def get_prompt(self, agent: "ConversableAgent", messages: list[dict[str, Any]]) -> str:
        """Return the static prompt string.

        Args:
            agent: The agent evaluating the condition (not used)
            messages: The conversation history (not used)

        Returns:
            The static prompt string
        """
        return self.prompt


class ContextStrLLMCondition(LLMCondition):
    """Context variable-based LLM condition.

    This condition uses a ContextStr object with context variable placeholders that
    will be substituted before being evaluated by an LLM.
    """

    context_str: ContextStr

    def __init__(self, context_str: ContextStr, **data: Any) -> None:
        """Initialize with a context string as a positional parameter.

        Args:
            context_str: The ContextStr object with variable placeholders
            data: Additional data for the parent class
        """
        super().__init__(context_str=context_str, **data)

    def get_prompt(self, agent: "ConversableAgent", messages: list[dict[str, Any]]) -> str:
        """Return the prompt with context variables substituted.

        Args:
            agent: The agent evaluating the condition (provides context variables)
            messages: The conversation history (not used)

        Returns:
            The prompt with context variables substituted
        """
        result = self.context_str.format(agent.context_variables)
        return result if result is not None else ""
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Any

from pydantic import BaseModel

from .context_expression import ContextExpression

if TYPE_CHECKING:
    # Avoid circular import
    from ..conversable_agent import ConversableAgent

__all__ = ["AvailableCondition", "ExpressionAvailableCondition", "StringAvailableCondition"]


class AvailableCondition(BaseModel):
    """Protocol for determining if a condition is available to be evaluated."""

    def is_available(self, agent: "ConversableAgent", messages: list[dict[str, Any]]) -> bool:
        """Determine if the condition should be considered for evaluation.

        Args:
            agent: The agent evaluating the condition
            messages: The conversation history

        Returns:
            True if the condition should be evaluated, False otherwise
        """
        raise NotImplementedError("Requires subclasses to implement.")


class StringAvailableCondition(AvailableCondition):
    """String-based available condition.

    This condition checks if a named context variable exists and is truthy.
    """

    context_variable: str

    def __init__(self, context_variable: str, **data: Any) -> None:
        """Initialize with a context variable name as a positional parameter.

        Args:
            context_variable: The name of the context variable to check
            data: Additional data for the parent class
        """
        super().__init__(context_variable=context_variable, **data)

    def is_available(self, agent: "ConversableAgent", messages: list[dict[str, Any]]) -> bool:
        """Check if the named context variable is truthy.

        Args:
            agent: The agent with context variables
            messages: The conversation history (not used)

        Returns:
            True if the variable exists and is truthy, False otherwise
        """
        return bool(agent.context_variables.get(self.context_variable, False))


class ExpressionAvailableCondition(AvailableCondition):
    """Expression-based available condition.

    This condition evaluates a ContextExpression against the context variables.
    """

    expression: ContextExpression

    def __init__(self, expression: ContextExpression, **data: Any) -> None:
        """Initialize with an expression as a positional parameter.

        Args:
            expression: The context expression to evaluate
            data: Additional data for the parent class
        """
        super().__init__(expression=expression, **data)

    def is_available(self, agent: "ConversableAgent", messages: list[dict[str, Any]]) -> bool:
        """Evaluate the expression against the context variables.

        Args:
            agent: The agent with context variables
            messages: The conversation history (not used)

        Returns:
            Boolean result of the expression evaluation
        """
        return self.expression.evaluate(agent.context_variables)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

import re
from typing import Any


class SafeguardValidator:
    """Validator for safeguard policy format and content."""

    def __init__(self, policy: dict[str, Any]):
        """Initialize validator with policy.

        Args:
            policy: The safeguard policy to validate
        """
        self.policy = policy

    def validate_policy_structure(self) -> None:
        """Validate policy format and syntax only."""
        if not isinstance(self.policy, dict):
            raise ValueError("Policy must be a dictionary")

        # Validate inter-agent safeguards
        if "inter_agent_safeguards" in self.policy:
            self._validate_inter_agent_safeguards()

        # Validate environment safeguards
        if "agent_environment_safeguards" in self.policy:
            self._validate_environment_safeguards()

    def validate_policy_complete(self, agent_names: list[str], agent_tool_mapping: dict[str, list[str]]) -> None:
        """Validate agent and tool names (assumes policy structure already validated).

        Args:
            agent_names: List of available agent names for validation
            agent_tool_mapping: Mapping of agent names to their tool names
        """
        # Validate agent names
        self.validate_agent_names(agent_names)

        # Validate tool names if any tools exist
        if any(tools for tools in agent_tool_mapping.values()):
            self.validate_tool_names(agent_tool_mapping, agent_names)

    def _validate_inter_agent_safeguards(self) -> None:
        """Validate inter-agent safeguards section."""
        inter_agent = self.policy["inter_agent_safeguards"]
        if not isinstance(inter_agent, dict):
            raise ValueError("inter_agent_safeguards must be a dictionary")

        # Validate agent_transitions
        if "agent_transitions" in inter_agent:
            if not isinstance(inter_agent["agent_transitions"], list):
                raise ValueError("agent_transitions must be a list")

            for i, rule in enumerate(inter_agent["agent_transitions"]):
                if not isinstance(rule, dict):
                    raise ValueError(f"agent_transitions[{i}] must be a dictionary")

                # Required fields
                required_fields = ["message_source", "message_destination"]
                for field in required_fields:
                    if field not in rule:
                        raise ValueError(f"agent_transitions[{i}] missing required field: {field}")

                # Check method validation - no default, must be explicit
                if "check_method" not in rule:
                    raise ValueError(f"agent_transitions[{i}] missing required field: check_method")
                check_method = rule["check_method"]
                if check_method not in ["llm", "regex"]:
                    raise ValueError(
                        f"agent_transitions[{i}] invalid check_method: {check_method}. Must be 'llm' or 'regex'"
                    )

                # LLM-specific validation
                if check_method == "llm":
                    if "custom_prompt" not in rule and "disallow_item" not in rule:
                        raise ValueError(
                            f"agent_transitions[{i}] with check_method 'llm' must have either 'custom_prompt' or 'disallow_item'"
                        )
                    if "disallow_item" in rule and not isinstance(rule["disallow_item"], list):
                        raise ValueError(f"agent_transitions[{i}] disallow_item must be a list")

                # Regex-specific validation
                if check_method == "regex":
                    if "pattern" not in rule:
                        raise ValueError(f"agent_transitions[{i}] with check_method 'regex' must have 'pattern'")
                    if not isinstance(rule["pattern"], str):
                        raise ValueError(f"agent_transitions[{i}] pattern must be a string")

                    # Test regex pattern validity
                    try:
                        re.compile(rule["pattern"])
                    except re.error as e:
                        raise ValueError(f"agent_transitions[{i}] invalid regex pattern '{rule['pattern']}': {e}")

                # Validate action - no default, must be explicit
                if "violation_response" not in rule and "action" not in rule:
                    raise ValueError(f"agent_transitions[{i}] missing required field: violation_response or action")
                action = rule.get("violation_response", rule.get("action"))
                if action not in ["block", "mask", "warning"]:
                    raise ValueError(
                        f"agent_transitions[{i}] invalid action: {action}. Must be 'block', 'mask', or 'warning'"
                    )

        # Validate groupchat_message_check
        if "groupchat_message_check" in inter_agent:
            rule = inter_agent["groupchat_message_check"]
            if not isinstance(rule, dict):
                raise ValueError("groupchat_message_check must be a dictionary")
            if "disallow_item" in rule and not isinstance(rule["disallow_item"], list):
                raise ValueError("groupchat_message_check disallow_item must be a list")

    def _validate_environment_safeguards(self) -> None:
        """Validate environment safeguards section."""
        env_rules = self.policy["agent_environment_safeguards"]
        if not isinstance(env_rules, dict):
            raise ValueError("agent_environment_safeguards must be a dictionary")

        # Validate tool_interaction rules
        if "tool_interaction" in env_rules:
            if not isinstance(env_rules["tool_interaction"], list):
                raise ValueError("tool_interaction must be a list")

            for i, rule in enumerate(env_rules["tool_interaction"]):
                if not isinstance(rule, dict):
                    raise ValueError(f"tool_interaction[{i}] must be a dictionary")

                # Check method validation - no default, must be explicit
                if "check_method" not in rule:
                    raise ValueError(f"tool_interaction[{i}] missing required field: check_method")
                check_method = rule["check_method"]
                if check_method not in ["llm", "regex"]:
                    raise ValueError(
                        f"tool_interaction[{i}] invalid check_method: {check_method}. Must be 'llm' or 'regex'"
                    )

                # Validate action - no default, must be explicit
                if "violation_response" not in rule and "action" not in rule:
                    raise ValueError(f"tool_interaction[{i}] missing required field: violation_response or action")
                action = rule.get("violation_response", rule.get("action"))
                if action not in ["block", "mask", "warning"]:
                    raise ValueError(
                        f"tool_interaction[{i}] invalid action: {action}. Must be 'block', 'mask', or 'warning'"
                    )

                # All tool_interaction rules must have message_source and message_destination
                if "message_source" not in rule or "message_destination" not in rule:
                    raise ValueError(f"tool_interaction[{i}] must have 'message_source' and 'message_destination'")

                if check_method == "llm":
                    # LLM-based checking requires either custom_prompt or disallow_item
                    if "custom_prompt" not in rule and "disallow_item" not in rule:
                        raise ValueError(
                            f"tool_interaction[{i}] with check_method 'llm' must have either 'custom_prompt' or 'disallow_item'"
                        )
                    if "disallow_item" in rule and not isinstance(rule["disallow_item"], list):
                        raise ValueError(f"tool_interaction[{i}] disallow_item must be a list")

                elif check_method == "regex":
                    # Regex-based checking requires pattern
                    if "pattern" not in rule:
                        raise ValueError(f"tool_interaction[{i}] with check_method 'regex' must have 'pattern'")
                    if not isinstance(rule["pattern"], str):
                        raise ValueError(f"tool_interaction[{i}] pattern must be a string")
                    # Test regex pattern validity
                    try:
                        re.compile(rule["pattern"])
                    except re.error as e:
                        raise ValueError(f"tool_interaction[{i}] invalid regex pattern '{rule['pattern']}': {e}")

        # Validate llm_interaction rules
        if "llm_interaction" in env_rules:
            if not isinstance(env_rules["llm_interaction"], list):
                raise ValueError("llm_interaction must be a list")

            for i, rule in enumerate(env_rules["llm_interaction"]):
                if not isinstance(rule, dict):
                    raise ValueError(f"llm_interaction[{i}] must be a dictionary")

                # Check method validation - no default, must be explicit
                if "check_method" not in rule:
                    raise ValueError(f"llm_interaction[{i}] missing required field: check_method")
                check_method = rule["check_method"]
                if check_method not in ["llm", "regex"]:
                    raise ValueError(
                        f"llm_interaction[{i}] invalid check_method: {check_method}. Must be 'llm' or 'regex'"
                    )

                # Validate action - no default, must be explicit
                if "action" not in rule:
                    raise ValueError(f"llm_interaction[{i}] missing required field: action")
                action = rule["action"]
                if action not in ["block", "mask", "warning"]:
                    raise ValueError(
                        f"llm_interaction[{i}] invalid action: {action}. Must be 'block', 'mask', or 'warning'"
                    )

                # All llm_interaction rules must have message_source and message_destination
                if "message_source" not in rule or "message_destination" not in rule:
                    raise ValueError(f"llm_interaction[{i}] must have 'message_source' and 'message_destination'")

                if check_method == "llm":
                    # LLM-based checking requires either custom_prompt or disallow_item
                    if "custom_prompt" not in rule and "disallow_item" not in rule:
                        raise ValueError(
                            f"llm_interaction[{i}] with check_method 'llm' must have either 'custom_prompt' or 'disallow_item'"
                        )
                    if "disallow_item" in rule and not isinstance(rule["disallow_item"], list):
                        raise ValueError(f"llm_interaction[{i}] disallow_item must be a list")

                elif check_method == "regex":
                    # Regex-based checking requires pattern
                    if "pattern" not in rule:
                        raise ValueError(f"llm_interaction[{i}] with check_method 'regex' must have 'pattern'")
                    if not isinstance(rule["pattern"], str):
                        raise ValueError(f"llm_interaction[{i}] pattern must be a string")
                    # Test regex pattern validity
                    try:
                        re.compile(rule["pattern"])
                    except re.error as e:
                        raise ValueError(f"llm_interaction[{i}] invalid regex pattern '{rule['pattern']}': {e}")

        # Validate user_interaction rules
        if "user_interaction" in env_rules:
            if not isinstance(env_rules["user_interaction"], list):
                raise ValueError("user_interaction must be a list")

            for i, rule in enumerate(env_rules["user_interaction"]):
                if not isinstance(rule, dict):
                    raise ValueError(f"user_interaction[{i}] must be a dictionary")

                # Check method validation - no default, must be explicit
                if "check_method" not in rule:
                    raise ValueError(f"user_interaction[{i}] missing required field: check_method")
                check_method = rule["check_method"]
                if check_method not in ["llm", "regex"]:
                    raise ValueError(
                        f"user_interaction[{i}] invalid check_method: {check_method}. Must be 'llm' or 'regex'"
                    )

                # Validate action - no default, must be explicit
                if "action" not in rule:
                    raise ValueError(f"user_interaction[{i}] missing required field: action")
                action = rule["action"]
                if action not in ["block", "mask", "warning"]:
                    raise ValueError(
                        f"user_interaction[{i}] invalid action: {action}. Must be 'block', 'mask', or 'warning'"
                    )

                # All user_interaction rules must have message_source and message_destination
                if "message_source" not in rule or "message_destination" not in rule:
                    raise ValueError(f"user_interaction[{i}] must have 'message_source' and 'message_destination'")

                if check_method == "llm":
                    # LLM-based checking requires either custom_prompt or disallow_item
                    if "custom_prompt" not in rule and "disallow_item" not in rule:
                        raise ValueError(
                            f"user_interaction[{i}] with check_method 'llm' must have either 'custom_prompt' or 'disallow_item'"
                        )
                    if "disallow_item" in rule and not isinstance(rule["disallow_item"], list):
                        raise ValueError(f"user_interaction[{i}] disallow_item must be a list")

                elif check_method == "regex":
                    # Regex-based checking requires pattern
                    if "pattern" not in rule:
                        raise ValueError(f"user_interaction[{i}] with check_method 'regex' must have 'pattern'")
                    if not isinstance(rule["pattern"], str):
                        raise ValueError(f"user_interaction[{i}] pattern must be a string")
                    # Test regex pattern validity
                    try:
                        re.compile(rule["pattern"])
                    except re.error as e:
                        raise ValueError(f"user_interaction[{i}] invalid regex pattern '{rule['pattern']}': {e}")

    def validate_agent_names(self, agent_names: list[str]) -> None:
        """Validate that agent names referenced in policy actually exist."""
        available_agents = set(agent_names)

        # Check inter-agent safeguards
        if "inter_agent_safeguards" in self.policy:
            inter_agent = self.policy["inter_agent_safeguards"]

            # Check agent_transitions
            for i, rule in enumerate(inter_agent.get("agent_transitions", [])):
                src_agent = rule.get("message_source")
                dst_agent = rule.get("message_destination")

                # Skip wildcard patterns
                if src_agent != "*" and src_agent not in available_agents:
                    raise ValueError(
                        f"agent_transitions[{i}] references unknown source agent: '{src_agent}'. Available agents: {sorted(available_agents)}"
                    )

                if dst_agent != "*" and dst_agent not in available_agents:
                    raise ValueError(
                        f"agent_transitions[{i}] references unknown destination agent: '{dst_agent}'. Available agents: {sorted(available_agents)}"
                    )

        # Check environment safeguards
        if "agent_environment_safeguards" in self.policy:
            env_rules = self.policy["agent_environment_safeguards"]

            # Check tool_interaction rules - only support message_source/message_destination format
            for i, rule in enumerate(env_rules.get("tool_interaction", [])):
                # Only validate message_source/message_destination format
                if "message_source" in rule and "message_destination" in rule:
                    # Skip detailed validation since we can't distinguish agent vs tool names
                    pass
                elif "pattern" in rule and "message_source" not in rule:
                    # Simple pattern rules are allowed
                    pass
                else:
                    raise ValueError(
                        f"tool_interaction[{i}] must use either (message_source, message_destination) or pattern-only format"
                    )

            # Check llm_interaction rules
            for i, rule in enumerate(env_rules.get("llm_interaction", [])):
                # New format
                if "message_source" in rule and "message_destination" in rule:
                    src = rule["message_source"]
                    dst = rule["message_destination"]

                    # Check agent references (LLM interactions have agent <-> llm)
                    if src != "llm" and src.lower() != "llm" and src not in available_agents:
                        raise ValueError(
                            f"llm_interaction[{i}] references unknown agent: '{src}'. Available agents: {sorted(available_agents)}"
                        )
                    if dst != "llm" and dst.lower() != "llm" and dst not in available_agents:
                        raise ValueError(
                            f"llm_interaction[{i}] references unknown agent: '{dst}'. Available agents: {sorted(available_agents)}"
                        )

                elif "agent_name" in rule:
                    agent_name = rule["agent_name"]
                    if agent_name not in available_agents:
                        raise ValueError(
                            f"llm_interaction[{i}] references unknown agent: '{agent_name}'. Available agents: {sorted(available_agents)}"
                        )

            # Check user_interaction rules
            for i, rule in enumerate(env_rules.get("user_interaction", [])):
                agent_name = rule.get("agent")
                if agent_name and agent_name not in available_agents:
                    raise ValueError(
                        f"user_interaction[{i}] references unknown agent: '{agent_name}'. Available agents: {sorted(available_agents)}"
                    )

    def validate_tool_names(self, agent_tool_mapping: dict[str, list[str]], agent_names: list[str]) -> None:
        """Validate that tool names referenced in policy actually exist and belong to the correct agents.

        Args:
            agent_tool_mapping: Dict mapping agent names to their tool names
            agent_names: List of available agent names
        """
        available_agents = set(agent_names)
        # Get all available tools across all agents
        all_available_tools = set()
        for tools in agent_tool_mapping.values():
            all_available_tools.update(tools)

        # Check environment safeguards for tool references
        if "agent_environment_safeguards" in self.policy:
            env_rules = self.policy["agent_environment_safeguards"]

            # Check tool_interaction rules
            for i, rule in enumerate(env_rules.get("tool_interaction", [])):
                # Check message_source/message_destination format
                if "message_source" in rule and "message_destination" in rule:
                    src = rule["message_source"]
                    dst = rule["message_destination"]

                    # Validate agent-tool relationships
                    self._validate_agent_tool_relationship(
                        i, "message_source", src, dst, available_agents, agent_tool_mapping, all_available_tools
                    )

    def _validate_agent_tool_relationship(
        self,
        rule_index: int,
        src_field: str,
        src: str,
        dst: str,
        available_agents: set[str],
        agent_tool_mapping: dict[str, list[str]],
        all_available_tools: set[str],
    ) -> None:
        """Validate that agent-tool relationships in policy are correct."""
        # Skip wildcards and special cases
        if src == "*" or dst == "*" or src.lower() == "user" or dst.lower() == "user":
            return

        # Case 1: Agent -> Tool (agent uses tool)
        if src in available_agents and dst not in available_agents:
            # dst should be a tool that belongs to src agent
            if dst not in all_available_tools:
                raise ValueError(
                    f"tool_interaction[{rule_index}] references unknown tool: '{dst}'. Available tools: {sorted(all_available_tools)}"
                )
            if dst not in agent_tool_mapping.get(src, []):
                agent_tools = agent_tool_mapping.get(src, [])
                raise ValueError(
                    f"tool_interaction[{rule_index}] agent '{src}' does not have access to tool '{dst}'. "
                    f"Agent's tools: {sorted(agent_tools)}"
                )

        # Case 2: Tool -> Agent (tool output to agent)
        elif src not in available_agents and dst in available_agents:
            # src should be a tool that belongs to dst agent
            if src not in all_available_tools:
                raise ValueError(
                    f"tool_interaction[{rule_index}] references unknown tool: '{src}'. Available tools: {sorted(all_available_tools)}"
                )
            if src not in agent_tool_mapping.get(dst, []):
                agent_tools = agent_tool_mapping.get(dst, [])
                raise ValueError(
                    f"tool_interaction[{rule_index}] agent '{dst}' does not have access to tool '{src}'. "
                    f"Agent's tools: {sorted(agent_tools)}"
                )

        # Case 3: Tool -> Tool (unusual, but validate both exist)
        elif src not in available_agents and dst not in available_agents:
            if src not in all_available_tools:
                raise ValueError(
                    f"tool_interaction[{rule_index}] references unknown tool: '{src}'. Available tools: {sorted(all_available_tools)}"
                )
            if dst not in all_available_tools:
                raise ValueError(
                    f"tool_interaction[{rule_index}] references unknown tool: '{dst}'. Available tools: {sorted(all_available_tools)}"
                )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

from collections.abc import Callable
from typing import Any
from uuid import UUID

from termcolor import colored

from ....events.base_event import BaseEvent, wrap_event


@wrap_event
class SafeguardEvent(BaseEvent):
    """Event for safeguard actions"""

    event_type: str  # e.g., "load", "check", "violation", "action"
    message: str
    source_agent: str | None = None
    target_agent: str | None = None
    guardrail_type: str | None = None
    action: str | None = None
    content_preview: str | None = None

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        event_type: str,
        message: str,
        source_agent: str | None = None,
        target_agent: str | None = None,
        guardrail_type: str | None = None,
        action: str | None = None,
        content_preview: str | None = None,
    ):
        super().__init__(
            uuid=uuid,
            event_type=event_type,
            message=message,
            source_agent=source_agent,
            target_agent=target_agent,
            guardrail_type=guardrail_type,
            action=action,
            content_preview=content_preview,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        # Choose color based on event type
        color = "green"
        if self.event_type == "load":
            color = "green"
        elif self.event_type == "check":
            color = "cyan"
        elif self.event_type == "violation":
            color = "red"
        elif self.event_type == "action":
            color = "yellow"

        # Choose emoji based on event type
        emoji = ""
        if self.event_type == "load":
            emoji = ""
        elif self.event_type == "check":
            emoji = ""
        elif self.event_type == "violation":
            emoji = ""
        elif self.event_type == "action":
            if self.action == "block":
                emoji = ""
            elif self.action == "mask":
                emoji = ""
            elif self.action == "warning":
                emoji = ""
            else:
                emoji = ""

        # Create header based on event type (skip for load events)
        if self.event_type == "check":
            header = f"***** Safeguard Check: {self.message} *****"
            f(colored(header, color), flush=True)
        elif self.event_type == "violation":
            header = "***** Safeguard Violation: DETECTED *****"
            f(colored(header, color), flush=True)
        elif self.event_type == "action":
            header = f"***** Safeguard Enforcement Action: {self.action.upper() if self.action else 'APPLIED'} *****"
            f(colored(header, color), flush=True)

        # Format the output
        output_parts = [f"{emoji} {self.message}" if emoji else self.message]

        if self.source_agent and self.target_agent:
            output_parts.append(f"   From: {self.source_agent}")
            output_parts.append(f"   To: {self.target_agent}")
        elif self.source_agent:
            output_parts.append(f"   Agent: {self.source_agent}")

        if self.guardrail_type:
            output_parts.append(f"   Guardrail: {self.guardrail_type}")

        if self.action:
            output_parts.append(f"   Action: {self.action}")

        if self.content_preview:
            # Replace actual newlines with \n for display
            content_display = self.content_preview.replace("\n", "\\n").replace("\r", "\\r")
            output_parts.append(f"   Content: {content_display}")

        f(colored("\n".join(output_parts), color), flush=True)

        # Print footer with matching length (skip for load events)
        if self.event_type in ["check", "violation", "action"]:
            footer = "*" * len(header)
            f(colored(footer, color), flush=True)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

import json
import re
from collections.abc import Callable
from typing import Any

from ....io.base import IOStream
from ....llm_config import LLMConfig
from ..guardrails import LLMGuardrail, RegexGuardrail
from ..targets.transition_target import TransitionTarget
from .events import SafeguardEvent


class SafeguardEnforcer:
    """Main safeguard enforcer - executes safeguard policies"""

    def __init__(
        self,
        policy: dict[str, Any] | str,
        safeguard_llm_config: LLMConfig | dict[str, Any] | None = None,
        mask_llm_config: LLMConfig | dict[str, Any] | None = None,
    ):
        """Initialize the safeguard enforcer.

        Args:
            policy: Safeguard policy dict or path to JSON file
            safeguard_llm_config: LLM configuration for safeguard checks
            mask_llm_config: LLM configuration for masking
        """
        self.policy = self._load_policy(policy)
        self.safeguard_llm_config = safeguard_llm_config
        self.mask_llm_config = mask_llm_config

        # Validate policy format before proceeding
        self._validate_policy()

        # Create mask agent for content masking
        if self.mask_llm_config:
            from ...conversable_agent import ConversableAgent

            self.mask_agent = ConversableAgent(
                name="mask_agent",
                system_message="You are a agent responsible for masking sensitive information.",
                llm_config=self.mask_llm_config,
                human_input_mode="NEVER",
                max_consecutive_auto_reply=1,
            )

        # Parse safeguard rules
        self.inter_agent_rules = self._parse_inter_agent_rules()
        self.environment_rules = self._parse_environment_rules()

        # Send load event
        self._send_safeguard_event(
            event_type="load",
            message=f"Loaded {len(self.inter_agent_rules)} inter-agent and {len(self.environment_rules)} environment safeguard rules",
        )

    def _send_safeguard_event(
        self,
        event_type: str,
        message: str,
        source_agent: str | None = None,
        target_agent: str | None = None,
        guardrail_type: str | None = None,
        action: str | None = None,
        content_preview: str | None = None,
    ) -> None:
        """Send a safeguard event to the IOStream."""
        iostream = IOStream.get_default()
        event = SafeguardEvent(
            event_type=event_type,
            message=message,
            source_agent=source_agent,
            target_agent=target_agent,
            guardrail_type=guardrail_type,
            action=action,
            content_preview=content_preview,
        )
        iostream.send(event)

    def _load_policy(self, policy: dict[str, Any] | str) -> dict[str, Any]:
        """Load policy from file or use provided dict."""
        if isinstance(policy, str):
            with open(policy) as f:
                result: dict[str, Any] = json.load(f)
                return result
        return policy

    def _validate_policy(self) -> None:
        """Validate policy format and syntax."""
        from .validator import SafeguardValidator

        validator = SafeguardValidator(self.policy)
        validator.validate_policy_structure()

    def _parse_inter_agent_rules(self) -> list[dict[str, Any]]:
        """Parse inter-agent safeguard rules from policy."""
        rules = []
        inter_agent = self.policy.get("inter_agent_safeguards", {})

        # Agent transitions
        for rule in inter_agent.get("agent_transitions", []):
            # Create guardrail based on check_method
            check_method = rule.get("check_method", "regex")
            guardrail: LLMGuardrail | RegexGuardrail | None = None
            action = rule.get("violation_response", rule.get("action", "block"))  # Support both field names

            if check_method == "llm":
                if not self.safeguard_llm_config:
                    raise ValueError(
                        f"safeguard_llm_config is required for LLM-based guardrail: {rule['message_source']} -> {rule['message_destination']}"
                    )

                # Handle different LLM check types
                if "custom_prompt" in rule:
                    # Custom prompt for LLM guardrail
                    condition = rule["custom_prompt"]

                elif "disallow_item" in rule:
                    # Disallow items for LLM guardrail
                    condition = f"Check if this content contains any of these disallowed categories: {', '.join(rule['disallow_item'])}"

                else:
                    raise ValueError(
                        f"Either custom_prompt or disallow_item must be provided for LLM guardrail: {rule['message_source']} -> {rule['message_destination']}"
                    )

                # Create LLM guardrail - handle dict config by converting to LLMConfig
                llm_config = self.safeguard_llm_config
                if isinstance(llm_config, dict):
                    llm_config = LLMConfig(config_list=[llm_config])

                guardrail = LLMGuardrail(
                    name=f"llm_guard_{rule['message_source']}_{rule['message_destination']}",
                    condition=condition,
                    target=TransitionTarget(),
                    llm_config=llm_config,
                    activation_message=rule.get("activation_message", "LLM detected violation"),
                )

            elif check_method == "regex":
                if "pattern" in rule:
                    # Regex pattern guardrail
                    guardrail = RegexGuardrail(
                        name=f"regex_guard_{rule['message_source']}_{rule['message_destination']}",
                        condition=rule["pattern"],
                        target=TransitionTarget(),
                        activation_message=rule.get("activation_message", "Regex pattern matched"),
                    )

            # Add rule with guardrail
            parsed_rule = {
                "type": "agent_transition",
                "source": rule["message_source"],
                "target": rule["message_destination"],
                "action": action,
                "guardrail": guardrail,
                "activation_message": rule.get("activation_message", "Content blocked by safeguard"),
            }

            # Keep legacy fields for backward compatibility
            if "disallow_item" in rule:
                parsed_rule["disallow"] = rule["disallow_item"]
            if "pattern" in rule:
                parsed_rule["pattern"] = rule["pattern"]
            if "custom_prompt" in rule:
                parsed_rule["custom_prompt"] = rule["custom_prompt"]

            rules.append(parsed_rule)

        # Groupchat message check
        if "groupchat_message_check" in inter_agent:
            rule = inter_agent["groupchat_message_check"]
            rules.append({
                "type": "groupchat_message",
                "source": "*",
                "target": "*",
                "action": rule.get("pet_action", "block"),
                "disallow": rule.get("disallow_item", []),
            })

        return rules

    def _parse_environment_rules(self) -> list[dict[str, Any]]:
        """Parse agent-environment safeguard rules from policy."""
        rules = []
        env_rules = self.policy.get("agent_environment_safeguards", {})

        # Tool interaction rules
        for rule in env_rules.get("tool_interaction", []):
            check_method = rule.get("check_method", "regex")  # default to regex for backward compatibility
            action = rule.get("violation_response", rule.get("action", "block"))

            if check_method == "llm":
                # LLM-based tool interaction rule - requires message_source/message_destination
                if "message_source" not in rule or "message_destination" not in rule:
                    raise ValueError(
                        "tool_interaction with check_method 'llm' must have 'message_source' and 'message_destination'"
                    )

                parsed_rule = {
                    "type": "tool_interaction",
                    "message_source": rule["message_source"],
                    "message_destination": rule["message_destination"],
                    "check_method": "llm",
                    "action": action,
                    "activation_message": rule.get("activation_message", "LLM blocked tool output"),
                }

                # Add LLM-specific parameters
                if "custom_prompt" in rule:
                    parsed_rule["custom_prompt"] = rule["custom_prompt"]
                elif "disallow_item" in rule:
                    parsed_rule["disallow"] = rule["disallow_item"]

                rules.append(parsed_rule)

            elif check_method == "regex":
                # Regex pattern-based rule - now requires message_source/message_destination
                if "message_source" not in rule or "message_destination" not in rule:
                    raise ValueError(
                        "tool_interaction with check_method 'regex' must have 'message_source' and 'message_destination'"
                    )
                if "pattern" not in rule:
                    raise ValueError("tool_interaction with check_method 'regex' must have 'pattern'")

                rules.append({
                    "type": "tool_interaction",
                    "message_source": rule["message_source"],
                    "message_destination": rule["message_destination"],
                    "check_method": "regex",
                    "pattern": rule["pattern"],
                    "action": action,
                    "activation_message": rule.get("activation_message", "Content blocked by safeguard"),
                })
            else:
                raise ValueError(
                    "tool_interaction rule must have check_method 'llm' or 'regex' with appropriate parameters"
                )

        # LLM interaction rules
        for rule in env_rules.get("llm_interaction", []):
            check_method = rule.get("check_method", "regex")  # default to regex for backward compatibility
            action = rule.get("action", "block")

            # All llm_interaction rules now require message_source/message_destination
            if "message_source" not in rule or "message_destination" not in rule:
                raise ValueError("llm_interaction rule must have 'message_source' and 'message_destination'")

            if check_method == "llm":
                # LLM-based LLM interaction rule
                parsed_rule = {
                    "type": "llm_interaction",
                    "message_source": rule["message_source"],
                    "message_destination": rule["message_destination"],
                    "check_method": "llm",
                    "action": action,
                    "activation_message": rule.get("activation_message", "LLM blocked content"),
                }

                # Add LLM-specific parameters
                if "custom_prompt" in rule:
                    parsed_rule["custom_prompt"] = rule["custom_prompt"]
                elif "disallow_item" in rule:
                    parsed_rule["disallow_item"] = rule["disallow_item"]
                else:
                    raise ValueError(
                        "llm_interaction with check_method 'llm' must have either 'custom_prompt' or 'disallow_item'"
                    )

                rules.append(parsed_rule)

            elif check_method == "regex":
                # Regex-based LLM interaction rule
                if "pattern" not in rule:
                    raise ValueError("llm_interaction with check_method 'regex' must have 'pattern'")

                rules.append({
                    "type": "llm_interaction",
                    "message_source": rule["message_source"],
                    "message_destination": rule["message_destination"],
                    "check_method": "regex",
                    "pattern": rule["pattern"],
                    "action": action,
                    "activation_message": rule.get("activation_message", "Content blocked by safeguard"),
                })
            else:
                raise ValueError(
                    "llm_interaction rule must have check_method 'llm' or 'regex' with appropriate parameters"
                )

        # User interaction rules
        for rule in env_rules.get("user_interaction", []):
            check_method = rule.get("check_method", "llm")  # default to llm for backward compatibility
            action = rule.get("action", "block")

            # All user_interaction rules now require message_source/message_destination
            if "message_source" not in rule or "message_destination" not in rule:
                raise ValueError("user_interaction rule must have 'message_source' and 'message_destination'")

            if check_method == "llm":
                # LLM-based user interaction rule
                parsed_rule = {
                    "type": "user_interaction",
                    "message_source": rule["message_source"],
                    "message_destination": rule["message_destination"],
                    "check_method": "llm",
                    "action": action,
                }

                # Add LLM-specific parameters
                if "custom_prompt" in rule:
                    parsed_rule["custom_prompt"] = rule["custom_prompt"]
                elif "disallow_item" in rule:
                    parsed_rule["disallow_item"] = rule["disallow_item"]
                else:
                    raise ValueError(
                        "user_interaction with check_method 'llm' must have either 'custom_prompt' or 'disallow_item'"
                    )

                rules.append(parsed_rule)

            elif check_method == "regex":
                # Regex-based user interaction rule
                if "pattern" not in rule:
                    raise ValueError("user_interaction with check_method 'regex' must have 'pattern'")

                rules.append({
                    "type": "user_interaction",
                    "message_source": rule["message_source"],
                    "message_destination": rule["message_destination"],
                    "check_method": "regex",
                    "pattern": rule["pattern"],
                    "action": action,
                })
            else:
                raise ValueError(
                    "user_interaction rule must have check_method 'llm' or 'regex' with appropriate parameters"
                )

        return rules

    def create_agent_hooks(self, agent_name: str) -> dict[str, Callable[..., Any]]:
        """Create hook functions for a specific agent, only for rule types that exist."""
        hooks = {}

        # Check if we have any tool interaction rules that apply to this agent
        agent_tool_rules = [
            rule
            for rule in self.environment_rules
            if rule["type"] == "tool_interaction"
            and (
                rule.get("message_destination") == agent_name
                or rule.get("message_source") == agent_name
                or rule.get("agent_name") == agent_name
                or "message_destination" not in rule
            )
        ]  # Simple pattern rules apply to all

        if agent_tool_rules:

            def tool_input_hook(tool_input: dict[str, Any]) -> dict[str, Any] | None:
                result = self._check_tool_interaction(agent_name, tool_input, "input")
                return result if result is not None else tool_input

            def tool_output_hook(tool_input: dict[str, Any]) -> dict[str, Any] | None:
                result = self._check_tool_interaction(agent_name, tool_input, "output")
                return result if result is not None else tool_input

            hooks["safeguard_tool_inputs"] = tool_input_hook
            hooks["safeguard_tool_outputs"] = tool_output_hook

        # Check if we have any LLM interaction rules that apply to this agent
        agent_llm_rules = [
            rule
            for rule in self.environment_rules
            if rule["type"] == "llm_interaction"
            and (
                rule.get("message_destination") == agent_name
                or rule.get("message_source") == agent_name
                or rule.get("agent_name") == agent_name
                or "message_destination" not in rule
            )
        ]  # Simple pattern rules apply to all

        if agent_llm_rules:

            def llm_input_hook(tool_input: dict[str, Any]) -> dict[str, Any] | None:
                # Extract messages from the data structure if needed
                messages = tool_input if isinstance(tool_input, list) else tool_input.get("messages", tool_input)
                result = self._check_llm_interaction(agent_name, messages, "input")
                if isinstance(result, list) and isinstance(tool_input, dict) and "messages" in tool_input:
                    return {**tool_input, "messages": result}
                elif isinstance(result, dict):
                    return result
                elif result is not None and not isinstance(result, dict):
                    # Convert string or other types to dict format
                    return {"content": str(result), "role": "function"}
                elif result is not None and isinstance(result, dict) and result != tool_input:
                    # Return the modified dict result
                    return result
                return tool_input

            def llm_output_hook(tool_input: dict[str, Any]) -> dict[str, Any] | None:
                result = self._check_llm_interaction(agent_name, tool_input, "output")
                if isinstance(result, dict):
                    return result
                elif result is not None and not isinstance(result, dict):
                    # Convert string or other types to dict format
                    return {"content": str(result), "role": "function"}
                elif result is not None and isinstance(result, dict) and result != tool_input:
                    # Return the modified dict result
                    return result
                return tool_input

            hooks["safeguard_llm_inputs"] = llm_input_hook
            hooks["safeguard_llm_outputs"] = llm_output_hook

        # Check if we have any user interaction rules that apply to this agent
        agent_user_rules = [
            rule
            for rule in self.environment_rules
            if rule["type"] == "user_interaction" and rule.get("message_destination") == agent_name
        ]

        if agent_user_rules:

            def human_input_hook(tool_input: dict[str, Any]) -> dict[str, Any] | None:
                # Extract human input from data structure
                human_input = tool_input.get("content", str(tool_input))
                result = self._check_user_interaction(agent_name, human_input)
                if result != human_input and isinstance(tool_input, dict):
                    return {**tool_input, "content": result}
                return tool_input if result == human_input else {"content": result}

            hooks["safeguard_human_inputs"] = human_input_hook

        # Check if we have any inter-agent rules that apply to this agent
        # Note: For group chats, inter-agent communication is handled by GroupChat._run_inter_agent_guardrails()
        # But for direct agent-to-agent communication, we need the process_message_before_send hook
        agent_inter_rules = [
            rule
            for rule in self.inter_agent_rules
            if (
                rule.get("source") == agent_name
                or rule.get("target") == agent_name
                or rule.get("source") == "*"
                or rule.get("target") == "*"
            )
        ]

        if agent_inter_rules:

            def message_before_send_hook(
                sender: Any, message: dict[str, Any] | str, recipient: Any, silent: Any = None
            ) -> dict[str, Any] | str:
                _ = silent  # Unused parameter
                result = self._check_inter_agent_communication(sender.name, recipient.name, message)
                return result

            hooks["process_message_before_send"] = message_before_send_hook  # type: ignore[assignment]

        return hooks

    def _check_llm_violation(
        self, content: str, disallow_items: list[str] | None = None, custom_prompt: str | None = None
    ) -> tuple[bool, str]:
        """Check if content violates LLM-based safeguard conditions."""
        if not self.safeguard_llm_config:
            raise ValueError("safeguard_llm_config is required for LLM-based safeguard checks")

        # Determine condition based on available parameters
        if custom_prompt:
            condition = custom_prompt
        elif disallow_items:
            condition = (
                f"Check if this content contains any of these disallowed categories: {', '.join(disallow_items)}"
            )
        else:
            raise ValueError("Either custom_prompt or disallow_items must be provided")

        # Create LLM guardrail for checking
        # Handle dict config by converting to LLMConfig
        llm_config = self.safeguard_llm_config
        if isinstance(llm_config, dict):
            llm_config = LLMConfig(config_list=[llm_config])

        from ..targets.transition_target import TransitionTarget

        guardrail = LLMGuardrail(
            name="temp_safeguard_check",
            condition=condition,
            target=TransitionTarget(),
            llm_config=llm_config,
            activation_message="Content violates safeguard conditions",
        )

        try:
            result = guardrail.check(content)
            return result.activated, result.justification
        except Exception as e:
            raise RuntimeError(f"Safeguard check failed: {e}")

    def _check_regex_violation(self, content: str, pattern: str) -> tuple[bool, str]:
        """Check if content matches a regex pattern."""
        try:
            if re.search(pattern, content, re.IGNORECASE):
                return True, f"Content matched pattern: {pattern}"
        except re.error as e:
            raise ValueError(f"Invalid regex pattern '{pattern}': {e}")

        return False, "No pattern match"

    def _apply_action(
        self,
        action: str,
        content: str | dict[str, Any] | list[Any],
        disallow_items: list[str],
        explanation: str,
        custom_message: str | None = None,
        pattern: str | None = None,
        guardrail_type: str | None = None,
        source_agent: str | None = None,
        target_agent: str | None = None,
        content_preview: str | None = None,
    ) -> str | dict[str, Any] | list[Any]:
        """Apply the specified action to content."""
        message = custom_message or explanation

        if action == "block":
            self._send_safeguard_event(
                event_type="action",
                message=f"BLOCKED: {message}",
                action="block",
                source_agent=source_agent,
                target_agent=target_agent,
                content_preview=content_preview,
            )
            return self._handle_blocked_content(content, message)
        elif action == "mask":
            self._send_safeguard_event(
                event_type="action",
                message=f"MASKED: {message}",
                action="mask",
                source_agent=source_agent,
                target_agent=target_agent,
                content_preview=content_preview,
            )

            def mask_func(text: str) -> str:
                return self._mask_content(text, disallow_items, explanation, pattern)

            return self._handle_masked_content(content, mask_func)
        elif action == "warning":
            self._send_safeguard_event(
                event_type="action",
                message=f"WARNING: {message}",
                action="warning",
                source_agent=source_agent,
                target_agent=target_agent,
                content_preview=content_preview,
            )
            return content
        else:
            return content

    def _mask_content(
        self, content: str, disallow_items: list[str], explanation: str, pattern: str | None = None
    ) -> str:
        """Mask sensitive content using LLM, pattern-based, or simple replacement."""
        # If we have a specific pattern from a regex guardrail, use it first
        if pattern:
            try:
                masked = re.sub(pattern, "[SENSITIVE_INFO]", content, flags=re.IGNORECASE)
                if masked != content:  # Only return if something was actually masked
                    return masked
            except re.error as e:
                raise ValueError(f"Pattern masking failed: {e}")

        # Try LLM-based masking if available
        if self.mask_agent and disallow_items:
            mask_prompt = f"""
            Mask the sensitive information in this content with [SENSITIVE_INFO]:

            Content: {content}
            Sensitive categories: {", ".join(disallow_items)}
            Reason: {explanation}

            Return only the masked content, nothing else.
            """

            try:
                response = self.mask_agent.generate_oai_reply(messages=[{"role": "user", "content": mask_prompt}])

                if response[0] and response[1]:
                    masked = response[1].get("content", content) if isinstance(response[1], dict) else str(response[1])
                    return masked
            except Exception as e:
                raise ValueError(f"LLM masking failed: {e}")

        return masked

    def _handle_blocked_content(
        self, content: str | dict[str, Any] | list[Any], block_message: str
    ) -> str | dict[str, Any] | list[Any]:
        """Handle blocked content based on its structure."""
        block_msg = f" BLOCKED: {block_message}"

        if isinstance(content, dict):
            blocked_content = content.copy()

            # Handle tool_responses (like in tool outputs)
            if "tool_responses" in blocked_content and blocked_content["tool_responses"]:
                blocked_content["content"] = block_msg
                blocked_content["tool_responses"] = [
                    {**response, "content": block_msg} for response in blocked_content["tool_responses"]
                ]
            # Handle tool_calls (like in tool inputs)
            elif "tool_calls" in blocked_content and blocked_content["tool_calls"]:
                blocked_content["tool_calls"] = [
                    {**tool_call, "function": {**tool_call["function"], "arguments": block_msg}}
                    for tool_call in blocked_content["tool_calls"]
                ]
            # Handle regular content
            elif "content" in blocked_content:
                blocked_content["content"] = block_msg
            # Handle arguments (for some tool formats)
            elif "arguments" in blocked_content:
                blocked_content["arguments"] = block_msg
            else:
                # Default case - add content field
                blocked_content["content"] = block_msg

            return blocked_content

        elif isinstance(content, list):
            # Handle list of messages (like LLM inputs)
            blocked_list = []
            for item in content:
                if isinstance(item, dict):
                    blocked_item = item.copy()
                    if "content" in blocked_item:
                        blocked_item["content"] = block_msg
                    if "tool_calls" in blocked_item:
                        blocked_item["tool_calls"] = [
                            {**tool_call, "function": {**tool_call["function"], "arguments": block_msg}}
                            for tool_call in blocked_item["tool_calls"]
                        ]
                    if "tool_responses" in blocked_item:
                        blocked_item["tool_responses"] = [
                            {**response, "content": block_msg} for response in blocked_item["tool_responses"]
                        ]
                    blocked_list.append(blocked_item)
                else:
                    blocked_list.append({"content": block_msg, "role": "function"})
            return blocked_list

        else:
            # String or other content - return as function message
            return {"content": block_msg, "role": "function"}

    def _handle_masked_content(
        self, content: str | dict[str, Any] | list[Any], mask_func: Callable[[str], str]
    ) -> str | dict[str, Any] | list[Any]:
        """Handle masked content based on its structure."""
        if isinstance(content, dict):
            masked_content = content.copy()

            # Handle tool_responses
            if "tool_responses" in masked_content and masked_content["tool_responses"]:
                if "content" in masked_content:
                    masked_content["content"] = mask_func(str(masked_content["content"]))
                masked_content["tool_responses"] = [
                    {**response, "content": mask_func(str(response.get("content", "")))}
                    for response in masked_content["tool_responses"]
                ]
            # Handle tool_calls
            elif "tool_calls" in masked_content and masked_content["tool_calls"]:
                masked_content["tool_calls"] = [
                    {
                        **tool_call,
                        "function": {
                            **tool_call["function"],
                            "arguments": mask_func(str(tool_call["function"].get("arguments", ""))),
                        },
                    }
                    for tool_call in masked_content["tool_calls"]
                ]
            # Handle regular content
            elif "content" in masked_content:
                masked_content["content"] = mask_func(str(masked_content["content"]))
            # Handle arguments
            elif "arguments" in masked_content:
                masked_content["arguments"] = mask_func(str(masked_content["arguments"]))

            return masked_content

        elif isinstance(content, list):
            # Handle list of messages
            masked_list = []
            for item in content:
                if isinstance(item, dict):
                    masked_item = item.copy()
                    if "content" in masked_item:
                        masked_item["content"] = mask_func(str(masked_item["content"]))
                    if "tool_calls" in masked_item:
                        masked_item["tool_calls"] = [
                            {
                                **tool_call,
                                "function": {
                                    **tool_call["function"],
                                    "arguments": mask_func(str(tool_call["function"].get("arguments", ""))),
                                },
                            }
                            for tool_call in masked_item["tool_calls"]
                        ]
                    if "tool_responses" in masked_item:
                        masked_item["tool_responses"] = [
                            {**response, "content": mask_func(str(response.get("content", "")))}
                            for response in masked_item["tool_responses"]
                        ]
                    masked_list.append(masked_item)
                else:
                    # For non-dict items, wrap the masked content in a dict
                    masked_item_content: str = mask_func(str(item))
                    masked_list.append({"content": masked_item_content, "role": "function"})
            return masked_list

        else:
            # String content
            return mask_func(str(content))

    def _check_inter_agent_communication(
        self, sender_name: str, recipient_name: str, message: str | dict[str, Any]
    ) -> str | dict[str, Any]:
        """Check inter-agent communication."""
        content = message.get("content", "") if isinstance(message, dict) else str(message)

        for rule in self.inter_agent_rules:
            if rule["type"] == "agent_transition":
                # Check if this rule applies
                source_match = rule["source"] == "*" or rule["source"] == sender_name
                target_match = rule["target"] == "*" or rule["target"] == recipient_name

                if source_match and target_match:
                    # Prepare content preview
                    content_preview = content[:100] + ("..." if len(content) > 100 else "")

                    # Use guardrail if available
                    if "guardrail" in rule and rule["guardrail"]:
                        # Send single check event with guardrail info
                        self._send_safeguard_event(
                            event_type="check",
                            message="Checking inter-agent communication",
                            source_agent=sender_name,
                            target_agent=recipient_name,
                            guardrail_type=type(rule["guardrail"]).__name__,
                            # action=rule.get('action', 'N/A'),
                            content_preview=content_preview,
                        )

                        try:
                            result = rule["guardrail"].check(content)
                            if result.activated:
                                self._send_safeguard_event(
                                    event_type="violation",
                                    message=f"VIOLATION DETECTED: {result.justification}",
                                    source_agent=sender_name,
                                    target_agent=recipient_name,
                                    guardrail_type=type(rule["guardrail"]).__name__,
                                    content_preview=content_preview,
                                )
                                # Pass the pattern if it's a regex guardrail
                                pattern = rule.get("pattern") if isinstance(rule["guardrail"], RegexGuardrail) else None
                                action_result = self._apply_action(
                                    action=rule["action"],
                                    content=message,
                                    disallow_items=[],
                                    explanation=result.justification,
                                    custom_message=rule.get("activation_message", result.justification),
                                    pattern=pattern,
                                    guardrail_type=type(rule["guardrail"]).__name__,
                                    source_agent=sender_name,
                                    target_agent=recipient_name,
                                    content_preview=content_preview,
                                )
                                if isinstance(action_result, (str, dict)):
                                    return action_result
                                else:
                                    return message
                            else:
                                # Content passed - no additional event needed, already sent check event above
                                pass
                        except Exception as e:
                            raise ValueError(f"Guardrail check failed: {e}")

                    # Handle legacy pattern-based rules
                    elif "pattern" in rule and rule["pattern"]:
                        # Send single check event for pattern-based rules
                        self._send_safeguard_event(
                            event_type="check",
                            message="Checking inter-agent communication",
                            source_agent=sender_name,
                            target_agent=recipient_name,
                            guardrail_type="RegexGuardrail",
                            # action=rule.get('action', 'N/A'),
                            content_preview=content_preview,
                        )
                        is_violation, explanation = self._check_regex_violation(content, rule["pattern"])
                        if is_violation:
                            result_value = self._apply_action(
                                action=rule["action"],
                                content=message,
                                disallow_items=[],
                                explanation=explanation,
                                custom_message=rule.get("activation_message"),
                                pattern=rule["pattern"],
                                guardrail_type="RegexGuardrail",
                                source_agent=sender_name,
                                target_agent=recipient_name,
                                content_preview=content_preview,
                            )
                            if isinstance(result_value, (str, dict)):
                                return result_value
                            else:
                                return message
                        else:
                            pass

                    # Handle legacy disallow-based rules and custom prompts
                    elif "disallow" in rule or "custom_prompt" in rule:
                        # Send single check event for LLM-based legacy rules
                        self._send_safeguard_event(
                            event_type="check",
                            message="Checking inter-agent communication",
                            source_agent=sender_name,
                            target_agent=recipient_name,
                            guardrail_type="LLMGuardrail",
                            # action=rule.get('action', 'N/A'),
                            content_preview=content_preview,
                        )
                        if "custom_prompt" in rule:
                            is_violation, explanation = self._check_llm_violation(
                                content, custom_prompt=rule["custom_prompt"]
                            )
                        else:
                            is_violation, explanation = self._check_llm_violation(
                                content, disallow_items=rule["disallow"]
                            )

                        if is_violation:
                            result_value = self._apply_action(
                                action=rule["action"],
                                content=message,
                                disallow_items=rule.get("disallow", []),
                                explanation=explanation,
                                custom_message=None,
                                pattern=None,
                                guardrail_type="LLMGuardrail",
                                source_agent=sender_name,
                                target_agent=recipient_name,
                                content_preview=content_preview,
                            )
                            if isinstance(result_value, (str, dict)):
                                return result_value
                            else:
                                return message
                        else:
                            pass

        return message

    def _check_interaction(
        self,
        interaction_type: str,
        source_name: str,
        dest_name: str,
        content: str,
        data: str | dict[str, Any] | list[dict[str, Any]],
        context_info: str,
    ) -> str | dict[str, Any] | list[dict[str, Any]] | None:
        """Unified method to check any type of interaction."""
        for rule in self.environment_rules:
            if (
                rule["type"] == interaction_type
                and "message_source" in rule
                and "message_destination" in rule
                and rule["message_source"] == source_name
                and rule["message_destination"] == dest_name
            ):
                content_preview = content[:100] + ("..." if len(content) > 100 else "")
                check_method = rule.get("check_method", "regex")
                guardrail_type = "LLMGuardrail" if check_method == "llm" else "RegexGuardrail"

                # Send check event
                self._send_safeguard_event(
                    event_type="check",
                    message=f"Checking {interaction_type.replace('_', ' ')}: {context_info}",
                    source_agent=source_name,
                    target_agent=dest_name,
                    guardrail_type=guardrail_type,
                    content_preview=content_preview,
                )

                # Perform check based on method
                is_violation, explanation = self._perform_check(rule, content, check_method)

                if is_violation:
                    # Send violation event
                    self._send_safeguard_event(
                        event_type="violation",
                        message=f"{guardrail_type.replace('Guardrail', '').upper()} VIOLATION: {explanation}",
                        source_agent=source_name,
                        target_agent=dest_name,
                        guardrail_type=guardrail_type,
                        content_preview=content_preview,
                    )

                    # Apply action
                    result = self._apply_action(
                        action=rule["action"],
                        content=data,
                        disallow_items=rule.get("disallow_item", []),
                        explanation=explanation,
                        custom_message=rule.get("activation_message"),
                        pattern=rule.get("pattern"),
                        guardrail_type=guardrail_type,
                        source_agent=source_name,
                        target_agent=dest_name,
                        content_preview=content_preview,
                    )
                    return result

        return None

    def _perform_check(self, rule: dict[str, Any], content: str, check_method: str) -> tuple[bool, str]:
        """Perform the actual check based on the method."""
        if check_method == "llm":
            if not self.safeguard_llm_config:
                raise ValueError(
                    f"safeguard_llm_config is required for LLM-based {rule['type']} rule: {rule['message_source']} -> {rule['message_destination']}"
                )

            if "custom_prompt" in rule:
                return self._check_llm_violation(content, custom_prompt=rule["custom_prompt"])
            elif "disallow_item" in rule:
                return self._check_llm_violation(content, disallow_items=rule["disallow_item"])
            else:
                raise ValueError(
                    f"Either custom_prompt or disallow_item must be provided for LLM-based {rule['type']}: {rule['message_source']} -> {rule['message_destination']}"
                )

        elif check_method == "regex":
            if "pattern" not in rule:
                raise ValueError(
                    f"pattern is required for regex-based {rule['type']}: {rule['message_source']} -> {rule['message_destination']}"
                )
            return self._check_regex_violation(content, rule["pattern"])

        else:
            raise ValueError(f"Unsupported check_method: {check_method}")

    def _check_tool_interaction(self, agent_name: str, data: dict[str, Any], direction: str) -> dict[str, Any]:
        """Check tool interactions."""
        # Extract tool name from data
        tool_name = data.get("name", data.get("tool_name", ""))

        # Determine source/destination based on direction
        if direction == "output":
            source_name, dest_name = tool_name, agent_name
            content = str(data.get("content", ""))
        else:  # input
            source_name, dest_name = agent_name, tool_name
            content = str(data.get("arguments", ""))

        result = self._check_interaction(
            interaction_type="tool_interaction",
            source_name=source_name,
            dest_name=dest_name,
            content=content,
            data=data,
            context_info=f"{agent_name} <-> {tool_name} ({direction})",
        )

        if result is not None:
            if isinstance(result, dict):
                return result
            else:
                # Convert string or list result back to dict format
                return {"content": str(result), "name": tool_name}
        return data

    def _check_llm_interaction(
        self, agent_name: str, data: str | dict[str, Any] | list[dict[str, Any]], direction: str
    ) -> str | dict[str, Any] | list[dict[str, Any]]:
        """Check LLM interactions."""
        content = str(data)

        # Determine source/destination based on direction
        if direction == "input":
            source_name, dest_name = agent_name, "llm"
        else:  # output
            source_name, dest_name = "llm", agent_name

        result = self._check_interaction(
            interaction_type="llm_interaction",
            source_name=source_name,
            dest_name=dest_name,
            content=content,
            data=data,
            context_info=f"{agent_name} <-> llm ({direction})",
        )

        return result if result is not None else data

    def _check_user_interaction(self, agent_name: str, user_input: str) -> str | None:
        """Check user interactions."""
        result = self._check_interaction(
            interaction_type="user_interaction",
            source_name="user",
            dest_name=agent_name,
            content=user_input,
            data=user_input,
            context_info=f"user <-> {agent_name}",
        )

        if result is not None and isinstance(result, str):
            return result
        return user_input

    def check_and_act(
        self, src_agent_name: str, dst_agent_name: str, message_content: str | dict[str, Any]
    ) -> str | dict[str, Any] | None:
        """Check and act on inter-agent communication for GroupChat integration.

        This method is called by GroupChat._run_inter_agent_guardrails to check
        messages between agents and potentially modify or block them.

        Args:
            src_agent_name: Name of the source agent
            dst_agent_name: Name of the destination agent
            message_content: The message content to check

        Returns:
            Optional replacement message if a safeguard triggers, None otherwise
        """
        # Store original content for comparison
        original_content = (
            message_content.get("content", "") if isinstance(message_content, dict) else str(message_content)
        )

        result = self._check_inter_agent_communication(src_agent_name, dst_agent_name, message_content)

        if result != original_content:
            # Return the complete modified message structure to preserve tool_calls/tool_responses pairing
            return result

        return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

"""Safeguards module for agent safety and compliance.

This module provides functionality for applying, managing, and enforcing
safeguards on agent interactions including inter-agent communication,
tool interactions, LLM interactions, and user interactions.
"""

from .api import apply_safeguard_policy, reset_safeguard_policy
from .enforcer import SafeguardEnforcer
from .events import SafeguardEvent

__all__ = [
    "SafeguardEnforcer",
    "SafeguardEvent",
    "apply_safeguard_policy",
    "reset_safeguard_policy",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

from typing import TYPE_CHECKING, Any

from ....llm_config import LLMConfig

# Import types that are safe to import at runtime
from ...agent import Agent
from ...conversable_agent import ConversableAgent
from ...groupchat import GroupChatManager

if TYPE_CHECKING:
    from .enforcer import SafeguardEnforcer


def reset_safeguard_policy(
    *,
    agents: list[ConversableAgent] | None = None,
    groupchat_manager: GroupChatManager | None = None,
) -> None:
    """Reset/remove all safeguards from agents and groupchat managers.

    This function removes all safeguard hooks and inter-agent guardrails that were
    previously applied by apply_safeguard_policy. Events are sent to track the reset process.

    Args:
        agents: List of agents to remove safeguards from (optional if groupchat_manager provided)
        groupchat_manager: GroupChatManager to remove safeguards from (optional if agents provided)

    Example:
        ```python
        from autogen.agentchat.group.safeguards import reset_safeguard_policy

        # Remove safeguards from agents
        reset_safeguard_policy(agents=[agent1, agent2, agent3])

        # Or remove from GroupChatManager
        reset_safeguard_policy(groupchat_manager=manager)
        ```
    """
    from ....events.print_event import PrintEvent
    from ....io.base import IOStream

    iostream = IOStream.get_default()

    # Send initial reset event
    iostream.send(PrintEvent("Resetting safeguards..."))
    # Determine which agents to remove safeguards from
    target_agents: list[ConversableAgent | Agent] = []

    if groupchat_manager:
        if not isinstance(groupchat_manager, GroupChatManager):
            raise ValueError("groupchat_manager must be an instance of GroupChatManager")

        target_agents.extend([agent for agent in groupchat_manager.groupchat.agents if hasattr(agent, "hook_lists")])

        agent_names = [agent.name for agent in target_agents]
        iostream.send(PrintEvent(f" Found {len(target_agents)} agents in GroupChat: {', '.join(agent_names)}"))

        # Clear inter-agent guardrails from the groupchat
        if hasattr(groupchat_manager.groupchat, "_inter_agent_guardrails"):
            guardrail_count = len(groupchat_manager.groupchat._inter_agent_guardrails)
            if guardrail_count > 0:
                iostream.send(PrintEvent(f" Clearing {guardrail_count} inter-agent guardrails from GroupChat"))
            groupchat_manager.groupchat._inter_agent_guardrails.clear()

    elif agents:
        target_agents.extend(agents)
        agent_names = [agent.name for agent in target_agents]
        iostream.send(PrintEvent(f" Resetting safeguards for {len(target_agents)} agents: {', '.join(agent_names)}"))
    else:
        raise ValueError("Either agents or groupchat_manager must be provided")

    # Remove safeguard hooks from each agent
    safeguard_hook_names = [
        "safeguard_tool_inputs",
        "safeguard_tool_outputs",
        "safeguard_llm_inputs",
        "safeguard_llm_outputs",
        "safeguard_human_inputs",
        "process_message_before_send",  # Inter-agent communication hooks for direct agent communication
    ]

    for agent in target_agents:
        if hasattr(agent, "hook_lists"):
            # Use the agent's reset_safeguards method for agent-to-environment safeguards
            if hasattr(agent, "reset_safeguards"):
                agent.reset_safeguards()
            else:
                # Fallback to manual clearing for older agent versions
                for hook_name in safeguard_hook_names:
                    if hook_name in agent.hook_lists:
                        # Clear all hooks in safeguard-specific hook lists
                        agent.hook_lists[hook_name].clear()

            # Manually clear inter-agent safeguards (process_message_before_send)
            if "process_message_before_send" in agent.hook_lists:
                agent.hook_lists["process_message_before_send"].clear()
        else:
            raise ValueError(
                f"Agent {agent.name} does not support hooks. Please ensure it inherits from ConversableAgent."
            )

    # Send completion event
    iostream.send(PrintEvent(f" Safeguard reset completed for {len(target_agents)} agents"))


def apply_safeguard_policy(
    *,
    agents: list[ConversableAgent] | None = None,
    groupchat_manager: GroupChatManager | None = None,
    policy: dict[str, Any] | str,
    safeguard_llm_config: LLMConfig | None = None,
    mask_llm_config: LLMConfig | None = None,
) -> SafeguardEnforcer:
    """Apply safeguards to agents using a policy file.

    This is the main function for applying safeguards. It supports the policy format
    with 'inter_agent_safeguards' and 'agent_environment_safeguards' sections.

    Args:
        agents: List of agents to apply safeguards to (optional if groupchat_manager provided)
        groupchat_manager: GroupChatManager to apply safeguards to (optional if agents provided)
        policy: Safeguard policy dict or path to JSON file
        safeguard_llm_config: LLM configuration for safeguard checks
        mask_llm_config: LLM configuration for masking

    Returns:
        SafeguardEnforcer instance for further configuration

    Example:
        ```python
        from autogen.agentchat.group.safeguards import apply_safeguard_policy

        # Apply safeguards to agents
        safeguard_enforcer = apply_safeguard_policy(
            agents=[agent1, agent2, agent3],
            policy="path/to/policy.json",
            safeguard_llm_config=safeguard_llm_config,
        )

        # Or apply to GroupChatManager
        safeguard_enforcer = apply_safeguard_policy(
            groupchat_manager=manager,
            policy="path/to/policy.json",
            safeguard_llm_config=safeguard_llm_config,
            mask_llm_config=mask_llm_config,
        )
        ```
    """
    from .enforcer import SafeguardEnforcer

    enforcer = SafeguardEnforcer(
        policy=policy,
        safeguard_llm_config=safeguard_llm_config,
        mask_llm_config=mask_llm_config,
    )

    # Determine which agents to apply safeguards to
    target_agents: list[ConversableAgent | Agent] = []
    all_agent_names = []

    if groupchat_manager:
        if not isinstance(groupchat_manager, GroupChatManager):
            raise ValueError("groupchat_manager must be an instance of GroupChatManager")

        target_agents.extend([agent for agent in groupchat_manager.groupchat.agents if hasattr(agent, "hook_lists")])
        all_agent_names = [agent.name for agent in groupchat_manager.groupchat.agents]
        all_agent_names.append(groupchat_manager.name)

        # Register inter-agent guardrails with the groupchat
        # Ensure the list exists and append our enforcer
        if not hasattr(groupchat_manager.groupchat, "_inter_agent_guardrails"):
            groupchat_manager.groupchat._inter_agent_guardrails = []
        groupchat_manager.groupchat._inter_agent_guardrails.clear()  # Clear any existing
        groupchat_manager.groupchat._inter_agent_guardrails.append(enforcer)
    elif agents:
        target_agents.extend(agents)
        all_agent_names = [agent.name for agent in agents]
    else:
        raise ValueError("Either agents or groupchat_manager must be provided")

    # Build agent-to-tool mapping for validation
    agent_tool_mapping = {}
    for agent in target_agents:
        agent_tools = []

        # Get tools from the tools property (Tool objects)
        if hasattr(agent, "tools"):
            for tool in agent.tools:
                agent_tools.append(tool.name)

        # Get tools from function_map (functions registered with @register_for_execution)
        if hasattr(agent, "function_map"):
            agent_tools.extend(agent.function_map.keys())

        agent_tool_mapping[agent.name] = agent_tools

    # Validate policy including agent names and tool names
    try:
        from .validator import SafeguardValidator

        validator = SafeguardValidator(enforcer.policy)  # Use enforcer's loaded policy dict
        validator.validate_policy_complete(agent_names=all_agent_names, agent_tool_mapping=agent_tool_mapping)
    except ValueError as e:
        raise ValueError(f"Policy validation failed: {e}")

    # Apply hooks to each agent
    for agent in target_agents:
        if hasattr(agent, "hook_lists"):
            hooks = enforcer.create_agent_hooks(agent.name)
            for hook_name, hook_func in hooks.items():
                if hook_name in agent.hook_lists:
                    agent.hook_lists[hook_name].append(hook_func)
        else:
            raise ValueError(
                f"Agent {agent.name} does not support hooks. Please ensure it inherits from ConversableAgent."
            )

    return enforcer
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
__all__: list[str] = []

from .available_condition import ExpressionAvailableCondition, StringAvailableCondition
from .context_condition import ExpressionContextCondition, StringContextCondition
from .context_expression import ContextExpression
from .context_str import ContextStr
from .context_variables import ContextVariables
from .handoffs import Handoffs
from .llm_condition import ContextStrLLMCondition, StringLLMCondition
from .on_condition import OnCondition
from .on_context_condition import OnContextCondition
from .reply_result import ReplyResult
from .speaker_selection_result import SpeakerSelectionResult
from .targets.group_chat_target import GroupChatConfig, GroupChatTarget

"""
from .targets.group_manager_target import (
    GroupManagerSelectionMessageContextStr,
    GroupManagerSelectionMessageString,
    GroupManagerTarget,
)
"""
from .targets.transition_target import (
    AgentNameTarget,
    AgentTarget,
    AskUserTarget,
    NestedChatTarget,
    RevertToUserTarget,
    StayTarget,
    TerminateTarget,
)

__all__ = [
    "AgentNameTarget",
    "AgentTarget",
    "AskUserTarget",
    "ContextExpression",
    "ContextStr",
    "ContextStrLLMCondition",
    "ContextVariables",
    "ExpressionAvailableCondition",
    "ExpressionContextCondition",
    "GroupChatConfig",
    "GroupChatTarget",
    # "GroupManagerSelectionMessageContextStr",
    # "GroupManagerSelectionMessageString",
    # "GroupManagerTarget",
    "Handoffs",
    "NestedChatTarget",
    "OnCondition",
    "OnContextCondition",
    "ReplyResult",
    "RevertToUserTarget",
    "SpeakerSelectionResult",
    "StayTarget",
    "StringAvailableCondition",
    "StringContextCondition",
    "StringLLMCondition",
    "TerminateTarget",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from pydantic import BaseModel

from .available_condition import AvailableCondition
from .context_condition import ContextCondition
from .targets.transition_target import TransitionTarget

__all__ = [
    "OnContextCondition",
]


class OnContextCondition(BaseModel):  # noqa: N801
    """Defines a condition for transitioning to another agent or nested chats using context variables and the ContextExpression class.

    This is for context variable-based condition evaluation (does not use the agent's LLM).

    These are evaluated before the OnCondition and after work conditions.

    Args:
        target (TransitionTarget): The transition (essentially an agent) to hand off to.
        condition (Optional[ContextCondition]): The context variable based condition for transitioning to the target agent. If None, the condition always evaluates to True.
        available (AvailableCondition): Optional condition to determine if this OnCondition is included for the LLM to evaluate based on context variables using classes like StringAvailableCondition and ContextExpressionAvailableCondition.
    """

    target: TransitionTarget
    condition: ContextCondition | None = None
    available: AvailableCondition | None = None

    def has_target_type(self, target_type: type) -> bool:
        """Check if the target type matches the specified type.

        Args:
            target_type (type): The target type to check against. Should be a subclass of TransitionTarget.

        Returns:
            bool: True if the target type matches, False otherwise
        """
        return isinstance(self.target, target_type)

    def target_requires_wrapping(self) -> bool:
        """Check if the target requires wrapping in an agent.

        Returns:
            bool: True if the target requires wrapping, False otherwise
        """
        return self.target.needs_agent_wrapper()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from pydantic import BaseModel

from .context_variables import ContextVariables

__all__ = ["ContextStr"]


class ContextStr(BaseModel):
    """A string that requires context variable substitution.

    Use the format method to substitute context variables into the string.
    """

    """The string to be substituted with context variables. It is expected that the string will contain `{var}` placeholders and that string format will be able to replace all values."""
    template: str

    def format(self, context_variables: ContextVariables) -> str | None:
        """Substitute context variables into the string.

        Args:
            context_variables (ContextVariables): The context variables to substitute into the string.

        Returns:
            Optional[str]: The formatted string with context variables substituted.
        """
        context = context_variables.to_dict()

        if not context:
            return self.template

        return self.template.format(**context)

    def __str__(self) -> str:
        return f"ContextStr, unformatted: {self.template}"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import json
import re
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any

from pydantic import BaseModel, Field

from ...oai.client import OpenAIWrapper

if TYPE_CHECKING:
    from ...llm_config import LLMConfig
    from .targets.transition_target import TransitionTarget


class GuardrailResult(BaseModel):
    """Represents the outcome of a guardrail check."""

    activated: bool
    justification: str = Field(default="No justification provided")

    def __str__(self) -> str:
        return f"Guardrail Result: {self.activated}\nJustification: {self.justification}"

    @staticmethod
    def parse(text: str) -> "GuardrailResult":
        """Parses a JSON string into a GuardrailResult object.

        Args:
            text (str): The JSON string to parse.

        Returns:
            GuardrailResult: The parsed GuardrailResult object.
        """
        try:
            data = json.loads(text)
            return GuardrailResult(**data)
        except (json.JSONDecodeError, ValueError) as e:
            raise ValueError(f"Failed to parse GuardrailResult from text: {text}") from e


class Guardrail(ABC):
    """Abstract base class for guardrails."""

    def __init__(
        self, name: str, condition: str, target: "TransitionTarget", activation_message: str | None = None
    ) -> None:
        self.name = name
        self.condition = condition
        self.target = target
        self.activation_message = (
            activation_message if activation_message else f"Guardrail '{name}' has been activated."
        )

    @abstractmethod
    def check(
        self,
        context: str | list[dict[str, Any]],
    ) -> GuardrailResult:
        """Checks the text against the guardrail and returns a GuardrailResult.

        Args:
            context (Union[str, list[dict[str, Any]]]): The context to check against the guardrail.

        Returns:
            GuardrailResult: The result of the guardrail check.
        """
        pass


class LLMGuardrail(Guardrail):
    """Guardrail that uses an LLM to check the context."""

    def __init__(
        self,
        name: str,
        condition: str,
        target: "TransitionTarget",
        llm_config: "LLMConfig",
        activation_message: str | None = None,
    ) -> None:
        super().__init__(name, condition, target, activation_message)

        if not llm_config:
            raise ValueError("LLMConfig is required.")

        self.llm_config = llm_config.deepcopy()
        setattr(self.llm_config, "response_format", GuardrailResult)
        self.client = OpenAIWrapper(**self.llm_config.model_dump())

        self.check_prompt = f"""You are a guardrail that checks if a condition is met in the conversation you are given.
You will activate the guardrail only if the condition is met.

**Condition: {self.condition}**"""

    def check(
        self,
        context: str | list[dict[str, Any]],
    ) -> GuardrailResult:
        """Checks the context against the guardrail using an LLM.

        Args:
            context (Union[str, list[dict[str, Any]]]): The context to check against the guardrail.

        Returns:
            GuardrailResult: The result of the guardrail check.
        """
        # Set the check prompt as the system message
        check_messages = [{"role": "system", "content": self.check_prompt}]
        # If context is a string, wrap it in a user message and append it
        if isinstance(context, str):
            check_messages.append({"role": "user", "content": context})
        # If context is a list of messages, append them
        elif isinstance(context, list):
            check_messages.extend(context)
        else:
            raise ValueError("Context must be a string or a list of messages.")
        # Call the LLM with the check messages
        response = self.client.create(messages=check_messages)
        return GuardrailResult.parse(response.choices[0].message.content)  # type: ignore


class RegexGuardrail(Guardrail):
    """Guardrail that checks the context against a regular expression."""

    def __init__(
        self,
        name: str,
        condition: str,
        target: "TransitionTarget",
        activation_message: str | None = None,
    ) -> None:
        super().__init__(name, condition, target, activation_message)
        # Compile the regular expression condition
        try:
            self.regex = re.compile(condition)
        except re.error as e:
            raise ValueError(f"Invalid regex pattern '{condition}': {str(e)}")

    def check(
        self,
        context: str | list[dict[str, Any]],
    ) -> GuardrailResult:
        """Checks the context against the guardrail using a regular expression.

        Args:
            context (Union[str, list[dict[str, Any]]]): The context to check against the guardrail.

        Returns:
            GuardrailResult: The result of the guardrail check.
        """
        # Create a list of the messages to check
        if isinstance(context, str):
            messages = [context]
        elif isinstance(context, list):
            messages = [message.get("content", "") for message in context]
        else:
            raise ValueError("Context must be a string or a list of messages.")

        # Check each message against the regex
        for message in messages:
            match = self.regex.search(message)
            # If a match is found, activate the guardrail and return the result
            if match:
                activated = True
                justification = f"Match found -> {match.group(0)}"
                return GuardrailResult(activated=activated, justification=justification)
        return GuardrailResult(activated=False, justification="No match found in the context.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING

from pydantic import BaseModel

from ..agent import Agent

if TYPE_CHECKING:
    # Avoid circular import
    from ..groupchat import GroupChat


class SpeakerSelectionResult(BaseModel):
    """Represents a speaker selection result that will be returned to GroupChat._prepare_and_select_agents to determine the next speaker.

    This class can return an Agent, a None to end the conversation, or a string for a speaker selection method.
    """

    terminate: bool | None = None
    agent_name: str | None = None
    speaker_selection_method: str | None = None

    def get_speaker_selection_result(self, groupchat: "GroupChat") -> Agent | str | None:
        """Get the speaker selection result. If None, the conversation will end."""
        if self.agent_name is not None:
            # Find the agent by name in the groupchat
            for agent in groupchat.agents:
                if agent.name == self.agent_name:
                    return agent
            raise ValueError(f"Agent '{self.agent_name}' not found in groupchat.")
        elif self.speaker_selection_method is not None:
            return self.speaker_selection_method
        elif self.terminate is not None and self.terminate:
            return None
        else:
            raise ValueError(
                "Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided."
            )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import ast
import re
from dataclasses import dataclass

from ...doc_utils import export_module
from .context_variables import ContextVariables


@dataclass
@export_module("autogen")
class ContextExpression:
    """A class to evaluate logical expressions using context variables.\n
    \n
        Args:\n
            expression (str): A string containing a logical expression with context variable references.\n
                - Variable references use ${var_name} syntax: ${logged_in}, ${attempts}\n
                - String literals can use normal quotes: 'hello', "world"\n
                - Supported operators:\n
                    - Logical: not/!, and/&, or/|\n
                    - Comparison: >, <, >=, <=, ==, !=\n
                - Supported functions:\n
                    - len(${var_name}): Gets the length of a list, string, or other collection\n
                - Parentheses can be used for grouping\n
                - Examples:\n
                    - "not ${logged_in} and ${is_admin} or ${guest_checkout}"\n
                    - "!${logged_in} & ${is_admin} | ${guest_checkout}"\n
                    - "len(${orders}) > 0 & ${user_active}"\n
                    - "len(${cart_items}) == 0 | ${checkout_started}"\n
    \n
        Raises:\n
            SyntaxError: If the expression cannot be parsed\n
            ValueError: If the expression contains disallowed operations\n
    """

    expression: str

    def __post_init__(self) -> None:
        # Validate the expression immediately upon creation
        try:
            # Extract variable references and replace with placeholders
            self._variable_names = self._extract_variable_names(self.expression)

            # Convert symbolic operators to Python keywords
            python_expr = self._convert_to_python_syntax(self.expression)

            # Sanitize for AST parsing
            sanitized_expr = self._prepare_for_ast(python_expr)

            # Use ast to parse and validate the expression
            self._ast = ast.parse(sanitized_expr, mode="eval")

            # Verify it only contains allowed operations
            self._validate_operations(self._ast.body)

            # Store the Python-syntax version for evaluation
            self._python_expr = python_expr

        except SyntaxError as e:
            raise SyntaxError(f"Invalid expression syntax in '{self.expression}': {str(e)}")
        except Exception as e:
            raise ValueError(f"Error validating expression '{self.expression}': {str(e)}")

    def _extract_variable_names(self, expr: str) -> list[str]:
        """Extract all variable references ${var_name} from the expression."""
        # Find all patterns like ${var_name}
        matches = re.findall(r"\${([^}]*)}", expr)
        return matches

    def _convert_to_python_syntax(self, expr: str) -> str:
        """Convert symbolic operators to Python keywords."""
        # We need to be careful about operators inside string literals
        # First, temporarily replace string literals with placeholders
        string_literals = []

        def replace_string_literal(match: re.Match[str]) -> str:
            string_literals.append(match.group(0))
            return f"__STRING_LITERAL_{len(string_literals) - 1}__"

        # Replace both single and double quoted strings
        expr_without_strings = re.sub(r"'[^']*'|\"[^\"]*\"", replace_string_literal, expr)

        # Handle the NOT operator (!) - no parentheses handling needed
        # Replace standalone ! before variables or expressions
        expr_without_strings = re.sub(r"!\s*(\${|\()", "not \\1", expr_without_strings)

        # Handle AND and OR operators - simpler approach without parentheses handling
        expr_without_strings = re.sub(r"\s+&\s+", " and ", expr_without_strings)
        expr_without_strings = re.sub(r"\s+\|\s+", " or ", expr_without_strings)

        # Now put string literals back
        for i, literal in enumerate(string_literals):
            expr_without_strings = expr_without_strings.replace(f"__STRING_LITERAL_{i}__", literal)

        return expr_without_strings

    def _prepare_for_ast(self, expr: str) -> str:
        """Convert the expression to valid Python for AST parsing by replacing variables with placeholders."""
        # Replace ${var_name} with var_name for AST parsing
        processed_expr = expr
        for var_name in self._variable_names:
            processed_expr = processed_expr.replace(f"${{{var_name}}}", var_name)

        return processed_expr

    def _validate_operations(self, node: ast.AST) -> None:
        """Recursively validate that only allowed operations exist in the AST."""
        allowed_node_types = (
            # Boolean operations
            ast.BoolOp,
            ast.UnaryOp,
            ast.And,
            ast.Or,
            ast.Not,
            # Comparison operations
            ast.Compare,
            ast.Eq,
            ast.NotEq,
            ast.Lt,
            ast.LtE,
            ast.Gt,
            ast.GtE,
            # Basic nodes
            ast.Name,
            ast.Load,
            ast.Constant,
            ast.Expression,
            # Support for basic numeric operations in comparisons
            ast.Num,
            ast.NameConstant,
            # Support for negative numbers
            ast.USub,
            ast.UnaryOp,
            # Support for string literals
            ast.Str,
            ast.Constant,
            # Support for function calls (specifically len())
            ast.Call,
        )

        if not isinstance(node, allowed_node_types):
            raise ValueError(f"Operation type {type(node).__name__} is not allowed in logical expressions")

        # Special validation for function calls - only allow len()
        if isinstance(node, ast.Call):
            if not (isinstance(node.func, ast.Name) and node.func.id == "len"):
                raise ValueError(f"Only the len() function is allowed, got: {getattr(node.func, 'id', 'unknown')}")
            if len(node.args) != 1:
                raise ValueError(f"len() function must have exactly one argument, got {len(node.args)}")

        # Special validation for Compare nodes
        if isinstance(node, ast.Compare):
            for op in node.ops:
                if not isinstance(op, (ast.Eq, ast.NotEq, ast.Lt, ast.LtE, ast.Gt, ast.GtE)):
                    raise ValueError(f"Comparison operator {type(op).__name__} is not allowed")

        # Recursively check child nodes
        for child in ast.iter_child_nodes(node):
            self._validate_operations(child)

    def evaluate(self, context_variables: ContextVariables) -> bool:
        """Evaluate the expression using the provided context variables.

        Args:
            context_variables: Dictionary of context variables to use for evaluation

        Returns:
            bool: The result of evaluating the expression

        Raises:
            KeyError: If a variable referenced in the expression is not found in the context
        """
        # Create a modified expression that we can safely evaluate
        eval_expr = self._python_expr  # Use the Python-syntax version

        # First, handle len() functions with variable references inside
        len_pattern = r"len\(\${([^}]*)}\)"
        len_matches = list(re.finditer(len_pattern, eval_expr))

        # Process all len() operations first
        for match in len_matches:
            var_name = match.group(1)
            # Check if variable exists in context, raise KeyError if not
            if not context_variables.contains(var_name):
                raise KeyError(f"Missing context variable: '{var_name}'")

            var_value = context_variables.get(var_name)

            # Calculate the length - works for lists, strings, dictionaries, etc.
            try:
                length_value = len(var_value)  # type: ignore[arg-type]
            except TypeError:
                # If the value doesn't support len(), treat as 0
                length_value = 0

            # Replace the len() expression with the actual length
            full_match = match.group(0)
            eval_expr = eval_expr.replace(full_match, str(length_value))

        # Then replace remaining variable references with their values
        for var_name in self._variable_names:
            # Skip variables that were already processed in len() expressions
            if any(m.group(1) == var_name for m in len_matches):
                continue

            # Check if variable exists in context, raise KeyError if not
            if not context_variables.contains(var_name):
                raise KeyError(f"Missing context variable: '{var_name}'")

            # Get the value from context
            var_value = context_variables.get(var_name)

            # Format the value appropriately based on its type
            if isinstance(var_value, (bool, int, float)):
                formatted_value = str(var_value)
            elif isinstance(var_value, str):
                formatted_value = f"'{var_value}'"  # Quote strings
            elif isinstance(var_value, (list, dict, tuple)):
                # For collections, convert to their boolean evaluation
                formatted_value = str(bool(var_value))
            else:
                formatted_value = str(var_value)

            # Replace the variable reference with the formatted value
            eval_expr = eval_expr.replace(f"${{{var_name}}}", formatted_value)

        try:
            return eval(eval_expr)  # type: ignore[no-any-return]
        except Exception as e:
            raise ValueError(
                f"Error evaluating expression '{self.expression}' (are you sure you're using ${{my_context_variable_key}}): {str(e)}"
            )

    def __str__(self) -> str:
        return f"ContextExpression('{self.expression}')"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from pydantic import BaseModel

from ...doc_utils import export_module
from .available_condition import AvailableCondition
from .llm_condition import LLMCondition
from .targets.transition_target import TransitionTarget

__all__ = [
    "OnCondition",
]


@export_module("autogen")
class OnCondition(BaseModel):  # noqa: N801
    """Defines a condition for transitioning to another agent or nested chats.\n
    \n
        This is for LLM-based condition evaluation where these conditions are translated into tools and attached to the agent.\n
    \n
        These are evaluated after the OnCondition conditions but before the after work condition.\n
    \n
        Args:\n
            target (TransitionTarget): The transition (essentially an agent) to hand off to.\n
            condition (LLMCondition): The condition for transitioning to the target agent, evaluated by the LLM.\n
            available (AvailableCondition): Optional condition to determine if this OnCondition is included for the LLM to evaluate based on context variables using classes like StringAvailableCondition and ContextExpressionAvailableCondition.\n
            llm_function_name (Optional[str]): The name of the LLM function to use for this condition.\n
    """

    target: TransitionTarget
    condition: LLMCondition
    available: AvailableCondition | None = None
    llm_function_name: str | None = None

    def has_target_type(self, target_type: type) -> bool:
        """Check if the target type matches the specified type.

        Args:
            target_type (type): The target type to check against, which should be a subclass of TransitionTarget

        Returns:
            bool: True if the target type matches, False otherwise
        """
        return isinstance(self.target, target_type)

    def target_requires_wrapping(self) -> bool:
        """Check if the target requires wrapping in an agent.

        Returns:
            bool: True if the target requires wrapping, False otherwise
        """
        return self.target.needs_agent_wrapper()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from typing import Any

from pydantic import BaseModel

from .context_expression import ContextExpression
from .context_variables import ContextVariables

__all__ = ["ContextCondition", "ExpressionContextCondition", "StringContextCondition"]


class ContextCondition(BaseModel):
    """Protocol for conditions evaluated directly using context variables."""

    def evaluate(self, context_variables: ContextVariables) -> bool:
        """Evaluate the condition to a boolean result.

        Args:
            context_variables: The context variables to evaluate against

        Returns:
            Boolean result of the condition evaluation
        """
        raise NotImplementedError("Requires subclasses to implement.")


class StringContextCondition(ContextCondition):
    """Simple string-based context condition.

    This condition checks if a named context variable exists and is truthy.
    """

    variable_name: str

    def evaluate(self, context_variables: ContextVariables) -> bool:
        """Check if the named context variable is truthy.

        Args:
            context_variables: The context variables to check against

        Returns:
            True if the variable exists and is truthy, False otherwise
        """
        return bool(context_variables.get(self.variable_name, False))


class ExpressionContextCondition(ContextCondition):
    """Complex expression-based context condition.

    This condition evaluates a ContextExpression against the context variables.
    """

    expression: ContextExpression

    def __init__(self, expression: ContextExpression, **data: Any) -> None:
        """Initialize with an expression as a positional parameter.

        Args:
            expression: The context expression to evaluate
            data: Additional data for the parent class
        """
        super().__init__(expression=expression, **data)

    def evaluate(self, context_variables: ContextVariables) -> bool:
        """Evaluate the expression against the context variables.

        Args:
            context_variables: The context variables to evaluate against

        Returns:
            Boolean result of the expression evaluation
        """
        return self.expression.evaluate(context_variables)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import overload

from pydantic import BaseModel, Field

from .on_condition import OnCondition
from .on_context_condition import OnContextCondition
from .targets.transition_target import TransitionTarget

__all__ = ["Handoffs"]


class Handoffs(BaseModel):
    """Container for all handoff transition conditions of a ConversableAgent.\n
    \n
        Three types of conditions can be added, each with a different order and time of use:\n
        1. OnContextConditions (evaluated without an LLM)\n
        2. OnConditions (evaluated with an LLM)\n
        3. After work TransitionTarget (if no other transition is triggered)\n
    \n
        Supports method chaining:\n
        agent.handoffs.add_context_conditions([condition1])\n
                       .add_llm_condition(condition2)\n
                       .set_after_work(after_work)\n
    """

    context_conditions: list[OnContextCondition] = Field(default_factory=list)
    llm_conditions: list[OnCondition] = Field(default_factory=list)
    after_works: list[OnContextCondition] = Field(default_factory=list)

    def add_context_condition(self, condition: OnContextCondition) -> "Handoffs":
        """Add a single context condition.

        Args:
            condition: The OnContextCondition to add

        Returns:
            Self for method chaining
        """
        # Validate that it is an OnContextCondition
        if not isinstance(condition, OnContextCondition):
            raise TypeError(f"Expected an OnContextCondition instance, got {type(condition).__name__}")

        self.context_conditions.append(condition)
        return self

    def add_context_conditions(self, conditions: list[OnContextCondition]) -> "Handoffs":
        """Add multiple context conditions.

        Args:
            conditions: List of OnContextConditions to add

        Returns:
            Self for method chaining
        """
        # Validate that it is a list of OnContextConditions
        if not all(isinstance(condition, OnContextCondition) for condition in conditions):
            raise TypeError("All conditions must be of type OnContextCondition")

        self.context_conditions.extend(conditions)
        return self

    def add_llm_condition(self, condition: OnCondition) -> "Handoffs":
        """Add a single LLM condition.

        Args:
            condition: The OnCondition to add

        Returns:
            Self for method chaining
        """
        # Validate that it is an OnCondition
        if not isinstance(condition, OnCondition):
            raise TypeError(f"Expected an OnCondition instance, got {type(condition).__name__}")

        self.llm_conditions.append(condition)
        return self

    def add_llm_conditions(self, conditions: list[OnCondition]) -> "Handoffs":
        """Add multiple LLM conditions.

        Args:
            conditions: List of OnConditions to add

        Returns:
            Self for method chaining
        """
        # Validate that it is a list of OnConditions
        if not all(isinstance(condition, OnCondition) for condition in conditions):
            raise TypeError("All conditions must be of type OnCondition")

        self.llm_conditions.extend(conditions)
        return self

    def set_after_work(self, target: TransitionTarget) -> "Handoffs":
        """Set the after work target (replaces all after_works with single entry).

        For backward compatibility, this creates an OnContextCondition with no condition (always true).

        Args:
            target: The after work TransitionTarget to set

        Returns:
            Self for method chaining
        """
        if not isinstance(target, TransitionTarget):
            raise TypeError(f"Expected a TransitionTarget instance, got {type(target).__name__}")

        # Create OnContextCondition with no condition (always true)
        after_work_condition = OnContextCondition(target=target, condition=None)
        self.after_works = [after_work_condition]
        return self

    def add_after_work(self, condition: OnContextCondition) -> "Handoffs":
        """Add a single after-work condition.

        If the condition has condition=None, it will replace any existing
        condition=None entry and be placed at the end.

        Args:
            condition: The OnContextCondition to add

        Returns:
            Self for method chaining
        """
        if not isinstance(condition, OnContextCondition):
            raise TypeError(f"Expected an OnContextCondition instance, got {type(condition).__name__}")

        if condition.condition is None:
            # Remove any existing condition=None entries
            self.after_works = [c for c in self.after_works if c.condition is not None]
            # Add the new one at the end
            self.after_works.append(condition)
        else:
            # For regular conditions, check if we need to move condition=None to the end
            none_conditions = [c for c in self.after_works if c.condition is None]
            if none_conditions:
                # Remove the None condition temporarily
                self.after_works = [c for c in self.after_works if c.condition is not None]
                # Add the new regular condition
                self.after_works.append(condition)
                # Re-add the None condition at the end
                self.after_works.append(none_conditions[0])
            else:
                # No None condition exists, just append
                self.after_works.append(condition)

        return self

    def add_after_works(self, conditions: list[OnContextCondition]) -> "Handoffs":
        """Add multiple after-work conditions.

        Special handling for condition=None entries:
        - Only one condition=None entry is allowed (the fallback)
        - It will always be placed at the end of the list
        - If multiple condition=None entries are provided, only the last one is kept

        Args:
            conditions: List of OnContextConditions to add

        Returns:
            Self for method chaining
        """
        # Validate that it is a list of OnContextConditions
        if not all(isinstance(condition, OnContextCondition) for condition in conditions):
            raise TypeError("All conditions must be of type OnContextCondition")

        # Separate conditions with None and without None
        none_conditions = [c for c in conditions if c.condition is None]
        regular_conditions = [c for c in conditions if c.condition is not None]

        # Remove any existing condition=None entries
        self.after_works = [c for c in self.after_works if c.condition is not None]

        # Add regular conditions
        self.after_works.extend(regular_conditions)

        # Add at most one None condition at the end
        if none_conditions:
            self.after_works.append(none_conditions[-1])  # Use the last one if multiple provided

        return self

    @overload
    def add(self, condition: OnContextCondition) -> "Handoffs": ...

    @overload
    def add(self, condition: OnCondition) -> "Handoffs": ...

    def add(self, condition: OnContextCondition | OnCondition) -> "Handoffs":
        """Add a single condition (OnContextCondition or OnCondition).

        Args:
            condition: The condition to add (OnContextCondition or OnCondition)

        Raises:
            TypeError: If the condition type is not supported

        Returns:
            Self for method chaining
        """
        # This add method is a helper method designed to make it easier for
        # adding handoffs without worrying about the specific type.
        if isinstance(condition, OnContextCondition):
            return self.add_context_condition(condition)
        elif isinstance(condition, OnCondition):
            return self.add_llm_condition(condition)
        else:
            raise TypeError(f"Unsupported condition type: {type(condition).__name__}")

    def add_many(self, conditions: list[OnContextCondition | OnCondition]) -> "Handoffs":
        """Add multiple conditions of any supported types (OnContextCondition and OnCondition).

        Args:
            conditions: List of conditions to add

        Raises:
            TypeError: If an unsupported condition type is provided

        Returns:
            Self for method chaining
        """
        # This add_many method is a helper method designed to make it easier for
        # adding handoffs without worrying about the specific type.
        context_conditions = []
        llm_conditions = []

        for condition in conditions:
            if isinstance(condition, OnContextCondition):
                context_conditions.append(condition)
            elif isinstance(condition, OnCondition):
                llm_conditions.append(condition)
            else:
                raise TypeError(f"Unsupported condition type: {type(condition).__name__}")

        if context_conditions:
            self.add_context_conditions(context_conditions)
        if llm_conditions:
            self.add_llm_conditions(llm_conditions)

        return self

    def clear(self) -> "Handoffs":
        """Clear all handoff conditions.

        Returns:
            Self for method chaining
        """
        self.context_conditions.clear()
        self.llm_conditions.clear()
        self.after_works.clear()
        return self

    def get_llm_conditions_by_target_type(self, target_type: type) -> list[OnCondition]:
        """Get OnConditions for a specific target type.

        Args:
            target_type: The type of condition to retrieve

        Returns:
            List of conditions of the specified type, or None if none exist
        """
        return [on_condition for on_condition in self.llm_conditions if on_condition.has_target_type(target_type)]

    def get_context_conditions_by_target_type(self, target_type: type) -> list[OnContextCondition]:
        """Get OnContextConditions for a specific target type.

        Args:
            target_type: The type of condition to retrieve

        Returns:
            List of conditions of the specified type, or None if none exist
        """
        return [
            on_context_condition
            for on_context_condition in self.context_conditions
            if on_context_condition.has_target_type(target_type)
        ]

    def get_llm_conditions_requiring_wrapping(self) -> list[OnCondition]:
        """Get LLM conditions that have targets that require wrapping.

        Returns:
            List of LLM conditions that require wrapping
        """
        return [condition for condition in self.llm_conditions if condition.target_requires_wrapping()]

    def get_context_conditions_requiring_wrapping(self) -> list[OnContextCondition]:
        """Get context conditions that have targets that require wrapping.

        Returns:
            List of context conditions that require wrapping
        """
        return [condition for condition in self.context_conditions if condition.target_requires_wrapping()]

    def set_llm_function_names(self) -> None:
        """Set the LLM function names for all LLM conditions, creating unique names for each function."""
        for i, condition in enumerate(self.llm_conditions):
            # Function names are made unique and allow multiple OnCondition's to the same agent
            condition.llm_function_name = f"transfer_to_{condition.target.normalized_name()}_{i + 1}"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


__all__ = ["ReplyResult"]


from pydantic import BaseModel

from .context_variables import ContextVariables
from .targets.transition_target import TransitionTarget


class ReplyResult(BaseModel):
    """Result of a tool call that is used to provide the return message and the target to transition to."""

    message: str
    target: TransitionTarget | None = None
    context_variables: ContextVariables | None = None

    def __str__(self) -> str:
        """The string representation for ReplyResult will be just the message."""
        return self.message
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import inspect
from collections.abc import Callable
from copy import deepcopy
from typing import Annotated, Any

from ...oai import OpenAIWrapper
from ...tools import Depends, Tool
from ...tools.dependency_injection import inject_params, on
from ..agent import Agent
from ..conversable_agent import ConversableAgent
from .context_variables import __CONTEXT_VARIABLES_PARAM_NAME__, ContextVariables
from .reply_result import ReplyResult
from .targets.transition_target import TransitionTarget

__TOOL_EXECUTOR_NAME__ = "_Group_Tool_Executor"


class GroupToolExecutor(ConversableAgent):
    """Tool executor for the group chat initiated with initiate_group_chat"""

    def __init__(self) -> None:
        super().__init__(
            name=__TOOL_EXECUTOR_NAME__,
            system_message="Tool Execution, do not use this agent directly.",
            human_input_mode="NEVER",
            code_execution_config=False,
        )

        # Store the next target from a tool call
        self._group_next_target: TransitionTarget | None = None

        # Primary tool reply function for handling the tool reply and the ReplyResult and TransitionTarget returns
        self.register_reply([Agent, None], self._generate_group_tool_reply, remove_other_reply_funcs=True)

    def set_next_target(self, next_target: TransitionTarget) -> None:
        """Sets the next target to transition to, used in the determine_next_agent function."""
        self._group_next_target = next_target

    def get_next_target(self) -> TransitionTarget:
        """Gets the next target to transition to."""
        """Returns the next target to transition to, if it exists."""
        if self._group_next_target is None:
            raise ValueError(
                "No next target set. Please set a next target before calling this method. Use has_next_target() to check if a next target exists."
            )
        return self._group_next_target

    def has_next_target(self) -> bool:
        """Checks if there is a next target to transition to."""
        return self._group_next_target is not None

    def clear_next_target(self) -> None:
        """Clears the next target to transition to."""
        self._group_next_target = None

    def _modify_context_variables_param(
        self, f: Callable[..., Any], context_variables: ContextVariables
    ) -> Callable[..., Any]:
        """Modifies the context_variables parameter to use dependency injection and link it to the group context variables.

        This essentially changes:
        def some_function(some_variable: int, context_variables: ContextVariables) -> str:

        to:

        def some_function(some_variable: int, context_variables: Annotated[ContextVariables, Depends(on(self.context_variables))]) -> str:
        """
        sig = inspect.signature(f)

        # Check if context_variables parameter exists and update it if so
        if __CONTEXT_VARIABLES_PARAM_NAME__ in sig.parameters:
            new_params = []
            for name, param in sig.parameters.items():
                if name == __CONTEXT_VARIABLES_PARAM_NAME__:
                    # Replace with new annotation using Depends
                    new_param = param.replace(annotation=Annotated[ContextVariables, Depends(on(context_variables))])
                    new_params.append(new_param)
                else:
                    new_params.append(param)

            # Update signature
            new_sig = sig.replace(parameters=new_params)
            f.__signature__ = new_sig  # type: ignore[attr-defined]

        return f

    def _change_tool_context_variables_to_depends(
        self, agent: ConversableAgent, current_tool: Tool, context_variables: ContextVariables
    ) -> None:
        """Checks for the context_variables parameter in the tool and updates it to use dependency injection."""
        # If the tool has a context_variables parameter, remove the tool and reregister it without the parameter
        if __CONTEXT_VARIABLES_PARAM_NAME__ in current_tool.tool_schema["function"]["parameters"]["properties"]:
            # We'll replace the tool, so start with getting the underlying function
            tool_func = current_tool._func

            # Remove the Tool from the agent
            name = current_tool._name
            description = current_tool._description
            agent.remove_tool_for_llm(current_tool)

            # Recreate the tool without the context_variables parameter
            tool_func = self._modify_context_variables_param(current_tool._func, context_variables)
            tool_func = inject_params(tool_func)
            new_tool = ConversableAgent._create_tool_if_needed(
                func_or_tool=tool_func, name=name, description=description
            )

            # Re-register with the agent
            agent.register_for_llm()(new_tool)

    def register_agents_functions(self, agents: list[ConversableAgent], context_variables: ContextVariables) -> None:
        """Adds the functions of the agents to the group tool executor."""
        for agent in agents:
            # As we're moving towards tools and away from function maps, this may not be used
            self._function_map.update(agent._function_map)

            # Update any agent tools that have context_variables parameters to use Dependency Injection
            for tool in agent.tools:
                self._change_tool_context_variables_to_depends(agent, tool, context_variables)

            # Add all tools to the Tool Executor agent
            for tool in agent.tools:
                self.register_for_execution(serialize=False, silent_override=True)(tool)

    def _generate_group_tool_reply(
        self,
        agent: ConversableAgent,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, dict[str, Any] | None]:
        """Pre-processes and generates tool call replies.

        This function:
        1. Adds context_variables back to the tool call for the function, if necessary.
        2. Generates the tool calls reply.
        3. Updates context_variables and next_agent based on the tool call response.
        """
        if config is None:
            config = agent  # type: ignore[assignment]
        if messages is None:
            messages = agent._oai_messages[sender]

        message = messages[-1]
        if "tool_calls" in message:
            tool_call_count = len(message["tool_calls"])

            # Loop through tool calls individually (so context can be updated after each function call)
            next_target: TransitionTarget | None = None
            tool_responses_inner = []
            contents = []
            for index in range(tool_call_count):
                message_copy = deepcopy(message)

                # 1. add context_variables to the tool call arguments
                tool_call = message_copy["tool_calls"][index]

                # Ensure we are only executing the one tool at a time
                message_copy["tool_calls"] = [tool_call]

                # 2. generate tool calls reply
                _, tool_message = agent.generate_tool_calls_reply([message_copy])

                if tool_message is None:
                    raise ValueError("Tool call did not return a message")

                # 3. update context_variables and next_agent, convert content to string
                for tool_response in tool_message["tool_responses"]:
                    content = tool_response.get("content")

                    # Tool Call returns that are a target are either a ReplyResult or a TransitionTarget are the next agent
                    if isinstance(content, ReplyResult):
                        if content.context_variables and content.context_variables.to_dict() != {}:
                            agent.context_variables.update(content.context_variables.to_dict())
                        if content.target is not None:
                            next_target = content.target
                    elif isinstance(content, TransitionTarget):
                        next_target = content

                    # Serialize the content to a string
                    if content is not None:
                        tool_response["content"] = str(content)

                    tool_responses_inner.append(tool_response)
                    contents.append(str(tool_response["content"]))

            self._group_next_target = next_target  # type: ignore[attr-defined]

            # Put the tool responses and content strings back into the response message
            # Caters for multiple tool calls
            if tool_message is None:
                raise ValueError("Tool call did not return a message")

            tool_message["tool_responses"] = tool_responses_inner
            tool_message["content"] = "\n".join(contents)

            return True, tool_message
        return False, None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
import json
import logging
import random
import re
import sys
from collections.abc import Callable
from dataclasses import dataclass, field
from typing import Any, Literal

from ..code_utils import content_str
from ..doc_utils import export_module
from ..events.agent_events import (
    ClearAgentsHistoryEvent,
    GroupChatResumeEvent,
    GroupChatRunChatEvent,
    SelectSpeakerEvent,
    SelectSpeakerInvalidInputEvent,
    SelectSpeakerTryCountExceededEvent,
    SpeakerAttemptFailedMultipleAgentsEvent,
    SpeakerAttemptFailedNoAgentsEvent,
    SpeakerAttemptSuccessfulEvent,
    TerminationEvent,
)
from ..exception_utils import AgentNameConflictError, NoEligibleSpeakerError, UndefinedNextAgentError
from ..graph_utils import check_graph_validity, invert_disallowed_to_allowed
from ..io.base import IOStream
from ..llm_config import LLMConfig, ModelClient
from ..runtime_logging import log_new_agent, logging_enabled
from .agent import Agent
from .contrib.capabilities import transform_messages
from .conversable_agent import ConversableAgent

logger = logging.getLogger(__name__)

SELECT_SPEAKER_PROMPT_TEMPLATE = (
    "Read the above conversation. Then select the next role from {agentlist} to play. Only return the role."
)


@dataclass
@export_module("autogen")
class GroupChat:
    """(In preview) A group chat class that contains the following data fields:

    - agents: a list of participating agents.
    - messages: a list of messages in the group chat.
    - max_round: the maximum number of rounds.
    - admin_name: the name of the admin agent if there is one. Default is "Admin".
        KeyBoardInterrupt will make the admin agent take over.
    - func_call_filter: whether to enforce function call filter. Default is True.
        When set to True and when a message is a function call suggestion,
        the next speaker will be chosen from an agent which contains the corresponding function name
        in its `function_map`.
    - select_speaker_message_template: customize the select speaker message (used in "auto" speaker selection), which appears first in the message context and generally includes the agent descriptions and list of agents. If the string contains "`{roles}`" it will replaced with the agent's and their role descriptions. If the string contains "`{agentlist}`" it will be replaced with a comma-separated list of agent names in square brackets. The default value is:
        "You are in a role play game. The following roles are available:
                `{roles}`.
                Read the following conversation.
                Then select the next role from `{agentlist}` to play. Only return the role."
    - select_speaker_prompt_template: customize the select speaker prompt (used in "auto" speaker selection), which appears last in the message context and generally includes the list of agents and guidance for the LLM to select the next agent. If the string contains "`{agentlist}`" it will be replaced with a comma-separated list of agent names in square brackets. The default value is:
        "Read the above conversation. Then select the next role from `{agentlist}` to play. Only return the role."
        To ignore this prompt being used, set this to None. If set to None, ensure your instructions for selecting a speaker are in the select_speaker_message_template string.
    - select_speaker_auto_multiple_template: customize the follow-up prompt used when selecting a speaker fails with a response that contains multiple agent names. This prompt guides the LLM to return just one agent name. Applies only to "auto" speaker selection method. If the string contains "`{agentlist}`" it will be replaced with a comma-separated list of agent names in square brackets. The default value is:
        "You provided more than one name in your text, please return just the name of the next speaker. To determine the speaker use these prioritised rules:
                1. If the context refers to themselves as a speaker e.g. "As the..." , choose that speaker's name
                2. If it refers to the "next" speaker name, choose that name
                3. Otherwise, choose the first provided speaker's name in the context
                The names are case-sensitive and should not be abbreviated or changed.
                Respond with ONLY the name of the speaker and DO NOT provide a reason."
    - select_speaker_auto_none_template: customize the follow-up prompt used when selecting a speaker fails with a response that contains no agent names. This prompt guides the LLM to return an agent name and provides a list of agent names. Applies only to "auto" speaker selection method. If the string contains "`{agentlist}`" it will be replaced with a comma-separated list of agent names in square brackets. The default value is:
        "You didn't choose a speaker. As a reminder, to determine the speaker use these prioritised rules:
                1. If the context refers to themselves as a speaker e.g. "As the..." , choose that speaker's name
                2. If it refers to the "next" speaker name, choose that name
                3. Otherwise, choose the first provided speaker's name in the context
                The names are case-sensitive and should not be abbreviated or changed.
                The only names that are accepted are `{agentlist}`.
                Respond with ONLY the name of the speaker and DO NOT provide a reason."
    - speaker_selection_method: the method for selecting the next speaker. Default is "auto".
        Could be any of the following (case insensitive), will raise ValueError if not recognized:
        - "auto": the next speaker is selected automatically by LLM.
        - "manual": the next speaker is selected manually by user input.
        - "random": the next speaker is selected randomly.
        - "round_robin": the next speaker is selected in a round robin fashion, i.e., iterating in the same order as provided in `agents`.
        - a customized speaker selection function (Callable): the function will be called to select the next speaker.
            The function should take the last speaker and the group chat as input and return one of the following:
                1. an `Agent` class, it must be one of the agents in the group chat.
                2. a string from ['auto', 'manual', 'random', 'round_robin'] to select a default method to use.
                3. None, which would terminate the conversation gracefully.
            ```python
            def custom_speaker_selection_func(
                last_speaker: Agent, groupchat: GroupChat
            ) -> Union[Agent, str, None]:
            ```
    - max_retries_for_selecting_speaker: the maximum number of times the speaker selection requery process will run.
        If, during speaker selection, multiple agent names or no agent names are returned by the LLM as the next agent, it will be queried again up to the maximum number
        of times until a single agent is returned or it exhausts the maximum attempts.
        Applies only to "auto" speaker selection method.
        Default is 2.
    - select_speaker_transform_messages: (optional) the message transformations to apply to the nested select speaker agent-to-agent chat messages.
        Takes a TransformMessages object, defaults to None and is only utilised when the speaker selection method is "auto".
    - select_speaker_auto_verbose: whether to output the select speaker responses and selections.
        If set to True, the outputs from the two agents in the nested select speaker chat will be output, along with
        whether the responses were successful, or not, in selecting an agent.
        Applies only to "auto" speaker selection method.
    - allow_repeat_speaker: whether to allow the same speaker to speak consecutively.
        Default is True, in which case all speakers are allowed to speak consecutively.
        If `allow_repeat_speaker` is a list of Agents, then only those listed agents are allowed to repeat.
        If set to False, then no speakers are allowed to repeat.
        `allow_repeat_speaker` and `allowed_or_disallowed_speaker_transitions` are mutually exclusive.
    - allowed_or_disallowed_speaker_transitions: dict.
        The keys are source agents, and the values are agents that the key agent can/can't transit to,
        depending on speaker_transitions_type. Default is None, which means all agents can transit to all other agents.
        `allow_repeat_speaker` and `allowed_or_disallowed_speaker_transitions` are mutually exclusive.
    - speaker_transitions_type: whether the speaker_transitions_type is a dictionary containing lists of allowed agents or disallowed agents.
        "allowed" means the `allowed_or_disallowed_speaker_transitions` is a dictionary containing lists of allowed agents.
        If set to "disallowed", then the `allowed_or_disallowed_speaker_transitions` is a dictionary containing lists of disallowed agents.
        Must be supplied if `allowed_or_disallowed_speaker_transitions` is not None.
    - enable_clear_history: enable possibility to clear history of messages for agents manually by providing
        "clear history" phrase in user prompt. This is experimental feature.
        See description of GroupChatManager.clear_agents_history function for more info.
    - send_introductions: send a round of introductions at the start of the group chat, so agents know who they can speak to (default: False)
    - select_speaker_auto_model_client_cls: Custom model client class for the internal speaker select agent used during 'auto' speaker selection (optional)
    - select_speaker_auto_llm_config: LLM config for the internal speaker select agent used during 'auto' speaker selection (optional)
    - role_for_select_speaker_messages: sets the role name for speaker selection when in 'auto' mode, typically 'user' or 'system'. (default: 'system')
    """

    agents: list[Agent]
    messages: list[dict[str, Any]] = field(default_factory=list)
    max_round: int = 10
    admin_name: str = "Admin"
    func_call_filter: bool = True
    speaker_selection_method: Literal["auto", "manual", "random", "round_robin"] | Callable[..., Any] = "auto"
    max_retries_for_selecting_speaker: int = 2
    allow_repeat_speaker: bool | list[Agent] | None = None
    allowed_or_disallowed_speaker_transitions: dict[str, Any] | None = None
    speaker_transitions_type: Literal["allowed", "disallowed", None] = None
    enable_clear_history: bool = False
    send_introductions: bool = False
    select_speaker_message_template: str = """You are in a role play game. The following roles are available:
                {roles}.
                Read the following conversation.
                Then select the next role from {agentlist} to play. Only return the role."""
    select_speaker_prompt_template: str = SELECT_SPEAKER_PROMPT_TEMPLATE
    select_speaker_auto_multiple_template: str = """You provided more than one name in your text, please return just the name of the next speaker. To determine the speaker use these prioritised rules:
    1. If the context refers to themselves as a speaker e.g. "As the..." , choose that speaker's name
    2. If it refers to the "next" speaker name, choose that name
    3. Otherwise, choose the first provided speaker's name in the context
    The names are case-sensitive and should not be abbreviated or changed.
    Respond with ONLY the name of the speaker and DO NOT provide a reason."""
    select_speaker_auto_none_template: str = """You didn't choose a speaker. As a reminder, to determine the speaker use these prioritised rules:
    1. If the context refers to themselves as a speaker e.g. "As the..." , choose that speaker's name
    2. If it refers to the "next" speaker name, choose that name
    3. Otherwise, choose the first provided speaker's name in the context
    The names are case-sensitive and should not be abbreviated or changed.
    The only names that are accepted are {agentlist}.
    Respond with ONLY the name of the speaker and DO NOT provide a reason."""
    select_speaker_transform_messages: transform_messages.TransformMessages | None = None
    select_speaker_auto_verbose: bool | None = False
    select_speaker_auto_model_client_cls: ModelClient | list[ModelClient] | None = None
    select_speaker_auto_llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None
    role_for_select_speaker_messages: str | None = "system"

    _VALID_SPEAKER_SELECTION_METHODS = ["auto", "manual", "random", "round_robin"]
    _VALID_SPEAKER_TRANSITIONS_TYPE = ["allowed", "disallowed", None]

    # Define a class attribute for the default introduction message
    DEFAULT_INTRO_MSG = (
        "Hello everyone. We have assembled a great team today to answer questions and solve tasks. In attendance are:"
    )

    allowed_speaker_transitions_dict: dict[str, list[Agent]] = field(init=False)
    _inter_agent_guardrails: list = field(default_factory=list, init=False)

    def __post_init__(self):
        # Post init steers clears of the automatically generated __init__ method from dataclass

        if self.allow_repeat_speaker is not None and not isinstance(self.allow_repeat_speaker, (bool, list)):
            raise ValueError("GroupChat allow_repeat_speaker should be a bool or a list of Agents.")

        # Here, we create allowed_speaker_transitions_dict from the supplied allowed_or_disallowed_speaker_transitions and speaker_transitions_type, and lastly checks for validity.

        # Check input
        if self.speaker_transitions_type is not None:
            self.speaker_transitions_type = self.speaker_transitions_type.lower()

        if self.speaker_transitions_type not in self._VALID_SPEAKER_TRANSITIONS_TYPE:
            raise ValueError(
                f"GroupChat speaker_transitions_type is set to '{self.speaker_transitions_type}'. "
                f"It should be one of {self._VALID_SPEAKER_TRANSITIONS_TYPE} (case insensitive). "
            )

        # If both self.allowed_or_disallowed_speaker_transitions is None and self.allow_repeat_speaker is None, set allow_repeat_speaker to True to ensure backward compatibility
        # Discussed in https://github.com/microsoft/autogen/pull/857#discussion_r1451541204
        if self.allowed_or_disallowed_speaker_transitions is None and self.allow_repeat_speaker is None:
            self.allow_repeat_speaker = True

        # self.allowed_or_disallowed_speaker_transitions and self.allow_repeat_speaker are mutually exclusive parameters.
        # Discussed in https://github.com/microsoft/autogen/pull/857#discussion_r1451266661
        if self.allowed_or_disallowed_speaker_transitions is not None and self.allow_repeat_speaker is not None:
            raise ValueError(
                "Don't provide both allowed_or_disallowed_speaker_transitions and allow_repeat_speaker in group chat. "
                "Please set one of them to None."
            )

        # Asks the user to specify whether the speaker_transitions_type is allowed or disallowed if speaker_transitions_type is supplied
        # Discussed in https://github.com/microsoft/autogen/pull/857#discussion_r1451259524
        if self.allowed_or_disallowed_speaker_transitions is not None and self.speaker_transitions_type is None:
            raise ValueError(
                "GroupChat allowed_or_disallowed_speaker_transitions is not None, but speaker_transitions_type is None. "
                "Please set speaker_transitions_type to either 'allowed' or 'disallowed'."
            )

        # Inferring self.allowed_speaker_transitions_dict
        # Create self.allowed_speaker_transitions_dict if allowed_or_disallowed_speaker_transitions is None, using allow_repeat_speaker
        if self.allowed_or_disallowed_speaker_transitions is None:
            self.allowed_speaker_transitions_dict = {}

            # Create a fully connected allowed_speaker_transitions_dict not including self loops
            for agent in self.agents:
                self.allowed_speaker_transitions_dict[agent] = [
                    other_agent for other_agent in self.agents if other_agent != agent
                ]

            # If self.allow_repeat_speaker is True, add self loops to all agents
            if self.allow_repeat_speaker is True:
                for agent in self.agents:
                    self.allowed_speaker_transitions_dict[agent].append(agent)

            # Else if self.allow_repeat_speaker is a list of Agents, add self loops to the agents in the list
            elif isinstance(self.allow_repeat_speaker, list):
                for agent in self.allow_repeat_speaker:
                    self.allowed_speaker_transitions_dict[agent].append(agent)

        # Create self.allowed_speaker_transitions_dict if allowed_or_disallowed_speaker_transitions is not None, using allowed_or_disallowed_speaker_transitions
        else:
            # Process based on speaker_transitions_type
            if self.speaker_transitions_type == "allowed":
                self.allowed_speaker_transitions_dict = self.allowed_or_disallowed_speaker_transitions
            else:
                # Logic for processing disallowed allowed_or_disallowed_speaker_transitions to allowed_speaker_transitions_dict
                self.allowed_speaker_transitions_dict = invert_disallowed_to_allowed(
                    self.allowed_or_disallowed_speaker_transitions, self.agents
                )

        # Check for validity
        check_graph_validity(
            allowed_speaker_transitions_dict=self.allowed_speaker_transitions_dict,
            agents=self.agents,
        )

        # Check select speaker messages, prompts, roles, and retries have values
        if self.select_speaker_message_template is None or len(self.select_speaker_message_template) == 0:
            raise ValueError("select_speaker_message_template cannot be empty or None.")

        if self.select_speaker_prompt_template is not None and len(self.select_speaker_prompt_template) == 0:
            self.select_speaker_prompt_template = None

        if self.role_for_select_speaker_messages is None or len(self.role_for_select_speaker_messages) == 0:
            raise ValueError("role_for_select_speaker_messages cannot be empty or None.")

        if self.select_speaker_auto_multiple_template is None or len(self.select_speaker_auto_multiple_template) == 0:
            raise ValueError("select_speaker_auto_multiple_template cannot be empty or None.")

        if self.select_speaker_auto_none_template is None or len(self.select_speaker_auto_none_template) == 0:
            raise ValueError("select_speaker_auto_none_template cannot be empty or None.")

        if self.max_retries_for_selecting_speaker is None or len(self.role_for_select_speaker_messages) == 0:
            raise ValueError("role_for_select_speaker_messages cannot be empty or None.")

        # Validate max select speakers retries
        if self.max_retries_for_selecting_speaker is None or not isinstance(
            self.max_retries_for_selecting_speaker, int
        ):
            raise ValueError("max_retries_for_selecting_speaker cannot be None or non-int")
        elif self.max_retries_for_selecting_speaker < 0:
            raise ValueError("max_retries_for_selecting_speaker must be greater than or equal to zero")

        # Load message transforms here (load once for the Group Chat so we don't have to re-initiate it and it maintains the cache across subsequent select speaker calls)
        if self.select_speaker_transform_messages is not None:
            if isinstance(self.select_speaker_transform_messages, transform_messages.TransformMessages):
                self._speaker_selection_transforms = self.select_speaker_transform_messages
            else:
                raise ValueError("select_speaker_transform_messages must be None or MessageTransforms.")
        else:
            self._speaker_selection_transforms = None

        # Validate select_speaker_auto_verbose
        if self.select_speaker_auto_verbose is None or not isinstance(self.select_speaker_auto_verbose, bool):
            raise ValueError("select_speaker_auto_verbose cannot be None or non-bool")

    @property
    def agent_names(self) -> list[str]:
        """Return the names of the agents in the group chat."""
        return [agent.name for agent in self.agents]

    def reset(self):
        """Reset the group chat."""
        self.messages.clear()

    def append(self, message: dict[str, Any], speaker: Agent):
        """Append a message to the group chat.
        We cast the content to str here so that it can be managed by text-based
        model.
        """
        # set the name to speaker's name if the role is not function
        # if the role is tool, it is OK to modify the name
        if message["role"] != "function":
            message["name"] = speaker.name
        if not isinstance(message["content"], str) and not isinstance(message["content"], list):
            message["content"] = str(message["content"])
        message["content"] = content_str(message["content"])
        self.messages.append(message)

    def agent_by_name(self, name: str, recursive: bool = False, raise_on_name_conflict: bool = False) -> Agent | None:
        """Returns the agent with a given name. If recursive is True, it will search in nested teams."""
        agents = self.nested_agents() if recursive else self.agents
        filtered_agents = [agent for agent in agents if agent.name == name]

        if raise_on_name_conflict and len(filtered_agents) > 1:
            raise AgentNameConflictError()

        return filtered_agents[0] if filtered_agents else None

    def nested_agents(self) -> list[Agent]:
        """Returns all agents in the group chat manager."""
        agents = self.agents.copy()
        for agent in agents:
            if isinstance(agent, GroupChatManager):
                # Recursive call for nested teams
                agents.extend(agent.groupchat.nested_agents())
        return agents

    def next_agent(self, agent: Agent, agents: list[Agent] | None = None) -> Agent:
        """Return the next agent in the list."""
        if agents is None:
            agents = self.agents

        # Ensure the provided list of agents is a subset of self.agents
        if not set(agents).issubset(set(self.agents)):
            raise UndefinedNextAgentError()

        # What index is the agent? (-1 if not present)
        idx = self.agent_names.index(agent.name) if agent.name in self.agent_names else -1

        # Return the next agent
        if agents == self.agents:
            return agents[(idx + 1) % len(agents)]
        else:
            offset = idx + 1
            for i in range(len(self.agents)):
                if self.agents[(offset + i) % len(self.agents)] in agents:
                    return self.agents[(offset + i) % len(self.agents)]

        # Explicitly handle cases where no valid next agent exists in the provided subset.
        raise UndefinedNextAgentError()

    def select_speaker_msg(self, agents: list[Agent] | None = None) -> str:
        """Return the system message for selecting the next speaker. This is always the *first* message in the context."""
        if agents is None:
            agents = self.agents

        roles = self._participant_roles(agents)
        agentlist = f"{[agent.name for agent in agents]}"

        return_msg = self.select_speaker_message_template.format(roles=roles, agentlist=agentlist)
        return return_msg

    def select_speaker_prompt(self, agents: list[Agent] | None = None) -> str:
        """Return the floating system prompt selecting the next speaker.
        This is always the *last* message in the context.
        Will return None if the select_speaker_prompt_template is None.
        """
        if self.select_speaker_prompt_template is None:
            return None

        if agents is None:
            agents = self.agents

        agentlist = f"{[agent.name for agent in agents]}"

        return_prompt = f"{self.select_speaker_prompt_template}".replace("{agentlist}", agentlist)
        return return_prompt

    def introductions_msg(self, agents: list[Agent] | None = None) -> str:
        """Return the system message for selecting the next speaker. This is always the *first* message in the context."""
        if agents is None:
            agents = self.agents

        # Use the class attribute instead of a hardcoded string
        intro_msg = self.DEFAULT_INTRO_MSG
        participant_roles = self._participant_roles(agents)

        return f"{intro_msg}\n\n{participant_roles}"

    def manual_select_speaker(self, agents: list[Agent] | None = None) -> Agent | None:
        """Manually select the next speaker."""
        iostream = IOStream.get_default()

        if agents is None:
            agents = self.agents

        iostream.send(SelectSpeakerEvent(agents=agents))

        try_count = 0
        # Assume the user will enter a valid number within 3 tries, otherwise use auto selection to avoid blocking.
        while try_count <= 3:
            try_count += 1
            if try_count >= 3:
                iostream.send(SelectSpeakerTryCountExceededEvent(try_count=try_count, agents=agents))
                break
            try:
                i = iostream.input(
                    "Enter the number of the next speaker (enter nothing or `q` to use auto selection): "
                )
                if i == "" or i == "q":
                    break
                i = int(i)
                if i > 0 and i <= len(agents):
                    return agents[i - 1]
                else:
                    raise ValueError
            except ValueError:
                iostream.send(SelectSpeakerInvalidInputEvent(agents=agents))
        return None

    def random_select_speaker(self, agents: list[Agent] | None = None) -> Agent | None:
        """Randomly select the next speaker."""
        if agents is None:
            agents = self.agents
        return random.choice(agents)

    def _prepare_and_select_agents(
        self,
        last_speaker: Agent,
    ) -> tuple[Agent | None, list[Agent], list[dict[str, Any]] | None]:
        # If self.speaker_selection_method is a callable, call it to get the next speaker.
        # If self.speaker_selection_method is a string, return it.
        speaker_selection_method = self.speaker_selection_method
        if isinstance(self.speaker_selection_method, Callable):
            selected_agent = self.speaker_selection_method(last_speaker, self)
            if selected_agent is None:
                raise NoEligibleSpeakerError(
                    "Custom speaker selection function returned None. Terminating conversation."
                )
            elif isinstance(selected_agent, Agent):
                if selected_agent in self.agents:
                    return selected_agent, self.agents, None
                else:
                    raise ValueError(
                        f"Custom speaker selection function returned an agent {selected_agent.name} not in the group chat."
                    )
            elif isinstance(selected_agent, str):
                # If returned a string, assume it is a speaker selection method
                speaker_selection_method = selected_agent
            else:
                raise ValueError(
                    f"Custom speaker selection function returned an object of type {type(selected_agent)} instead of Agent or str."
                )

        if speaker_selection_method.lower() not in self._VALID_SPEAKER_SELECTION_METHODS:
            raise ValueError(
                f"GroupChat speaker_selection_method is set to '{speaker_selection_method}'. "
                f"It should be one of {self._VALID_SPEAKER_SELECTION_METHODS} (case insensitive). "
            )

        # If provided a list, make sure the agent is in the list
        allow_repeat_speaker = (
            self.allow_repeat_speaker
            if isinstance(self.allow_repeat_speaker, bool) or self.allow_repeat_speaker is None
            else last_speaker in self.allow_repeat_speaker
        )

        agents = self.agents
        n_agents = len(agents)
        # Warn if GroupChat is underpopulated
        if n_agents < 2:
            raise ValueError(
                f"GroupChat is underpopulated with {n_agents} agents. "
                "Please add more agents to the GroupChat or use direct communication instead."
            )
        elif n_agents == 2 and speaker_selection_method.lower() != "round_robin" and allow_repeat_speaker:
            logger.warning(
                f"GroupChat is underpopulated with {n_agents} agents. "
                "Consider setting speaker_selection_method to 'round_robin' or allow_repeat_speaker to False, "
                "or use direct communication, unless repeated speaker is desired."
            )

        if (
            self.func_call_filter
            and self.messages
            and ("function_call" in self.messages[-1] or "tool_calls" in self.messages[-1])
        ):
            funcs = []
            if "function_call" in self.messages[-1]:
                funcs += [self.messages[-1]["function_call"]["name"]]
            if "tool_calls" in self.messages[-1]:
                funcs += [
                    tool["function"]["name"] for tool in self.messages[-1]["tool_calls"] if tool["type"] == "function"
                ]

            # find agents with the right function_map which contains the function name
            agents = [agent for agent in self.agents if agent.can_execute_function(funcs)]
            if len(agents) == 1:
                # only one agent can execute the function
                return agents[0], agents, None
            elif not agents:
                # find all the agents with function_map
                agents = [agent for agent in self.agents if agent.function_map]
                if len(agents) == 1:
                    return agents[0], agents, None
                elif not agents:
                    raise ValueError(
                        f"No agent can execute the function {', '.join(funcs)}. "
                        "Please check the function_map of the agents."
                    )
        # remove the last speaker from the list to avoid selecting the same speaker if allow_repeat_speaker is False
        agents = [agent for agent in agents if agent != last_speaker] if allow_repeat_speaker is False else agents

        # Filter agents with allowed_speaker_transitions_dict

        is_last_speaker_in_group = last_speaker in self.agents

        # this condition means last_speaker is a sink in the graph, then no agents are eligible
        if last_speaker not in self.allowed_speaker_transitions_dict and is_last_speaker_in_group:
            raise NoEligibleSpeakerError(
                f"Last speaker {last_speaker.name} is not in the allowed_speaker_transitions_dict."
            )
        # last_speaker is not in the group, so all agents are eligible
        elif last_speaker not in self.allowed_speaker_transitions_dict and not is_last_speaker_in_group:
            graph_eligible_agents = []
        else:
            # Extract agent names from the list of agents
            graph_eligible_agents = [
                agent for agent in agents if agent in self.allowed_speaker_transitions_dict[last_speaker]
            ]

        # If there is only one eligible agent, just return it to avoid the speaker selection prompt
        if len(graph_eligible_agents) == 1:
            return graph_eligible_agents[0], graph_eligible_agents, None

        # If there are no eligible agents, return None, which means all agents will be taken into consideration in the next step
        if len(graph_eligible_agents) == 0:
            graph_eligible_agents = None

        # Use the selected speaker selection method
        select_speaker_messages = None
        if speaker_selection_method.lower() == "manual":
            selected_agent = self.manual_select_speaker(graph_eligible_agents)
        elif speaker_selection_method.lower() == "round_robin":
            selected_agent = self.next_agent(last_speaker, graph_eligible_agents)
        elif speaker_selection_method.lower() == "random":
            selected_agent = self.random_select_speaker(graph_eligible_agents)
        else:  # auto
            selected_agent = None
            select_speaker_messages = self.messages.copy()
            # If last message is a tool call or function call, blank the call so the api doesn't throw
            if select_speaker_messages[-1].get("function_call", False):
                select_speaker_messages[-1] = dict(select_speaker_messages[-1], function_call=None)
            if select_speaker_messages[-1].get("tool_calls", False):
                select_speaker_messages[-1] = dict(select_speaker_messages[-1], tool_calls=None)
        return selected_agent, graph_eligible_agents, select_speaker_messages

    def select_speaker(self, last_speaker: Agent, selector: ConversableAgent) -> Agent:
        """Select the next speaker (with requery)."""
        # Prepare the list of available agents and select an agent if selection method allows (non-auto)
        selected_agent, agents, messages = self._prepare_and_select_agents(last_speaker)
        if selected_agent:
            return selected_agent
        elif self.speaker_selection_method == "manual":
            # An agent has not been selected while in manual mode, so move to the next agent
            return self.next_agent(last_speaker)

        # auto speaker selection with 2-agent chat
        return self._auto_select_speaker(last_speaker, selector, messages if messages else self.messages, agents)

    async def a_select_speaker(self, last_speaker: Agent, selector: ConversableAgent) -> Agent:
        """Select the next speaker (with requery), asynchronously."""
        selected_agent, agents, messages = self._prepare_and_select_agents(last_speaker)
        if selected_agent:
            return selected_agent
        elif self.speaker_selection_method == "manual":
            # An agent has not been selected while in manual mode, so move to the next agent
            return self.next_agent(last_speaker)

        # auto speaker selection with 2-agent chat
        return await self.a_auto_select_speaker(last_speaker, selector, messages if messages else self.messages, agents)

    def _finalize_speaker(self, last_speaker: Agent, final: bool, name: str, agents: list[Agent] | None) -> Agent:
        if not final:
            # the LLM client is None, thus no reply is generated. Use round robin instead.
            return self.next_agent(last_speaker, agents)

        # If exactly one agent is mentioned, use it. Otherwise, leave the OAI response unmodified
        mentions = self._mentioned_agents(name, agents)
        if len(mentions) == 1:
            name = next(iter(mentions))
        else:
            logger.warning(
                f"GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned:\n{name}"
            )

        # Return the result
        agent = self.agent_by_name(name)
        return agent if agent else self.next_agent(last_speaker, agents)

    def _register_client_from_config(self, agent: Agent, config: dict):
        model_client_cls_to_match = config.get("model_client_cls")
        if model_client_cls_to_match:
            if not self.select_speaker_auto_model_client_cls:
                raise ValueError(
                    "A custom model was detected in the config but no 'model_client_cls' "
                    "was supplied for registration in GroupChat."
                )

            if isinstance(self.select_speaker_auto_model_client_cls, list):
                # Register the first custom model client class matching the name specified in the config
                matching_model_cls = [
                    client_cls
                    for client_cls in self.select_speaker_auto_model_client_cls
                    if client_cls.__name__ == model_client_cls_to_match
                ]
                if len(set(matching_model_cls)) > 1:
                    raise RuntimeError(
                        f"More than one unique 'model_client_cls' with __name__ '{model_client_cls_to_match}'."
                    )
                if not matching_model_cls:
                    raise ValueError(
                        "No model's __name__ matches the model client class "
                        f"'{model_client_cls_to_match}' specified in select_speaker_auto_llm_config."
                    )
                select_speaker_auto_model_client_cls = matching_model_cls[0]
            else:
                # Register the only custom model client
                select_speaker_auto_model_client_cls = self.select_speaker_auto_model_client_cls

            agent.register_model_client(select_speaker_auto_model_client_cls)

    def _register_custom_model_clients(self, agent: ConversableAgent):
        if not self.select_speaker_auto_llm_config:
            return

        config_format_is_list = "config_list" in self.select_speaker_auto_llm_config
        if config_format_is_list:
            for config in self.select_speaker_auto_llm_config["config_list"]:
                self._register_client_from_config(agent, config)
        elif not config_format_is_list:
            self._register_client_from_config(agent, self.select_speaker_auto_llm_config)

    def _create_internal_agents(
        self, agents, max_attempts, messages, validate_speaker_name, selector: ConversableAgent | None = None
    ):
        checking_agent = ConversableAgent("checking_agent", default_auto_reply=max_attempts)

        # Register the speaker validation function with the checking agent
        checking_agent.register_reply(
            [ConversableAgent, None],
            reply_func=validate_speaker_name,  # Validate each response
            remove_other_reply_funcs=True,
        )

        # Override the selector's config if one was passed as a parameter to this class
        speaker_selection_llm_config = self.select_speaker_auto_llm_config or selector.llm_config

        if speaker_selection_llm_config is False:
            raise ValueError(
                "The group chat's internal speaker selection agent does not have an LLM configuration. Please provide a valid LLM config to the group chat's GroupChatManager or set it with the select_speaker_auto_llm_config parameter."
            )

        # Agent for selecting a single agent name from the response
        speaker_selection_agent = ConversableAgent(
            "speaker_selection_agent",
            system_message=self.select_speaker_msg(agents),
            chat_messages={checking_agent: messages},
            llm_config=speaker_selection_llm_config,
            human_input_mode="NEVER",
            # Suppresses some extra terminal outputs, outputs will be handled by select_speaker_auto_verbose
        )

        # Register any custom model passed in select_speaker_auto_llm_config with the speaker_selection_agent
        self._register_custom_model_clients(speaker_selection_agent)

        return checking_agent, speaker_selection_agent

    def _auto_select_speaker(
        self,
        last_speaker: Agent,
        selector: ConversableAgent,
        messages: list[dict[str, Any]] | None,
        agents: list[Agent] | None,
    ) -> Agent:
        """Selects next speaker for the "auto" speaker selection method. Utilises its own two-agent chat to determine the next speaker and supports requerying.

        Speaker selection for "auto" speaker selection method:
        1. Create a two-agent chat with a speaker selector agent and a speaker validator agent, like a nested chat
        2. Inject the group messages into the new chat
        3. Run the two-agent chat, evaluating the result of response from the speaker selector agent:
            - If a single agent is provided then we return it and finish. If not, we add an additional message to this nested chat in an attempt to guide the LLM to a single agent response
        4. Chat continues until a single agent is nominated or there are no more attempts left
        5. If we run out of turns and no single agent can be determined, the next speaker in the list of agents is returned

        Args:
            last_speaker: The previous speaker in the group chat
            selector: The ConversableAgent that initiated the speaker selection
            messages: Current chat messages
            agents: Valid list of agents for speaker selection

        Returns:
            A counter for mentioned agents.
        """
        # If no agents are passed in, assign all the group chat's agents
        if agents is None:
            agents = self.agents

        # The maximum number of speaker selection attempts (including requeries)
        # is the initial speaker selection attempt plus the maximum number of retries.
        # We track these and use them in the validation function as we can't
        # access the max_turns from within validate_speaker_name.
        max_attempts = 1 + self.max_retries_for_selecting_speaker
        attempts_left = max_attempts
        attempt = 0

        # Registered reply function for checking_agent, checks the result of the response for agent names
        def validate_speaker_name(recipient, messages, sender, config) -> tuple[bool, str | dict[str, Any] | None]:
            # The number of retries left, starting at max_retries_for_selecting_speaker
            nonlocal attempts_left
            nonlocal attempt

            attempt = attempt + 1
            attempts_left = attempts_left - 1

            return self._validate_speaker_name(recipient, messages, sender, config, attempts_left, attempt, agents)

        # Two-agent chat for speaker selection

        # Agent for checking the response from the speaker_select_agent
        checking_agent, speaker_selection_agent = self._create_internal_agents(
            agents, max_attempts, messages, validate_speaker_name, selector
        )

        # Create the starting message
        if self.select_speaker_prompt_template is not None:
            start_message = {
                "content": self.select_speaker_prompt(agents),
                "name": "checking_agent",
                "override_role": self.role_for_select_speaker_messages,
            }
        else:
            start_message = messages[-1]

        # Add the message transforms, if any, to the speaker selection agent
        if self._speaker_selection_transforms is not None:
            self._speaker_selection_transforms.add_to_agent(speaker_selection_agent)

        # Run the speaker selection chat
        result = checking_agent.initiate_chat(
            speaker_selection_agent,
            cache=None,  # don't use caching for the speaker selection chat
            message=start_message,
            max_turns=2
            * max(1, max_attempts),  # Limiting the chat to the number of attempts, including the initial one
            clear_history=False,
            silent=not self.select_speaker_auto_verbose,  # Base silence on the verbose attribute
        )

        return self._process_speaker_selection_result(result, last_speaker, agents)

    async def a_auto_select_speaker(
        self,
        last_speaker: Agent,
        selector: ConversableAgent,
        messages: list[dict[str, Any]] | None,
        agents: list[Agent] | None,
    ) -> Agent:
        """(Asynchronous) Selects next speaker for the "auto" speaker selection method. Utilises its own two-agent chat to determine the next speaker and supports requerying.

        Speaker selection for "auto" speaker selection method:
        1. Create a two-agent chat with a speaker selector agent and a speaker validator agent, like a nested chat
        2. Inject the group messages into the new chat
        3. Run the two-agent chat, evaluating the result of response from the speaker selector agent:
            - If a single agent is provided then we return it and finish. If not, we add an additional message to this nested chat in an attempt to guide the LLM to a single agent response
        4. Chat continues until a single agent is nominated or there are no more attempts left
        5. If we run out of turns and no single agent can be determined, the next speaker in the list of agents is returned

        Args:
            last_speaker: The previous speaker in the group chat
            selector: The ConversableAgent that initiated the speaker selection
            messages: Current chat messages
            agents: Valid list of agents for speaker selection

        Returns:
            A counter for mentioned agents.
        """
        # If no agents are passed in, assign all the group chat's agents
        if agents is None:
            agents = self.agents

        # The maximum number of speaker selection attempts (including requeries)
        # We track these and use them in the validation function as we can't
        # access the max_turns from within validate_speaker_name
        max_attempts = 1 + self.max_retries_for_selecting_speaker
        attempts_left = max_attempts
        attempt = 0

        # Registered reply function for checking_agent, checks the result of the response for agent names
        def validate_speaker_name(recipient, messages, sender, config) -> tuple[bool, str | dict[str, Any] | None]:
            # The number of retries left, starting at max_retries_for_selecting_speaker
            nonlocal attempts_left
            nonlocal attempt

            attempt = attempt + 1
            attempts_left = attempts_left - 1

            return self._validate_speaker_name(recipient, messages, sender, config, attempts_left, attempt, agents)

        # Two-agent chat for speaker selection

        # Agent for checking the response from the speaker_select_agent
        checking_agent, speaker_selection_agent = self._create_internal_agents(
            agents, max_attempts, messages, validate_speaker_name, selector
        )

        # Create the starting message
        if self.select_speaker_prompt_template is not None:
            start_message = {
                "content": self.select_speaker_prompt(agents),
                "override_role": self.role_for_select_speaker_messages,
            }
        else:
            start_message = messages[-1]

        # Add the message transforms, if any, to the speaker selection agent
        if self._speaker_selection_transforms is not None:
            self._speaker_selection_transforms.add_to_agent(speaker_selection_agent)

        # Run the speaker selection chat
        result = await checking_agent.a_initiate_chat(
            speaker_selection_agent,
            cache=None,  # don't use caching for the speaker selection chat
            message=start_message,
            max_turns=2
            * max(1, max_attempts),  # Limiting the chat to the number of attempts, including the initial one
            clear_history=False,
            silent=not self.select_speaker_auto_verbose,  # Base silence on the verbose attribute
        )

        return self._process_speaker_selection_result(result, last_speaker, agents)

    def _validate_speaker_name(
        self, recipient, messages, sender, config, attempts_left, attempt, agents
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Validates the speaker response for each round in the internal 2-agent
        chat within the  auto select speaker method.

        Used by auto_select_speaker and a_auto_select_speaker.
        """
        # Validate the speaker name selected
        if messages and (name := messages[-1].get("content")):
            mentions = self._mentioned_agents(name.strip(), agents)
        else:
            mentions = []
        no_of_mentions = len(mentions)

        # Output the query and requery results
        if self.select_speaker_auto_verbose:
            iostream = IOStream.get_default()
            if no_of_mentions == 1:
                # Success on retry, we have just one name mentioned
                iostream.send(
                    SpeakerAttemptSuccessfulEvent(
                        mentions=mentions,
                        attempt=attempt,
                        attempts_left=attempts_left,
                        select_speaker_auto_verbose=self.select_speaker_auto_verbose,
                    )
                )
            elif no_of_mentions == 1:
                iostream.send(
                    SpeakerAttemptFailedMultipleAgentsEvent(
                        mentions=mentions,
                        attempt=attempt,
                        attempts_left=attempts_left,
                        select_speaker_auto_verbose=self.select_speaker_auto_verbose,
                    )
                )
            else:
                iostream.send(
                    SpeakerAttemptFailedNoAgentsEvent(
                        mentions=mentions,
                        attempt=attempt,
                        attempts_left=attempts_left,
                        select_speaker_auto_verbose=self.select_speaker_auto_verbose,
                    )
                )

        if no_of_mentions == 1:
            # Success on retry, we have just one name mentioned
            selected_agent_name = next(iter(mentions))

            # Add the selected agent to the response so we can return it
            messages.append({"role": "user", "content": f"[AGENT SELECTED]{selected_agent_name}"})

        elif no_of_mentions > 1:
            # More than one name on requery so add additional reminder prompt for next retry

            if attempts_left:
                # Message to return to the chat for the next attempt
                agentlist = f"{[agent.name for agent in agents]}"

                return True, {
                    "content": self.select_speaker_auto_multiple_template.format(agentlist=agentlist),
                    "name": "checking_agent",
                    "override_role": self.role_for_select_speaker_messages,
                }
            else:
                # Final failure, no attempts left
                messages.append({
                    "role": "user",
                    "content": f"[AGENT SELECTION FAILED]Select speaker attempt #{attempt} of {attempt + attempts_left} failed as it returned multiple names.",
                })

        else:
            # No names at all on requery so add additional reminder prompt for next retry

            if attempts_left:
                # Message to return to the chat for the next attempt
                agentlist = f"{[agent.name for agent in agents]}"

                return True, {
                    "content": self.select_speaker_auto_none_template.format(agentlist=agentlist),
                    "name": "checking_agent",
                    "override_role": self.role_for_select_speaker_messages,
                }
            else:
                # Final failure, no attempts left
                messages.append({
                    "role": "user",
                    "content": f"[AGENT SELECTION FAILED]Select speaker attempt #{attempt} of {attempt + attempts_left} failed as it did not include any agent names.",
                })

        return True, None

    def _process_speaker_selection_result(self, result, last_speaker: ConversableAgent, agents: list[Agent] | None):
        """Checks the result of the auto_select_speaker function, returning the
        agent to speak.

        Used by auto_select_speaker and a_auto_select_speaker.
        """
        if len(result.chat_history) > 0:
            # Use the final message, which will have the selected agent or reason for failure
            final_message = result.chat_history[-1]["content"]

            if "[AGENT SELECTED]" in final_message:
                # Have successfully selected an agent, return it
                return self.agent_by_name(final_message.replace("[AGENT SELECTED]", ""))

            else:  # "[AGENT SELECTION FAILED]"
                # Failed to select an agent, so we'll select the next agent in the list
                next_agent = self.next_agent(last_speaker, agents)

                # No agent, return the failed reason
                return next_agent

    def _participant_roles(self, agents: list[Agent] = None) -> str:
        # Default to all agents registered
        if agents is None:
            agents = self.agents

        roles = []
        for agent in agents:
            if agent.description.strip() == "":
                logger.warning(
                    f"The agent '{agent.name}' has an empty description, and may not work well with GroupChat."
                )
            roles.append(f"{agent.name}: {agent.description}".strip())
        return "\n".join(roles)

    def _mentioned_agents(self, message_content: str | list, agents: list[Agent] | None) -> dict:
        """Counts the number of times each agent is mentioned in the provided message content.
        Agent names will match under any of the following conditions (all case-sensitive):
        - Exact name match
        - If the agent name has underscores it will match with spaces instead (e.g. 'Story_writer' == 'Story writer')
        - If the agent name has underscores it will match with '\\_' instead of '_' (e.g. 'Story_writer' == 'Story\\_writer')

        Args:
            message_content (Union[str, List]): The content of the message, either as a single string or a list of strings.
            agents (List[Agent]): A list of Agent objects, each having a 'name' attribute to be searched in the message content.

        Returns:
            Dict: a counter for mentioned agents.
        """
        if agents is None:
            agents = self.agents

        # Cast message content to str
        if isinstance(message_content, dict):
            message_content = message_content["content"]
        message_content = content_str(message_content)

        mentions = {}
        for agent in agents:
            # Finds agent mentions, taking word boundaries into account,
            # accommodates escaping underscores and underscores as spaces
            regex = (
                r"(?<=\W)("
                + re.escape(agent.name)
                + r"|"
                + re.escape(agent.name.replace("_", " "))
                + r"|"
                + re.escape(agent.name.replace("_", r"\_"))
                + r")(?=\W)"
            )
            count = len(re.findall(regex, f" {message_content} "))  # Pad the message to help with matching
            if count > 0:
                mentions[agent.name] = count
        return mentions

    def _run_input_guardrails(
        self,
        agent: "ConversableAgent",
        messages: list[dict[str, Any]] | None = None,
    ) -> str | None:
        """Run input guardrails for an agent before the reply is generated.

        Args:
            agent (ConversableAgent): The agent whose input guardrails to run.
            messages (Optional[list[dict[str, Any]]]): The messages to check against the guardrails.
        """
        for guardrail in agent.input_guardrails:
            guardrail_result = guardrail.check(context=messages)

            if guardrail_result.activated:
                guardrail.target.activate_target(self)
                return f"{guardrail.activation_message}\nJustification: {guardrail_result.justification}"
        return None

    def _run_output_guardrails(self, agent: "ConversableAgent", reply: str) -> None:
        """Run output guardrails for an agent after the reply is generated.

        Args:
            agent (ConversableAgent): The agent whose output guardrails to run.
            reply (str): The reply generated by the agent.
        """
        for guardrail in agent.output_guardrails:
            guardrail_result = guardrail.check(context=reply)

            if guardrail_result.activated:
                guardrail.target.activate_target(self)
                return f"{guardrail.activation_message}\nJustification: {guardrail_result.justification}"
        return None

    def _run_inter_agent_guardrails(
        self,
        *,
        src_agent_name: str,
        dst_agent_name: str,
        message_content: str,
    ) -> str | None:
        """Run policy-driven inter-agent guardrails, if any are configured.

        Returns optional replacement content when a guardrail triggers.
        """
        guardrails = getattr(self, "_inter_agent_guardrails", None)
        if not guardrails:
            return None
        for gr in guardrails:
            reply = gr.check_and_act(
                src_agent_name=src_agent_name,
                dst_agent_name=dst_agent_name,
                message_content=message_content,
            )
            if reply is not None:
                return reply
        return None


@export_module("autogen")
class GroupChatManager(ConversableAgent):
    """(In preview) A chat manager agent that can manage a group chat of multiple agents."""

    def __init__(
        self,
        groupchat: GroupChat,
        name: str | None = "chat_manager",
        # unlimited consecutive auto reply by default
        max_consecutive_auto_reply: int | None = sys.maxsize,
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "NEVER",
        system_message: str | list | None = "Group chat manager.",
        silent: bool = False,
        **kwargs: Any,
    ):
        if (
            kwargs.get("llm_config")
            and isinstance(kwargs["llm_config"], dict)
            and (kwargs["llm_config"].get("functions") or kwargs["llm_config"].get("tools"))
        ):
            raise ValueError(
                "GroupChatManager is not allowed to make function/tool calls. Please remove the 'functions' or 'tools' config in 'llm_config' you passed in."
            )

        super().__init__(
            name=name,
            max_consecutive_auto_reply=max_consecutive_auto_reply,
            human_input_mode=human_input_mode,
            system_message=system_message,
            **kwargs,
        )
        if logging_enabled():
            log_new_agent(self, locals())
        # Store groupchat
        self._groupchat = groupchat

        self._last_speaker = None
        self._silent = silent

        # Order of register_reply is important.
        # Allow sync chat if initiated using initiate_chat
        self.register_reply(Agent, GroupChatManager.run_chat, config=groupchat, reset_config=GroupChat.reset)
        # Allow async chat if initiated using a_initiate_chat
        self.register_reply(
            Agent,
            GroupChatManager.a_run_chat,
            config=groupchat,
            reset_config=GroupChat.reset,
            ignore_async_in_sync_chat=True,
        )

    @property
    def groupchat(self) -> GroupChat:
        """Returns the group chat managed by the group chat manager."""
        return self._groupchat

    def chat_messages_for_summary(self, agent: Agent) -> list[dict[str, Any]]:
        """The list of messages in the group chat as a conversation to summarize.
        The agent is ignored.
        """
        return self._groupchat.messages

    def _prepare_chat(
        self,
        recipient: ConversableAgent,
        clear_history: bool,
        prepare_recipient: bool = True,
        reply_at_receive: bool = True,
    ) -> None:
        super()._prepare_chat(recipient, clear_history, prepare_recipient, reply_at_receive)

        if clear_history:
            self._groupchat.reset()

        for agent in self._groupchat.agents:
            if (recipient != agent or prepare_recipient) and isinstance(agent, ConversableAgent):
                agent._prepare_chat(self, clear_history, False, reply_at_receive)

    @property
    def last_speaker(self) -> Agent:
        """Return the agent who sent the last message to group chat manager.

        In a group chat, an agent will always send a message to the group chat manager, and the group chat manager will
        send the message to all other agents in the group chat. So, when an agent receives a message, it will always be
        from the group chat manager. With this property, the agent receiving the message can know who actually sent the
        message.

        Example:
        ```python
        from autogen import ConversableAgent
        from autogen import GroupChat, GroupChatManager


        def print_messages(recipient, messages, sender, config):
            # Print the message immediately
            print(f"Sender: {sender.name} | Recipient: {recipient.name} | Message: {messages[-1].get('content')}")
            print(f"Real Sender: {sender.last_speaker.name}")
            assert sender.last_speaker.name in messages[-1].get("content")
            return False, None  # Required to ensure the agent communication flow continues


        agent_a = ConversableAgent("agent A", default_auto_reply="I'm agent A.")
        agent_b = ConversableAgent("agent B", default_auto_reply="I'm agent B.")
        agent_c = ConversableAgent("agent C", default_auto_reply="I'm agent C.")
        for agent in [agent_a, agent_b, agent_c]:
            agent.register_reply([ConversableAgent, None], reply_func=print_messages, config=None)
        group_chat = GroupChat(
            [agent_a, agent_b, agent_c],
            messages=[],
            max_round=6,
            speaker_selection_method="random",
            allow_repeat_speaker=True,
        )
        chat_manager = GroupChatManager(group_chat)
        groupchat_result = agent_a.initiate_chat(chat_manager, message="Hi, there, I'm agent A.")
        ```
        """
        return self._last_speaker

    def run_chat(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: GroupChat | None = None,
    ) -> tuple[bool, str | None]:
        """Run a group chat."""
        iostream = IOStream.get_default()

        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        speaker = sender
        groupchat = config
        send_introductions = getattr(groupchat, "send_introductions", False)
        silent = getattr(self, "_silent", False)
        termination_reason = None

        if send_introductions:
            # Broadcast the intro
            intro = groupchat.introductions_msg()
            for agent in groupchat.agents:
                self.send(intro, agent, request_reply=False, silent=True)
            # NOTE: We do not also append to groupchat.messages,
            # since groupchat handles its own introductions

        if self.client_cache is not None:
            for a in groupchat.agents:
                a.previous_cache = a.client_cache
                a.client_cache = self.client_cache
        for i in range(groupchat.max_round):
            self._last_speaker = speaker
            groupchat.append(message, speaker)
            # broadcast the message to all agents except the speaker
            for agent in groupchat.agents:
                if agent != speaker:
                    inter_reply = groupchat._run_inter_agent_guardrails(
                        src_agent_name=speaker.name,
                        dst_agent_name=agent.name,
                        message_content=message,
                    )
                    if inter_reply is not None:
                        replacement = (
                            {"content": inter_reply, "name": speaker.name}
                            if not isinstance(inter_reply, dict)
                            else inter_reply
                        )
                        self.send(replacement, agent, request_reply=False, silent=True)
                    else:
                        self.send(message, agent, request_reply=False, silent=True)

            if self._is_termination_msg(message):
                # The conversation is over
                termination_reason = f"Termination message condition on the GroupChatManager '{self.name}' met"
                break
            elif i == groupchat.max_round - 1:
                # It's the last round
                termination_reason = f"Maximum rounds ({groupchat.max_round}) reached"
                break
            try:
                # select the next speaker
                speaker = groupchat.select_speaker(speaker, self)
                if not silent:
                    iostream = IOStream.get_default()
                    iostream.send(GroupChatRunChatEvent(speaker=speaker, silent=silent))

                guardrails_activated = False
                guardrails_reply = groupchat._run_input_guardrails(speaker, speaker._oai_messages[self])

                if guardrails_reply is not None:
                    # if a guardrail has been activated, then the next target has been set and the guardrail reply will be sent
                    guardrails_activated = True
                    reply = guardrails_reply
                else:
                    # let the speaker speak
                    reply = speaker.generate_reply(sender=self)
            except KeyboardInterrupt:
                # let the admin agent speak if interrupted
                if groupchat.admin_name in groupchat.agent_names:
                    # admin agent is one of the participants
                    speaker = groupchat.agent_by_name(groupchat.admin_name)
                    reply = speaker.generate_reply(sender=self)
                else:
                    # admin agent is not found in the participants
                    raise
            except NoEligibleSpeakerError:
                # No eligible speaker, terminate the conversation
                termination_reason = "No next speaker selected"
                break

            if reply is None:
                # no reply is generated, exit the chat
                termination_reason = "No reply generated"
                break

            if not guardrails_activated:
                # if the input guardrails were not activated, and the agent returned a reply
                guardrails_reply = groupchat._run_output_guardrails(speaker, reply)

                if guardrails_reply is not None:
                    # if a guardrail has been activated, then the next target has been set and the guardrail reply will be sent
                    guardrails_activated = True
                    reply = guardrails_reply

            # check for "clear history" phrase in reply and activate clear history function if found
            if (
                groupchat.enable_clear_history
                and isinstance(reply, dict)
                and reply["content"]
                and "CLEAR HISTORY" in reply["content"].upper()
            ):
                reply["content"] = self.clear_agents_history(reply, groupchat)

            # The speaker sends the message without requesting a reply
            speaker.send(reply, self, request_reply=False, silent=silent)
            message = self.last_message(speaker)
        if self.client_cache is not None:
            for a in groupchat.agents:
                a.client_cache = a.previous_cache
                a.previous_cache = None

        if termination_reason:
            iostream.send(
                TerminationEvent(
                    termination_reason=termination_reason, sender=self, recipient=speaker if speaker else None
                )
            )

        return True, None

    async def a_run_chat(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: GroupChat | None = None,
    ):
        """Run a group chat asynchronously."""
        iostream = IOStream.get_default()

        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        speaker = sender
        groupchat = config
        send_introductions = getattr(groupchat, "send_introductions", False)
        silent = getattr(self, "_silent", False)
        termination_reason = None

        if send_introductions:
            # Broadcast the intro
            intro = groupchat.introductions_msg()
            for agent in groupchat.agents:
                await self.a_send(intro, agent, request_reply=False, silent=True)
            # NOTE: We do not also append to groupchat.messages,
            # since groupchat handles its own introductions

        if self.client_cache is not None:
            for a in groupchat.agents:
                a.previous_cache = a.client_cache
                a.client_cache = self.client_cache
        for i in range(groupchat.max_round):
            groupchat.append(message, speaker)
            self._last_speaker = speaker

            if self._is_termination_msg(message):
                # The conversation is over
                termination_reason = f"Termination message condition on the GroupChatManager '{self.name}' met"
                break

            # broadcast the message to all agents except the speaker
            for agent in groupchat.agents:
                if agent != speaker:
                    await self.a_send(message, agent, request_reply=False, silent=True)
            if i == groupchat.max_round - 1:
                # the last round
                termination_reason = f"Maximum rounds ({groupchat.max_round}) reached"
                break
            try:
                # select the next speaker
                speaker = await groupchat.a_select_speaker(speaker, self)
                if not silent:
                    iostream.send(GroupChatRunChatEvent(speaker=speaker, silent=silent))

                guardrails_activated = False
                guardrails_reply = groupchat._run_input_guardrails(speaker, speaker._oai_messages[self])

                if guardrails_reply is not None:
                    # if a guardrail has been activated, then the next target has been set and the guardrail reply will be sent
                    guardrails_activated = True
                    reply = guardrails_reply
                else:
                    # let the speaker speak
                    reply = await speaker.a_generate_reply(sender=self)
            except KeyboardInterrupt:
                # let the admin agent speak if interrupted
                if groupchat.admin_name in groupchat.agent_names:
                    # admin agent is one of the participants
                    speaker = groupchat.agent_by_name(groupchat.admin_name)
                    reply = await speaker.a_generate_reply(sender=self)
                else:
                    # admin agent is not found in the participants
                    raise
            except NoEligibleSpeakerError:
                # No eligible speaker, terminate the conversation
                termination_reason = "No next speaker selected"
                break

            if reply is None:
                # no reply is generated, exit the chat
                termination_reason = "No reply generated"
                break

            if not guardrails_activated:
                # if the input guardrails were not activated, and the agent returned a reply
                guardrails_reply = groupchat._run_output_guardrails(speaker, reply)

                if guardrails_reply is not None:
                    # if a guardrail has been activated, then the next target has been set and the guardrail reply will be sent
                    guardrails_activated = True
                    reply = guardrails_reply

            # check for "clear history" phrase in reply and activate clear history function if found
            if (
                groupchat.enable_clear_history
                and isinstance(reply, dict)
                and reply["content"]
                and "CLEAR HISTORY" in reply["content"].upper()
            ):
                reply["content"] = self.clear_agents_history(reply, groupchat)

            # The speaker sends the message without requesting a reply
            await speaker.a_send(reply, self, request_reply=False, silent=silent)
            message = self.last_message(speaker)
        if self.client_cache is not None:
            for a in groupchat.agents:
                a.client_cache = a.previous_cache
                a.previous_cache = None

        if termination_reason:
            iostream.send(
                TerminationEvent(
                    termination_reason=termination_reason, sender=self, recipient=speaker if speaker else None
                )
            )

        return True, None

    def resume(
        self,
        messages: list[dict[str, Any]] | str,
        remove_termination_string: str | Callable[[str], str] | None = None,
        silent: bool | None = False,
    ) -> tuple[ConversableAgent, dict[str, Any]]:
        """Resumes a group chat using the previous messages as a starting point. Requires the agents, group chat, and group chat manager to be established
        as per the original group chat.

        Args:
            messages: The content of the previous chat's messages, either as a Json string or a list of message dictionaries.
            remove_termination_string: Remove the termination string from the last message to prevent immediate termination
                If a string is provided, this string will be removed from last message.
                If a function is provided, the last message will be passed to this function.
            silent: (Experimental) whether to print the messages for this conversation. Default is False.

        Returns:
            A tuple containing the last agent who spoke and their message
        """
        # Convert messages from string to messages list, if needed
        if isinstance(messages, str):
            messages = self.messages_from_string(messages)
        elif isinstance(messages, list) and all(isinstance(item, dict) for item in messages):
            messages = copy.deepcopy(messages)
        else:
            raise Exception("Messages is not of type str or List[Dict]")

        # Clean up the objects, ensuring there are no messages in the agents and group chat

        # Clear agent message history
        for agent in self._groupchat.agents:
            if isinstance(agent, ConversableAgent):
                agent.clear_history()

        # Clear Manager message history
        self.clear_history()

        # Clear GroupChat messages
        self._groupchat.reset()

        # Validation of message and agents

        try:
            self._valid_resume_messages(messages)
        except:
            raise

        # Load the messages into the group chat
        for i, message in enumerate(messages):
            if "name" in message:
                message_speaker_agent = self._groupchat.agent_by_name(message["name"])
            else:
                # If there's no name, assign the group chat manager (this is an indication the ChatResult messages was used instead of groupchat.messages as state)
                message_speaker_agent = self
                message["name"] = self.name

            # If it wasn't an agent speaking, it may be the manager
            if not message_speaker_agent and message["name"] == self.name:
                message_speaker_agent = self

            # Add previous messages to each agent (except the last message, as we'll kick off the conversation with it)
            if i != len(messages) - 1:
                for agent in self._groupchat.agents:
                    if agent.name == message["name"]:
                        # An agent`s message is sent to the Group Chat Manager
                        agent.send(message, self, request_reply=False, silent=True)
                    else:
                        # Otherwise, messages are sent from the Group Chat Manager to the agent
                        self.send(message, agent, request_reply=False, silent=True)

                # Add previous message to the new groupchat, if it's an admin message the name may not match so add the message directly
                if message_speaker_agent:
                    self._groupchat.append(message, message_speaker_agent)
                else:
                    self._groupchat.messages.append(message)

            # Last speaker agent
            last_speaker_name = message["name"]

            # Last message to check for termination (we could avoid this by ignoring termination check for resume in the future)
            last_message = message

        # Get last speaker as an agent
        previous_last_agent = self._groupchat.agent_by_name(name=last_speaker_name)

        # If we didn't match a last speaker agent, we check that it's the group chat's admin name and assign the manager, if so
        if not previous_last_agent and (
            last_speaker_name == self._groupchat.admin_name or last_speaker_name == self.name
        ):
            previous_last_agent = self

        # Termination removal and check
        self._process_resume_termination(remove_termination_string, messages)

        if not silent:
            iostream = IOStream.get_default()
            iostream.send(GroupChatResumeEvent(last_speaker_name=last_speaker_name, events=messages, silent=silent))

        # Update group chat settings for resuming
        self._groupchat.send_introductions = False

        return previous_last_agent, last_message

    async def a_resume(
        self,
        messages: list[dict[str, Any]] | str,
        remove_termination_string: str | Callable[[str], str] | None = None,
        silent: bool | None = False,
    ) -> tuple[ConversableAgent, dict[str, Any]]:
        """Resumes a group chat using the previous messages as a starting point, asynchronously. Requires the agents, group chat, and group chat manager to be established
        as per the original group chat.

        Args:
            messages: The content of the previous chat's messages, either as a Json string or a list of message dictionaries.
            remove_termination_string: Remove the termination string from the last message to prevent immediate termination
                If a string is provided, this string will be removed from last message.
                If a function is provided, the last message will be passed to this function, and the function returns the string after processing.
            silent: (Experimental) whether to print the messages for this conversation. Default is False.

        Returns:
            A tuple containing the last agent who spoke and their message
        """
        # Convert messages from string to messages list, if needed
        if isinstance(messages, str):
            messages = self.messages_from_string(messages)
        elif isinstance(messages, list) and all(isinstance(item, dict) for item in messages):
            messages = copy.deepcopy(messages)
        else:
            raise Exception("Messages is not of type str or List[Dict]")

        # Clean up the objects, ensuring there are no messages in the agents and group chat

        # Clear agent message history
        for agent in self._groupchat.agents:
            if isinstance(agent, ConversableAgent):
                agent.clear_history()

        # Clear Manager message history
        self.clear_history()

        # Clear GroupChat messages
        self._groupchat.reset()

        # Validation of message and agents

        try:
            self._valid_resume_messages(messages)
        except:
            raise

        # Load the messages into the group chat
        for i, message in enumerate(messages):
            if "name" in message:
                message_speaker_agent = self._groupchat.agent_by_name(message["name"])
            else:
                # If there's no name, assign the group chat manager (this is an indication the ChatResult messages was used instead of groupchat.messages as state)
                message_speaker_agent = self
                message["name"] = self.name

            # If it wasn't an agent speaking, it may be the manager
            if not message_speaker_agent and message["name"] == self.name:
                message_speaker_agent = self

            # Add previous messages to each agent (except the last message, as we'll kick off the conversation with it)
            if i != len(messages) - 1:
                for agent in self._groupchat.agents:
                    if agent.name == message["name"]:
                        # An agent`s message is sent to the Group Chat Manager
                        await agent.a_send(message, self, request_reply=False, silent=True)
                    else:
                        # Otherwise, messages are sent from the Group Chat Manager to the agent
                        await self.a_send(message, agent, request_reply=False, silent=True)

                # Add previous message to the new groupchat, if it's an admin message the name may not match so add the message directly
                if message_speaker_agent:
                    self._groupchat.append(message, message_speaker_agent)
                else:
                    self._groupchat.messages.append(message)

            # Last speaker agent
            last_speaker_name = message["name"]

            # Last message to check for termination (we could avoid this by ignoring termination check for resume in the future)
            last_message = message

        # Get last speaker as an agent
        previous_last_agent = self._groupchat.agent_by_name(name=last_speaker_name)

        # If we didn't match a last speaker agent, we check that it's the group chat's admin name and assign the manager, if so
        if not previous_last_agent and (
            last_speaker_name == self._groupchat.admin_name or last_speaker_name == self.name
        ):
            previous_last_agent = self

        # Termination removal and check
        self._process_resume_termination(remove_termination_string, messages)

        if not silent:
            iostream = IOStream.get_default()
            iostream.send(GroupChatResumeEvent(last_speaker_name=last_speaker_name, events=messages, silent=silent))

        # Update group chat settings for resuming
        self._groupchat.send_introductions = False

        return previous_last_agent, last_message

    def _valid_resume_messages(self, messages: list[dict[str, Any]]):
        """Validates the messages used for resuming

        Args:
            messages (List[Dict]): list of messages to resume with

        Returns:
            - bool: Whether they are valid for resuming
        """
        # Must have messages to start with, otherwise they should run run_chat
        if not messages:
            raise Exception(
                "Cannot resume group chat as no messages were provided. Use GroupChatManager.run_chat or ConversableAgent.initiate_chat to start a new chat."
            )

        # Check that all agents in the chat messages exist in the group chat
        for message in messages:
            if message.get("name") and (
                not self._groupchat.agent_by_name(message["name"])
                and not message["name"] == self._groupchat.admin_name  # ignore group chat's name
                and not message["name"] == self.name  # ignore group chat manager's name
            ):
                raise Exception(f"Agent name in message doesn't exist as agent in group chat: {message['name']}")

    def _process_resume_termination(
        self, remove_termination_string: str | Callable[[str], str], messages: list[dict[str, Any]]
    ):
        """Removes termination string, if required, and checks if termination may occur.

        Args:
            remove_termination_string: Remove the termination string from the last message to prevent immediate termination
                If a string is provided, this string will be removed from last message.
                If a function is provided, the last message will be passed to this function, and the function returns the string after processing.
            messages: List of chat messages

        Returns:
            None
        """
        last_message = messages[-1]

        # Replace any given termination string in the last message
        if isinstance(remove_termination_string, str):

            def _remove_termination_string(content: str) -> str:
                return content.replace(remove_termination_string, "")

        else:
            _remove_termination_string = remove_termination_string

        if _remove_termination_string and messages[-1].get("content"):
            messages[-1]["content"] = _remove_termination_string(messages[-1]["content"])

        # Check if the last message meets termination (if it has one)
        if self._is_termination_msg and self._is_termination_msg(last_message):
            logger.warning("WARNING: Last message meets termination criteria and this may terminate the chat.")

    def messages_from_string(self, message_string: str) -> list[dict[str, Any]]:
        """Reads the saved state of messages in Json format for resume and returns as a messages list

        Args:
            message_string: Json string, the saved state

        Returns:
            A list of messages
        """
        try:
            state = json.loads(message_string)
        except json.JSONDecodeError:
            raise Exception("Messages string is not a valid JSON string")

        return state

    def messages_to_string(self, messages: list[dict[str, Any]]) -> str:
        """Converts the provided messages into a Json string that can be used for resuming the chat.
        The state is made up of a list of messages

        Args:
            messages: set of messages to convert to a string

        Returns:
            A JSON representation of the messages which can be persisted for resuming later
        """
        return json.dumps(messages)

    def _raise_exception_on_async_reply_functions(self) -> None:
        """Raise an exception if any async reply functions are registered.

        Raises:
            RuntimeError: if any async reply functions are registered.
        """
        super()._raise_exception_on_async_reply_functions()

        for agent in self._groupchat.agents:
            agent._raise_exception_on_async_reply_functions()

    def clear_agents_history(self, reply: dict[str, Any], groupchat: GroupChat) -> str:
        """Clears history of messages for all agents or a selected one. Can preserve a selected number of last messages.\n
        \n
        This function is called when the user manually provides the "clear history" phrase in their reply.\n
        When "clear history" is provided, the history of messages for all agents is cleared.\n
        When "clear history `<agent_name>`" is provided, the history of messages for the selected agent is cleared.\n
        When "clear history `<nr_of_messages_to_preserve>`" is provided, the history of messages for all agents is cleared\n
        except for the last `<nr_of_messages_to_preserve>` messages.\n
        When "clear history `<agent_name>` `<nr_of_messages_to_preserve>`" is provided, the history of messages for the selected\n
        agent is cleared except for the last `<nr_of_messages_to_preserve>` messages.\n
        The phrase "clear history" and optional arguments are cut out from the reply before it is passed to the chat.\n
        \n
        Args:\n
            reply (dict): reply message dict to analyze.\n
            groupchat (GroupChat): GroupChat object.\n
        """
        iostream = IOStream.get_default()

        reply_content = reply["content"]
        # Split the reply into words
        words = reply_content.split()
        # Find the position of "clear" to determine where to start processing
        clear_word_index = next(i for i in reversed(range(len(words))) if words[i].upper() == "CLEAR")
        # Extract potential agent name and steps
        words_to_check = words[clear_word_index + 2 : clear_word_index + 4]
        nr_messages_to_preserve = None
        nr_messages_to_preserve_provided = False
        agent_to_memory_clear = None

        for word in words_to_check:
            if word.isdigit():
                nr_messages_to_preserve = int(word)
                nr_messages_to_preserve_provided = True
            elif word[:-1].isdigit():  # for the case when number of messages is followed by dot or other sign
                nr_messages_to_preserve = int(word[:-1])
                nr_messages_to_preserve_provided = True
            else:
                for agent in groupchat.agents:
                    if agent.name == word or agent.name == word[:-1]:
                        agent_to_memory_clear = agent
                        break
        # preserve last tool call message if clear history called inside of tool response
        if "tool_responses" in reply and not nr_messages_to_preserve:
            nr_messages_to_preserve = 1
            logger.warning(
                "The last tool call message will be saved to prevent errors caused by tool response without tool call."
            )
        # clear history
        iostream.send(
            ClearAgentsHistoryEvent(agent=agent_to_memory_clear, nr_events_to_preserve=nr_messages_to_preserve)
        )
        if agent_to_memory_clear:
            agent_to_memory_clear.clear_history(nr_messages_to_preserve=nr_messages_to_preserve)
        else:
            if nr_messages_to_preserve:
                # clearing history for groupchat here
                temp = groupchat.messages[-nr_messages_to_preserve:]
                groupchat.messages.clear()
                groupchat.messages.extend(temp)
            else:
                # clearing history for groupchat here
                groupchat.messages.clear()
            # clearing history for agents
            for agent in groupchat.agents:
                agent.clear_history(nr_messages_to_preserve=nr_messages_to_preserve)

        # Reconstruct the reply without the "clear history" command and parameters
        skip_words_number = 2 + int(bool(agent_to_memory_clear)) + int(nr_messages_to_preserve_provided)
        reply_content = " ".join(words[:clear_word_index] + words[clear_word_index + skip_words_number :])

        return reply_content
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from collections.abc import Callable
from typing import Any, Literal

from ..doc_utils import export_module
from ..llm_config import LLMConfig
from ..runtime_logging import log_new_agent, logging_enabled
from .conversable_agent import ConversableAgent


@export_module("autogen")
class UserProxyAgent(ConversableAgent):
    """(In preview) A proxy agent for the user, that can execute code and provide feedback to the other agents.\n
    \n
        UserProxyAgent is a subclass of ConversableAgent configured with `human_input_mode` to ALWAYS\n
        and `llm_config` to False. By default, the agent will prompt for human input every time a message is received.\n
        Code execution is enabled by default. LLM-based auto reply is disabled by default.\n
        To modify auto reply, register a method with [`register_reply`](../ConversableAgent#register-reply).\n
        To modify the way to get human input, override `get_human_input` method.\n
        To modify the way to execute code blocks, single code block, or function call, override `execute_code_blocks`,\n
        `run_code`, and `execute_function` methods respectively.\n
    """

    # Default UserProxyAgent.description values, based on human_input_mode
    DEFAULT_USER_PROXY_AGENT_DESCRIPTIONS = {
        "ALWAYS": "An attentive HUMAN user who can answer questions about the task, and can perform tasks such as running Python code or inputting command line commands at a Linux terminal and reporting back the execution results.",
        "TERMINATE": "A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.",
        "NEVER": "A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).",
    }

    def __init__(
        self,
        name: str,
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        max_consecutive_auto_reply: int | None = None,
        human_input_mode: Literal["ALWAYS", "TERMINATE", "NEVER"] = "ALWAYS",
        function_map: dict[str, Callable[..., Any]] | None = None,
        code_execution_config: dict[str, Any] | Literal[False] = {},
        default_auto_reply: str | dict[str, Any] | None = "",
        llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = False,
        system_message: str | list[str] | None = "",
        description: str | None = None,
        **kwargs: Any,
    ):
        """Initialize a UserProxyAgent.

        Args:
        name (str): name of the agent.\n
        is_termination_msg (function): a function that takes a message in the form of a dictionary\n
            and returns a boolean value indicating if this received message is a termination message.\n
            The dict can contain the following keys: "content", "role", "name", "function_call".\n
        max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.\n
            default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n
            The limit only plays a role when human_input_mode is not "ALWAYS".\n
        human_input_mode (str): whether to ask for human inputs every time a message is received.\n
            Possible values are "ALWAYS", "TERMINATE", "NEVER".\n
            (1) When "ALWAYS", the agent prompts for human input every time a message is received.\n
                Under this mode, the conversation stops when the human input is "exit",\n
                or when is_termination_msg is True and there is no human input.\n
            (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or\n
                the number of auto reply reaches the max_consecutive_auto_reply.\n
            (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops\n
                when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n
        function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions.\n
        code_execution_config (dict or False): config for the code execution.\n
            To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n
            - work_dir (Optional, str): The working directory for the code execution.\n
                If None, a default working directory will be used.\n
                The default working directory is the "extensions" directory under\n
                "path_to_autogen".\n
            - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n
                Default is True, which means the code will be executed in a docker container. A default list of images will be used.\n
                If a list or a str of image name(s) is provided, the code will be executed in a docker container\n
                with the first image successfully pulled.\n
                If False, the code will be executed in the current environment.\n
                We strongly recommend using docker for code execution.\n
            - timeout (Optional, int): The maximum execution time in seconds.\n
            - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.\n
        default_auto_reply (str or dict or None): the default auto reply message when no code execution or llm based reply is generated.\n
        llm_config (LLMConfig or dict or False or None): llm inference configuration.\n
            Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create)\n
            for available options.\n
            Default to False, which disables llm-based auto reply.\n
            When set to None, will use self.DEFAULT_CONFIG, which defaults to False.\n
        system_message (str or List): system message for ChatCompletion inference.\n
            Only used when llm_config is not False. Use it to reprogram the agent.\n
        description (str): a short description of the agent. This description is used by other agents\n
            (e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)\n
        **kwargs (dict): Please refer to other kwargs in\n
            [ConversableAgent](https://docs.ag2.ai/latest/docs/api-reference/autogen/ConversableAgent).\n
        """
        super().__init__(
            name=name,
            system_message=system_message,
            is_termination_msg=is_termination_msg,
            max_consecutive_auto_reply=max_consecutive_auto_reply,
            human_input_mode=human_input_mode,
            function_map=function_map,
            code_execution_config=code_execution_config,
            llm_config=llm_config,
            default_auto_reply=default_auto_reply,
            description=(
                description if description is not None else self.DEFAULT_USER_PROXY_AGENT_DESCRIPTIONS[human_input_mode]
            ),
            **kwargs,
        )

        if logging_enabled():
            log_new_agent(self, locals())
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from .agent import Agent, LLMAgent
from .assistant_agent import AssistantAgent
from .chat import ChatResult, a_initiate_chats, initiate_chats

# Imported last to avoid circular imports
from .contrib.swarm_agent import (
    a_initiate_swarm_chat,
    a_run_swarm,
    run_swarm,
)
from .conversable_agent import ConversableAgent, UpdateSystemMessage, register_function
from .group.multi_agent_chat import a_initiate_group_chat, a_run_group_chat, initiate_group_chat, run_group_chat
from .groupchat import GroupChat, GroupChatManager
from .user_proxy_agent import UserProxyAgent
from .utils import gather_usage_summary

__all__ = [
    "Agent",
    "AssistantAgent",
    "ChatResult",
    "ConversableAgent",
    "GroupChat",
    "GroupChatManager",
    "LLMAgent",
    "UpdateSystemMessage",
    "UserProxyAgent",
    "a_initiate_chats",
    "a_initiate_group_chat",
    "a_initiate_swarm_chat",
    "a_run_group_chat",
    "a_run_swarm",
    "gather_usage_summary",
    "initiate_chats",
    "initiate_group_chat",
    "register_function",
    "run_group_chat",
    "run_swarm",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import json
import logging
import warnings
from typing import Any

import requests

from ...code_utils import content_str
from ...formatting_utils import colored
from ...import_utils import optional_import_block, require_optional_import
from ...llm_config import LLMConfig
from ..agent import Agent
from .img_utils import get_image_data, llava_formatter
from .multimodal_conversable_agent import MultimodalConversableAgent

with optional_import_block():
    import replicate

logger = logging.getLogger(__name__)

# we will override the following variables later.
SEP = "###"

DEFAULT_LLAVA_SYS_MSG = "You are an AI agent and you can view images."


class LLaVAAgent(MultimodalConversableAgent):
    def __init__(
        self,
        name: str,
        system_message: tuple[str, list] | None = DEFAULT_LLAVA_SYS_MSG,
        *args,
        **kwargs: Any,
    ):
        """Args:
        name (str): agent name.
        system_message (str): system message for the ChatCompletion inference.
            Please override this attribute if you want to reprogram the agent.
        **kwargs (dict): Please refer to other kwargs in
            [ConversableAgent](/docs/api-reference/autogen/ConversableAgent#conversableagent).
        """
        super().__init__(
            name,
            system_message=system_message,
            *args,
            **kwargs,
        )

        assert self.llm_config is not None, "llm_config must be provided."
        self.register_reply([Agent, None], reply_func=LLaVAAgent._image_reply, position=2)

    def _image_reply(self, messages=None, sender=None, config=None):
        # Note: we did not use "llm_config" yet.

        if all((messages is None, sender is None)):
            error_msg = f"Either {messages=} or {sender=} must be provided."
            logger.error(error_msg)
            raise AssertionError(error_msg)

        if messages is None:
            messages = self._oai_messages[sender]

        # The formats for LLaVA and GPT are different. So, we manually handle them here.
        images = []
        prompt = content_str(self.system_message) + "\n"
        for msg in messages:
            role = "Human" if msg["role"] == "user" else "Assistant"
            # pdb.set_trace()
            images += [d["image_url"]["url"] for d in msg["content"] if d["type"] == "image_url"]
            content_prompt = content_str(msg["content"])
            prompt += f"{SEP}{role}: {content_prompt}\n"
        prompt += "\n" + SEP + "Assistant: "

        # TODO: PIL to base64
        images = [get_image_data(im) for im in images]
        print(colored(prompt, "blue"))

        out = ""
        retry = 10
        while len(out) == 0 and retry > 0:
            # image names will be inferred automatically from llava_call
            if "max_new_tokens" in self.llm_config:
                warnings.warn(
                    (
                        "`max_new_tokens` is deprecated in `llm_config` for llava agents. "
                        "Use `max_tokens` instead. "
                        "Scheduled for removal in 0.10.0 version."
                    ),
                    DeprecationWarning,
                )
                max_tokens = self.llm_config["max_new_tokens"]
            else:
                max_tokens = self.llm_config.get("max_tokens")

            out = llava_call_binary(
                prompt=prompt,
                images=images,
                config_list=self.llm_config["config_list"],
                temperature=self.llm_config.get("temperature", 0.5),
                max_new_tokens=max_tokens or 2000,
            )
            retry -= 1

        assert out != "", "Empty response from LLaVA."

        return True, out


@require_optional_import("replicate", "lmm")
def _llava_call_binary_with_config(
    prompt: str,
    images: list[Any],
    config: dict[str, Any],
    max_new_tokens: int = 1000,
    temperature: float = 0.5,
    seed: int = 1,
):
    if config["base_url"].find("0.0.0.0") >= 0 or config["base_url"].find("localhost") >= 0:
        llava_mode = "local"
    else:
        llava_mode = "remote"

    if llava_mode == "local":
        headers = {"User-Agent": "LLaVA Client"}
        pload = {
            "model": config["model"],
            "prompt": prompt,
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "stop": SEP,
            "images": images,
        }

        response = requests.post(
            config["base_url"].rstrip("/") + "/worker_generate_stream", headers=headers, json=pload, stream=False
        )

        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b"\0"):
            if chunk:
                data = json.loads(chunk.decode("utf-8"))
                output = data["text"].split(SEP)[-1]
    elif llava_mode == "remote":
        # The Replicate version of the model only support 1 image for now.
        img = "data:image/jpeg;base64," + images[0]
        response = replicate.run(
            config["base_url"], input={"image": img, "prompt": prompt.replace("<image>", " "), "seed": seed}
        )
        # The yorickvp/llava-13b model can stream output as it's running.
        # The predict method returns an iterator, and you can iterate over that output.
        output = ""
        for item in response:
            # https://replicate.com/yorickvp/llava-13b/versions/2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591/api#output-schema
            output += item

    # Remove the prompt and the space.
    output = output.replace(prompt, "").strip().rstrip()
    return output


@require_optional_import("replicate", "lmm")
def llava_call_binary(
    prompt: str,
    images: list[Any],
    config_list: list[dict[str, Any]],
    max_new_tokens: int = 1000,
    temperature: float = 0.5,
    seed: int = 1,
):
    # TODO 1: add caching around the LLaVA call to save compute and cost
    # TODO 2: add `seed` to ensure reproducibility. The seed is not working now.

    for config in config_list:
        try:
            return _llava_call_binary_with_config(prompt, images, config, max_new_tokens, temperature, seed)
        except Exception as e:
            print(f"Error: {e}")
            continue


def llava_call(prompt: str, llm_config: LLMConfig | dict) -> str:
    """Makes a call to the LLaVA service to generate text based on a given prompt"""
    prompt, images = llava_formatter(prompt, order_image_tokens=False)

    for im in images:
        if len(im) == 0:
            raise RuntimeError("An image is empty!")

    return llava_call_binary(
        prompt,
        images,
        config_list=llm_config["config_list"],
        max_new_tokens=llm_config.get("max_new_tokens", 2000),
        temperature=llm_config.get("temperature", 0.5),
        seed=llm_config.get("seed"),
    )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
# ruff: noqa: E722
import copy
import traceback
from collections.abc import Callable
from contextlib import suppress
from typing import Any, Literal

from ... import Agent, ConversableAgent, GroupChat, GroupChatManager, OpenAIWrapper
from ...llm_config import LLMConfig


class SocietyOfMindAgent(ConversableAgent):
    """(In preview) A single agent that runs a Group Chat as an inner monologue.
    At the end of the conversation (termination for any reason), the SocietyOfMindAgent
    applies the response_preparer method on the entire inner monologue message history to
    extract a final answer for the reply.

    Most arguments are inherited from ConversableAgent. New arguments are:
        chat_manager (GroupChatManager): the group chat manager that will be running the inner monologue
        response_preparer (Optional, Callable or String): If response_preparer is a callable function, then
                it should have the signature:
                    f( self: SocietyOfMindAgent, messages: List[Dict])
                where `self` is this SocietyOfMindAgent, and `messages` is a list of inner-monologue messages.
                The function should return a string representing the final response (extracted or prepared)
                from that history.
                If response_preparer is a string, then it should be the LLM prompt used to extract the final
                message from the inner chat transcript.
                The default response_preparer depends on if an llm_config is provided. If llm_config is False,
                then the response_preparer deterministically returns the last message in the inner-monolgue. If
                llm_config is set to anything else, then a default LLM prompt is used.
    """

    def __init__(
        self,
        name: str,
        chat_manager: GroupChatManager,
        response_preparer: str | Callable[..., Any] | None = None,
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        max_consecutive_auto_reply: int | None = None,
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "TERMINATE",
        function_map: dict[str, Callable[..., Any]] | None = None,
        code_execution_config: dict[str, Any] | Literal[False] = False,
        llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = False,
        default_auto_reply: str | dict[str, Any] | None = "",
        **kwargs: Any,
    ):
        super().__init__(
            name=name,
            system_message="",
            is_termination_msg=is_termination_msg,
            max_consecutive_auto_reply=max_consecutive_auto_reply,
            human_input_mode=human_input_mode,
            function_map=function_map,
            code_execution_config=code_execution_config,
            llm_config=llm_config,
            default_auto_reply=default_auto_reply,
            **kwargs,
        )

        self.update_chat_manager(chat_manager)

        # response_preparer default depends on if the llm_config is set, and if a client was created
        if response_preparer is None:
            if self.client is not None:
                response_preparer = "Output a standalone response to the original request, without mentioning any of the intermediate discussion."
            else:

                def response_preparer(agent, messages):
                    return messages[-1]["content"].replace("TERMINATE", "").strip()

        # Create the response_preparer callable, if given only a prompt string
        if isinstance(response_preparer, str):
            self.response_preparer = lambda agent, messages: agent._llm_response_preparer(response_preparer, messages)
        else:
            self.response_preparer = response_preparer

        # NOTE: Async reply functions are not yet supported with this contrib agent
        self._reply_func_list = []
        self.register_reply([Agent, None], SocietyOfMindAgent.generate_inner_monologue_reply)
        self.register_reply([Agent, None], ConversableAgent.generate_code_execution_reply)
        self.register_reply([Agent, None], ConversableAgent.generate_function_call_reply)
        self.register_reply([Agent, None], ConversableAgent.check_termination_and_human_reply)

    def _llm_response_preparer(self, prompt, messages):
        """Default response_preparer when provided with a string prompt, rather than a callable.

        Args:
            prompt (str): The prompt used to extract the final response from the transcript.
            messages (list): The messages generated as part of the inner monologue group chat.
        """
        _messages = [
            {
                "role": "system",
                "content": """Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:""",
            }
        ]

        for message in messages:
            message = copy.deepcopy(message)
            message["role"] = "user"

            # Convert tool and function calls to basic messages to avoid an error on the LLM call
            if "content" not in message:
                message["content"] = ""

            if "tool_calls" in message:
                del message["tool_calls"]
            if "tool_responses" in message:
                del message["tool_responses"]
            if "function_call" in message and message["content"] == "":
                with suppress(KeyError):
                    message["content"] = (
                        message["function_call"]["name"] + "(" + message["function_call"]["arguments"] + ")"
                    )
                del message["function_call"]

            # Add the modified message to the transcript
            _messages.append(message)

        _messages.append({
            "role": "system",
            "content": prompt,
        })

        response = self.client.create(context=None, messages=_messages, cache=self.client_cache, agent=self.name)
        extracted_response = self.client.extract_text_or_completion_object(response)[0]
        if not isinstance(extracted_response, str):
            return str(extracted_response.model_dump(mode="dict"))
        else:
            return extracted_response

    @property
    def chat_manager(self) -> GroupChatManager | None:
        """Return the group chat manager."""
        return self._chat_manager

    def update_chat_manager(self, chat_manager: GroupChatManager | None):
        """Update the chat manager.

        Args:
            chat_manager (GroupChatManager): the group chat manager
        """
        self._chat_manager = chat_manager

        # Awkward, but due to object cloning, there's no better way to do this
        # Read the GroupChat object from the callback
        self._group_chat = None
        if self._chat_manager is not None:
            for item in self._chat_manager._reply_func_list:
                if isinstance(item["config"], GroupChat):
                    self._group_chat = item["config"]
                    break

    def generate_inner_monologue_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Generate a reply by running the group chat"""
        if self.chat_manager is None:
            return False, None
        if messages is None:
            messages = self._oai_messages[sender]

        # We want to clear the inner monolgue, keeping only the exteranl chat for context.
        # Reset all the counters and histories, then populate agents with necessary context from the external chat
        self.chat_manager.reset()
        self.update_chat_manager(self.chat_manager)

        external_history = []
        if len(messages) > 1:
            external_history = messages[0 : len(messages) - 1]  # All but the current message

        for agent in self._group_chat.agents:
            agent.reset()
            for message in external_history:
                # Assign each message a name
                attributed_message = message.copy()
                if "name" not in attributed_message:
                    if attributed_message["role"] == "assistant":
                        attributed_message["name"] = self.name
                    else:
                        attributed_message["name"] = sender.name

                self.chat_manager.send(attributed_message, agent, request_reply=False, silent=True)

        try:
            self.initiate_chat(self.chat_manager, message=messages[-1], clear_history=False)
        except:
            traceback.print_exc()

        response_preparer = self.response_preparer
        return True, response_preparer(self, self._group_chat.messages)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import warnings
from typing import Any

from typing_extensions import deprecated

from ..agent import Agent
from ..assistant_agent import AssistantAgent


@deprecated("The RetrieveAssistantAgent is deprecated. Please use the AssistantAgent instead.")
class RetrieveAssistantAgent(AssistantAgent):
    """(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.

    RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.
    The default system message is designed to solve a task with LLM,
    including suggesting python code blocks and debugging.
    `human_input_mode` is default to "NEVER"
    and `code_execution_config` is default to False.
    This agent doesn't execute code by default, and expects the user to execute the code.
    """

    def __init__(self, *args, **kwargs):
        warnings.warn(
            "The RetrieveAssistantAgent is deprecated. Please use the AssistantAgent instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        super().__init__(*args, **kwargs)
        self.register_reply(Agent, RetrieveAssistantAgent._generate_retrieve_assistant_reply)

    def _generate_retrieve_assistant_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        if "exitcode: 0 (execution succeeded)" in message.get("content", ""):
            # Terminate the conversation when the code execution succeeds. Although sometimes even when the
            # code execution succeeds, the task is not solved, but it's hard to tell. If the human_input_mode
            # of RetrieveUserProxyAgent is "TERMINATE" or "ALWAYS", user can still continue the conversation.
            return True, "TERMINATE"
        elif (
            "UPDATE CONTEXT" in message.get("content", "")[-20:].upper()
            or "UPDATE CONTEXT" in message.get("content", "")[:20].upper()
        ):
            return True, "UPDATE CONTEXT"
        else:
            return False, None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
from typing import Any

from ... import OpenAIWrapper
from ...code_utils import content_str
from .. import Agent, ConversableAgent
from ..contrib.img_utils import (
    gpt4v_formatter,
    message_formatter_pil_to_b64,
)

DEFAULT_LMM_SYS_MSG = """You are a helpful AI assistant."""
DEFAULT_MODEL = "gpt-4-vision-preview"


class MultimodalConversableAgent(ConversableAgent):
    DEFAULT_CONFIG = {
        "model": DEFAULT_MODEL,
    }

    def __init__(
        self,
        name: str,
        system_message: str | list | None = DEFAULT_LMM_SYS_MSG,
        is_termination_msg: str = None,
        *args,
        **kwargs: Any,
    ):
        """Args:
        name (str): agent name.
        system_message (str): system message for the OpenAIWrapper inference.
            Please override this attribute if you want to reprogram the agent.
        **kwargs (dict): Please refer to other kwargs in
            [ConversableAgent](/docs/api-reference/autogen/ConversableAgent#conversableagent).
        """
        super().__init__(
            name,
            system_message,
            is_termination_msg=is_termination_msg,
            *args,
            **kwargs,
        )
        # call the setter to handle special format.
        self.update_system_message(system_message)
        self._is_termination_msg = (
            is_termination_msg
            if is_termination_msg is not None
            else (lambda x: content_str(x.get("content")) == "TERMINATE")
        )

        # Override the `generate_oai_reply`
        self.replace_reply_func(ConversableAgent.generate_oai_reply, MultimodalConversableAgent.generate_oai_reply)
        self.replace_reply_func(
            ConversableAgent.a_generate_oai_reply,
            MultimodalConversableAgent.a_generate_oai_reply,
        )

    def update_system_message(self, system_message: dict[str, Any] | list[str] | str):
        """Update the system message.

        Args:
            system_message (str): system message for the OpenAIWrapper inference.
        """
        self._oai_system_message[0]["content"] = self._message_to_dict(system_message)["content"]
        self._oai_system_message[0]["role"] = "system"

    @staticmethod
    def _message_to_dict(message: dict[str, Any] | list[str] | str) -> dict:
        """Convert a message to a dictionary. This implementation
        handles the GPT-4V formatting for easier prompts.

        The message can be a string, a dictionary, or a list of dictionaries:
            - If it's a string, it will be cast into a list and placed in the 'content' field.
            - If it's a list, it will be directly placed in the 'content' field.
            - If it's a dictionary, it is already in message dict format. The 'content' field of this dictionary
            will be processed using the gpt4v_formatter.
        """
        if isinstance(message, str):
            return {"content": gpt4v_formatter(message, img_format="pil")}
        if isinstance(message, list):
            return {"content": message}
        if isinstance(message, dict):
            assert "content" in message, "The message dict must have a `content` field"
            if isinstance(message["content"], str):
                message = copy.deepcopy(message)
                message["content"] = gpt4v_formatter(message["content"], img_format="pil")
            try:
                content_str(message["content"])
            except (TypeError, ValueError) as e:
                print("The `content` field should be compatible with the content_str function!")
                raise e
            return message
        raise ValueError(f"Unsupported message type: {type(message)}")

    def generate_oai_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Generate a reply using autogen.oai."""
        client = self.client if config is None else config
        if client is None:
            return False, None
        if messages is None:
            messages = self._oai_messages[sender]

        messages_with_b64_img = message_formatter_pil_to_b64(self._oai_system_message + messages)

        # TODO: #1143 handle token limit exceeded error
        response = client.create(
            context=messages[-1].pop("context", None), messages=messages_with_b64_img, agent=self.name
        )

        # TODO: line 301, line 271 is converting messages to dict. Can be removed after ChatCompletionMessage_to_dict is merged.
        extracted_response = client.extract_text_or_completion_object(response)[0]
        if not isinstance(extracted_response, str):
            extracted_response = extracted_response.model_dump()
        return True, extracted_response
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
import sys
from typing import Any, Protocol

import tiktoken
from termcolor import colored

from .... import token_count_utils
from ....cache import AbstractCache, Cache
from ....types import MessageContentType
from . import transforms_util
from .text_compressors import LLMLingua, TextCompressor


class MessageTransform(Protocol):
    """Defines a contract for message transformation.

    Classes implementing this protocol should provide an `apply_transform` method
    that takes a list of messages and returns the transformed list.
    """

    def apply_transform(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Applies a transformation to a list of messages.

        Args:
            messages: A list of dictionaries representing messages.

        Returns:
            A new list of dictionaries containing the transformed messages.
        """
        ...

    def get_logs(
        self, pre_transform_messages: list[dict[str, Any]], post_transform_messages: list[dict[str, Any]]
    ) -> tuple[str, bool]:
        """Creates the string including the logs of the transformation

        Alongside the string, it returns a boolean indicating whether the transformation had an effect or not.

        Args:
            pre_transform_messages: A list of dictionaries representing messages before the transformation.
            post_transform_messages: A list of dictionaries representig messages after the transformation.

        Returns:
            A tuple with a string with the logs and a flag indicating whether the transformation had an effect or not.
        """
        ...


class MessageHistoryLimiter:
    """Limits the number of messages considered by an agent for response generation.

    This transform keeps only the most recent messages up to the specified maximum number of messages (max_messages).
    It trims the conversation history by removing older messages, retaining only the most recent messages.
    """

    def __init__(
        self,
        max_messages: int | None = None,
        keep_first_message: bool = False,
        exclude_names: list[str] | None = None,
    ):
        """Args:
        max_messages Optional[int]: Maximum number of messages to keep in the context. Must be greater than 0 if not None.
        keep_first_message bool: Whether to keep the original first message in the conversation history.
            Defaults to False.
        exclude_names Optional[list[str]]: List of message sender names to exclude from the message history.
            Messages from these senders will be filtered out before applying the message limit. Defaults to None.
        """
        self._validate_max_messages(max_messages)
        self._max_messages = max_messages
        self._keep_first_message = keep_first_message
        self._exclude_names = exclude_names

    def apply_transform(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Truncates the conversation history to the specified maximum number of messages.

        This method returns a new list containing the most recent messages up to the specified
        maximum number of messages (max_messages). If max_messages is None, it returns the
        original list of messages unmodified.

        Args:
            messages (List[Dict]): The list of messages representing the conversation history.

        Returns:
            List[Dict]: A new list containing the most recent messages up to the specified maximum.
        """
        exclude_names = getattr(self, "_exclude_names", None)

        filtered = [msg for msg in messages if msg.get("name") not in exclude_names] if exclude_names else messages

        if self._max_messages is None or len(filtered) <= self._max_messages:
            return filtered

        truncated_messages = []
        remaining_count = self._max_messages

        # Start with the first message if we need to keep it
        if self._keep_first_message and filtered:
            truncated_messages = [filtered[0]]
            remaining_count -= 1

        # Loop through messages in reverse
        for i in range(len(filtered) - 1, 0, -1):
            if remaining_count > 1:
                truncated_messages.insert(1 if self._keep_first_message else 0, filtered[i])
            if remaining_count == 1:  # noqa: SIM102
                # If there's only 1 slot left and it's a 'tools' message, ignore it.
                if filtered[i].get("role") != "tool":
                    truncated_messages.insert(1, filtered[i])

            remaining_count -= 1
            if remaining_count == 0:
                break

        return truncated_messages

    def get_logs(
        self, pre_transform_messages: list[dict[str, Any]], post_transform_messages: list[dict[str, Any]]
    ) -> tuple[str, bool]:
        pre_transform_messages_len = len(pre_transform_messages)
        post_transform_messages_len = len(post_transform_messages)

        if post_transform_messages_len < pre_transform_messages_len:
            logs_str = (
                f"Removed {pre_transform_messages_len - post_transform_messages_len} messages. "
                f"Number of messages reduced from {pre_transform_messages_len} to {post_transform_messages_len}."
            )
            return logs_str, True
        return "No messages were removed.", False

    def _validate_max_messages(self, max_messages: int | None):
        if max_messages is not None and max_messages < 1:
            raise ValueError("max_messages must be None or greater than 1")


class MessageTokenLimiter:
    """Truncates messages to meet token limits for efficient processing and response generation.

    This transformation applies two levels of truncation to the conversation history:

    1. Truncates each individual message to the maximum number of tokens specified by max_tokens_per_message.
    2. Truncates the overall conversation history to the maximum number of tokens specified by max_tokens.

    NOTE: Tokens are counted using the encoder for the specified model. Different models may yield different token
        counts for the same text.

    NOTE: For multimodal LLMs, the token count may be inaccurate as it does not account for the non-text input
        (e.g images).

    The truncation process follows these steps in order:

    1. The minimum tokens threshold (`min_tokens`) is checked (0 by default). If the total number of tokens in messages
        is less than this threshold, then the messages are returned as is. In other case, the following process is applied.
    2. Messages are processed in reverse order (newest to oldest).
    3. Individual messages are truncated based on max_tokens_per_message. For multimodal messages containing both text
        and other types of content, only the text content is truncated.
    4. The overall conversation history is truncated based on the max_tokens limit. Once the accumulated token count
        exceeds this limit, the current message being processed get truncated to meet the total token count and any
        remaining messages get discarded.
    5. The truncated conversation history is reconstructed by prepending the messages to a new list to preserve the
        original message order.
    """

    def __init__(
        self,
        max_tokens_per_message: int | None = None,
        max_tokens: int | None = None,
        min_tokens: int | None = None,
        model: str = "gpt-3.5-turbo-0613",
        filter_dict: dict[str, Any] | None = None,
        exclude_filter: bool = True,
    ):
        """Args:
        max_tokens_per_message (None or int): Maximum number of tokens to keep in each message.
            Must be greater than or equal to 0 if not None.
        max_tokens (Optional[int]): Maximum number of tokens to keep in the chat history.
            Must be greater than or equal to 0 if not None.
        min_tokens (Optional[int]): Minimum number of tokens in messages to apply the transformation.
            Must be greater than or equal to 0 if not None.
        model (str): The target OpenAI model for tokenization alignment.
        filter_dict (None or dict): A dictionary to filter out messages that you want/don't want to compress.
            If None, no filters will be applied.
        exclude_filter (bool): If exclude filter is True (the default value), messages that match the filter will be
            excluded from token truncation. If False, messages that match the filter will be truncated.
        """
        self._model = model
        self._max_tokens_per_message = self._validate_max_tokens(max_tokens_per_message)
        self._max_tokens = self._validate_max_tokens(max_tokens)
        self._min_tokens = self._validate_min_tokens(min_tokens, max_tokens)
        self._filter_dict = filter_dict
        self._exclude_filter = exclude_filter

    def apply_transform(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Applies token truncation to the conversation history.

        Args:
            messages (List[Dict]): The list of messages representing the conversation history.

        Returns:
            List[Dict]: A new list containing the truncated messages up to the specified token limits.
        """
        assert self._max_tokens_per_message is not None
        assert self._max_tokens is not None
        assert self._min_tokens is not None

        # if the total number of tokens in the messages is less than the min_tokens, return the messages as is
        if not transforms_util.min_tokens_reached(messages, self._min_tokens):
            return messages

        temp_messages = copy.deepcopy(messages)
        processed_messages = []
        processed_messages_tokens = 0

        for msg in reversed(temp_messages):
            # Some messages may not have content.
            if not transforms_util.is_content_right_type(msg.get("content")):
                processed_messages.insert(0, msg)
                continue

            if not transforms_util.should_transform_message(msg, self._filter_dict, self._exclude_filter):
                processed_messages.insert(0, msg)
                processed_messages_tokens += transforms_util.count_text_tokens(msg["content"])
                continue

            expected_tokens_remained = self._max_tokens - processed_messages_tokens - self._max_tokens_per_message

            # If adding this message would exceed the token limit, truncate the last message to meet the total token
            # limit and discard all remaining messages
            if expected_tokens_remained < 0:
                msg["content"] = self._truncate_str_to_tokens(
                    msg["content"], self._max_tokens - processed_messages_tokens
                )
                processed_messages.insert(0, msg)
                break

            msg["content"] = self._truncate_str_to_tokens(msg["content"], self._max_tokens_per_message)
            msg_tokens = transforms_util.count_text_tokens(msg["content"])

            # prepend the message to the list to preserve order
            processed_messages_tokens += msg_tokens
            processed_messages.insert(0, msg)

        return processed_messages

    def get_logs(
        self, pre_transform_messages: list[dict[str, Any]], post_transform_messages: list[dict[str, Any]]
    ) -> tuple[str, bool]:
        pre_transform_messages_tokens = sum(
            transforms_util.count_text_tokens(msg["content"]) for msg in pre_transform_messages if "content" in msg
        )
        post_transform_messages_tokens = sum(
            transforms_util.count_text_tokens(msg["content"]) for msg in post_transform_messages if "content" in msg
        )

        if post_transform_messages_tokens < pre_transform_messages_tokens:
            logs_str = (
                f"Truncated {pre_transform_messages_tokens - post_transform_messages_tokens} tokens. "
                f"Number of tokens reduced from {pre_transform_messages_tokens} to {post_transform_messages_tokens}"
            )
            return logs_str, True
        return "No tokens were truncated.", False

    def _truncate_str_to_tokens(self, contents: str | list, n_tokens: int) -> str | list:
        if isinstance(contents, str):
            return self._truncate_tokens(contents, n_tokens)
        elif isinstance(contents, list):
            return self._truncate_multimodal_text(contents, n_tokens)
        else:
            raise ValueError(f"Contents must be a string or a list of dictionaries. Received type: {type(contents)}")

    def _truncate_multimodal_text(self, contents: list[dict[str, Any]], n_tokens: int) -> list[dict[str, Any]]:
        """Truncates text content within a list of multimodal elements, preserving the overall structure."""
        tmp_contents = []
        for content in contents:
            if content["type"] == "text":
                truncated_text = self._truncate_tokens(content["text"], n_tokens)
                tmp_contents.append({"type": "text", "text": truncated_text})
            else:
                tmp_contents.append(content)
        return tmp_contents

    def _truncate_tokens(self, text: str, n_tokens: int) -> str:
        encoding = tiktoken.encoding_for_model(self._model)  # Get the appropriate tokenizer

        encoded_tokens = encoding.encode(text)
        truncated_tokens = encoded_tokens[:n_tokens]
        truncated_text = encoding.decode(truncated_tokens)  # Decode back to text

        return truncated_text

    def _validate_max_tokens(self, max_tokens: int | None = None) -> int | None:
        if max_tokens is not None and max_tokens < 0:
            raise ValueError("max_tokens and max_tokens_per_message must be None or greater than or equal to 0")

        try:
            allowed_tokens = token_count_utils.get_max_token_limit(self._model)
        except Exception:
            print(colored(f"Model {self._model} not found in token_count_utils.", "yellow"))
            allowed_tokens = None

        if max_tokens is not None and allowed_tokens is not None and max_tokens > allowed_tokens:
            print(
                colored(
                    f"Max token was set to {max_tokens}, but {self._model} can only accept {allowed_tokens} tokens. Capping it to {allowed_tokens}.",
                    "yellow",
                )
            )
            return allowed_tokens

        return max_tokens if max_tokens is not None else sys.maxsize

    def _validate_min_tokens(self, min_tokens: int | None, max_tokens: int | None) -> int:
        if min_tokens is None:
            return 0
        if min_tokens < 0:
            raise ValueError("min_tokens must be None or greater than or equal to 0.")
        if max_tokens is not None and min_tokens > max_tokens:
            raise ValueError("min_tokens must not be more than max_tokens.")
        return min_tokens


class TextMessageCompressor:
    """A transform for compressing text messages in a conversation history.

    It uses a specified text compression method to reduce the token count of messages, which can lead to more efficient
    processing and response generation by downstream models.
    """

    def __init__(
        self,
        text_compressor: TextCompressor | None = None,
        min_tokens: int | None = None,
        compression_params: dict = {},
        cache: AbstractCache | None = None,
        filter_dict: dict[str, Any] | None = None,
        exclude_filter: bool = True,
    ):
        """Args:
        text_compressor (TextCompressor or None): An instance of a class that implements the TextCompressor
            protocol. If None, it defaults to LLMLingua.
        min_tokens (int or None): Minimum number of tokens in messages to apply the transformation. Must be greater
            than or equal to 0 if not None. If None, no threshold-based compression is applied.
        compression_args (dict): A dictionary of arguments for the compression method. Defaults to an empty
            dictionary.
        cache (None or AbstractCache): The cache client to use to store and retrieve previously compressed messages.
            If None, no caching will be used.
        filter_dict (None or dict): A dictionary to filter out messages that you want/don't want to compress.
            If None, no filters will be applied.
        exclude_filter (bool): If exclude filter is True (the default value), messages that match the filter will be
            excluded from compression. If False, messages that match the filter will be compressed.
        """
        if text_compressor is None:
            text_compressor = LLMLingua()

        self._validate_min_tokens(min_tokens)

        self._text_compressor = text_compressor
        self._min_tokens = min_tokens
        self._compression_args = compression_params
        self._filter_dict = filter_dict
        self._exclude_filter = exclude_filter

        if cache is None:
            self._cache = Cache.disk()
        else:
            self._cache = cache

        # Optimizing savings calculations to optimize log generation
        self._recent_tokens_savings = 0

    def apply_transform(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Applies compression to messages in a conversation history based on the specified configuration.

        The function processes each message according to the `compression_args` and `min_tokens` settings, applying
        the specified compression configuration and returning a new list of messages with reduced token counts
        where possible.

        Args:
            messages (List[Dict]): A list of message dictionaries to be compressed.

        Returns:
            List[Dict]: A list of dictionaries with the message content compressed according to the configured
                method and scope.
        """
        # Make sure there is at least one message
        if not messages:
            return messages

        # if the total number of tokens in the messages is less than the min_tokens, return the messages as is
        if not transforms_util.min_tokens_reached(messages, self._min_tokens):
            return messages

        total_savings = 0
        processed_messages = messages.copy()
        for message in processed_messages:
            # Some messages may not have content.
            if not transforms_util.is_content_right_type(message.get("content")):
                continue

            if not transforms_util.should_transform_message(message, self._filter_dict, self._exclude_filter):
                continue

            if transforms_util.is_content_text_empty(message["content"]):
                continue

            cache_key = transforms_util.cache_key(message["content"], self._min_tokens)
            cached_content = transforms_util.cache_content_get(self._cache, cache_key)
            if cached_content is not None:
                message["content"], savings = cached_content
            else:
                message["content"], savings = self._compress(message["content"])

            transforms_util.cache_content_set(self._cache, cache_key, message["content"], savings)

            assert isinstance(savings, int)
            total_savings += savings

        self._recent_tokens_savings = total_savings
        return processed_messages

    def get_logs(
        self, pre_transform_messages: list[dict[str, Any]], post_transform_messages: list[dict[str, Any]]
    ) -> tuple[str, bool]:
        if self._recent_tokens_savings > 0:
            return f"{self._recent_tokens_savings} tokens saved with text compression.", True
        else:
            return "No tokens saved with text compression.", False

    def _compress(self, content: MessageContentType) -> tuple[MessageContentType, int]:
        """Compresses the given text or multimodal content using the specified compression method."""
        if isinstance(content, str):
            return self._compress_text(content)
        elif isinstance(content, list):
            return self._compress_multimodal(content)
        else:
            return content, 0

    def _compress_multimodal(self, content: MessageContentType) -> tuple[MessageContentType, int]:
        tokens_saved = 0
        for item in content:
            if isinstance(item, dict) and "text" in item:
                item["text"], savings = self._compress_text(item["text"])
                tokens_saved += savings

            elif isinstance(item, str):
                item, savings = self._compress_text(item)
                tokens_saved += savings

        return content, tokens_saved

    def _compress_text(self, text: str) -> tuple[str, int]:
        """Compresses the given text using the specified compression method."""
        compressed_text = self._text_compressor.compress_text(text, **self._compression_args)

        savings = 0
        if "origin_tokens" in compressed_text and "compressed_tokens" in compressed_text:
            savings = compressed_text["origin_tokens"] - compressed_text["compressed_tokens"]

        return compressed_text["compressed_prompt"], savings

    def _validate_min_tokens(self, min_tokens: int | None):
        if min_tokens is not None and min_tokens <= 0:
            raise ValueError("min_tokens must be greater than 0 or None")


class TextMessageContentName:
    """A transform for including the agent's name in the content of a message.

    How to create and apply the transform:
    # Imports
    from autogen.agentchat.contrib.capabilities import transform_messages, transforms

    # Create Transform
    name_transform = transforms.TextMessageContentName(position="start", format_string="'{name}' said:\n")

    # Create the TransformMessages
    context_handling = transform_messages.TransformMessages(
                transforms=[
                    name_transform
                ]
            )

    # Add it to an agent so when they run inference it will apply to the messages
    context_handling.add_to_agent(my_agent)
    """

    def __init__(
        self,
        position: str = "start",
        format_string: str = "{name}:\n",
        deduplicate: bool = True,
        filter_dict: dict[str, Any] | None = None,
        exclude_filter: bool = True,
    ):
        """Args:
        position (str): The position to add the name to the content. The possible options are 'start' or 'end'. Defaults to 'start'.
        format_string (str): The f-string to format the message name with. Use '{name}' as a placeholder for the agent's name. Defaults to '{name}:\n' and must contain '{name}'.
        deduplicate (bool): Whether to deduplicate the formatted string so it doesn't appear twice (sometimes the LLM will add it to new messages itself). Defaults to True.
        filter_dict (None or dict): A dictionary to filter out messages that you want/don't want to compress.
            If None, no filters will be applied.
        exclude_filter (bool): If exclude filter is True (the default value), messages that match the filter will be
            excluded from compression. If False, messages that match the filter will be compressed.
        """
        assert isinstance(position, str) and position in ["start", "end"]
        assert isinstance(format_string, str) and "{name}" in format_string
        assert isinstance(deduplicate, bool) and deduplicate is not None

        self._position = position
        self._format_string = format_string
        self._deduplicate = deduplicate
        self._filter_dict = filter_dict
        self._exclude_filter = exclude_filter

        # Track the number of messages changed for logging
        self._messages_changed = 0

    def apply_transform(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Applies the name change to the message based on the position and format string.

        Args:
            messages (List[Dict]): A list of message dictionaries.

        Returns:
            List[Dict]: A list of dictionaries with the message content updated with names.
        """
        # Make sure there is at least one message
        if not messages:
            return messages

        messages_changed = 0
        processed_messages = copy.deepcopy(messages)
        for message in processed_messages:
            # Some messages may not have content.
            if not transforms_util.is_content_right_type(
                message.get("content")
            ) or not transforms_util.is_content_right_type(message.get("name")):
                continue

            if not transforms_util.should_transform_message(message, self._filter_dict, self._exclude_filter):
                continue

            if transforms_util.is_content_text_empty(message["content"]) or transforms_util.is_content_text_empty(
                message["name"]
            ):
                continue

            # Get and format the name in the content
            content = message["content"]
            formatted_name = self._format_string.format(name=message["name"])

            if self._position == "start":
                if not self._deduplicate or not content.startswith(formatted_name):
                    message["content"] = f"{formatted_name}{content}"

                    messages_changed += 1
            else:
                if not self._deduplicate or not content.endswith(formatted_name):
                    message["content"] = f"{content}{formatted_name}"

                    messages_changed += 1

        self._messages_changed = messages_changed
        return processed_messages

    def get_logs(
        self, pre_transform_messages: list[dict[str, Any]], post_transform_messages: list[dict[str, Any]]
    ) -> tuple[str, bool]:
        if self._messages_changed > 0:
            return f"{self._messages_changed} message(s) changed to incorporate name.", True
        else:
            return "No messages changed to incorporate name.", False
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any, Protocol

from ....import_utils import optional_import_block, require_optional_import

with optional_import_block() as result:
    import llmlingua
    from llmlingua import PromptCompressor


class TextCompressor(Protocol):
    """Defines a protocol for text compression to optimize agent interactions."""

    def compress_text(self, text: str, **compression_params) -> dict[str, Any]:
        """This method takes a string as input and returns a dictionary containing the compressed text and other
        relevant information. The compressed text should be stored under the 'compressed_text' key in the dictionary.
        To calculate the number of saved tokens, the dictionary should include 'origin_tokens' and 'compressed_tokens' keys.
        """
        ...


@require_optional_import("llmlingua", "long-context")
class LLMLingua:
    """Compresses text messages using LLMLingua for improved efficiency in processing and response generation.

    NOTE: The effectiveness of compression and the resultant token savings can vary based on the content of the messages
    and the specific configurations used for the PromptCompressor.
    """

    def __init__(
        self,
        prompt_compressor_kwargs: dict = {
            "model_name": "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
            "use_llmlingua2": True,
            "device_map": "cpu",
        },
        structured_compression: bool = False,
    ) -> None:
        """Args:
            prompt_compressor_kwargs (dict): A dictionary of keyword arguments for the PromptCompressor. Defaults to a
                dictionary with model_name set to "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
                use_llmlingua2 set to True, and device_map set to "cpu".
            structured_compression (bool): A flag indicating whether to use structured compression. If True, the
                structured_compress_prompt method of the PromptCompressor is used. Otherwise, the compress_prompt method
                is used. Defaults to False.
                dictionary.

        Raises:
            ImportError: If the llmlingua library is not installed.
        """
        self._prompt_compressor = PromptCompressor(**prompt_compressor_kwargs)

        assert isinstance(self._prompt_compressor, llmlingua.PromptCompressor)
        self._compression_method = (
            self._prompt_compressor.structured_compress_prompt
            if structured_compression
            else self._prompt_compressor.compress_prompt
        )

    def compress_text(self, text: str, **compression_params) -> dict[str, Any]:
        return self._compression_method([text], **compression_params)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
from collections.abc import Callable
from typing import Any

from ....code_utils import content_str
from ....oai.client import OpenAIWrapper
from ...assistant_agent import ConversableAgent
from ..img_utils import (
    convert_base64_to_data_uri,
    get_image_data,
    get_pil_image,
    gpt4v_formatter,
)
from .agent_capability import AgentCapability

DEFAULT_DESCRIPTION_PROMPT = (
    "Write a detailed caption for this image. "
    "Pay special attention to any details that might be useful or relevant "
    "to the ongoing conversation."
)


class VisionCapability(AgentCapability):
    """We can add vision capability to regular ConversableAgent, even if the agent does not have the multimodal capability,
    such as GPT-3.5-turbo agent, Llama, Orca, or Mistral agents. This vision capability will invoke a LMM client to describe
    the image (captioning) before sending the information to the agent's actual client.

        The vision capability will hook to the ConversableAgent's `process_last_received_message`.

        Some technical details:
        When the agent (who has the vision capability) received an message, it will:
        1. _process_received_message:
            a. _append_oai_message
        2. generate_reply: if the agent is a MultimodalAgent, it will also use the image tag.
            a. hook process_last_received_message (NOTE: this is where the vision capability will be hooked to.)
            b. hook process_all_messages_before_reply
        3. send:
            a. hook process_message_before_send
            b. _append_oai_message
    """

    def __init__(
        self,
        lmm_config: dict[str, Any],
        description_prompt: str | None = DEFAULT_DESCRIPTION_PROMPT,
        custom_caption_func: Callable = None,
    ) -> None:
        """Initializes a new instance, setting up the configuration for interacting with
        a Language Multimodal (LMM) client and specifying optional parameters for image
        description and captioning.

        Args:
            lmm_config (Dict): Configuration for the LMM client, which is used to call
                the LMM service for describing the image. This must be a dictionary containing
                the necessary configuration parameters. If `lmm_config` is False or an empty dictionary,
                it is considered invalid, and initialization will assert.
            description_prompt (Optional[str], optional): The prompt to use for generating
                descriptions of the image. This parameter allows customization of the
                prompt passed to the LMM service. Defaults to `DEFAULT_DESCRIPTION_PROMPT` if not provided.
            custom_caption_func (Callable, optional): A callable that, if provided, will be used
                to generate captions for images. This allows for custom captioning logic outside
                of the standard LMM service interaction.
                The callable should take three parameters as input:
                    1. an image URL (or local location)
                    2. image_data (a PIL image)
                    3. lmm_client (to call remote LMM)
                and then return a description (as string).
                If not provided, captioning will rely on the LMM client configured via `lmm_config`.
                If provided, we will not run the default self._get_image_caption method.

        Raises:
            AssertionError: If neither a valid `lmm_config` nor a `custom_caption_func` is provided,
                an AssertionError is raised to indicate that the Vision Capability requires
                one of these to be valid for operation.
        """
        self._lmm_config = lmm_config
        self._description_prompt = description_prompt
        self._parent_agent = None

        if lmm_config:
            self._lmm_client = OpenAIWrapper(**lmm_config)
        else:
            self._lmm_client = None

        self._custom_caption_func = custom_caption_func
        assert self._lmm_config or custom_caption_func, (
            "Vision Capability requires a valid lmm_config or custom_caption_func."
        )

    def add_to_agent(self, agent: ConversableAgent) -> None:
        self._parent_agent = agent

        # Append extra info to the system message.
        agent.update_system_message(agent.system_message + "\nYou've been given the ability to interpret images.")

        # Register a hook for processing the last message.
        agent.register_hook(hookable_method="process_last_received_message", hook=self.process_last_received_message)

    def process_last_received_message(self, content: str | list[dict[str, Any]]) -> str:
        """Processes the last received message content by normalizing and augmenting it
        with descriptions of any included images. The function supports input content
        as either a string or a list of dictionaries, where each dictionary represents
        a content item (e.g., text, image). If the content contains image URLs, it
        fetches the image data, generates a caption for each image, and inserts the
        caption into the augmented content.

        The function aims to transform the content into a format compatible with GPT-4V
        multimodal inputs, specifically by formatting strings into PIL-compatible
        images if needed and appending text descriptions for images. This allows for
        a more accessible presentation of the content, especially in contexts where
        images cannot be displayed directly.

        Args:
            content (Union[str, List[dict[str, Any]]]): The last received message content, which
                can be a plain text string or a list of dictionaries representing
                different types of content items (e.g., text, image_url).

        Returns:
            str: The augmented message content

        Raises:
            AssertionError: If an item in the content list is not a dictionary.

        Examples:
            Assuming `self._get_image_caption(img_data)` returns
            "A beautiful sunset over the mountains" for the image.

        - Input as String:
            content = "Check out this cool photo!"
            Output: "Check out this cool photo!"
            (Content is a string without an image, remains unchanged.)

        - Input as String, with image location:
            content = "What's weather in this cool photo: `<img http://example.com/photo.jpg>`"
            Output: "What's weather in this cool photo: `<img http://example.com/photo.jpg>` in case you can not see, the caption of this image is:
            A beautiful sunset over the mountains\n"
            (Caption added after the image)

        - Input as List with Text Only:
            content = `[{"type": "text", "text": "Here's an interesting fact."}]`
            Output: "Here's an interesting fact."
            (No images in the content, it remains unchanged.)

        - Input as List with Image URL:
            ```python
            content = [
                {"type": "text", "text": "What's weather in this cool photo:"},
                {"type": "image_url", "image_url": {"url": "http://example.com/photo.jpg"}},
            ]
            ```
            Output: "What's weather in this cool photo: `<img http://example.com/photo.jpg>` in case you can not see, the caption of this image is:
            A beautiful sunset over the mountains\n"
            (Caption added after the image)
        """
        copy.deepcopy(content)
        # normalize the content into the gpt-4v format for multimodal
        # we want to keep the URL format to keep it concise.
        if isinstance(content, str):
            content = gpt4v_formatter(content, img_format="url")

        aug_content: str = ""
        for item in content:
            assert isinstance(item, dict)
            if item["type"] == "text":
                aug_content += item["text"]
            elif item["type"] == "image_url":
                img_url = item["image_url"]["url"]
                img_caption = ""

                if self._custom_caption_func:
                    img_caption = self._custom_caption_func(img_url, get_pil_image(img_url), self._lmm_client)
                elif self._lmm_client:
                    img_data = get_image_data(img_url)
                    img_caption = self._get_image_caption(img_data)
                else:
                    img_caption = ""

                aug_content += f"<img {img_url}> in case you can not see, the caption of this image is: {img_caption}\n"
            else:
                print(f"Warning: the input type should either be `test` or `image_url`. Skip {item['type']} here.")

        return aug_content

    def _get_image_caption(self, img_data: str) -> str:
        """Args:
            img_data (str): base64 encoded image data.

        Returns:
            str: caption for the given image.
        """
        response = self._lmm_client.create(
            context=None,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self._description_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": convert_base64_to_data_uri(img_data),
                            },
                        },
                    ],
                }
            ],
        )
        description = response.choices[0].message.content
        return content_str(description)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import os
import pickle
from typing import Any

from ....formatting_utils import colored
from ....import_utils import optional_import_block, require_optional_import
from ....llm_config import LLMConfig
from ...assistant_agent import ConversableAgent
from ..text_analyzer_agent import TextAnalyzerAgent
from .agent_capability import AgentCapability

with optional_import_block():
    import chromadb
    from chromadb.config import Settings


class Teachability(AgentCapability):
    """Teachability uses a vector database to give an agent the ability to remember user teachings,
    where the user is any caller (human or not) sending messages to the teachable agent.
    Teachability is designed to be composable with other agent capabilities.
    To make any conversable agent teachable, instantiate both the agent and the Teachability class,
    then pass the agent to teachability.add_to_agent(agent).
    Note that teachable agents in a group chat must be given unique path_to_db_dir values.

    When adding Teachability to an agent, the following are modified:
    - The agent's system message is appended with a note about the agent's new ability.
    - A hook is added to the agent's `process_last_received_message` hookable method,
    and the hook potentially modifies the last of the received messages to include earlier teachings related to the message.
    Added teachings do not propagate into the stored message history.
    If new user teachings are detected, they are added to new memos in the vector database.
    """

    def __init__(
        self,
        verbosity: int | None = 0,
        reset_db: bool | None = False,
        path_to_db_dir: str | None = "./tmp/teachable_agent_db",
        recall_threshold: float | None = 1.5,
        max_num_retrievals: int | None = 10,
        llm_config: LLMConfig | dict[str, Any] | bool | None = None,
    ):
        """Args:
        verbosity (Optional, int): # 0 (default) for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.
        reset_db (Optional, bool): True to clear the DB before starting. Default False.
        path_to_db_dir (Optional, str): path to the directory where this particular agent's DB is stored. Default "./tmp/teachable_agent_db"
        recall_threshold (Optional, float): The maximum distance for retrieved memos, where 0.0 is exact match. Default 1.5. Larger values allow more (but less relevant) memos to be recalled.
        max_num_retrievals (Optional, int): The maximum number of memos to retrieve from the DB. Default 10.
        llm_config (LLMConfig or dict or False): llm inference configuration passed to TextAnalyzerAgent.
            If None, TextAnalyzerAgent uses llm_config from the teachable agent.
        """
        self.verbosity = verbosity
        self.path_to_db_dir = path_to_db_dir
        self.recall_threshold = recall_threshold
        self.max_num_retrievals = max_num_retrievals
        self.llm_config = llm_config

        self.analyzer = None
        self.teachable_agent = None

        # Create the memo store.
        self.memo_store = MemoStore(self.verbosity, reset_db, self.path_to_db_dir)

    def add_to_agent(self, agent: ConversableAgent):
        """Adds teachability to the given agent."""
        self.teachable_agent = agent

        # Register a hook for processing the last message.
        agent.register_hook(hookable_method="process_last_received_message", hook=self.process_last_received_message)

        # Was an llm_config passed to the constructor?
        if self.llm_config is None:
            # No. Use the agent's llm_config.
            self.llm_config = agent.llm_config
        assert self.llm_config, "Teachability requires a valid llm_config."

        # Create the analyzer agent.
        self.analyzer = TextAnalyzerAgent(llm_config=self.llm_config)

        # Append extra info to the system message.
        agent.update_system_message(
            agent.system_message
            + "\nYou've been given the special ability to remember user teachings from prior conversations."
        )

    def prepopulate_db(self):
        """Adds a few arbitrary memos to the DB."""
        self.memo_store.prepopulate()

    def process_last_received_message(self, text: dict[str, Any] | str):
        """Appends any relevant memos to the message text, and stores any apparent teachings in new memos.
        Uses TextAnalyzerAgent to make decisions about memo storage and retrieval.
        """
        # Try to retrieve relevant memos from the DB.
        expanded_text = text
        if self.memo_store.last_memo_id > 0:
            expanded_text = self._consider_memo_retrieval(text)

        # Try to store any user teachings in new memos to be used in the future.
        self._consider_memo_storage(text)

        # Return the (possibly) expanded message text.
        return expanded_text

    def _consider_memo_storage(self, comment: dict[str, Any] | str):
        """Decides whether to store something from one user comment in the DB."""
        memo_added = False

        # Check for a problem-solution pair.
        response = self._analyze(
            comment,
            "Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.",
        )
        if "yes" in response.lower():
            # Can we extract advice?
            advice = self._analyze(
                comment,
                "Briefly copy any advice from the TEXT that may be useful for a similar but different task in the future. But if no advice is present, just respond with 'none'.",
            )
            if "none" not in advice.lower():
                # Yes. Extract the task.
                task = self._analyze(
                    comment,
                    "Briefly copy just the task from the TEXT, then stop. Don't solve it, and don't include any advice.",
                )
                # Generalize the task.
                general_task = self._analyze(
                    task,
                    "Summarize very briefly, in general terms, the type of task described in the TEXT. Leave out details that might not appear in a similar problem.",
                )
                # Add the task-advice (problem-solution) pair to the vector DB.
                if self.verbosity >= 1:
                    print(colored("\nREMEMBER THIS TASK-ADVICE PAIR", "light_yellow"))
                self.memo_store.add_input_output_pair(general_task, advice)
                memo_added = True

        # Check for information to be learned.
        response = self._analyze(
            comment,
            "Does the TEXT contain information that could be committed to memory? Answer with just one word, yes or no.",
        )
        if "yes" in response.lower():
            # Yes. What question would this information answer?
            question = self._analyze(
                comment,
                "Imagine that the user forgot this information in the TEXT. How would they ask you for this information? Include no other text in your response.",
            )
            # Extract the information.
            answer = self._analyze(
                comment, "Copy the information from the TEXT that should be committed to memory. Add no explanation."
            )
            # Add the question-answer pair to the vector DB.
            if self.verbosity >= 1:
                print(colored("\nREMEMBER THIS QUESTION-ANSWER PAIR", "light_yellow"))
            self.memo_store.add_input_output_pair(question, answer)
            memo_added = True

        # Were any memos added?
        if memo_added:
            # Yes. Save them to disk.
            self.memo_store._save_memos()

    def _consider_memo_retrieval(self, comment: dict[str, Any] | str):
        """Decides whether to retrieve memos from the DB, and add them to the chat context."""
        # First, use the comment directly as the lookup key.
        if self.verbosity >= 1:
            print(colored("\nLOOK FOR RELEVANT MEMOS, AS QUESTION-ANSWER PAIRS", "light_yellow"))
        memo_list = self._retrieve_relevant_memos(comment)

        # Next, if the comment involves a task, then extract and generalize the task before using it as the lookup key.
        response = self._analyze(
            comment,
            "Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.",
        )
        if "yes" in response.lower():
            if self.verbosity >= 1:
                print(colored("\nLOOK FOR RELEVANT MEMOS, AS TASK-ADVICE PAIRS", "light_yellow"))
            # Extract the task.
            task = self._analyze(
                comment, "Copy just the task from the TEXT, then stop. Don't solve it, and don't include any advice."
            )
            # Generalize the task.
            general_task = self._analyze(
                task,
                "Summarize very briefly, in general terms, the type of task described in the TEXT. Leave out details that might not appear in a similar problem.",
            )
            # Append any relevant memos.
            memo_list.extend(self._retrieve_relevant_memos(general_task))

        # De-duplicate the memo list.
        memo_list = list(set(memo_list))

        # Append the memos to the text of the last message.
        return comment + self._concatenate_memo_texts(memo_list)

    def _retrieve_relevant_memos(self, input_text: str) -> list:
        """Returns semantically related memos from the DB."""
        memo_list = self.memo_store.get_related_memos(
            input_text, n_results=self.max_num_retrievals, threshold=self.recall_threshold
        )

        if self.verbosity >= 1:  # noqa: SIM102
            # Was anything retrieved?
            if len(memo_list) == 0:
                # No. Look at the closest memo.
                print(colored("\nTHE CLOSEST MEMO IS BEYOND THE THRESHOLD:", "light_yellow"))
                self.memo_store.get_nearest_memo(input_text)
                print()  # Print a blank line. The memo details were printed by get_nearest_memo().

        # Create a list of just the memo output_text strings.
        memo_list = [memo[1] for memo in memo_list]
        return memo_list

    def _concatenate_memo_texts(self, memo_list: list) -> str:
        """Concatenates the memo texts into a single string for inclusion in the chat context."""
        memo_texts = ""
        if len(memo_list) > 0:
            info = "\n# Memories that might help\n"
            for memo in memo_list:
                info = info + "- " + memo + "\n"
            if self.verbosity >= 1:
                print(colored("\nMEMOS APPENDED TO LAST MESSAGE...\n" + info + "\n", "light_yellow"))
            memo_texts = memo_texts + "\n" + info
        return memo_texts

    def _analyze(self, text_to_analyze: dict[str, Any] | str, analysis_instructions: dict[str, Any] | str):
        """Asks TextAnalyzerAgent to analyze the given text according to specific instructions."""
        self.analyzer.reset()  # Clear the analyzer's list of messages.
        self.teachable_agent.send(
            recipient=self.analyzer, message=text_to_analyze, request_reply=False, silent=(self.verbosity < 2)
        )  # Put the message in the analyzer's list.
        self.teachable_agent.send(
            recipient=self.analyzer, message=analysis_instructions, request_reply=True, silent=(self.verbosity < 2)
        )  # Request the reply.
        return self.teachable_agent.last_message(self.analyzer)["content"]


@require_optional_import("chromadb", "teachable")
class MemoStore:
    """Provides memory storage and retrieval for a teachable agent, using a vector database.
    Each DB entry (called a memo) is a pair of strings: an input text and an output text.
    The input text might be a question, or a task to perform.
    The output text might be an answer to the question, or advice on how to perform the task.
    Vector embeddings are currently supplied by Chroma's default Sentence Transformers.
    """

    def __init__(
        self,
        verbosity: int | None = 0,
        reset: bool | None = False,
        path_to_db_dir: str | None = "./tmp/teachable_agent_db",
    ):
        """Args:
        - verbosity (Optional, int): 1 to print memory operations, 0 to omit them. 3+ to print memo lists.
        - reset (Optional, bool): True to clear the DB before starting. Default False.
        - path_to_db_dir (Optional, str): path to the directory where the DB is stored.
        """
        self.verbosity = verbosity
        self.path_to_db_dir = path_to_db_dir

        # Load or create the vector DB on disk.
        settings = Settings(
            anonymized_telemetry=False, allow_reset=True, is_persistent=True, persist_directory=path_to_db_dir
        )
        self.db_client = chromadb.Client(settings)
        self.vec_db = self.db_client.create_collection("memos", get_or_create=True)  # The collection is the DB.

        # Load or create the associated memo dict on disk.
        self.path_to_dict = os.path.join(path_to_db_dir, "uid_text_dict.pkl")
        self.uid_text_dict = {}
        self.last_memo_id = 0
        if (not reset) and os.path.exists(self.path_to_dict):
            print(colored("\nLOADING MEMORY FROM DISK", "light_green"))
            print(colored(f"    Location = {self.path_to_dict}", "light_green"))
            with open(self.path_to_dict, "rb") as f:
                self.uid_text_dict = pickle.load(f)
                self.last_memo_id = len(self.uid_text_dict)
                if self.verbosity >= 3:
                    self.list_memos()

        # Clear the DB if requested.
        if reset:
            self.reset_db()

    def list_memos(self):
        """Prints the contents of MemoStore."""
        print(colored("LIST OF MEMOS", "light_green"))
        for uid, text in self.uid_text_dict.items():
            input_text, output_text = text
            print(
                colored(
                    f"  ID: {uid}\n    INPUT TEXT: {input_text}\n    OUTPUT TEXT: {output_text}",
                    "light_green",
                )
            )

    def _save_memos(self):
        """Saves self.uid_text_dict to disk."""
        with open(self.path_to_dict, "wb") as file:
            pickle.dump(self.uid_text_dict, file)

    def reset_db(self):
        """Forces immediate deletion of the DB's contents, in memory and on disk."""
        print(colored("\nCLEARING MEMORY", "light_green"))
        self.db_client.delete_collection("memos")
        self.vec_db = self.db_client.create_collection("memos")
        self.uid_text_dict = {}
        self._save_memos()

    def add_input_output_pair(self, input_text: str, output_text: str):
        """Adds an input-output pair to the vector DB."""
        self.last_memo_id += 1
        self.vec_db.add(documents=[input_text], ids=[str(self.last_memo_id)])
        self.uid_text_dict[str(self.last_memo_id)] = input_text, output_text
        if self.verbosity >= 1:
            print(
                colored(
                    f"\nINPUT-OUTPUT PAIR ADDED TO VECTOR DATABASE:\n  ID\n    {self.last_memo_id}\n  INPUT\n    {input_text}\n  OUTPUT\n    {output_text}\n",
                    "light_yellow",
                )
            )
        if self.verbosity >= 3:
            self.list_memos()

    def get_nearest_memo(self, query_text: str):
        """Retrieves the nearest memo to the given query text."""
        results = self.vec_db.query(query_texts=[query_text], n_results=1)
        uid, input_text, distance = results["ids"][0][0], results["documents"][0][0], results["distances"][0][0]
        input_text_2, output_text = self.uid_text_dict[uid]
        assert input_text == input_text_2
        if self.verbosity >= 1:
            print(
                colored(
                    f"\nINPUT-OUTPUT PAIR RETRIEVED FROM VECTOR DATABASE:\n  INPUT1\n    {input_text}\n  OUTPUT\n    {output_text}\n  DISTANCE\n    {distance}",
                    "light_yellow",
                )
            )
        return input_text, output_text, distance

    def get_related_memos(self, query_text: str, n_results: int, threshold: int | float):
        """Retrieves memos that are related to the given query text within the specified distance threshold."""
        if n_results > len(self.uid_text_dict):
            n_results = len(self.uid_text_dict)
        results = self.vec_db.query(query_texts=[query_text], n_results=n_results)
        memos = []
        num_results = len(results["ids"][0])
        for i in range(num_results):
            uid, input_text, distance = results["ids"][0][i], results["documents"][0][i], results["distances"][0][i]
            if distance < threshold:
                input_text_2, output_text = self.uid_text_dict[uid]
                assert input_text == input_text_2
                if self.verbosity >= 1:
                    print(
                        colored(
                            f"\nINPUT-OUTPUT PAIR RETRIEVED FROM VECTOR DATABASE:\n  INPUT1\n    {input_text}\n  OUTPUT\n    {output_text}\n  DISTANCE\n    {distance}",
                            "light_yellow",
                        )
                    )
                memos.append((input_text, output_text, distance))
        return memos

    def prepopulate(self):
        """Adds a few arbitrary examples to the vector DB, just to make retrieval less trivial."""
        if self.verbosity >= 1:
            print(colored("\nPREPOPULATING MEMORY", "light_green"))
        examples = []
        examples.append({"text": "When I say papers I mean research papers, which are typically pdfs.", "label": "yes"})
        examples.append({"text": "Please verify that each paper you listed actually uses langchain.", "label": "no"})
        examples.append({"text": "Tell gpt the output should still be latex code.", "label": "no"})
        examples.append({"text": "Hint: convert pdfs to text and then answer questions based on them.", "label": "yes"})
        examples.append({
            "text": "To create a good PPT, include enough content to make it interesting.",
            "label": "yes",
        })
        examples.append({
            "text": "No, for this case the columns should be aspects and the rows should be frameworks.",
            "label": "no",
        })
        examples.append({"text": "When writing code, remember to include any libraries that are used.", "label": "yes"})
        examples.append({"text": "Please summarize the papers by Eric Horvitz on bounded rationality.", "label": "no"})
        examples.append({"text": "Compare the h-index of Daniel Weld and Oren Etzioni.", "label": "no"})
        examples.append({
            "text": "Double check to be sure that the columns in a table correspond to what was asked for.",
            "label": "yes",
        })
        for example in examples:
            self.add_input_output_pair(example["text"], example["label"])
        self._save_memos()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from ....agentchat import ConversableAgent
from ....tools import Tool


class ToolsCapability:
    """Adding a list of tools as composable capabilities to a single agent.
    This class can be inherited from to allow code to run at the point of creating or adding the capability.

    Note: both caller and executor of the tools are the same agent.
    """

    def __init__(self, tool_list: list[Tool]):
        self.tools = list(tool_list)

    def add_to_agent(self, agent: ConversableAgent):
        """Add tools to the given agent."""
        for tool in self.tools:
            tool.register_tool(agent=agent)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

__all__: list[str] = []
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from collections.abc import Hashable
from typing import Any

from .... import token_count_utils
from ....cache.abstract_cache_base import AbstractCache
from ....oai.openai_utils import filter_config
from ....types import MessageContentType


def cache_key(content: MessageContentType, *args: Hashable) -> str:
    """Calculates the cache key for the given message content and any other hashable args.

    Args:
        content (MessageContentType): The message content to calculate the cache key for.
        *args: Any additional hashable args to include in the cache key.
    """
    str_keys = [str(key) for key in (content, *args)]
    return "".join(str_keys)


def cache_content_get(cache: AbstractCache | None, key: str) -> tuple[MessageContentType, ...] | None:
    """Retrieves cached content from the cache.

    Args:
        cache (None or AbstractCache): The cache to retrieve the content from. If None, the cache is ignored.
        key (str): The key to retrieve the content from.
    """
    if cache:
        cached_value = cache.get(key)
        if cached_value:
            return cached_value


def cache_content_set(cache: AbstractCache | None, key: str, content: MessageContentType, *extra_values):
    """Sets content into the cache.

    Args:
        cache (None or AbstractCache): The cache to set the content into. If None, the cache is ignored.
        key (str): The key to set the content into.
        content (MessageContentType): The message content to set into the cache.
        *extra_values: Additional values to be passed to the cache.
    """
    if cache:
        cache_value = (content, *extra_values)
        cache.set(key, cache_value)


def min_tokens_reached(messages: list[dict[str, Any]], min_tokens: int | None) -> bool:
    """Returns True if the total number of tokens in the messages is greater than or equal to the specified value.

    Args:
        messages (List[Dict]): A list of messages to check.
        min_tokens (None or int): The minimum number of tokens to check for.
    """
    if not min_tokens:
        return True

    messages_tokens = sum(count_text_tokens(msg["content"]) for msg in messages if "content" in msg)
    return messages_tokens >= min_tokens


def count_text_tokens(content: MessageContentType) -> int:
    """Calculates the number of text tokens in the given message content.

    Args:
        content (MessageContentType): The message content to calculate the number of text tokens for.
    """
    token_count = 0
    if isinstance(content, str):
        token_count = token_count_utils.count_token(content)
    elif isinstance(content, list):
        for item in content:
            if isinstance(item, str):
                token_count += token_count_utils.count_token(item)
            else:
                token_count += count_text_tokens(item.get("text", ""))
    return token_count


def is_content_right_type(content: Any) -> bool:
    """A helper function to check if the passed in content is of the right type."""
    return isinstance(content, (str, list))


def is_content_text_empty(content: MessageContentType) -> bool:
    """Checks if the content of the message does not contain any text.

    Args:
        content (MessageContentType): The message content to check.
    """
    if isinstance(content, str):
        return content == ""
    elif isinstance(content, list):
        texts = []
        for item in content:
            if isinstance(item, str):
                texts.append(item)
            elif isinstance(item, dict):
                texts.append(item.get("text", ""))
        return not any(texts)
    else:
        return True


def should_transform_message(message: dict[str, Any], filter_dict: dict[str, Any] | None, exclude: bool) -> bool:
    """Validates whether the transform should be applied according to the filter dictionary.

    Args:
        message (Dict[str, Any]): The message to validate.
        filter_dict (None or Dict[str, Any]): The filter dictionary to validate against. If None, the transform is always applied.
        exclude (bool): Whether to exclude messages that match the filter dictionary.
    """
    if not filter_dict:
        return True

    return len(filter_config([message], filter_dict, exclude)) > 0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
from typing import TYPE_CHECKING, Any

from ....formatting_utils import colored
from .transforms import MessageTransform

if TYPE_CHECKING:
    from ...conversable_agent import ConversableAgent


class TransformMessages:
    """Agent capability for transforming messages before reply generation.

    This capability allows you to apply a series of message transformations to
    a ConversableAgent's incoming messages before they are processed for response
    generation. This is useful for tasks such as:

    - Limiting the number of messages considered for context.
    - Truncating messages to meet token limits.
    - Filtering sensitive information.
    - Customizing message formatting.

    To use `TransformMessages`:

    1. Create message transformations (e.g., `MessageHistoryLimiter`, `MessageTokenLimiter`).
    2. Instantiate `TransformMessages` with a list of these transformations.
    3. Add the `TransformMessages` instance to your `ConversableAgent` using `add_to_agent`.

    NOTE: Order of message transformations is important. You could get different results based on
        the order of transformations.

    Example:
        ```python
        from agentchat import ConversableAgent
        from agentchat.contrib.capabilities import TransformMessages, MessageHistoryLimiter, MessageTokenLimiter

        max_messages = MessageHistoryLimiter(max_messages=2)
        truncate_messages = MessageTokenLimiter(max_tokens=500)
        transform_messages = TransformMessages(transforms=[max_messages, truncate_messages])

        agent = ConversableAgent(...)
        transform_messages.add_to_agent(agent)
        ```
    """

    def __init__(self, *, transforms: list[MessageTransform] = [], verbose: bool = True):
        """Args:
        transforms: A list of message transformations to apply.
        verbose: Whether to print logs of each transformation or not.
        """
        self._transforms = transforms
        self._verbose = verbose

    def add_to_agent(self, agent: "ConversableAgent"):
        """Adds the message transformations capability to the specified ConversableAgent.

        This function performs the following modifications to the agent:

        1. Registers a hook that automatically transforms all messages before they are processed for
            response generation.
        """
        agent.register_hook(hookable_method="process_all_messages_before_reply", hook=self._transform_messages)

    def _transform_messages(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        post_transform_messages = copy.deepcopy(messages)
        system_message = None

        if messages[0]["role"] == "system":
            system_message = copy.deepcopy(messages[0])
            post_transform_messages.pop(0)

        for transform in self._transforms:
            # deepcopy in case pre_transform_messages will later be used for logs printing
            pre_transform_messages = (
                copy.deepcopy(post_transform_messages) if self._verbose else post_transform_messages
            )
            post_transform_messages = transform.apply_transform(pre_transform_messages)

            if self._verbose:
                logs_str, had_effect = transform.get_logs(pre_transform_messages, post_transform_messages)
                if had_effect:
                    print(colored(logs_str, "yellow"))

        if system_message:
            post_transform_messages.insert(0, system_message)

        return post_transform_messages
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from ...assistant_agent import ConversableAgent


class AgentCapability:
    """Base class for composable capabilities that can be added to an agent."""

    def __init__(self):
        pass

    def add_to_agent(self, agent: ConversableAgent):
        """Adds a particular capability to the given agent. Must be implemented by the capability subclass.
        An implementation will typically call agent.register_hook() one or more times. See teachability.py as an example.
        """
        raise NotImplementedError
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import re
from typing import Any, Literal, Optional, Protocol

from .... import Agent, ConversableAgent, code_utils
from ....cache import AbstractCache
from ....import_utils import optional_import_block, require_optional_import
from ....llm_config import LLMConfig
from .. import img_utils
from ..capabilities.agent_capability import AgentCapability
from ..text_analyzer_agent import TextAnalyzerAgent

with optional_import_block():
    from PIL.Image import Image
    from openai import OpenAI

SYSTEM_MESSAGE = "You've been given the special ability to generate images."
DESCRIPTION_MESSAGE = "This agent has the ability to generate images."

PROMPT_INSTRUCTIONS = """In detail, please summarize the provided prompt to generate the image described in the TEXT.
DO NOT include any advice. RESPOND like the following example:
EXAMPLE: Blue background, 3D shapes, ...
"""


class ImageGenerator(Protocol):
    """This class defines an interface for image generators.

    Concrete implementations of this protocol must provide a `generate_image` method that takes a string prompt as
    input and returns a PIL Image object.

    NOTE: Current implementation does not allow you to edit a previously existing image.
    """

    def generate_image(self, prompt: str) -> "Image":
        """Generates an image based on the provided prompt.

        Args:
          prompt: A string describing the desired image.

        Returns:
          A PIL Image object representing the generated image.

        Raises:
          ValueError: If the image generation fails.
        """
        ...

    def cache_key(self, prompt: str) -> str:
        """Generates a unique cache key for the given prompt.

        This key can be used to store and retrieve generated images based on the prompt.

        Args:
          prompt: A string describing the desired image.

        Returns:
          A unique string that can be used as a cache key.
        """
        ...


@require_optional_import("PIL", "unknown")
@require_optional_import("openai>=1.66.2", "openai")
class DalleImageGenerator:
    """Generates images using OpenAI's DALL-E models.

    This class provides a convenient interface for generating images based on textual prompts using OpenAI's DALL-E
    models. It allows you to specify the DALL-E model, resolution, quality, and the number of images to generate.

    Note: Current implementation does not allow you to edit a previously existing image.
    """

    def __init__(
        self,
        llm_config: LLMConfig | dict[str, Any],
        resolution: Literal["256x256", "512x512", "1024x1024", "1792x1024", "1024x1792"] = "1024x1024",
        quality: Literal["standard", "hd"] = "standard",
        num_images: int = 1,
    ):
        """Args:
        llm_config (LLMConfig or dict): llm config, must contain a valid dalle model and OpenAI API key in config_list.
        resolution (str): The resolution of the image you want to generate. Must be one of "256x256", "512x512", "1024x1024", "1792x1024", "1024x1792".
        quality (str): The quality of the image you want to generate. Must be one of "standard", "hd".
        num_images (int): The number of images to generate.
        """
        config_list = llm_config["config_list"]
        _validate_dalle_model(config_list[0]["model"])
        _validate_resolution_format(resolution)

        self._model = config_list[0]["model"]
        self._resolution = resolution
        self._quality = quality
        self._num_images = num_images
        self._dalle_client = OpenAI(api_key=config_list[0]["api_key"])

    def generate_image(self, prompt: str) -> "Image":
        response = self._dalle_client.images.generate(
            model=self._model,
            prompt=prompt,
            size=self._resolution,
            quality=self._quality,
            n=self._num_images,
        )

        image_url = response.data[0].url
        if image_url is None:
            raise ValueError("Failed to generate image.")

        return img_utils.get_pil_image(image_url)

    def cache_key(self, prompt: str) -> str:
        keys = (prompt, self._model, self._resolution, self._quality, self._num_images)
        return ",".join([str(k) for k in keys])


@require_optional_import("PIL", "unknown")
class ImageGeneration(AgentCapability):
    """This capability allows a ConversableAgent to generate images based on the message received from other Agents.

    1. Utilizes a TextAnalyzerAgent to analyze incoming messages to identify requests for image generation and
        extract relevant details.
    2. Leverages the provided ImageGenerator (e.g., DalleImageGenerator) to create the image.
    3. Optionally caches generated images for faster retrieval in future conversations.

    NOTE: This capability increases the token usage of the agent, as it uses TextAnalyzerAgent to analyze every
        message received by the agent.

    Example:
        ```python
        import autogen
        from autogen.agentchat.contrib.capabilities.image_generation import ImageGeneration

        # Assuming you have llm configs configured for the LLMs you want to use and Dalle.
        # Create the agent
        agent = autogen.ConversableAgent(
            name="dalle", llm_config={...}, max_consecutive_auto_reply=3, human_input_mode="NEVER"
        )

        # Create an ImageGenerator with desired settings
        dalle_gen = generate_images.DalleImageGenerator(llm_config={...})

        # Add the ImageGeneration capability to the agent
        agent.add_capability(ImageGeneration(image_generator=dalle_gen))
        ```
    """

    def __init__(
        self,
        image_generator: ImageGenerator,
        cache: AbstractCache | None = None,
        text_analyzer_llm_config: LLMConfig | dict[str, Any] | None = None,
        text_analyzer_instructions: str = PROMPT_INSTRUCTIONS,
        verbosity: int = 0,
        register_reply_position: int = 2,
    ):
        """Args:
        image_generator (ImageGenerator): The image generator you would like to use to generate images.
        cache (None or AbstractCache): The cache client to use to store and retrieve generated images. If None,
            no caching will be used.
        text_analyzer_llm_config (LLMConfig or Dict or None): The LLM config for the text analyzer. If None, the LLM config will
            be retrieved from the agent you're adding the ability to.
        text_analyzer_instructions (str): Instructions provided to the TextAnalyzerAgent used to analyze
            incoming messages and extract the prompt for image generation. The default instructions focus on
            summarizing the prompt. You can customize the instructions to achieve more granular control over prompt
            extraction.
            Example: 'Extract specific details from the message, like desired objects, styles, or backgrounds.'
        verbosity (int): The verbosity level. Defaults to 0 and must be greater than or equal to 0. The text
            analyzer llm calls will be silent if verbosity is less than 2.
        register_reply_position (int): The position of the reply function in the agent's list of reply functions.
            This capability registers a new reply function to handle messages with image generation requests.
            Defaults to 2 to place it after the check termination and human reply for a ConversableAgent.
        """
        self._image_generator = image_generator
        self._cache = cache
        self._text_analyzer_llm_config = text_analyzer_llm_config
        self._text_analyzer_instructions = text_analyzer_instructions
        self._verbosity = verbosity
        self._register_reply_position = register_reply_position

        self._agent: ConversableAgent | None = None
        self._text_analyzer: TextAnalyzerAgent | None = None

    def add_to_agent(self, agent: ConversableAgent):
        """Adds the Image Generation capability to the specified ConversableAgent.

        This function performs the following modifications to the agent:

        1. Registers a reply function: A new reply function is registered with the agent to handle messages that
           potentially request image generation. This function analyzes the message and triggers image generation if
           necessary.
        2. Creates an Agent (TextAnalyzerAgent): This is used to analyze messages for image generation requirements.
        3. Updates System Message: The agent's system message is updated to include a message indicating the
           capability to generate images has been added.
        4. Updates Description: The agent's description is updated to reflect the addition of the Image Generation
           capability. This might be helpful in certain use cases, like group chats.

        Args:
          agent (ConversableAgent): The ConversableAgent to add the capability to.
        """
        self._agent = agent

        agent.register_reply([Agent, None], self._image_gen_reply, position=self._register_reply_position)

        self._text_analyzer_llm_config = self._text_analyzer_llm_config or agent.llm_config
        self._text_analyzer = TextAnalyzerAgent(llm_config=self._text_analyzer_llm_config)

        agent.update_system_message(agent.system_message + "\n" + SYSTEM_MESSAGE)
        agent.description += "\n" + DESCRIPTION_MESSAGE

    def _image_gen_reply(
        self,
        recipient: ConversableAgent,
        messages: list[dict[str, Any]] | None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        if messages is None:
            return False, None

        last_message = code_utils.content_str(messages[-1]["content"])

        if not last_message:
            return False, None

        if self._should_generate_image(last_message):
            prompt = self._extract_prompt(last_message)

            image = self._cache_get(prompt)
            if image is None:
                image = self._image_generator.generate_image(prompt)
                self._cache_set(prompt, image)

            return True, self._generate_content_message(prompt, image)

        else:
            return False, None

    def _should_generate_image(self, message: str) -> bool:
        assert self._text_analyzer is not None

        instructions = """
        Does any part of the TEXT ask the agent to generate an image?
        The TEXT must explicitly mention that the image must be generated.
        Answer with just one word, yes or no.
        """
        analysis = self._text_analyzer.analyze_text(message, instructions)

        return "yes" in self._extract_analysis(analysis).lower()

    def _extract_prompt(self, last_message) -> str:
        assert self._text_analyzer is not None

        analysis = self._text_analyzer.analyze_text(last_message, self._text_analyzer_instructions)
        return self._extract_analysis(analysis)

    def _cache_get(self, prompt: str) -> Optional["Image"]:
        if self._cache:
            key = self._image_generator.cache_key(prompt)
            cached_value = self._cache.get(key)

            if cached_value:
                return img_utils.get_pil_image(cached_value)

    def _cache_set(self, prompt: str, image: "Image"):
        if self._cache:
            key = self._image_generator.cache_key(prompt)
            self._cache.set(key, img_utils.pil_to_data_uri(image))

    def _extract_analysis(self, analysis: str | dict[str, Any] | None) -> str:
        if isinstance(analysis, dict):
            return code_utils.content_str(analysis["content"])
        else:
            return code_utils.content_str(analysis)

    def _generate_content_message(self, prompt: str, image: "Image") -> dict[str, Any]:
        return {
            "content": [
                {"type": "text", "text": f"I generated an image with the prompt: {prompt}"},
                {"type": "image_url", "image_url": {"url": img_utils.pil_to_data_uri(image)}},
            ]
        }


# Helpers
def _validate_resolution_format(resolution: str):
    """Checks if a string is in a valid resolution format (e.g., "1024x768")."""
    pattern = r"^\d+x\d+$"  # Matches a pattern of digits, "x", and digits
    matched_resolution = re.match(pattern, resolution)
    if matched_resolution is None:
        raise ValueError(f"Invalid resolution format: {resolution}")


def _validate_dalle_model(model: str):
    if model not in ["dall-e-3", "dall-e-2"]:
        raise ValueError(f"Invalid DALL-E model: {model}. Must be 'dall-e-3' or 'dall-e-2'")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
import json
from typing import Any

from ... import OpenAIWrapper, filter_config
from ...code_utils import execute_code
from ...llm_config import LLMConfig

ADD_FUNC = {
    "type": "function",
    "function": {
        "name": "add_function",
        "description": "Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "The name of the function in the code implementation."},
                "description": {"type": "string", "description": "A short description of the function."},
                "arguments": {
                    "type": "string",
                    "description": 'JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { "url": { "type": "string", "description": "The URL", }}. Please avoid the error \'array schema missing items\' when using array type.',
                },
                "packages": {
                    "type": "string",
                    "description": "A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.",
                },
                "code": {
                    "type": "string",
                    "description": "The implementation in Python. Do not include the function declaration.",
                },
            },
            "required": ["name", "description", "arguments", "packages", "code"],
        },
    },
}

REVISE_FUNC = {
    "type": "function",
    "function": {
        "name": "revise_function",
        "description": "Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "The name of the function in the code implementation."},
                "description": {"type": "string", "description": "A short description of the function."},
                "arguments": {
                    "type": "string",
                    "description": 'JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { "url": { "type": "string", "description": "The URL", }}. Please avoid the error \'array schema missing items\' when using array type.',
                },
                "packages": {
                    "type": "string",
                    "description": "A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.",
                },
                "code": {
                    "type": "string",
                    "description": "The implementation in Python. Do not include the function declaration.",
                },
            },
            "required": ["name", "description", "arguments", "packages", "code"],
        },
    },
}

REMOVE_FUNC = {
    "type": "function",
    "function": {
        "name": "remove_function",
        "description": "Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "The name of the function in the code implementation."}
            },
            "required": ["name"],
        },
    },
}

OPT_PROMPT = """You are a function optimizer. Your task is to maintain a list of functions for the assistant according to the existing function list and conversation history that happens between the assistant and the user.
You can perform one of the following four actions to manipulate the function list using the functions you have:
1. Revise one existing function (using revise_function).
2. Remove one existing function (using remove_function).
3. Add one new function (using add_function).
4. Directly return "TERMINATE" to me if no more actions are needed for the current function list.

Below are the principles that you need to follow for taking these four actions.
(1) Revise one existing function:
1. Pay more attention to the failed tasks and corresponding error information, and optimize the function used in these tasks according to the conversation history if needed.
2. A failed function call can occur due to incorrect input arguments (missing arguments) or an incorrect function code implementation. You should focus more on the function code implementation and make it easy to get success function call.
3. Do not revise the function that you think works well and plays a critical role in solving the problems according to the conversation history. Only making revisions if needed.
4. Sometimes, a NameError may occur. To fix this error, you can either revise the name of the function in the code implementation or revise the name of the function call to make these two names consistent.
(2) Remove one existing function:
1. Only remove the function that you think is not needed anymore in future tasks.
(3) Add one new function:
1. The added function should be general enough to be used in future tasks. For instance, if you encounter a problem that this function can solve, or one step of it, you can use the generated function directly instead of starting from scratch
2. The added new function should solve a higher-level question that encompasses the original query and extend the code's functionality to make it more versatile and widely applicable.
3. Replace specific strings or variable names with general variables to enhance the tool's applicability to various queries. All names used inside the function should be passed in as arguments.
Below is an example of a function that potentially deserves to be added in solving MATH problems, which can be used to solve a higher-level question:
{{
    \"name\": \"evaluate_expression\",
    \"description\": \"Evaluate arithmetic or mathematical expressions provided as strings.\",
    \"arguments\": {{
        \"expression\": {{
            \"type\": \"string\",
            \"description\": \"The mathematical expression to evaluate.\"
        }}
    }},
    \"packages\": \"sympy\",
    \"code\": \"from sympy import sympify, SympifyError\\n\\ndef evaluate_expression(expression):\\n    try:\\n        result = sympify(expression)\\n        if result.is_number:\\n            result = float(result)\\n        else:\\n            result = str(result)\\n        return result\\n    except SympifyError as e:\\n        return str(e)\"
}}
(4) Directly return "TERMINATE":
If you think there is no need to perform any other actions for the current function list since the current list is optimal more actions will harm the performance in future tasks. Please directly reply to me with "TERMINATE".

One function signature includes the following five elements:
1. Function name
2. Function description
3. JSON schema of arguments encoded as a string
4. A list of package names imported by the function packages
5. The code implementation

Below are the signatures of the current functions:
List A: {best_functions}.
The following list are the function signatures that you have after taking {actions_num} actions to manipulate List A:
List B: {incumbent_functions}.

{accumulated_experience}

Here are {best_conversations_num} conversation histories of solving {best_conversations_num} tasks using List A.
History:
{best_conversations_history}

{statistic_informations}

According to the information I provide, please take one of four actions to manipulate list B using the functions you know.
Instead of returning TERMINATE directly or taking no action, you should try your best to optimize the function list. Only take no action if you really think the current list is optimal, as more actions will harm performance in future tasks.
Even adding a general function that can substitute the assistant's repeated suggestions of Python code with the same functionality could also be helpful.
"""


def execute_func(name, packages, code, **args):
    """The wrapper for generated functions."""
    pip_install = (
        f"""print("Installing package: {packages}")\nsubprocess.run(["pip", "-qq", "install", "{packages}"])"""
        if packages
        else ""
    )
    str = f"""
import subprocess
{pip_install}
print("Result of {name} function execution:")
{code}
args={args}
result={name}(**args)
if result is not None: print(result)
"""
    print(f"execute_code:\n{str}")
    result = execute_code(str, use_docker="shaokun529/evoagent:v1")
    if result[0] != 0:
        raise Exception("Error in executing function:" + result[1])
    print(f"Result: {result[1]}")
    return result[1]


class AgentOptimizer:
    """Base class for optimizing AG2 agents. Specifically, it is used to optimize the functions used in the agent.
    More information could be found in the following paper: https://arxiv.org/abs/2402.11359.
    """

    def __init__(
        self,
        max_actions_per_step: int,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        optimizer_model: str | None = "gpt-4-1106-preview",
    ):
        """(These APIs are experimental and may change in the future.)

        Args:
            max_actions_per_step (int): the maximum number of actions that the optimizer can take in one step.
            llm_config (LLMConfig or dict or None): llm inference configuration.
                If None, the current LLMConfig from context is used.
                Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create) for available options.
                When using OpenAI or Azure OpenAI endpoints, please specify a non-empty 'model' either in `llm_config` or in each config of 'config_list' in `llm_config`.
            optimizer_model: the model used for the optimizer.
        """
        self.max_actions_per_step = max_actions_per_step
        self._max_trials = 3
        self.optimizer_model = optimizer_model

        self._trial_conversations_history = []
        self._trial_conversations_performance = []
        self._trial_functions = []

        self._best_conversations_history = []
        self._best_conversations_performance = []
        self._best_functions = []

        self._failure_functions_performance = []
        self._best_performance = -1

        if llm_config is None:
            llm_config = LLMConfig.current
        assert isinstance(llm_config, (dict, LLMConfig)), "llm_config must be a dict or LLMConfig"
        llm_config = copy.deepcopy(llm_config)
        self.llm_config = llm_config
        if self.llm_config in [{}, {"config_list": []}, {"config_list": [{"model": ""}]}]:
            raise ValueError(
                "When using OpenAI or Azure OpenAI endpoints, specify a non-empty 'model' either in 'llm_config' or in each config of 'config_list'."
            )
        self.llm_config["config_list"] = filter_config(llm_config["config_list"], {"model": [self.optimizer_model]})
        self._client = OpenAIWrapper(**self.llm_config)

    def record_one_conversation(self, conversation_history: list[dict[str, Any]], is_satisfied: bool = None):
        """Record one conversation history.

        Args:
            conversation_history (List[Dict]): the chat messages of the conversation.
            is_satisfied (bool): whether the user is satisfied with the solution. If it is none, the user will be asked to input the satisfaction.
        """
        if is_satisfied is None:
            reply = input(
                "Please provide whether the user is satisfied with the solution. 1 represents satisfied. 0 represents not satisfied. Press enter to submit. \n"
            )
            assert reply in [
                "0",
                "1",
            ], "The input is invalid. Please input 1 or 0. 1 represents satisfied. 0 represents not satisfied."
            is_satisfied = reply == "1"
        self._trial_conversations_history.append({
            f"Conversation {len(self._trial_conversations_history)}": conversation_history
        })
        self._trial_conversations_performance.append({
            f"Conversation {len(self._trial_conversations_performance)}": 1 if is_satisfied else 0
        })

    def step(self):
        """One step of training. It will return register_for_llm and register_for_executor at each iteration,
        which are subsequently utilized to update the assistant and executor agents, respectively.
        See example: https://github.com/ag2ai/ag2/blob/main/notebook/agentchat_agentoptimizer.ipynb
        """
        performance = sum(sum(d.values()) for d in self._trial_conversations_performance) / len(
            self._trial_conversations_performance
        )

        if performance < self._best_performance:
            self._failure_functions_performance.append({"functions": self._trial_functions, "performance": performance})
            self._failure_functions_performance = sorted(
                self._failure_functions_performance, key=lambda x: x["performance"]
            )
        else:
            self._failure_functions_performance = []
            self._best_performance = performance
            self._best_functions = copy.deepcopy(self._trial_functions)
            self._best_conversations_history = copy.deepcopy(self._trial_conversations_history)
            self._best_conversations_performance = copy.deepcopy(self._trial_conversations_performance)
        self._trial_conversations_history = []
        self._trial_conversations_performance = []

        best_functions = copy.deepcopy(self._best_functions)
        incumbent_functions = copy.deepcopy(self._best_functions)
        failure_experience_prompt, statistic_prompt = self._construct_intermediate_prompt()

        for action_index in range(self.max_actions_per_step):
            prompt = OPT_PROMPT.format(
                best_conversations_history=self._best_conversations_history,
                best_conversations_num=len(self._best_conversations_history),
                actions_num=action_index,
                best_functions=best_functions,
                incumbent_functions=incumbent_functions,
                accumulated_experience=failure_experience_prompt,
                statistic_informations=statistic_prompt,
            )
            messages = [{"role": "user", "content": prompt}]
            for _ in range(self._max_trials):
                response = self._client.create(
                    messages=messages, tools=[ADD_FUNC, REVISE_FUNC, REMOVE_FUNC], tool_choice="auto"
                )
                actions = response.choices[0].message.tool_calls
                if self._validate_actions(actions, incumbent_functions):
                    break
            if actions is not None and self._validate_actions(actions, incumbent_functions):
                incumbent_functions = self._update_function_call(incumbent_functions, actions)

        remove_functions = list(
            {key for dictionary in self._trial_functions for key in dictionary}
            - {key for dictionary in incumbent_functions for key in dictionary}
        )

        register_for_llm = []
        register_for_exector = {}
        for name in remove_functions:
            register_for_llm.append({"func_sig": {"name": name}, "is_remove": True})
            register_for_exector.update({name: None})
        for func in incumbent_functions:
            register_for_llm.append({
                "func_sig": {
                    "name": func.get("name"),
                    "description": func.get("description"),
                    "parameters": {"type": "object", "properties": func.get("arguments")},
                },
                "is_remove": False,
            })
            register_for_exector.update({
                func.get("name"): lambda **args: execute_func(
                    func.get("name"), func.get("packages"), func.get("code"), **args
                )
            })

        self._trial_functions = incumbent_functions
        return register_for_llm, register_for_exector

    def reset_optimizer(self):
        """Reset the optimizer."""
        self._trial_conversations_history = []
        self._trial_conversations_performance = []
        self._trial_functions = []

        self._best_conversations_history = []
        self._best_conversations_performance = []
        self._best_functions = []

        self._best_performance = -1
        self._failure_functions_performance = []

    def _update_function_call(self, incumbent_functions, actions):
        """Update function call."""
        formatted_actions = []
        for action in actions:
            func = json.loads(action.function.arguments.strip('"'))
            func["action_name"] = action.function.name

            if func.get("action_name") == "remove_function":
                item = {
                    "action_name": func.get("action_name"),
                    "name": func.get("name"),
                }
            else:
                item = {
                    "action_name": func.get("action_name"),
                    "name": func.get("name"),
                    "description": func.get("description"),
                    "arguments": json.loads(func.get("arguments").strip('"')),
                    "packages": func.get("packages"),
                    "code": func.get("code"),
                }
            formatted_actions.append(item)
        actions = formatted_actions

        for action in actions:
            name, description, arguments, packages, code, action_name = (
                action.get("name"),
                action.get("description"),
                action.get("arguments"),
                action.get("packages"),
                action.get("code"),
                action.get("action_name"),
            )
            if action_name == "remove_function":
                incumbent_functions = [item for item in incumbent_functions if item["name"] != name]
            else:
                incumbent_functions = [item for item in incumbent_functions if item["name"] != name]
                incumbent_functions.append({
                    "name": name,
                    "description": description,
                    "arguments": arguments,
                    "packages": packages,
                    "code": code,
                })

        return incumbent_functions

    def _construct_intermediate_prompt(self):
        """Construct intermediate prompts."""
        if len(self._failure_functions_performance) != 0:
            failure_experience_prompt = "We also provide more examples for different functions and their corresponding performance (0-100).\n The following function signatures are arranged in ascending order based on their performance, where higher performance indicate better quality."
            failure_experience_prompt += "\n"
            for item in self._failure_functions_performance:
                failure_experience_prompt += "Function: \n" + str(item["functions"]) + "\n"
                failure_experience_prompt += "Performance: \n" + str(item["performance"]) + "\n"
        else:
            failure_experience_prompt = "\n"

        if len(self._best_conversations_performance) != 0:
            statistic_prompt = "The following table shows the statistical information for solving each task in each conversation and indicates, whether the result is satisfied by the users. 1 represents satisfied. 0 represents not satisfied."
            statistic_prompt += "\n"
            for item in self._best_conversations_performance:
                statistic_prompt += str(item) + "\n"
        else:
            statistic_prompt = "\n"

        return failure_experience_prompt, statistic_prompt

    def _validate_actions(self, actions, incumbent_functions):
        """Validate whether the proposed actions are feasible."""
        if actions is None:
            return True
        else:
            # val json format
            for action in actions:
                function_args = action.function.arguments
                try:
                    function_args = json.loads(function_args.strip('"'))
                    if "arguments" in function_args:
                        json.loads(function_args.get("arguments").strip('"'))
                except Exception as e:
                    print("JSON is invalid:", e)
                    return False
            # val syntax
            for action in actions:
                if action.function.name != "remove_function":
                    function_args = json.loads(action.function.arguments.strip('"'))
                    code = function_args.get("code")
                    try:
                        compile(code, "<string>", "exec")
                        print("successfully compiled")
                    except Exception as e:
                        print("Syntax is invalid:", e)
                        return False
            for action in actions:
                action_name = action.function.name
                if action_name == "remove_function":
                    function_args = json.loads(action.function.arguments.strip('"'))
                    if function_args.get("name") not in [item["name"] for item in incumbent_functions]:
                        print("The function you want to remove does not exist.")
                        return False
        return True
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import json

from pydantic import BaseModel


class Task(BaseModel):
    """Class representing a task for agent completion, includes example agent execution for criteria generation."""

    name: str
    description: str
    successful_response: str
    failed_response: str

    def get_sys_message(self):
        return f"""Task: {self.name}.
        Task description: {self.description}
        Task successful example: {self.successful_response}
        Task failed example: {self.failed_response}
        """

    @staticmethod
    def parse_json_str(task: str) -> "Task":
        """Create a Task object from a json object.

        Args:
            task (str): A json string that represents the task information.

        Returns:
            Task: A Task object that represents the json task information.
        """
        json_data = json.loads(task)
        name = json_data.get("name")
        description = json_data.get("description")
        successful_response = json_data.get("successful_response")
        failed_response = json_data.get("failed_response")
        return Task(name, description, successful_response, failed_response)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any

from ...conversable_agent import ConversableAgent


class QuantifierAgent(ConversableAgent):
    """An agent for quantifying the performance of a system using the provided criteria."""

    DEFAULT_SYSTEM_MESSAGE = """"You are a helpful assistant. You quantify the output of different tasks based on the given criteria.
    The criterion is given in a json list format where each element is a distinct criteria.
    The each element is a dictionary as follows {"name": name of the criterion, "description": criteria description , "accepted_values": possible accepted inputs for this key}
    You are going to quantify each of the crieria for a given task based on the task description.
    Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.
    Return only the dictionary, no code."""

    DEFAULT_DESCRIPTION = "An AI agent for quantifing the performance of a system using the provided criteria."

    def __init__(
        self,
        name="quantifier",
        system_message: str | None = DEFAULT_SYSTEM_MESSAGE,
        description: str | None = DEFAULT_DESCRIPTION,
        **kwargs: Any,
    ):
        """Args:
        name (str): agent name.
        system_message (str): system message for the ChatCompletion inference.
            Please override this attribute if you want to reprogram the agent.
        description (str): The description of the agent.
        **kwargs (dict): Please refer to other kwargs in
            [ConversableAgent](/docs/api-reference/autogen/ConversableAgent#conversableagent).
        """
        super().__init__(name=name, system_message=system_message, description=description, **kwargs)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any, Literal

from .... import GroupChat, GroupChatManager, UserProxyAgent
from ....llm_config import LLMConfig
from .criterion import Criterion
from .critic_agent import CriticAgent
from .quantifier_agent import QuantifierAgent
from .subcritic_agent import SubCriticAgent
from .task import Task


def generate_criteria(
    llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None,
    task: Task = None,
    additional_instructions: str = "",
    max_round=2,
    use_subcritic: bool = False,
):
    """Creates a list of criteria for evaluating the utility of a given task.

    Args:
        llm_config (LLMConfig or dict or bool): llm inference configuration.
        task (Task): The task to evaluate.
        additional_instructions (str): Additional instructions for the criteria agent.
        max_round (int): The maximum number of rounds to run the conversation.
        use_subcritic (bool): Whether to use the subcritic agent to generate subcriteria.

    Returns:
        list: A list of Criterion objects for evaluating the utility of the given task.
    """
    critic = CriticAgent(
        system_message=CriticAgent.DEFAULT_SYSTEM_MESSAGE + "\n" + additional_instructions,
        llm_config=llm_config,
    )

    critic_user = UserProxyAgent(
        name="critic_user",
        max_consecutive_auto_reply=0,  # terminate without auto-reply
        human_input_mode="NEVER",
        code_execution_config={"use_docker": False},
    )

    agents = [critic_user, critic]

    if use_subcritic:
        subcritic = SubCriticAgent(
            llm_config=llm_config,
        )
        agents.append(subcritic)

    groupchat = GroupChat(agents=agents, messages=[], max_round=max_round, speaker_selection_method="round_robin")
    critic_manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)

    critic_user.initiate_chat(critic_manager, message=task.get_sys_message())
    criteria = critic_user.last_message()
    content = criteria["content"]
    # need to strip out any extra code around the returned json
    content = content[content.find("[") : content.rfind("]") + 1]
    criteria = Criterion.parse_json_str(content)
    return criteria


def quantify_criteria(
    llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None,
    criteria: list[Criterion] = None,
    task: Task = None,
    test_case: str = "",
    ground_truth: str = "",
):
    """Quantifies the performance of a system using the provided criteria.

    Args:
        llm_config (LLMConfig or dict or bool): llm inference configuration.
        criteria ([Criterion]): A list of criteria for evaluating the utility of a given task.
        task (Task): The task to evaluate.
        test_case (str): The test case to evaluate.
        ground_truth (str): The ground truth for the test case.

    Returns:
        dict: A dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.
    """
    quantifier = QuantifierAgent(
        llm_config=llm_config,
    )

    quantifier_user = UserProxyAgent(
        name="quantifier_user",
        max_consecutive_auto_reply=0,  # terminate without auto-reply
        human_input_mode="NEVER",
        code_execution_config={"use_docker": False},
    )

    quantifier_user.initiate_chat(
        quantifier,
        message=task.get_sys_message()
        + "Evaluation dictionary: "
        + Criterion.write_json(criteria)
        + "actual test case to evaluate: "
        + test_case,
    )
    quantified_results = quantifier_user.last_message()
    return {"actual_success": ground_truth, "estimated_performance": quantified_results["content"]}
Agents for running the [AgentEval](https://docs.ag2.ai/latest/docs/blog/2023/11/20/AgentEval/) pipeline.

AgentEval is a process for evaluating a LLM-based system's performance on a given task.

When given a task to evaluate and a few example runs, the critic and subcritic agents create evaluation criteria for evaluating a system's solution. Once the criteria have been created, the quantifier agent can evaluate subsequent task solutions based on the generated criteria.

See our [blog post](https://docs.ag2.ai/blog/2024-06-21-AgentEval) for usage examples and general explanations.
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any

from ...conversable_agent import ConversableAgent


class CriticAgent(ConversableAgent):
    """An agent for creating list of criteria for evaluating the utility of a given task."""

    DEFAULT_SYSTEM_MESSAGE = """You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable and not redundant.
    Convert the evaluation criteria into a list where each item is a criteria which consists of the following dictionary as follows
    {"name": name of the criterion, "description": criteria description , "accepted_values": possible accepted inputs for this key}
    Make sure "accepted_values" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels and "description" includes the criterion description.
    Output just the criteria string you have created, no code.
    """

    DEFAULT_DESCRIPTION = "An AI agent for creating list criteria for evaluating the utility of a given task."

    def __init__(
        self,
        name="critic",
        system_message: str | None = DEFAULT_SYSTEM_MESSAGE,
        description: str | None = DEFAULT_DESCRIPTION,
        **kwargs: Any,
    ):
        """Args:
        name (str): agent name.
        system_message (str): system message for the ChatCompletion inference.
            Please override this attribute if you want to reprogram the agent.
        description (str): The description of the agent.
        **kwargs (dict): Please refer to other kwargs in
            [ConversableAgent](/docs/api-reference/autogen/ConversableAgent#conversableagent).
        """
        super().__init__(
            name=name,
            system_message=system_message,
            description=description,
            **kwargs,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import json

from pydantic import BaseModel


class Criterion(BaseModel):
    """A class that represents a criterion for agent evaluation."""

    name: str
    description: str
    accepted_values: list[str]
    sub_criteria: list[Criterion] = []

    @staticmethod
    def parse_json_str(criteria: str):
        """Create a list of Criterion objects from a json string.

        Args:
            criteria (str): Json string that represents the criteria
        returns:
            [Criterion]: A list of Criterion objects that represents the json criteria information.
        """
        return [Criterion(**crit) for crit in json.loads(criteria)]

    @staticmethod
    def write_json(criteria):
        """Create a json string from a list of Criterion objects.

        Args:
            criteria ([Criterion]): A list of Criterion objects.

        Returns:
            str: A json string that represents the list of Criterion objects.
        """
        return json.dumps([crit.model_dump() for crit in criteria], indent=2)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any

from ...conversable_agent import ConversableAgent


class SubCriticAgent(ConversableAgent):
    """An agent for creating subcriteria from a given list of criteria for evaluating the utility of a given task."""

    DEFAULT_SYSTEM_MESSAGE = """You are a helpful assistant to the critic agent. You suggest sub criteria for evaluating different tasks based on the criteria provided by the critic agent (if you feel it is needed).
        They should be distinguishable, quantifiable, and related to the overall theme of the critic's provided criteria.
        You operate by taking in the description of the criteria. You then create a new key called sub criteria where you provide the sub criteria for the given criteria.
        The value of the sub_criteria is a dictionary where the keys are the subcriteria and each value is as follows {"description": sub criteria description , "accepted_values": possible accepted inputs for this key}
        Do this for each criteria provided by the critic (removing the criteria's accepted values). "accepted_values" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. "description" includes the criterion description.
        Once you have created the sub criteria for the given criteria, you return the json (make sure to include the contents of the critic's dictionary in the final dictionary as well).
        Make sure to return a valid json and no code"""

    DEFAULT_DESCRIPTION = "An AI agent for creating subcriteria from a given list of criteria."

    def __init__(
        self,
        name="subcritic",
        system_message: str | None = DEFAULT_SYSTEM_MESSAGE,
        description: str | None = DEFAULT_DESCRIPTION,
        **kwargs: Any,
    ):
        """Args:
        name (str): agent name.
        system_message (str): system message for the ChatCompletion inference.
            Please override this attribute if you want to reprogram the agent.
        description (str): The description of the agent.
        **kwargs (dict): Please refer to other kwargs in
            [ConversableAgent](/docs/api-reference/autogen/ConversableAgent#conversableagent).
        """
        super().__init__(
            name=name,
            system_message=system_message,
            description=description,
            **kwargs,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
import copy
import inspect
import threading
import warnings
from collections.abc import Callable
from dataclasses import dataclass
from enum import Enum
from functools import partial
from types import MethodType
from typing import Annotated, Any, Literal

from pydantic import BaseModel, ConfigDict, field_serializer

from ...doc_utils import export_module
from ...events.agent_events import ErrorEvent, RunCompletionEvent
from ...io.base import IOStream
from ...io.run_response import AsyncRunResponse, AsyncRunResponseProtocol, RunResponse, RunResponseProtocol
from ...io.thread_io_stream import AsyncThreadIOStream, ThreadIOStream
from ...oai import OpenAIWrapper
from ...tools import Depends, Tool
from ...tools.dependency_injection import inject_params, on
from ..agent import Agent
from ..chat import ChatResult
from ..conversable_agent import ConversableAgent
from ..group.context_expression import ContextExpression
from ..group.context_str import ContextStr
from ..group.context_variables import __CONTEXT_VARIABLES_PARAM_NAME__, ContextVariables
from ..groupchat import SELECT_SPEAKER_PROMPT_TEMPLATE, GroupChat, GroupChatManager
from ..user_proxy_agent import UserProxyAgent

__all__ = [
    "AFTER_WORK",
    "ON_CONDITION",
    "AfterWork",
    "AfterWorkOption",
    "OnCondition",
    "OnContextCondition",
    "SwarmAgent",
    "a_initiate_swarm_chat",
    "create_swarm_transition",
    "initiate_swarm_chat",
    "register_hand_off",
    "run_swarm",
]


# Created tool executor's name
__TOOL_EXECUTOR_NAME__ = "_Swarm_Tool_Executor"


@export_module("autogen")
class AfterWorkOption(Enum):
    TERMINATE = "TERMINATE"
    REVERT_TO_USER = "REVERT_TO_USER"
    STAY = "STAY"
    SWARM_MANAGER = "SWARM_MANAGER"


@dataclass
@export_module("autogen")
class AfterWork:  # noqa: N801
    """Handles the next step in the conversation when an agent doesn't suggest a tool call or a handoff.

    Args:
        agent (Union[AfterWorkOption, ConversableAgent, str, Callable[..., Any]]): The agent to hand off to or the after work option. Can be a ConversableAgent, a string name of a ConversableAgent, an AfterWorkOption, or a Callable.
            The Callable signature is:
                def my_after_work_func(last_speaker: ConversableAgent, messages: list[dict[str, Any]], groupchat: GroupChat) -> Union[AfterWorkOption, ConversableAgent, str]:
        next_agent_selection_msg (Optional[Union[str, Callable[..., Any]]]): Optional message to use for the agent selection (in internal group chat), only valid for when agent is AfterWorkOption.SWARM_MANAGER.
            If a string, it will be used as a template and substitute the context variables.
            If a Callable, it should have the signature:
                def my_selection_message(agent: ConversableAgent, messages: list[dict[str, Any]]) -> str
    """

    agent: AfterWorkOption | ConversableAgent | str | Callable[..., Any]
    next_agent_selection_msg: str | ContextStr | Callable[[ConversableAgent, list[dict[str, Any]]], str] | None = None

    def __post_init__(self) -> None:
        if isinstance(self.agent, str):
            self.agent = AfterWorkOption(self.agent.upper())

        # next_agent_selection_msg is only valid for when agent is AfterWorkOption.SWARM_MANAGER, but isn't mandatory
        if self.next_agent_selection_msg is not None:
            if not (
                isinstance(self.next_agent_selection_msg, (str, ContextStr)) or callable(self.next_agent_selection_msg)
            ):
                raise ValueError("next_agent_selection_msg must be a string, ContextStr, or a Callable")

            if self.agent != AfterWorkOption.SWARM_MANAGER:
                warnings.warn(
                    "next_agent_selection_msg is only valid for agent=AfterWorkOption.SWARM_MANAGER. Ignoring the value.",
                    UserWarning,
                )
                self.next_agent_selection_msg = None


class AFTER_WORK(AfterWork):  # noqa: N801
    """Deprecated: Use AfterWork instead. This class will be removed in a future version (TBD)."""

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        warnings.warn(
            "AFTER_WORK is deprecated and will be removed in a future version (TBD). Use AfterWork instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        super().__init__(*args, **kwargs)


@dataclass
@export_module("autogen")
class OnCondition:  # noqa: N801
    """Defines a condition for transitioning to another agent or nested chats.

    This is for LLM-based condition evaluation where these conditions are translated into tools and attached to the agent.

    These are evaluated after the OnCondition conditions but before the AfterWork conditions.

    Args:
        target (Optional[Union[ConversableAgent, dict[str, Any]]]): The agent to hand off to or the nested chat configuration. Can be a ConversableAgent or a Dict.
            If a Dict, it should follow the convention of the nested chat configuration, with the exception of a carryover configuration which is unique to Swarms.
            Swarm Nested chat documentation: https://docs.ag2.ai/docs/user-guide/advanced-concepts/swarm-deep-dive#registering-handoffs-to-a-nested-chat
        condition (Optional[Union[str, ContextStr, Callable[[ConversableAgent, list[dict[str, Any]]], str]]]): The condition for transitioning to the target agent, evaluated by the LLM.
            If a string or Callable, no automatic context variable substitution occurs.
            If a ContextStr, context variable substitution occurs.
            The Callable signature is:
                def my_condition_string(agent: ConversableAgent, messages: list[Dict[str, Any]]) -> str
        available (Optional[Union[Callable[[ConversableAgent, list[dict[str, Any]]], bool], str, ContextExpression]]): Optional condition to determine if this OnCondition is included for the LLM to evaluate.
            If a string, it will look up the value of the context variable with that name, which should be a bool, to determine whether it should include this condition.
            If a ContextExpression, it will evaluate the logical expression against the context variables. Can use not, and, or, and comparison operators (>, <, >=, <=, ==, !=).
                Example: ContextExpression("not(${logged_in} and ${is_admin}) or (${guest_checkout})")
                Example with comparison: ContextExpression("${attempts} >= 3 or ${is_premium} == True or ${tier} == 'gold'")
            The Callable signature is:
                def my_available_func(agent: ConversableAgent, messages: list[Dict[str, Any]]) -> bool
    """

    target: ConversableAgent | dict[str, Any] | None = None
    condition: str | ContextStr | Callable[[ConversableAgent, list[dict[str, Any]]], str] | None = None
    available: Callable[[ConversableAgent, list[dict[str, Any]]], bool] | str | ContextExpression | None = None

    def __post_init__(self) -> None:
        # Ensure valid types
        if (self.target is not None) and (not isinstance(self.target, (ConversableAgent, dict))):
            raise ValueError("'target' must be a ConversableAgent or a dict")

        # Ensure they have a condition
        if isinstance(self.condition, str):
            if not self.condition.strip():
                raise ValueError("'condition' must be a non-empty string")
        else:
            if not isinstance(self.condition, ContextStr) and not callable(self.condition):
                raise ValueError("'condition' must be a string, ContextStr, or callable")

        if (self.available is not None) and (
            not (isinstance(self.available, (str, ContextExpression)) or callable(self.available))
        ):
            raise ValueError("'available' must be a callable, a string, or a ContextExpression")


class ON_CONDITION(OnCondition):  # noqa: N801
    """Deprecated: Use OnCondition instead. This class will be removed in a future version (TBD)."""

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        warnings.warn(
            "ON_CONDITION is deprecated and will be removed in a future version (TBD). Use OnCondition instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        super().__init__(*args, **kwargs)


@dataclass
@export_module("autogen")
class OnContextCondition:  # noqa: N801
    """Defines a condition for transitioning to another agent or nested chats using context variables and the ContextExpression class.

    This is for context variable-based condition evaluation (does not use the agent's LLM).

    These are evaluated before the OnCondition and AfterWork conditions.

    Args:
        target (Optional[Union[ConversableAgent, dict[str, Any]]]): The agent to hand off to or the nested chat configuration. Can be a ConversableAgent or a Dict.
            If a Dict, it should follow the convention of the nested chat configuration, with the exception of a carryover configuration which is unique to Swarms.
            Swarm Nested chat documentation: https://docs.ag2.ai/docs/user-guide/advanced-concepts/swarm-deep-dive#registering-handoffs-to-a-nested-chat
        condition (Optional[Union[str, ContextExpression]]): The condition for transitioning to the target agent, evaluated by the LLM.
            If a string, it needs to represent a context variable key and the value will be evaluated as a boolean
            If a ContextExpression, it will evaluate the logical expression against the context variables. If it is True, the transition will occur.
                Can use not, and, or, and comparison operators (>, <, >=, <=, ==, !=).
                Example: ContextExpression("not(${logged_in} and ${is_admin}) or (${guest_checkout})")
                Example with comparison: ContextExpression("${attempts} >= 3 or ${is_premium} == True or ${tier} == 'gold'")
        available (Optional[Union[Callable[[ConversableAgent, list[dict[str, Any]]], bool], str, ContextExpression]]): Optional condition to determine if this OnContextCondition is included for the LLM to evaluate.
            If a string, it will look up the value of the context variable with that name, which should be a bool, to determine whether it should include this condition.
            If a ContextExpression, it will evaluate the logical expression against the context variables. Can use not, and, or, and comparison operators (>, <, >=, <=, ==, !=).
            The Callable signature is:
                def my_available_func(agent: ConversableAgent, messages: list[Dict[str, Any]]) -> bool

    """

    target: ConversableAgent | dict[str, Any] | None = None
    condition: str | ContextExpression | None = None
    available: Callable[[ConversableAgent, list[dict[str, Any]]], bool] | str | ContextExpression | None = None

    def __post_init__(self) -> None:
        # Ensure valid types
        if (self.target is not None) and (not isinstance(self.target, (ConversableAgent, dict))):
            raise ValueError("'target' must be a ConversableAgent or a dict")

        # Ensure they have a condition
        if isinstance(self.condition, str):
            if not self.condition.strip():
                raise ValueError("'condition' must be a non-empty string")

            self._context_condition = ContextExpression("${" + self.condition + "}")
        else:
            if not isinstance(self.condition, ContextExpression):
                raise ValueError("'condition' must be a string on ContextExpression")

            self._context_condition = self.condition

        if (self.available is not None) and (
            not (isinstance(self.available, (str, ContextExpression)) or callable(self.available))
        ):
            raise ValueError("'available' must be a callable, a string, or a ContextExpression")


def _establish_swarm_agent(agent: ConversableAgent) -> None:
    """Establish the swarm agent with the swarm-related attributes and hooks. Not for the tool executor.

    Args:
        agent (ConversableAgent): The agent to establish as a swarm agent.
    """

    def _swarm_agent_str(self: ConversableAgent) -> str:
        """Customise the __str__ method to show the agent name for transition messages."""
        return f"Swarm agent --> {self.name}"

    agent._swarm_after_work = None  # type: ignore[attr-defined]
    agent._swarm_after_work_selection_msg = None  # type: ignore[attr-defined]

    # Store nested chats hand offs as we'll establish these in the initiate_swarm_chat
    # List of Dictionaries containing the nested_chats and condition
    agent._swarm_nested_chat_handoffs = []  # type: ignore[attr-defined]

    # Store conditional functions (and their OnCondition instances) to add/remove later when transitioning to this agent
    agent._swarm_conditional_functions = {}  # type: ignore[attr-defined]

    # Register the hook to update agent state (except tool executor)
    agent.register_hook("update_agent_state", _update_conditional_functions)

    # Store the OnContextConditions for evaluation (list[OnContextCondition])
    agent._swarm_oncontextconditions = []  # type: ignore[attr-defined]

    # Register a reply function to run Python function-based OnConditions before any other reply function
    agent.register_reply(trigger=([Agent, None]), reply_func=_run_oncontextconditions, position=0)

    agent._get_display_name = MethodType(_swarm_agent_str, agent)  # type: ignore[method-assign]

    # Mark this agent as established as a swarm agent
    agent._swarm_is_established = True  # type: ignore[attr-defined]


def _link_agents_to_swarm_manager(agents: list[Agent], group_chat_manager: Agent) -> None:
    """Link all agents to the GroupChatManager so they can access the underlying GroupChat and other agents.

    This is primarily used so that agents can set the tool executor's _swarm_next_agent attribute to control
    the next agent programmatically.

    Does not link the Tool Executor agent.
    """
    for agent in agents:
        agent._swarm_manager = group_chat_manager  # type: ignore[attr-defined]


def _run_oncontextconditions(
    agent: ConversableAgent,
    messages: list[dict[str, Any]] | None = None,
    sender: Agent | None = None,
    config: Any | None = None,
) -> tuple[bool, str | dict[str, Any] | None]:
    """Run OnContextConditions for an agent before any other reply function."""
    for on_condition in agent._swarm_oncontextconditions:  # type: ignore[attr-defined]
        is_available = True

        if on_condition.available is not None:
            if callable(on_condition.available):
                is_available = on_condition.available(agent, next(iter(agent.chat_messages.values())))
            elif isinstance(on_condition.available, str):
                is_available = agent.context_variables.get(on_condition.available) or False
            elif isinstance(on_condition.available, ContextExpression):
                is_available = on_condition.available.evaluate(agent.context_variables)

        if is_available and on_condition._context_condition.evaluate(agent.context_variables):
            # Condition has been met, we'll set the Tool Executor's _swarm_next_agent
            # attribute and that will be picked up on the next iteration when
            # _determine_next_agent is called
            for agent in agent._swarm_manager.groupchat.agents:  # type: ignore[attr-defined]
                if agent.name == __TOOL_EXECUTOR_NAME__:
                    agent._swarm_next_agent = on_condition.target  # type: ignore[attr-defined]
                    break

            if isinstance(on_condition.target, ConversableAgent):
                transfer_name = on_condition.target.name
            else:
                transfer_name = "a nested chat"

            return True, "[Handing off to " + transfer_name + "]"

    return False, None


def _modify_context_variables_param(f: Callable[..., Any], context_variables: ContextVariables) -> Callable[..., Any]:
    """Modifies the context_variables parameter to use dependency injection and link it to the swarm context variables.

    This essentially changes:
    def some_function(some_variable: int, context_variables: ContextVariables) -> str:

    to:

    def some_function(some_variable: int, context_variables: Annotated[ContextVariables, Depends(on(self.context_variables))]) -> str:
    """
    sig = inspect.signature(f)

    # Check if context_variables parameter exists and update it if so
    if __CONTEXT_VARIABLES_PARAM_NAME__ in sig.parameters:
        new_params = []
        for name, param in sig.parameters.items():
            if name == __CONTEXT_VARIABLES_PARAM_NAME__:
                # Replace with new annotation using Depends
                new_param = param.replace(annotation=Annotated[ContextVariables, Depends(on(context_variables))])
                new_params.append(new_param)
            else:
                new_params.append(param)

        # Update signature
        new_sig = sig.replace(parameters=new_params)
        f.__signature__ = new_sig  # type: ignore[attr-defined]

    return f


def _change_tool_context_variables_to_depends(
    agent: ConversableAgent, current_tool: Tool, context_variables: ContextVariables
) -> None:
    """Checks for the context_variables parameter in the tool and updates it to use dependency injection."""
    # If the tool has a context_variables parameter, remove the tool and reregister it without the parameter
    if __CONTEXT_VARIABLES_PARAM_NAME__ in current_tool.tool_schema["function"]["parameters"]["properties"]:
        # We'll replace the tool, so start with getting the underlying function
        tool_func = current_tool._func

        # Remove the Tool from the agent
        name = current_tool._name
        description = current_tool._description
        agent.remove_tool_for_llm(current_tool)

        # Recreate the tool without the context_variables parameter
        tool_func = _modify_context_variables_param(current_tool._func, context_variables)
        tool_func = inject_params(tool_func)
        new_tool = ConversableAgent._create_tool_if_needed(func_or_tool=tool_func, name=name, description=description)

        # Re-register with the agent
        agent.register_for_llm()(new_tool)


def _prepare_swarm_agents(
    initial_agent: ConversableAgent,
    agents: list[ConversableAgent],
    context_variables: ContextVariables,
    exclude_transit_message: bool = True,
) -> tuple[ConversableAgent, list[ConversableAgent]]:
    """Validates agents, create the tool executor, configure nested chats.

    Args:
        initial_agent (ConversableAgent): The first agent in the conversation.
        agents (list[ConversableAgent]): List of all agents in the conversation.
        context_variables (ContextVariables): Context variables to assign to all agents.
        exclude_transit_message (bool): Whether to exclude transit messages from the agents.

    Returns:
        ConversableAgent: The tool executor agent.
        list[ConversableAgent]: List of nested chat agents.
    """
    if not isinstance(initial_agent, ConversableAgent):
        raise ValueError("initial_agent must be a ConversableAgent")
    if not all(isinstance(agent, ConversableAgent) for agent in agents):
        raise ValueError("Agents must be a list of ConversableAgents")

    # Initialize all agents as swarm agents
    for agent in agents:
        if not hasattr(agent, "_swarm_is_established"):
            _establish_swarm_agent(agent)

    # Ensure all agents in hand-off after-works are in the passed in agents list
    for agent in agents:
        if (agent._swarm_after_work is not None and isinstance(agent._swarm_after_work.agent, ConversableAgent)) and (  # type: ignore[attr-defined]
            agent._swarm_after_work.agent not in agents  # type: ignore[attr-defined]
        ):
            raise ValueError("Agent in hand-off must be in the agents list")

    tool_execution = ConversableAgent(
        name=__TOOL_EXECUTOR_NAME__,
        system_message="Tool Execution, do not use this agent directly.",
    )
    _set_to_tool_execution(tool_execution)

    nested_chat_agents: list[ConversableAgent] = []
    for agent in agents:
        _create_nested_chats(agent, nested_chat_agents)

    # Update any agent's tools that have context_variables as a parameter
    # To use Dependency Injection

    # Update tool execution agent with all the functions from all the agents
    for agent in agents + nested_chat_agents:
        tool_execution._function_map.update(agent._function_map)

        # Add conditional functions to the tool_execution agent
        for func_name, (func, _) in agent._swarm_conditional_functions.items():  # type: ignore[attr-defined]
            tool_execution._function_map[func_name] = func

        # Update any agent tools that have context_variables parameters to use Dependency Injection
        for tool in agent.tools:
            _change_tool_context_variables_to_depends(agent, tool, context_variables)

        # Add all tools to the Tool Executor agent
        for tool in agent.tools:
            tool_execution.register_for_execution(serialize=False, silent_override=True)(tool)

    if exclude_transit_message:
        # get all transit functions names
        to_be_removed = []
        for agent in agents + nested_chat_agents:
            if hasattr(agent, "_swarm_conditional_functions"):
                to_be_removed += list(agent._swarm_conditional_functions.keys())

        # register hook to remove transit messages for swarm agents
        for agent in agents + nested_chat_agents:
            agent.register_hook("process_all_messages_before_reply", make_remove_function(to_be_removed))

    return tool_execution, nested_chat_agents


def _create_nested_chats(agent: ConversableAgent, nested_chat_agents: list[ConversableAgent]) -> None:
    """Create nested chat agents and register nested chats.

    Args:
        agent (ConversableAgent): The agent to create nested chat agents for, including registering the hand offs.
        nested_chat_agents (list[ConversableAgent]): List for all nested chat agents, appends to this.
    """

    def create_nested_chat_agent(agent: ConversableAgent, nested_chats: dict[str, Any]) -> ConversableAgent:
        """Create a nested chat agent for a nested chat configuration.

        Args:
            agent (ConversableAgent): The agent to create the nested chat agent for.
            nested_chats (dict[str, Any]): The nested chat configuration.

        Returns:
            The created nested chat agent.
        """
        # Create a nested chat agent specifically for this nested chat
        nested_chat_agent = ConversableAgent(name=f"nested_chat_{agent.name}_{i + 1}")

        nested_chat_agent.register_nested_chats(
            nested_chats["chat_queue"],
            reply_func_from_nested_chats=nested_chats.get("reply_func_from_nested_chats")
            or "summary_from_nested_chats",
            config=nested_chats.get("config"),
            trigger=lambda sender: True,
            position=0,
            use_async=nested_chats.get("use_async", False),
        )

        # After the nested chat is complete, transfer back to the parent agent
        register_hand_off(nested_chat_agent, AfterWork(agent=agent))

        return nested_chat_agent

    for i, nested_chat_handoff in enumerate(agent._swarm_nested_chat_handoffs):  # type: ignore[attr-defined]
        llm_nested_chats: dict[str, Any] = nested_chat_handoff["nested_chats"]

        # Create nested chat agent
        nested_chat_agent = create_nested_chat_agent(agent, llm_nested_chats)
        nested_chat_agents.append(nested_chat_agent)

        # Nested chat is triggered through an agent transfer to this nested chat agent
        condition = nested_chat_handoff["condition"]
        available = nested_chat_handoff["available"]
        register_hand_off(agent, OnCondition(target=nested_chat_agent, condition=condition, available=available))

    for i, nested_chat_context_handoff in enumerate(agent._swarm_oncontextconditions):  # type: ignore[attr-defined]
        if isinstance(nested_chat_context_handoff.target, dict):
            context_nested_chats: dict[str, Any] = nested_chat_context_handoff.target

            # Create nested chat agent
            nested_chat_agent = create_nested_chat_agent(agent, context_nested_chats)
            nested_chat_agents.append(nested_chat_agent)

            # Update the OnContextCondition, replacing the nested chat dictionary with the nested chat agent
            nested_chat_context_handoff.target = nested_chat_agent


def _process_initial_messages(
    messages: list[dict[str, Any]] | str,
    user_agent: UserProxyAgent | None,
    agents: list[ConversableAgent],
    nested_chat_agents: list[ConversableAgent],
) -> tuple[list[dict[str, Any]], Agent | None, list[str], list[Agent]]:
    """Process initial messages, validating agent names against messages, and determining the last agent to speak.

    Args:
        messages: Initial messages to process.
        user_agent: Optional user proxy agent passed in to a_/initiate_swarm_chat.
        agents: Agents in swarm.
        nested_chat_agents: List of nested chat agents.

    Returns:
        list[dict[str, Any]]: Processed message(s).
        Agent: Last agent to speak.
        list[str]: List of agent names.
        list[Agent]: List of temporary user proxy agents to add to GroupChat.
    """
    if isinstance(messages, str):
        messages = [{"role": "user", "content": messages}]

    swarm_agent_names = [agent.name for agent in agents + nested_chat_agents]

    # If there's only one message and there's no identified swarm agent
    # Start with a user proxy agent, creating one if they haven't passed one in
    last_agent: Agent | None
    temp_user_proxy: Agent | None = None
    temp_user_list: list[Agent] = []
    if len(messages) == 1 and "name" not in messages[0] and not user_agent:
        temp_user_proxy = UserProxyAgent(name="_User", code_execution_config=False)
        last_agent = temp_user_proxy
        temp_user_list.append(temp_user_proxy)
    else:
        last_message = messages[0]
        if "name" in last_message:
            if last_message["name"] in swarm_agent_names:
                last_agent = next(agent for agent in agents + nested_chat_agents if agent.name == last_message["name"])  # type: ignore[assignment]
            elif user_agent and last_message["name"] == user_agent.name:
                last_agent = user_agent
            else:
                raise ValueError(f"Invalid swarm agent name in last message: {last_message['name']}")
        else:
            last_agent = user_agent if user_agent else temp_user_proxy

    return messages, last_agent, swarm_agent_names, temp_user_list


def _setup_context_variables(
    tool_execution: ConversableAgent,
    agents: list[ConversableAgent],
    manager: GroupChatManager,
    context_variables: ContextVariables,
) -> None:
    """Assign a common context_variables reference to all agents in the swarm, including the tool executor and group chat manager.

    Args:
        tool_execution: The tool execution agent.
        agents: List of all agents in the conversation.
        manager: GroupChatManager instance.
        context_variables: Context variables to assign to all agents.
    """
    for agent in agents + [tool_execution] + [manager]:
        agent.context_variables = context_variables


def _cleanup_temp_user_messages(chat_result: ChatResult) -> None:
    """Remove temporary user proxy agent name from messages before returning.

    Args:
        chat_result: ChatResult instance.
    """
    for message in chat_result.chat_history:
        if "name" in message and message["name"] == "_User":
            del message["name"]


def _prepare_groupchat_auto_speaker(
    groupchat: GroupChat,
    last_swarm_agent: ConversableAgent,
    after_work_next_agent_selection_msg: str
    | ContextStr
    | Callable[[ConversableAgent, list[dict[str, Any]]], str]
    | None,
) -> None:
    """Prepare the group chat for auto speaker selection, includes updating or restore the groupchat speaker selection message.

    Tool Executor and Nested Chat agents will be removed from the available agents list.

    Args:
        groupchat (GroupChat): GroupChat instance.
        last_swarm_agent (ConversableAgent): The last swarm agent for which the LLM config is used
        after_work_next_agent_selection_msg (Union[str, ContextStr, Callable[..., Any]]): Optional message to use for the agent selection (in internal group chat).
            if a string, it will be use the string a the prompt template, no context variable substitution however '{agentlist}' will be substituted for a list of agents.
            if a ContextStr, it will substitute the agentlist first and then the context variables
            if a Callable, it will not substitute the agentlist or context variables, signature:
                def my_selection_message(agent: ConversableAgent, messages: list[dict[str, Any]]) -> str
    """

    def substitute_agentlist(template: str) -> str:
        # Run through group chat's string substitution first for {agentlist}
        # We need to do this so that the next substitution doesn't fail with agentlist
        # and we can remove the tool executor and nested chats from the available agents list
        agent_list = [
            agent
            for agent in groupchat.agents
            if agent.name != __TOOL_EXECUTOR_NAME__ and not agent.name.startswith("nested_chat_")
        ]

        groupchat.select_speaker_prompt_template = template
        return groupchat.select_speaker_prompt(agent_list)

    if after_work_next_agent_selection_msg is None:
        # If there's no selection message, restore the default and filter out the tool executor and nested chat agents
        groupchat.select_speaker_prompt_template = substitute_agentlist(SELECT_SPEAKER_PROMPT_TEMPLATE)
    elif isinstance(after_work_next_agent_selection_msg, str):
        # No context variable substitution for string, but agentlist will be substituted
        groupchat.select_speaker_prompt_template = substitute_agentlist(after_work_next_agent_selection_msg)
    elif isinstance(after_work_next_agent_selection_msg, ContextStr):
        # Replace the agentlist in the string first, putting it into a new ContextStr
        agent_list_replaced_string = ContextStr(
            template=substitute_agentlist(after_work_next_agent_selection_msg.template)
        )

        # Then replace the context variables
        groupchat.select_speaker_prompt_template = agent_list_replaced_string.format(  # type: ignore[assignment]
            last_swarm_agent.context_variables
        )
    elif callable(after_work_next_agent_selection_msg):
        groupchat.select_speaker_prompt_template = substitute_agentlist(
            after_work_next_agent_selection_msg(last_swarm_agent, groupchat.messages)
        )


def _determine_next_agent(
    last_speaker: ConversableAgent,
    groupchat: GroupChat,
    initial_agent: ConversableAgent,
    use_initial_agent: bool,
    tool_execution: ConversableAgent,
    swarm_agent_names: list[str],
    user_agent: UserProxyAgent | None,
    swarm_after_work: AfterWorkOption | Callable[..., Any] | None,
) -> Agent | Literal["auto"] | None:
    """Determine the next agent in the conversation.

    Args:
        last_speaker (ConversableAgent): The last agent to speak.
        groupchat (GroupChat): GroupChat instance.
        initial_agent (ConversableAgent): The initial agent in the conversation.
        use_initial_agent (bool): Whether to use the initial agent straight away.
        tool_execution (ConversableAgent): The tool execution agent.
        swarm_agent_names (list[str]): List of agent names.
        user_agent (UserProxyAgent): Optional user proxy agent.
        swarm_after_work (Union[AfterWorkOption, Callable[..., Any]]): Method to handle conversation continuation when an agent doesn't select the next agent.
    """
    if use_initial_agent:
        return initial_agent

    if "tool_calls" in groupchat.messages[-1]:
        return tool_execution

    after_work_condition = None

    if tool_execution._swarm_next_agent is not None:  # type: ignore[attr-defined]
        next_agent: Agent | None = tool_execution._swarm_next_agent  # type: ignore[attr-defined]
        tool_execution._swarm_next_agent = None  # type: ignore[attr-defined]

        if not isinstance(next_agent, AfterWorkOption):
            # Check for string, access agent from group chat.

            if isinstance(next_agent, str):
                if next_agent in swarm_agent_names:
                    next_agent = groupchat.agent_by_name(name=next_agent)
                else:
                    raise ValueError(
                        f"No agent found with the name '{next_agent}'. Ensure the agent exists in the swarm."
                    )

            return next_agent
        else:
            after_work_condition = next_agent

    # get the last swarm agent
    last_swarm_speaker = None
    for message in reversed(groupchat.messages):
        if "name" in message and message["name"] in swarm_agent_names and message["name"] != __TOOL_EXECUTOR_NAME__:
            agent = groupchat.agent_by_name(name=message["name"])
            if isinstance(agent, ConversableAgent):
                last_swarm_speaker = agent
                break
    if last_swarm_speaker is None:
        raise ValueError("No swarm agent found in the message history")

    # If the user last spoke, return to the agent prior
    if after_work_condition is None and (
        (user_agent and last_speaker == user_agent) or groupchat.messages[-1]["role"] == "tool"
    ):
        return last_swarm_speaker

    after_work_next_agent_selection_msg = None

    if after_work_condition is None:
        # Resolve after_work condition if one hasn't been passed in (agent-level overrides global)
        after_work_condition = (
            last_swarm_speaker._swarm_after_work  # type: ignore[attr-defined]
            if last_swarm_speaker._swarm_after_work is not None  # type: ignore[attr-defined]
            else swarm_after_work
        )

    if isinstance(after_work_condition, AfterWork):
        after_work_next_agent_selection_msg = after_work_condition.next_agent_selection_msg
        after_work_condition = after_work_condition.agent

    # Evaluate callable after_work
    if callable(after_work_condition):
        after_work_condition = after_work_condition(last_swarm_speaker, groupchat.messages, groupchat)

    if isinstance(after_work_condition, str):  # Agent name in a string
        if after_work_condition in swarm_agent_names:
            return groupchat.agent_by_name(name=after_work_condition)
        else:
            raise ValueError(f"Invalid agent name in after_work: {after_work_condition}")
    elif isinstance(after_work_condition, ConversableAgent):
        return after_work_condition
    elif isinstance(after_work_condition, AfterWorkOption):
        if after_work_condition == AfterWorkOption.TERMINATE:
            return None
        elif after_work_condition == AfterWorkOption.REVERT_TO_USER:
            return None if user_agent is None else user_agent
        elif after_work_condition == AfterWorkOption.STAY:
            return last_swarm_speaker
        elif after_work_condition == AfterWorkOption.SWARM_MANAGER:
            _prepare_groupchat_auto_speaker(groupchat, last_swarm_speaker, after_work_next_agent_selection_msg)
            return "auto"
    else:
        raise ValueError("Invalid After Work condition or return value from callable")


def create_swarm_transition(
    initial_agent: ConversableAgent,
    tool_execution: ConversableAgent,
    swarm_agent_names: list[str],
    user_agent: UserProxyAgent | None,
    swarm_after_work: AfterWorkOption | Callable[..., Any] | None,
) -> Callable[[ConversableAgent, GroupChat], Agent | Literal["auto"] | None]:
    """Creates a transition function for swarm chat with enclosed state for the use_initial_agent.

    Args:
        initial_agent (ConversableAgent): The first agent to speak
        tool_execution (ConversableAgent): The tool execution agent
        swarm_agent_names (list[str]): List of all agent names
        user_agent (UserProxyAgent): Optional user proxy agent
        swarm_after_work (Union[AfterWorkOption, Callable[..., Any]]): Swarm-level after work

    Returns:
        Callable transition function (for sync and async swarm chats)
    """
    # Create enclosed state, this will be set once per creation so will only be True on the first execution
    # of swarm_transition
    state = {"use_initial_agent": True}

    def swarm_transition(last_speaker: ConversableAgent, groupchat: GroupChat) -> Agent | Literal["auto"] | None:
        result = _determine_next_agent(
            last_speaker=last_speaker,
            groupchat=groupchat,
            initial_agent=initial_agent,
            use_initial_agent=state["use_initial_agent"],
            tool_execution=tool_execution,
            swarm_agent_names=swarm_agent_names,
            user_agent=user_agent,
            swarm_after_work=swarm_after_work,
        )
        state["use_initial_agent"] = False
        return result

    return swarm_transition


def _create_swarm_manager(
    groupchat: GroupChat, swarm_manager_args: dict[str, Any] | None, agents: list[ConversableAgent]
) -> GroupChatManager:
    """Create a GroupChatManager for the swarm chat utilising any arguments passed in and ensure an LLM Config exists if needed

    Args:
        groupchat (GroupChat): Swarm groupchat.
        swarm_manager_args (dict[str, Any]): Swarm manager arguments to create the GroupChatManager.
        agents (list[ConversableAgent]): List of agents in the swarm.

    Returns:
        GroupChatManager: GroupChatManager instance.
    """
    manager_args = (swarm_manager_args or {}).copy()
    if "groupchat" in manager_args:
        raise ValueError("'groupchat' cannot be specified in swarm_manager_args as it is set by initiate_swarm_chat")
    manager = GroupChatManager(groupchat, **manager_args)

    # Ensure that our manager has an LLM Config if we have any AfterWorkOption.SWARM_MANAGER after works
    if manager.llm_config is False:
        for agent in agents:
            if (
                agent._swarm_after_work  # type: ignore[attr-defined]
                and isinstance(agent._swarm_after_work.agent, AfterWorkOption)  # type: ignore[attr-defined]
                and agent._swarm_after_work.agent == AfterWorkOption.SWARM_MANAGER  # type: ignore[attr-defined]
            ):
                raise ValueError(
                    "The swarm manager doesn't have an LLM Config and it is required for AfterWorkOption.SWARM_MANAGER. Use the swarm_manager_args to specify the LLM Config for the swarm manager."
                )

    return manager


def make_remove_function(tool_msgs_to_remove: list[str]) -> Callable[[list[dict[str, Any]]], list[dict[str, Any]]]:
    """Create a function to remove messages with tool calls from the messages list.

    The returned function can be registered as a hook to "process_all_messages_before_reply"" to remove messages with tool calls.
    """

    def remove_messages(messages: list[dict[str, Any]], tool_msgs_to_remove: list[str]) -> list[dict[str, Any]]:
        copied = copy.deepcopy(messages)
        new_messages = []
        removed_tool_ids = []
        for message in copied:
            # remove tool calls
            if message.get("tool_calls") is not None:
                filtered_tool_calls = []
                for tool_call in message["tool_calls"]:
                    if tool_call.get("function") is not None and tool_call["function"]["name"] in tool_msgs_to_remove:
                        # remove
                        removed_tool_ids.append(tool_call["id"])
                    else:
                        filtered_tool_calls.append(tool_call)
                if len(filtered_tool_calls) > 0:
                    message["tool_calls"] = filtered_tool_calls
                else:
                    del message["tool_calls"]
                    if (
                        message.get("content") is None
                        or message.get("content") == ""
                        or message.get("content") == "None"
                    ):
                        continue  # if no tool call and no content, skip this message
                    # else: keep the message with tool_calls removed
            # remove corresponding tool responses
            elif message.get("tool_responses") is not None:
                filtered_tool_responses = []
                for tool_response in message["tool_responses"]:
                    if tool_response["tool_call_id"] not in removed_tool_ids:
                        filtered_tool_responses.append(tool_response)

                if len(filtered_tool_responses) > 0:
                    message["tool_responses"] = filtered_tool_responses
                else:
                    continue

            new_messages.append(message)

        return new_messages

    return partial(remove_messages, tool_msgs_to_remove=tool_msgs_to_remove)


@export_module("autogen")
def initiate_swarm_chat(
    initial_agent: ConversableAgent,
    messages: list[dict[str, Any]] | str,
    agents: list[ConversableAgent],
    user_agent: UserProxyAgent | None = None,
    swarm_manager_args: dict[str, Any] | None = None,
    max_rounds: int = 20,
    context_variables: ContextVariables | None = None,
    after_work: AfterWorkOption
    | Callable[[ConversableAgent, list[dict[str, Any]], GroupChat], AfterWorkOption | ConversableAgent | str]
    | None = AfterWorkOption.TERMINATE,
    exclude_transit_message: bool = True,
) -> tuple[ChatResult, ContextVariables, ConversableAgent]:
    """Initialize and run a swarm chat

    Args:
        initial_agent: The first receiving agent of the conversation.
        messages: Initial message(s).
        agents: list of swarm agents.
        user_agent: Optional user proxy agent for falling back to.
        swarm_manager_args: Optional group chat manager arguments used to establish the swarm's groupchat manager, required when AfterWorkOption.SWARM_MANAGER is used.
        max_rounds: Maximum number of conversation rounds.
        context_variables: Starting context variables.
        after_work: Method to handle conversation continuation when an agent doesn't select the next agent. If no agent is selected and no tool calls are output, we will use this method to determine the next agent.
            Must be a AfterWork instance (which is a dataclass accepting a ConversableAgent, AfterWorkOption, A str (of the AfterWorkOption)) or a callable.
            AfterWorkOption:
                - TERMINATE (Default): Terminate the conversation.
                - REVERT_TO_USER : Revert to the user agent if a user agent is provided. If not provided, terminate the conversation.
                - STAY : Stay with the last speaker.

            Callable: A custom function that takes the current agent, messages, and groupchat as arguments and returns an AfterWorkOption or a ConversableAgent (by reference or string name).
                ```python
                def custom_afterwork_func(last_speaker: ConversableAgent, messages: list[dict[str, Any]], groupchat: GroupChat) -> Union[AfterWorkOption, ConversableAgent, str]:
                ```
        exclude_transit_message:  all registered handoff function call and responses messages will be removed from message list before calling an LLM.
            Note: only with transition functions added with `register_handoff` will be removed. If you pass in a function to manage workflow, it will not be removed. You may register a cumstomized hook to `process_all_messages_before_reply` to remove that.

    Returns:
        ChatResult:     Conversations chat history.
        ContextVariables: Updated Context variables.
        ConversableAgent:     Last speaker.
    """
    context_variables = context_variables or ContextVariables()

    tool_execution, nested_chat_agents = _prepare_swarm_agents(
        initial_agent, agents, context_variables, exclude_transit_message
    )

    processed_messages, last_agent, swarm_agent_names, temp_user_list = _process_initial_messages(
        messages, user_agent, agents, nested_chat_agents
    )

    # Create transition function (has enclosed state for initial agent)
    swarm_transition = create_swarm_transition(
        initial_agent=initial_agent,
        tool_execution=tool_execution,
        swarm_agent_names=swarm_agent_names,
        user_agent=user_agent,
        swarm_after_work=after_work,
    )

    groupchat = GroupChat(
        agents=[tool_execution] + agents + nested_chat_agents + ([user_agent] if user_agent else temp_user_list),
        messages=[],
        max_round=max_rounds,
        speaker_selection_method=swarm_transition,
    )

    manager = _create_swarm_manager(groupchat, swarm_manager_args, agents)

    # Point all ConversableAgent's context variables to this function's context_variables
    _setup_context_variables(tool_execution, agents, manager, context_variables)

    # Link all agents with the GroupChatManager to allow access to the group chat
    # and other agents, particularly the tool executor for setting _swarm_next_agent
    _link_agents_to_swarm_manager(groupchat.agents, manager)  # Commented out as the function is not defined

    if len(processed_messages) > 1:
        last_agent, last_message = manager.resume(messages=processed_messages)
        clear_history = False
    else:
        last_message = processed_messages[0]
        clear_history = True

    if last_agent is None:
        raise ValueError("No agent selected to start the conversation")

    chat_result = last_agent.initiate_chat(  # type: ignore[attr-defined]
        manager,
        message=last_message,
        clear_history=clear_history,
    )

    _cleanup_temp_user_messages(chat_result)

    return chat_result, context_variables, manager.last_speaker  # type: ignore[return-value]


@export_module("autogen")
def run_swarm(
    initial_agent: ConversableAgent,
    messages: list[dict[str, Any]] | str,
    agents: list[ConversableAgent],
    user_agent: UserProxyAgent | None = None,
    swarm_manager_args: dict[str, Any] | None = None,
    max_rounds: int = 20,
    context_variables: ContextVariables | None = None,
    after_work: AfterWorkOption
    | Callable[[ConversableAgent, list[dict[str, Any]], GroupChat], AfterWorkOption | ConversableAgent | str]
    | None = AfterWorkOption.TERMINATE,
    exclude_transit_message: bool = True,
) -> RunResponseProtocol:
    iostream = ThreadIOStream()
    response = RunResponse(iostream, agents)  # type: ignore[arg-type]

    def stream_run(
        iostream: ThreadIOStream = iostream,
        response: RunResponse = response,
    ) -> None:
        with IOStream.set_default(iostream):
            try:
                chat_result, returned_context_variables, last_speaker = initiate_swarm_chat(
                    initial_agent=initial_agent,
                    messages=messages,
                    agents=agents,
                    user_agent=user_agent,
                    swarm_manager_args=swarm_manager_args,
                    max_rounds=max_rounds,
                    context_variables=context_variables,
                    after_work=after_work,
                    exclude_transit_message=exclude_transit_message,
                )

                IOStream.get_default().send(
                    RunCompletionEvent(  # type: ignore[call-arg]
                        history=chat_result.chat_history,
                        summary=chat_result.summary,
                        cost=chat_result.cost,
                        last_speaker=last_speaker.name,
                        context_variables=returned_context_variables,
                    )
                )
            except Exception as e:
                response.iostream.send(ErrorEvent(error=e))  # type: ignore[call-arg]

    threading.Thread(
        target=stream_run,
    ).start()

    return response


@export_module("autogen")
async def a_initiate_swarm_chat(
    initial_agent: ConversableAgent,
    messages: list[dict[str, Any]] | str,
    agents: list[ConversableAgent],
    user_agent: UserProxyAgent | None = None,
    swarm_manager_args: dict[str, Any] | None = None,
    max_rounds: int = 20,
    context_variables: ContextVariables | None = None,
    after_work: AfterWorkOption
    | Callable[[ConversableAgent, list[dict[str, Any]], GroupChat], AfterWorkOption | ConversableAgent | str]
    | None = AfterWorkOption.TERMINATE,
    exclude_transit_message: bool = True,
) -> tuple[ChatResult, ContextVariables, ConversableAgent]:
    """Initialize and run a swarm chat asynchronously

    Args:
        initial_agent: The first receiving agent of the conversation.
        messages: Initial message(s).
        agents: List of swarm agents.
        user_agent: Optional user proxy agent for falling back to.
        swarm_manager_args: Optional group chat manager arguments used to establish the swarm's groupchat manager, required when AfterWorkOption.SWARM_MANAGER is used.
        max_rounds: Maximum number of conversation rounds.
        context_variables: Starting context variables.
        after_work: Method to handle conversation continuation when an agent doesn't select the next agent. If no agent is selected and no tool calls are output, we will use this method to determine the next agent.
            Must be a AfterWork instance (which is a dataclass accepting a ConversableAgent, AfterWorkOption, A str (of the AfterWorkOption)) or a callable.
            AfterWorkOption:
                - TERMINATE (Default): Terminate the conversation.
                - REVERT_TO_USER : Revert to the user agent if a user agent is provided. If not provided, terminate the conversation.
                - STAY : Stay with the last speaker.

            Callable: A custom function that takes the current agent, messages, and groupchat as arguments and returns an AfterWorkOption or a ConversableAgent (by reference or string name).
                ```python
                def custom_afterwork_func(last_speaker: ConversableAgent, messages: list[dict[str, Any]], groupchat: GroupChat) -> Union[AfterWorkOption, ConversableAgent, str]:
                ```
        exclude_transit_message:  all registered handoff function call and responses messages will be removed from message list before calling an LLM.
            Note: only with transition functions added with `register_handoff` will be removed. If you pass in a function to manage workflow, it will not be removed. You may register a cumstomized hook to `process_all_messages_before_reply` to remove that.

    Returns:
        ChatResult:     Conversations chat history.
        ContextVariables: Updated Context variables.
        ConversableAgent:     Last speaker.
    """
    context_variables = context_variables or ContextVariables()
    tool_execution, nested_chat_agents = _prepare_swarm_agents(
        initial_agent, agents, context_variables, exclude_transit_message
    )

    processed_messages, last_agent, swarm_agent_names, temp_user_list = _process_initial_messages(
        messages, user_agent, agents, nested_chat_agents
    )

    # Create transition function (has enclosed state for initial agent)
    swarm_transition = create_swarm_transition(
        initial_agent=initial_agent,
        tool_execution=tool_execution,
        swarm_agent_names=swarm_agent_names,
        user_agent=user_agent,
        swarm_after_work=after_work,
    )

    groupchat = GroupChat(
        agents=[tool_execution] + agents + nested_chat_agents + ([user_agent] if user_agent else temp_user_list),
        messages=[],
        max_round=max_rounds,
        speaker_selection_method=swarm_transition,
    )

    manager = _create_swarm_manager(groupchat, swarm_manager_args, agents)

    # Point all ConversableAgent's context variables to this function's context_variables
    _setup_context_variables(tool_execution, agents, manager, context_variables)

    # Link all agents with the GroupChatManager to allow access to the group chat
    # and other agents, particularly the tool executor for setting _swarm_next_agent
    _link_agents_to_swarm_manager(groupchat.agents, manager)

    if len(processed_messages) > 1:
        last_agent, last_message = await manager.a_resume(messages=processed_messages)
        clear_history = False
    else:
        last_message = processed_messages[0]
        clear_history = True

    if last_agent is None:
        raise ValueError("No agent selected to start the conversation")

    chat_result = await last_agent.a_initiate_chat(  # type: ignore[attr-defined]
        manager,
        message=last_message,
        clear_history=clear_history,
    )

    _cleanup_temp_user_messages(chat_result)

    return chat_result, context_variables, manager.last_speaker  # type: ignore[return-value]


@export_module("autogen")
async def a_run_swarm(
    initial_agent: ConversableAgent,
    messages: list[dict[str, Any]] | str,
    agents: list[ConversableAgent],
    user_agent: UserProxyAgent | None = None,
    swarm_manager_args: dict[str, Any] | None = None,
    max_rounds: int = 20,
    context_variables: ContextVariables | None = None,
    after_work: AfterWorkOption
    | Callable[[ConversableAgent, list[dict[str, Any]], GroupChat], AfterWorkOption | ConversableAgent | str]
    | None = AfterWorkOption.TERMINATE,
    exclude_transit_message: bool = True,
) -> AsyncRunResponseProtocol:
    iostream = AsyncThreadIOStream()
    response = AsyncRunResponse(iostream, agents)  # type: ignore[arg-type]

    async def stream_run(
        iostream: AsyncThreadIOStream = iostream,
        response: AsyncRunResponse = response,
    ) -> None:
        with IOStream.set_default(iostream):
            try:
                chat_result, returned_context_variables, last_speaker = await a_initiate_swarm_chat(
                    initial_agent=initial_agent,
                    messages=messages,
                    agents=agents,
                    user_agent=user_agent,
                    swarm_manager_args=swarm_manager_args,
                    max_rounds=max_rounds,
                    context_variables=context_variables,
                    after_work=after_work,
                    exclude_transit_message=exclude_transit_message,
                )

                IOStream.get_default().send(
                    RunCompletionEvent(  # type: ignore[call-arg]
                        history=chat_result.chat_history,
                        summary=chat_result.summary,
                        cost=chat_result.cost,
                        last_speaker=last_speaker.name,
                        context_variables=returned_context_variables,
                    )
                )
            except Exception as e:
                response.iostream.send(ErrorEvent(error=e))  # type: ignore[call-arg]

    task = asyncio.create_task(stream_run())
    # prevent the task from being garbage collected
    response._task_ref = task  # type: ignore[attr-defined]
    return response


@export_module("autogen")
class SwarmResult(BaseModel):
    """Encapsulates the possible return values for a swarm agent function."""

    values: str = ""
    agent: ConversableAgent | AfterWorkOption | str | None = None
    context_variables: ContextVariables | None = None

    @field_serializer("agent", when_used="json")
    def serialize_agent(self, agent: ConversableAgent | str) -> str:
        if isinstance(agent, ConversableAgent):
            return agent.name
        return agent

    def model_post_init(self, __context: Any) -> None:
        # Initialise with a new ContextVariables object if not provided
        if self.context_variables is None:
            self.context_variables = ContextVariables()

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def __str__(self) -> str:
        return self.values


def _set_to_tool_execution(agent: ConversableAgent) -> None:
    """Set to a special instance of ConversableAgent that is responsible for executing tool calls from other swarm agents.
    This agent will be used internally and should not be visible to the user.

    It will execute the tool calls and update the referenced context_variables and next_agent accordingly.
    """
    agent._swarm_next_agent = None  # type: ignore[attr-defined]
    agent._reply_func_list.clear()
    agent.register_reply([Agent, None], _generate_swarm_tool_reply)


@export_module("autogen")
def register_hand_off(
    agent: ConversableAgent,
    hand_to: list[OnCondition | OnContextCondition | AfterWork] | OnCondition | OnContextCondition | AfterWork,
) -> None:
    """Register a function to hand off to another agent.

    Args:
        agent: The agent to register the hand off with.
        hand_to: A list of OnCondition's and an, optional, AfterWork condition

    Hand off template:
    def transfer_to_agent_name() -> ConversableAgent:
        return agent_name
    1. register the function with the agent
    2. register the schema with the agent, description set to the condition
    """
    # If the agent hasn't been established as a swarm agent, do so first
    if not hasattr(agent, "_swarm_is_established"):
        _establish_swarm_agent(agent)

    # Ensure that hand_to is a list or OnCondition or AfterWork
    if not isinstance(hand_to, (list, OnCondition, OnContextCondition, AfterWork)):
        raise ValueError("hand_to must be a list of OnCondition, OnContextCondition, or AfterWork")

    if isinstance(hand_to, (OnCondition, OnContextCondition, AfterWork)):
        hand_to = [hand_to]

    for transit in hand_to:
        if isinstance(transit, AfterWork):
            if not (isinstance(transit.agent, (AfterWorkOption, ConversableAgent, str)) or callable(transit.agent)):
                raise ValueError(f"Invalid AfterWork agent: {transit.agent}")
            agent._swarm_after_work = transit  # type: ignore[attr-defined]
            agent._swarm_after_work_selection_msg = transit.next_agent_selection_msg  # type: ignore[attr-defined]
        elif isinstance(transit, OnCondition):
            if isinstance(transit.target, ConversableAgent):
                # Transition to agent

                # Create closure with current loop transit value
                # to ensure the condition matches the one in the loop
                def make_transfer_function(current_transit: OnCondition) -> Callable[[], ConversableAgent]:
                    def transfer_to_agent() -> ConversableAgent:
                        return current_transit.target  # type: ignore[return-value]

                    return transfer_to_agent

                transfer_func = make_transfer_function(transit)

                # Store function to add/remove later based on it being 'available'
                # Function names are made unique and allow multiple OnCondition's to the same agent
                base_func_name = f"transfer_{agent.name}_to_{transit.target.name}"
                func_name = base_func_name
                count = 2
                while func_name in agent._swarm_conditional_functions:  # type: ignore[attr-defined]
                    func_name = f"{base_func_name}_{count}"
                    count += 1

                # Store function to add/remove later based on it being 'available'
                agent._swarm_conditional_functions[func_name] = (transfer_func, transit)  # type: ignore[attr-defined]

            elif isinstance(transit.target, dict):
                # Transition to a nested chat
                # We will store them here and establish them in the initiate_swarm_chat
                agent._swarm_nested_chat_handoffs.append({  # type: ignore[attr-defined]
                    "nested_chats": transit.target,
                    "condition": transit.condition,
                    "available": transit.available,
                })

        elif isinstance(transit, OnContextCondition):
            agent._swarm_oncontextconditions.append(transit)  # type: ignore[attr-defined]

        else:
            raise ValueError("Invalid hand off condition, must be either OnCondition or AfterWork")


def _update_conditional_functions(agent: ConversableAgent, messages: list[dict[str, Any]] | None = None) -> None:
    """Updates the agent's functions based on the OnCondition's available condition."""
    for func_name, (func, on_condition) in agent._swarm_conditional_functions.items():  # type: ignore[attr-defined]
        is_available = True

        if on_condition.available is not None:
            if callable(on_condition.available):
                is_available = on_condition.available(agent, next(iter(agent.chat_messages.values())))
            elif isinstance(on_condition.available, str):
                is_available = agent.context_variables.get(on_condition.available) or False
            elif isinstance(on_condition.available, ContextExpression):
                is_available = on_condition.available.evaluate(agent.context_variables)

        # first remove the function if it exists
        if func_name in agent._function_map:
            agent.update_tool_signature(func_name, is_remove=True)
            del agent._function_map[func_name]

        # then add the function if it is available, so that the function signature is updated
        if is_available:
            condition = on_condition.condition
            if isinstance(condition, ContextStr):
                condition = condition.format(context_variables=agent.context_variables)
            elif callable(condition):
                condition = condition(agent, messages)

            # TODO: Don't add it if it's already there
            agent._add_single_function(func, func_name, condition)


def _generate_swarm_tool_reply(
    agent: ConversableAgent,
    messages: list[dict[str, Any]] | None = None,
    sender: Agent | None = None,
    config: OpenAIWrapper | None = None,
) -> tuple[bool, dict[str, Any] | None]:
    """Pre-processes and generates tool call replies.

    This function:
    1. Adds context_variables back to the tool call for the function, if necessary.
    2. Generates the tool calls reply.
    3. Updates context_variables and next_agent based on the tool call response.
    """
    if config is None:
        config = agent  # type: ignore[assignment]
    if messages is None:
        messages = agent._oai_messages[sender]

    message = messages[-1]
    if "tool_calls" in message:
        tool_call_count = len(message["tool_calls"])

        # Loop through tool calls individually (so context can be updated after each function call)
        next_agent: Agent | None = None
        tool_responses_inner = []
        contents = []
        for index in range(tool_call_count):
            message_copy = copy.deepcopy(message)

            # 1. add context_variables to the tool call arguments
            tool_call = message_copy["tool_calls"][index]

            # Ensure we are only executing the one tool at a time
            message_copy["tool_calls"] = [tool_call]

            # 2. generate tool calls reply
            _, tool_message = agent.generate_tool_calls_reply([message_copy])

            if tool_message is None:
                raise ValueError("Tool call did not return a message")

            # 3. update context_variables and next_agent, convert content to string
            for tool_response in tool_message["tool_responses"]:
                content = tool_response.get("content")

                if isinstance(content, SwarmResult):
                    if content.context_variables is not None and content.context_variables.to_dict() != {}:
                        agent.context_variables.update(content.context_variables.to_dict())
                    if content.agent is not None:
                        next_agent = content.agent  # type: ignore[assignment]
                elif isinstance(content, Agent):
                    next_agent = content

                # Serialize the content to a string
                if content is not None:
                    tool_response["content"] = str(content)

                tool_responses_inner.append(tool_response)
                contents.append(str(tool_response["content"]))

        agent._swarm_next_agent = next_agent  # type: ignore[attr-defined]

        # Put the tool responses and content strings back into the response message
        # Caters for multiple tool calls
        if tool_message is None:
            raise ValueError("Tool call did not return a message")

        tool_message["tool_responses"] = tool_responses_inner
        tool_message["content"] = "\n".join(contents)

        return True, tool_message
    return False, None


class SwarmAgent(ConversableAgent):
    """SwarmAgent is deprecated and has been incorporated into ConversableAgent, use ConversableAgent instead. SwarmAgent will be removed in a future version (TBD)"""

    def __init__(self, *args: Any, **kwargs: Any):
        """Initializes a new instance of the SwarmAgent class.

        Args:
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.
        """
        warnings.warn(
            "SwarmAgent is deprecated and has been incorporated into ConversableAgent, use ConversableAgent instead. SwarmAgent will be removed in a future version (TBD).",
            DeprecationWarning,
            stacklevel=2,
        )

        super().__init__(*args, **kwargs)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import os
import re
import urllib.parse
from collections.abc import Callable
from typing import Any, Optional

from ....import_utils import optional_import_block, require_optional_import
from .base import Document, ItemID, QueryResults, VectorDB
from .utils import get_logger

with optional_import_block():
    import numpy as np
    import pgvector  # noqa: F401
    import psycopg
    from pgvector.psycopg import register_vector
    from sentence_transformers import SentenceTransformer

PGVECTOR_MAX_BATCH_SIZE = os.environ.get("PGVECTOR_MAX_BATCH_SIZE", 40000)
logger = get_logger(__name__)


@require_optional_import(["psycopg", "sentence_transformers", "numpy"], "retrievechat-pgvector")
class Collection:
    """A Collection object for PGVector.

    Attributes:
        client: The PGVector client.
        collection_name (str): The name of the collection. Default is "documents".
        embedding_function (Callable): The embedding function used to generate the vector representation.
            Default is None. SentenceTransformer("all-MiniLM-L6-v2").encode will be used when None.
            Models can be chosen from:
            https://huggingface.co/models?library=sentence-transformers
        metadata (Optional[dict[str, Any]]): The metadata of the collection.
        get_or_create (Optional): The flag indicating whether to get or create the collection.
    """

    def __init__(
        self,
        client: Any | None = None,
        collection_name: str = "ag2-docs",
        embedding_function: Callable[..., Any] | None = None,
        metadata: Any | None = None,
        get_or_create: Any | None = None,
    ):
        """Initialize the Collection object.

        Args:
            client: The PostgreSQL client.
            collection_name: The name of the collection. Default is "documents".
            embedding_function: The embedding function used to generate the vector representation.
            metadata: The metadata of the collection.
            get_or_create: The flag indicating whether to get or create the collection.

        Returns:
            None
        """
        self.client = client
        self.name = self.set_collection_name(collection_name)
        self.require_embeddings_or_documents = False
        self.ids = []
        if embedding_function:
            self.embedding_function = embedding_function
        else:
            self.embedding_function = SentenceTransformer("all-MiniLM-L6-v2").encode
        self.metadata = metadata if metadata else {"hnsw:space": "ip", "hnsw:construction_ef": 32, "hnsw:M": 16}
        self.documents = ""
        self.get_or_create = get_or_create
        # This will get the model dimension size by computing the embeddings dimensions
        sentences = [
            "The weather is lovely today in paradise.",
        ]
        embeddings = self.embedding_function(sentences)
        self.dimension = len(embeddings[0])

    def set_collection_name(self, collection_name) -> str:
        name = re.sub("-", "_", collection_name)
        self.name = name
        return self.name

    def add(
        self,
        ids: list[ItemID],
        documents: list[Document] | None,
        embeddings: list[Any] | None = None,
        metadatas: list[Any] | None = None,
    ) -> None:
        """Add documents to the collection.

        Args:
            ids (List[ItemID]): A list of document IDs.
            embeddings (List): A list of document embeddings. Optional
            metadatas (List): A list of document metadatas. Optional
            documents (List): A list of documents.

        Returns:
            None
        """
        cursor = self.client.cursor()
        sql_values = []
        if embeddings is not None and metadatas is not None:
            for doc_id, embedding, metadata, document in zip(ids, embeddings, metadatas, documents):
                metadata = re.sub("'", '"', str(metadata))
                sql_values.append((doc_id, embedding, metadata, document))
            sql_string = f"INSERT INTO {self.name} (id, embedding, metadatas, documents)\nVALUES (%s, %s, %s, %s);\n"
        elif embeddings is not None:
            for doc_id, embedding, document in zip(ids, embeddings, documents):
                sql_values.append((doc_id, embedding, document))
            sql_string = f"INSERT INTO {self.name} (id, embedding, documents) VALUES (%s, %s, %s);\n"
        elif metadatas is not None:
            for doc_id, metadata, document in zip(ids, metadatas, documents):
                metadata = re.sub("'", '"', str(metadata))
                embedding = self.embedding_function(document)
                sql_values.append((doc_id, metadata, embedding, document))
            sql_string = f"INSERT INTO {self.name} (id, metadatas, embedding, documents)\nVALUES (%s, %s, %s, %s);\n"
        else:
            for doc_id, document in zip(ids, documents):
                embedding = self.embedding_function(document)
                sql_values.append((doc_id, document, embedding))
            sql_string = f"INSERT INTO {self.name} (id, documents, embedding)\nVALUES (%s, %s, %s);\n"
        logger.debug(f"Add SQL String:\n{sql_string}\n{sql_values}")
        cursor.executemany(sql_string, sql_values)
        cursor.close()

    def upsert(
        self,
        ids: list[ItemID],
        documents: list[Document],
        embeddings: list[Any] | None = None,
        metadatas: list[Any] | None = None,
    ) -> None:
        """Upsert documents into the collection.

        Args:
            ids (List[ItemID]): A list of document IDs.
            documents (List): A list of documents.
            embeddings (List): A list of document embeddings.
            metadatas (List): A list of document metadatas.

        Returns:
            None
        """
        cursor = self.client.cursor()
        sql_values = []
        if embeddings is not None and metadatas is not None:
            for doc_id, embedding, metadata, document in zip(ids, embeddings, metadatas, documents):
                metadata = re.sub("'", '"', str(metadata))
                sql_values.append((doc_id, embedding, metadata, document, embedding, metadata, document))
            sql_string = (
                f"INSERT INTO {self.name} (id, embedding, metadatas, documents)\n"
                f"VALUES (%s, %s, %s, %s)\n"
                f"ON CONFLICT (id)\n"
                f"DO UPDATE SET embedding = %s,\n"
                f"metadatas = %s, documents = %s;\n"
            )
        elif embeddings is not None:
            for doc_id, embedding, document in zip(ids, embeddings, documents):
                sql_values.append((doc_id, embedding, document, embedding, document))
            sql_string = (
                f"INSERT INTO {self.name} (id, embedding, documents) "
                f"VALUES (%s, %s, %s) ON CONFLICT (id)\n"
                f"DO UPDATE SET embedding = %s, documents = %s;\n"
            )
        elif metadatas is not None:
            for doc_id, metadata, document in zip(ids, metadatas, documents):
                metadata = re.sub("'", '"', str(metadata))
                embedding = self.embedding_function(document)
                sql_values.append((doc_id, metadata, embedding, document, metadata, document, embedding))
            sql_string = (
                f"INSERT INTO {self.name} (id, metadatas, embedding, documents)\n"
                f"VALUES (%s, %s, %s, %s)\n"
                f"ON CONFLICT (id)\n"
                f"DO UPDATE SET metadatas = %s, documents = %s, embedding = %s;\n"
            )
        else:
            for doc_id, document in zip(ids, documents):
                embedding = self.embedding_function(document)
                sql_values.append((doc_id, document, embedding, document))
            sql_string = (
                f"INSERT INTO {self.name} (id, documents, embedding)\n"
                f"VALUES (%s, %s, %s)\n"
                f"ON CONFLICT (id)\n"
                f"DO UPDATE SET documents = %s;\n"
            )
        logger.debug(f"Upsert SQL String:\n{sql_string}\n{sql_values}")
        cursor.executemany(sql_string, sql_values)
        cursor.close()

    def count(self) -> int:
        """Get the total number of documents in the collection.

        Returns:
            int: The total number of documents.
        """
        cursor = self.client.cursor()
        query = f"SELECT COUNT(*) FROM {self.name}"
        cursor.execute(query)
        total = cursor.fetchone()[0]
        cursor.close()
        try:
            total = int(total)
        except (TypeError, ValueError):
            total = None
        return total

    def table_exists(self, table_name: str) -> bool:
        """Check if a table exists in the PostgreSQL database.

        Args:
            table_name (str): The name of the table to check.

        Returns:
            bool: True if the table exists, False otherwise.
        """
        cursor = self.client.cursor()
        cursor.execute(
            """
            SELECT EXISTS (
                SELECT 1
                FROM information_schema.tables
                WHERE table_name = %s
            )
            """,
            (table_name,),
        )
        exists = cursor.fetchone()[0]
        return exists

    def get(
        self,
        ids: str | None = None,
        include: str | None = None,
        where: str | None = None,
        limit: int | str | None = None,
        offset: int | str | None = None,
    ) -> list[Document]:
        """Retrieve documents from the collection.

        Args:
            ids (Optional[List]): A list of document IDs.
            include (Optional): The fields to include.
            where (Optional): Additional filtering criteria.
            limit (Optional): The maximum number of documents to retrieve.
            offset (Optional): The offset for pagination.

        Returns:
            List: The retrieved documents.
        """
        cursor = self.client.cursor()

        # Initialize variables for query components
        select_clause = "SELECT id, metadatas, documents, embedding"
        from_clause = f"FROM {self.name}"
        where_clause = ""
        limit_clause = ""
        offset_clause = ""

        # Handle include clause
        if include:
            select_clause = f"SELECT id, {', '.join(include)}, embedding"

        # Handle where clause
        if ids:
            where_clause = f"WHERE id IN ({', '.join(['%s' for _ in ids])})"
        elif where:
            where_clause = f"WHERE {where}"

        # Handle limit and offset clauses
        if limit:
            limit_clause = "LIMIT %s"
        if offset:
            offset_clause = "OFFSET %s"

        # Construct the full query
        query = f"{select_clause} {from_clause} {where_clause} {limit_clause} {offset_clause}"
        retrieved_documents = []
        try:
            # Execute the query with the appropriate values
            if ids is not None:
                cursor.execute(query, ids)
            else:
                query_params = []
                if limit:
                    query_params.append(limit)
                if offset:
                    query_params.append(offset)
                cursor.execute(query, query_params)

            retrieval = cursor.fetchall()
            for retrieved_document in retrieval:
                retrieved_documents.append(
                    Document(
                        id=retrieved_document[0].strip(),
                        metadata=retrieved_document[1],
                        content=retrieved_document[2],
                        embedding=retrieved_document[3],
                    )
                )
        except (psycopg.errors.UndefinedTable, psycopg.errors.UndefinedColumn) as e:
            logger.info(f"Error executing select on non-existent table: {self.name}. Creating it instead. Error: {e}")
            self.create_collection(collection_name=self.name, dimension=self.dimension)
            logger.info(f"Created table {self.name}")

        cursor.close()
        return retrieved_documents

    def update(self, ids: list[str], embeddings: list[Any], metadatas: list[Any], documents: list[Document]) -> None:
        """Update documents in the collection.

        Args:
            ids (List): A list of document IDs.
            embeddings (List): A list of document embeddings.
            metadatas (List): A list of document metadatas.
            documents (List): A list of documents.

        Returns:
            None
        """
        cursor = self.client.cursor()
        sql_values = []
        for doc_id, embedding, metadata, document in zip(ids, embeddings, metadatas, documents):
            sql_values.append((doc_id, embedding, metadata, document, doc_id, embedding, metadata, document))
        sql_string = (
            f"INSERT INTO {self.name} (id, embedding, metadata, document) "
            f"VALUES (%s, %s, %s, %s) "
            f"ON CONFLICT (id) "
            f"DO UPDATE SET id = %s, embedding = %s, "
            f"metadata = %s, document = %s;\n"
        )
        logger.debug(f"Upsert SQL String:\n{sql_string}\n")
        cursor.executemany(sql_string, sql_values)
        cursor.close()

    @staticmethod
    def euclidean_distance(arr1: list[float], arr2: list[float]) -> float:
        """Calculate the Euclidean distance between two vectors.

        Parameters:
        - arr1 (List[float]): The first vector.
        - arr2 (List[float]): The second vector.

        Returns:
        - float: The Euclidean distance between arr1 and arr2.
        """
        dist = np.linalg.norm(arr1 - arr2)
        return dist

    @staticmethod
    def cosine_distance(arr1: list[float], arr2: list[float]) -> float:
        """Calculate the cosine distance between two vectors.

        Parameters:
        - arr1 (List[float]): The first vector.
        - arr2 (List[float]): The second vector.

        Returns:
        - float: The cosine distance between arr1 and arr2.
        """
        dist = np.dot(arr1, arr2) / (np.linalg.norm(arr1) * np.linalg.norm(arr2))
        return dist

    @staticmethod
    def inner_product_distance(arr1: list[float], arr2: list[float]) -> float:
        """Calculate the Euclidean distance between two vectors.

        Parameters:
        - arr1 (List[float]): The first vector.
        - arr2 (List[float]): The second vector.

        Returns:
        - float: The Euclidean distance between arr1 and arr2.
        """
        dist = np.linalg.norm(arr1 - arr2)
        return dist

    def query(
        self,
        query_texts: list[str],
        collection_name: str | None = None,
        n_results: int | None = 10,
        distance_type: str | None = "euclidean",
        distance_threshold: float | None = -1,
        include_embedding: bool | None = False,
    ) -> QueryResults:
        """Query documents in the collection.

        Args:
            query_texts (List[str]): A list of query texts.
            collection_name (Optional[str]): The name of the collection.
            n_results (int): The maximum number of results to return.
            distance_type (Optional[str]): Distance search type - euclidean or cosine
            distance_threshold (Optional[float]): Distance threshold to limit searches
            include_embedding (Optional[bool]): Include embedding values in QueryResults
        Returns:
            QueryResults: The query results.
        """
        if collection_name:
            self.name = collection_name

        clause = "ORDER BY"
        if distance_threshold == -1:
            distance_threshold = ""
            clause = "ORDER BY"
        elif distance_threshold > 0:
            distance_threshold = f"< {distance_threshold}"
            clause = "WHERE"

        cursor = self.client.cursor()
        results = []
        for query_text in query_texts:
            vector = self.embedding_function(query_text, convert_to_tensor=False).tolist()
            if distance_type.lower() == "cosine":
                index_function = "<=>"
            elif distance_type.lower() == "euclidean":
                index_function = "<->"
            elif distance_type.lower() == "inner-product":
                index_function = "<#>"
            else:
                index_function = "<->"
            query = (
                f"SELECT id, documents, embedding, metadatas "
                f"FROM {self.name} "
                f"{clause} embedding {index_function} '{vector!s}' {distance_threshold} "
                f"LIMIT {n_results}"
            )
            cursor.execute(query)
            result = []
            for row in cursor.fetchall():
                fetched_document = Document(id=row[0].strip(), content=row[1], embedding=row[2], metadata=row[3])
                fetched_document_array = self.convert_string_to_array(array_string=fetched_document.get("embedding"))
                if distance_type.lower() == "cosine":
                    distance = self.cosine_distance(fetched_document_array, vector)
                elif distance_type.lower() == "euclidean":
                    distance = self.euclidean_distance(fetched_document_array, vector)
                elif distance_type.lower() == "inner-product":
                    distance = self.inner_product_distance(fetched_document_array, vector)
                else:
                    distance = self.euclidean_distance(fetched_document_array, vector)
                if not include_embedding:
                    fetched_document = Document(id=row[0].strip(), content=row[1], metadata=row[3])
                result.append((fetched_document, distance))
            results.append(result)
        cursor.close()
        logger.debug(f"Query Results: {results}")
        return results

    @staticmethod
    def convert_string_to_array(array_string: str) -> list[float]:
        """Convert a string representation of an array to a list of floats.

        Parameters:
        - array_string (str): The string representation of the array.

        Returns:
        - list: A list of floats parsed from the input string. If the input is
          not a string, it returns the input itself.
        """
        if not isinstance(array_string, str):
            return array_string
        array_string = array_string.strip("[]")
        array = [float(num) for num in array_string.split()]
        return array

    def modify(self, metadata, collection_name: str | None = None) -> None:
        """Modify metadata for the collection.

        Args:
            collection_name: The name of the collection.
            metadata: The new metadata.

        Returns:
            None
        """
        if collection_name:
            self.name = collection_name
        cursor = self.client.cursor()
        cursor.execute("UPDATE collectionsSET metadata = '%s'WHERE collection_name = '%s';", (metadata, self.name))
        cursor.close()

    def delete(self, ids: list[ItemID], collection_name: str | None = None) -> None:
        """Delete documents from the collection.

        Args:
            ids (List[ItemID]): A list of document IDs to delete.
            collection_name (str): The name of the collection to delete.

        Returns:
            None
        """
        if collection_name:
            self.name = collection_name
        cursor = self.client.cursor()
        id_placeholders = ", ".join(["%s" for _ in ids])
        cursor.execute(f"DELETE FROM {self.name} WHERE id IN ({id_placeholders});", ids)
        cursor.close()

    def delete_collection(self, collection_name: str | None = None) -> None:
        """Delete the entire collection.

        Args:
            collection_name (Optional[str]): The name of the collection to delete.

        Returns:
            None
        """
        if collection_name:
            self.name = collection_name
        cursor = self.client.cursor()
        cursor.execute(f"DROP TABLE IF EXISTS {self.name}")
        cursor.close()

    def create_collection(self, collection_name: str | None = None, dimension: str | int | None = None) -> None:
        """Create a new collection.

        Args:
            collection_name (Optional[str]): The name of the new collection.
            dimension (Optional[Union[str, int]]): The dimension size of the sentence embedding model

        Returns:
            None
        """
        if collection_name:
            self.name = collection_name

        if dimension:
            self.dimension = dimension
        elif self.dimension is None:
            self.dimension = 384

        cursor = self.client.cursor()
        cursor.execute(
            f"CREATE TABLE {self.name} ("
            f"documents text, id CHAR(8) PRIMARY KEY, metadatas JSONB, embedding vector({self.dimension}));"
            f"CREATE INDEX "
            f"ON {self.name} USING hnsw (embedding vector_l2_ops) WITH (m = {self.metadata['hnsw:M']}, "
            f"ef_construction = {self.metadata['hnsw:construction_ef']});"
            f"CREATE INDEX "
            f"ON {self.name} USING hnsw (embedding vector_cosine_ops) WITH (m = {self.metadata['hnsw:M']}, "
            f"ef_construction = {self.metadata['hnsw:construction_ef']});"
            f"CREATE INDEX "
            f"ON {self.name} USING hnsw (embedding vector_ip_ops) WITH (m = {self.metadata['hnsw:M']}, "
            f"ef_construction = {self.metadata['hnsw:construction_ef']});"
        )
        cursor.close()


@require_optional_import(["pgvector", "psycopg", "sentence_transformers"], "retrievechat-pgvector")
class PGVectorDB(VectorDB):
    """A vector database that uses PGVector as the backend."""

    def __init__(
        self,
        *,
        conn: Optional["psycopg.Connection"] = None,
        connection_string: str | None = None,
        host: str | None = None,
        port: int | str | None = None,
        dbname: str | None = None,
        username: str | None = None,
        password: str | None = None,
        connect_timeout: int | None = 10,
        embedding_function: Callable = None,
        metadata: dict[str, Any] | None = None,
    ) -> None:
        """Initialize the vector database.

        Note: connection_string or host + port + dbname must be specified

        Args:
            conn: psycopg.Connection | A customer connection object to connect to the database.
                A connection object may include additional key/values:
                https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING
            connection_string: "postgresql://username:password@hostname:port/database" | The PGVector connection string. Default is None.
            host: str | The host to connect to. Default is None.
            port: int | The port to connect to. Default is None.
            dbname: str | The database name to connect to. Default is None.
            username: str | The database username to use. Default is None.
            password: str | The database user password to use. Default is None.
            connect_timeout: int | The timeout to set for the connection. Default is 10.
            embedding_function: Callable | The embedding function used to generate the vector representation.
                Default is None. SentenceTransformer("all-MiniLM-L6-v2").encode will be used when None.
                Models can be chosen from:
                https://huggingface.co/models?library=sentence-transformers
            metadata: dict | The metadata of the vector database. Default is None. If None, it will use this
                setting: `{"hnsw:space": "ip", "hnsw:construction_ef": 30, "hnsw:M": 16}`. Creates Index on table
                using hnsw (embedding vector_l2_ops) WITH (m = hnsw:M) ef_construction = "hnsw:construction_ef".
                For more info: https://github.com/pgvector/pgvector?tab=readme-ov-file#hnsw
        Returns:
            None
        """
        self.client = self.establish_connection(
            conn=conn,
            connection_string=connection_string,
            host=host,
            port=port,
            dbname=dbname,
            username=username,
            password=password,
            connect_timeout=connect_timeout,
        )
        if embedding_function:
            self.embedding_function = embedding_function
        else:
            self.embedding_function = SentenceTransformer("all-MiniLM-L6-v2").encode
        self.metadata = metadata
        register_vector(self.client)
        self.active_collection = None

    def establish_connection(
        self,
        conn: Optional["psycopg.Connection"] = None,
        connection_string: str | None = None,
        host: str | None = None,
        port: int | str | None = None,
        dbname: str | None = None,
        username: str | None = None,
        password: str | None = None,
        connect_timeout: int | None = 10,
    ) -> "psycopg.Connection":
        """Establishes a connection to a PostgreSQL database using psycopg.

        Args:
            conn: An existing psycopg connection object. If provided, this connection will be used.
            connection_string: A string containing the connection information. If provided, a new connection will be established using this string.
            host: The hostname of the PostgreSQL server. Used if connection_string is not provided.
            port: The port number to connect to at the server host. Used if connection_string is not provided.
            dbname: The database name. Used if connection_string is not provided.
            username: The username to connect as. Used if connection_string is not provided.
            password: The user's password. Used if connection_string is not provided.
            connect_timeout: Maximum wait for connection, in seconds. The default is 10 seconds.

        Returns:
            A psycopg.Connection object representing the established connection.

        Raises:
            PermissionError if no credentials are supplied
            psycopg.Error: If an error occurs while trying to connect to the database.
        """
        try:
            if conn:
                self.client = conn
            elif connection_string:
                parsed_connection = urllib.parse.urlparse(connection_string)
                encoded_username = urllib.parse.quote(parsed_connection.username, safe="")
                encoded_password = urllib.parse.quote(parsed_connection.password, safe="")
                encoded_password = f":{encoded_password}@"
                encoded_host = urllib.parse.quote(parsed_connection.hostname, safe="")
                encoded_port = f":{parsed_connection.port}"
                encoded_database = urllib.parse.quote(parsed_connection.path[1:], safe="")
                connection_string_encoded = (
                    f"{parsed_connection.scheme}://{encoded_username}{encoded_password}"
                    f"{encoded_host}{encoded_port}/{encoded_database}"
                )
                self.client = psycopg.connect(conninfo=connection_string_encoded, autocommit=True)
            elif host:
                connection_string = ""
                if host:
                    encoded_host = urllib.parse.quote(host, safe="")
                    connection_string += f"host={encoded_host} "
                if port:
                    connection_string += f"port={port} "
                if dbname:
                    encoded_database = urllib.parse.quote(dbname, safe="")
                    connection_string += f"dbname={encoded_database} "
                if username:
                    encoded_username = urllib.parse.quote(username, safe="")
                    connection_string += f"user={encoded_username} "
                if password:
                    encoded_password = urllib.parse.quote(password, safe="")
                    connection_string += f"password={encoded_password} "

                self.client = psycopg.connect(
                    conninfo=connection_string,
                    connect_timeout=connect_timeout,
                    autocommit=True,
                )
            else:
                logger.error("Credentials were not supplied...")
                raise PermissionError
            self.client.execute("CREATE EXTENSION IF NOT EXISTS vector")
        except psycopg.Error as e:
            logger.error("Error connecting to the database: ", e)
            raise e
        return self.client

    def create_collection(
        self, collection_name: str, overwrite: bool = False, get_or_create: bool = True
    ) -> Collection:
        """Create a collection in the vector database.
        Case 1. if the collection does not exist, create the collection.
        Case 2. the collection exists, if overwrite is True, it will overwrite the collection.
        Case 3. the collection exists and overwrite is False, if get_or_create is True, it will get the collection,
            otherwise it raise a ValueError.

        Args:
            collection_name: str | The name of the collection.
            overwrite: bool | Whether to overwrite the collection if it exists. Default is False.
            get_or_create: bool | Whether to get the collection if it exists. Default is True.

        Returns:
            Collection | The collection object.
        """
        try:
            if self.active_collection and self.active_collection.name == collection_name:
                collection = self.active_collection
            else:
                collection = self.get_collection(collection_name)
        except ValueError:
            collection = None
        if collection is None:
            collection = Collection(
                client=self.client,
                collection_name=collection_name,
                embedding_function=self.embedding_function,
                get_or_create=get_or_create,
                metadata=self.metadata,
            )
            collection.set_collection_name(collection_name=collection_name)
            collection.create_collection(collection_name=collection_name)
            return collection
        elif overwrite:
            self.delete_collection(collection_name)
            collection = Collection(
                client=self.client,
                collection_name=collection_name,
                embedding_function=self.embedding_function,
                get_or_create=get_or_create,
                metadata=self.metadata,
            )
            collection.set_collection_name(collection_name=collection_name)
            collection.create_collection(collection_name=collection_name)
            return collection
        elif get_or_create:
            return collection
        elif not collection.table_exists(table_name=collection_name):
            collection = Collection(
                client=self.client,
                collection_name=collection_name,
                embedding_function=self.embedding_function,
                get_or_create=get_or_create,
                metadata=self.metadata,
            )
            collection.set_collection_name(collection_name=collection_name)
            collection.create_collection(collection_name=collection_name)
            return collection
        else:
            raise ValueError(f"Collection {collection_name} already exists.")

    def get_collection(self, collection_name: str = None) -> Collection:
        """Get the collection from the vector database.

        Args:
            collection_name: str | The name of the collection. Default is None. If None, return the
                current active collection.

        Returns:
            Collection | The collection object.
        """
        if collection_name is None:
            if self.active_collection is None:
                raise ValueError("No collection is specified.")
            else:
                logger.debug(
                    f"No collection is specified. Using current active collection {self.active_collection.name}."
                )
        else:
            if not (self.active_collection and self.active_collection.name == collection_name):
                self.active_collection = Collection(
                    client=self.client,
                    collection_name=collection_name,
                    embedding_function=self.embedding_function,
                )
        return self.active_collection

    def delete_collection(self, collection_name: str) -> None:
        """Delete the collection from the vector database.

        Args:
            collection_name: str | The name of the collection.

        Returns:
            None
        """
        if self.active_collection:
            self.active_collection.delete_collection(collection_name)
        else:
            collection = self.get_collection(collection_name)
            collection.delete_collection(collection_name)
        if self.active_collection and self.active_collection.name == collection_name:
            self.active_collection = None

    def _batch_insert(
        self, collection: Collection, embeddings=None, ids=None, metadatas=None, documents=None, upsert=False
    ) -> None:
        batch_size = int(PGVECTOR_MAX_BATCH_SIZE)
        default_metadata = {"hnsw:space": "ip", "hnsw:construction_ef": 32, "hnsw:M": 16}
        default_metadatas = [default_metadata] * min(batch_size, len(documents))
        for i in range(0, len(documents), min(batch_size, len(documents))):
            end_idx = i + min(batch_size, len(documents) - i)
            collection_kwargs = {
                "documents": documents[i:end_idx],
                "ids": ids[i:end_idx],
                "metadatas": metadatas[i:end_idx] if metadatas else default_metadatas,
                "embeddings": embeddings[i:end_idx] if embeddings else None,
            }
            if upsert:
                collection.upsert(**collection_kwargs)
            else:
                collection.add(**collection_kwargs)

    def insert_docs(self, docs: list[Document], collection_name: str = None, upsert: bool = False) -> None:
        """Insert documents into the collection of the vector database.

        Args:
            docs: List[Document] | A list of documents. Each document is a TypedDict `Document`.
            collection_name: str | The name of the collection. Default is None.
            upsert: bool | Whether to update the document if it exists. Default is False.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        if not docs:
            return
        if docs[0].get("content") is None:
            raise ValueError("The document content is required.")
        if docs[0].get("id") is None:
            raise ValueError("The document id is required.")
        documents = [doc.get("content") for doc in docs]
        ids = [doc.get("id") for doc in docs]

        collection = self.get_collection(collection_name)
        if docs[0].get("embedding") is None:
            logger.debug(
                "No content embedding is provided. "
                "Will use the VectorDB's embedding function to generate the content embedding."
            )
            embeddings = None
        else:
            embeddings = [doc.get("embedding") for doc in docs]
        metadatas = None if docs[0].get("metadata") is None else [doc.get("metadata") for doc in docs]

        self._batch_insert(collection, embeddings, ids, metadatas, documents, upsert)

    def update_docs(self, docs: list[Document], collection_name: str = None) -> None:
        """Update documents in the collection of the vector database.

        Args:
            docs: List[Document] | A list of documents.
            collection_name: str | The name of the collection. Default is None.

        Returns:
            None
        """
        self.insert_docs(docs, collection_name, upsert=True)

    def delete_docs(self, ids: list[ItemID], collection_name: str = None) -> None:
        """Delete documents from the collection of the vector database.

        Args:
            ids: List[ItemID] | A list of document ids. Each id is a typed `ItemID`.
            collection_name: str | The name of the collection. Default is None.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        collection = self.get_collection(collection_name)
        collection.delete(ids=ids, collection_name=collection_name)

    def retrieve_docs(
        self,
        queries: list[str],
        collection_name: str = None,
        n_results: int = 10,
        distance_threshold: float = -1,
    ) -> QueryResults:
        """Retrieve documents from the collection of the vector database based on the queries.

        Args:
            queries: List[str] | A list of queries. Each query is a string.
            collection_name: str | The name of the collection. Default is None.
            n_results: int | The number of relevant documents to return. Default is 10.
            distance_threshold: float | The threshold for the distance score, only distance smaller than it will be
                returned. Don't filter with it if `< 0`. Default is -1.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            QueryResults | The query results. Each query result is a list of list of tuples containing the document and
                the distance.
        """
        collection = self.get_collection(collection_name)
        if isinstance(queries, str):
            queries = [queries]
        results = collection.query(
            query_texts=queries,
            n_results=n_results,
            distance_threshold=distance_threshold,
        )
        logger.debug(f"Retrieve Docs Results:\n{results}")
        return results

    def get_docs_by_ids(
        self, ids: list[ItemID] = None, collection_name: str = None, include=None, **kwargs
    ) -> list[Document]:
        """Retrieve documents from the collection of the vector database based on the ids.

        Args:
            ids: List[ItemID] | A list of document ids. If None, will return all the documents. Default is None.
            collection_name: str | The name of the collection. Default is None.
            include: List[str] | The fields to include. Default is None.
                If None, will include ["metadatas", "documents"], ids will always be included.
            kwargs: dict | Additional keyword arguments.

        Returns:
            List[Document] | The results.
        """
        collection = self.get_collection(collection_name)
        include = include if include else ["metadatas", "documents"]
        results = collection.get(ids, include=include, **kwargs)
        logger.debug(f"Retrieve Documents by ID Results:\n{results}")
        return results
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import abc
import logging
from collections.abc import Sequence
from typing import Any

from ....import_utils import optional_import_block, require_optional_import
from .base import Document, ItemID, QueryResults, VectorDB
from .utils import get_logger

with optional_import_block():
    from fastembed import TextEmbedding
    from qdrant_client import QdrantClient, models


logger = get_logger(__name__)

Embeddings = Sequence[float] | Sequence[int]


class EmbeddingFunction(abc.ABC):
    @abc.abstractmethod
    def __call__(self, inputs: list[str]) -> list[Embeddings]:
        raise NotImplementedError


@require_optional_import("fastembed", "retrievechat-qdrant")
class FastEmbedEmbeddingFunction(EmbeddingFunction):
    """Embedding function implementation using FastEmbed - https://qdrant.github.io/fastembed."""

    def __init__(
        self,
        model_name: str = "BAAI/bge-small-en-v1.5",
        batch_size: int = 256,
        cache_dir: str | None = None,
        threads: int | None = None,
        parallel: int | None = None,
        **kwargs: Any,
    ):
        """Initialize fastembed.TextEmbedding.

        Args:
            model_name (str): The name of the model to use. Defaults to `"BAAI/bge-small-en-v1.5"`.
            batch_size (int): Batch size for encoding. Higher values will use more memory, but be faster.\
                                        Defaults to 256.
            cache_dir (str, optional): The path to the model cache directory.\
                                       Can also be set using the `FASTEMBED_CACHE_PATH` env variable.
            threads (int, optional): The number of threads single onnxruntime session can use.
            parallel (int, optional): If `>1`, data-parallel encoding will be used, recommended for large datasets.\
                                      If `0`, use all available cores.\
                                      If `None`, don't use data-parallel processing, use default onnxruntime threading.\
                                      Defaults to None.
            **kwargs: Additional options to pass to fastembed.TextEmbedding
        Raises:
            ValueError: If the model_name is not in the format `<org>/<model>` e.g. BAAI/bge-small-en-v1.5.
        """
        self._batch_size = batch_size
        self._parallel = parallel
        self._model = TextEmbedding(model_name=model_name, cache_dir=cache_dir, threads=threads, **kwargs)

    def __call__(self, inputs: list[str]) -> list[Embeddings]:
        embeddings = self._model.embed(inputs, batch_size=self._batch_size, parallel=self._parallel)

        return [embedding.tolist() for embedding in embeddings]


@require_optional_import("qdrant_client", "retrievechat-qdrant")
class QdrantVectorDB(VectorDB):
    """A vector database implementation that uses Qdrant as the backend."""

    def __init__(
        self,
        *,
        client=None,
        embedding_function: EmbeddingFunction = None,
        content_payload_key: str = "_content",
        metadata_payload_key: str = "_metadata",
        collection_options: dict = {},
        **kwargs: Any,
    ) -> None:
        """Initialize the vector database.

        Args:
            client: An instance of QdrantClient.
            embedding_function: The embedding function used to generate the vector representation
                of the documents. Defaults to FastEmbedEmbeddingFunction.
            content_payload_key: The key to use for the content payload. Default is "_content".
            metadata_payload_key: The key to use for the metadata payload. Default is "_metadata".
            collection_options: The options for creating the collection.
            **kwargs: Additional keyword arguments.
        """
        self.client: QdrantClient = client or QdrantClient(location=":memory:")
        self.embedding_function = embedding_function or FastEmbedEmbeddingFunction()
        self.collection_options = collection_options
        self.content_payload_key = content_payload_key
        self.metadata_payload_key = metadata_payload_key
        self.type = "qdrant"

    def create_collection(self, collection_name: str, overwrite: bool = False, get_or_create: bool = True) -> None:
        """Create a collection in the vector database.
        Case 1. if the collection does not exist, create the collection.
        Case 2. the collection exists, if overwrite is True, it will overwrite the collection.
        Case 3. the collection exists and overwrite is False, if get_or_create is True, it will get the collection,
            otherwise it raise a ValueError.

        Args:
            collection_name: str | The name of the collection.
            overwrite: bool | Whether to overwrite the collection if it exists. Default is False.
            get_or_create: bool | Whether to get the collection if it exists. Default is True.

        Returns:
            Any | The collection object.
        """
        embeddings_size = len(self.embedding_function(["test"])[0])

        if self.client.collection_exists(collection_name) and overwrite:
            self.client.delete_collection(collection_name)

        if not self.client.collection_exists(collection_name):
            self.client.create_collection(
                collection_name,
                vectors_config=models.VectorParams(size=embeddings_size, distance=models.Distance.COSINE),
                **self.collection_options,
            )
        elif not get_or_create:
            raise ValueError(f"Collection {collection_name} already exists.")

    def get_collection(self, collection_name: str = None):
        """Get the collection from the vector database.

        Args:
            collection_name: str | The name of the collection.

        Returns:
            Any | The collection object.
        """
        if collection_name is None:
            raise ValueError("The collection name is required.")

        return self.client.get_collection(collection_name)

    def delete_collection(self, collection_name: str) -> None:
        """Delete the collection from the vector database.

        Args:
            collection_name: str | The name of the collection.

        Returns:
            Any
        """
        return self.client.delete_collection(collection_name)

    def insert_docs(self, docs: list[Document], collection_name: str = None, upsert: bool = False) -> None:
        """Insert documents into the collection of the vector database.

        Args:
            docs: List[Document] | A list of documents. Each document is a TypedDict `Document`.
            collection_name: str | The name of the collection. Default is None.
            upsert: bool | Whether to update the document if it exists. Default is False.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        if not docs:
            return
        if any(doc.get("content") is None for doc in docs):
            raise ValueError("The document content is required.")
        if any(doc.get("id") is None for doc in docs):
            raise ValueError("The document id is required.")

        if not upsert and not self._validate_upsert_ids(collection_name, [doc["id"] for doc in docs]):
            logger.log("Some IDs already exist. Skipping insert", level=logging.WARN)

        self.client.upsert(collection_name, points=self._documents_to_points(docs))

    def update_docs(self, docs: list[Document], collection_name: str = None) -> None:
        if not docs:
            return
        if any(doc.get("id") is None for doc in docs):
            raise ValueError("The document id is required.")
        if any(doc.get("content") is None for doc in docs):
            raise ValueError("The document content is required.")
        if self._validate_update_ids(collection_name, [doc["id"] for doc in docs]):
            return self.client.upsert(collection_name, points=self._documents_to_points(docs))

        raise ValueError("Some IDs do not exist. Skipping update")

    def delete_docs(self, ids: list[ItemID], collection_name: str = None, **kwargs) -> None:
        """Delete documents from the collection of the vector database.

        Args:
            ids: List[ItemID] | A list of document ids. Each id is a typed `ItemID`.
            collection_name: str | The name of the collection. Default is None.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        self.client.delete(collection_name, ids)

    def retrieve_docs(
        self,
        queries: list[str],
        collection_name: str = None,
        n_results: int = 10,
        distance_threshold: float = 0,
        **kwargs: Any,
    ) -> QueryResults:
        """Retrieve documents from the collection of the vector database based on the queries.

        Args:
            queries: List[str] | A list of queries. Each query is a string.
            collection_name: str | The name of the collection. Default is None.
            n_results: int | The number of relevant documents to return. Default is 10.
            distance_threshold: float | The threshold for the distance score, only distance smaller than it will be
                returned. Don't filter with it if `< 0`. Default is 0.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            QueryResults | The query results. Each query result is a list of list of tuples containing the document and
                the distance.
        """
        embeddings = self.embedding_function(queries)
        requests = [
            models.SearchRequest(
                vector=embedding,
                limit=n_results,
                score_threshold=distance_threshold,
                with_payload=True,
                with_vector=False,
            )
            for embedding in embeddings
        ]

        batch_results = self.client.search_batch(collection_name, requests)
        return [self._scored_points_to_documents(results) for results in batch_results]

    def get_docs_by_ids(
        self, ids: list[ItemID] = None, collection_name: str = None, include=True, **kwargs
    ) -> list[Document]:
        """Retrieve documents from the collection of the vector database based on the ids.

        Args:
            ids: List[ItemID] | A list of document ids. If None, will return all the documents. Default is None.
            collection_name: str | The name of the collection. Default is None.
            include: List[str] | The fields to include. Default is True.
                If None, will include ["metadatas", "documents"], ids will always be included.
            kwargs: dict | Additional keyword arguments.

        Returns:
            List[Document] | The results.
        """
        if ids is None:
            results = self.client.scroll(collection_name=collection_name, with_payload=include, with_vectors=True)[0]
        else:
            results = self.client.retrieve(collection_name, ids=ids, with_payload=include, with_vectors=True)
        return [self._point_to_document(result) for result in results]

    def _point_to_document(self, point) -> Document:
        return {
            "id": point.id,
            "content": point.payload.get(self.content_payload_key, ""),
            "metadata": point.payload.get(self.metadata_payload_key, {}),
            "embedding": point.vector,
        }

    def _points_to_documents(self, points) -> list[Document]:
        return [self._point_to_document(point) for point in points]

    def _scored_point_to_document(self, scored_point: "models.ScoredPoint") -> tuple[Document, float]:
        return self._point_to_document(scored_point), scored_point.score

    def _documents_to_points(self, documents: list[Document]):
        contents = [document["content"] for document in documents]
        embeddings = self.embedding_function(contents)
        points = [
            models.PointStruct(
                id=documents[i]["id"],
                vector=embeddings[i],
                payload={
                    self.content_payload_key: documents[i].get("content"),
                    self.metadata_payload_key: documents[i].get("metadata"),
                },
            )
            for i in range(len(documents))
        ]
        return points

    def _scored_points_to_documents(self, scored_points: list["models.ScoredPoint"]) -> list[tuple[Document, float]]:
        return [self._scored_point_to_document(scored_point) for scored_point in scored_points]

    def _validate_update_ids(self, collection_name: str, ids: list[str]) -> bool:
        """Validates all the IDs exist in the collection"""
        retrieved_ids = [
            point.id for point in self.client.retrieve(collection_name, ids=ids, with_payload=False, with_vectors=False)
        ]

        if missing_ids := set(ids) - set(retrieved_ids):
            logger.log(f"Missing IDs: {missing_ids}. Skipping update", level=logging.WARN)
            return False

        return True

    def _validate_upsert_ids(self, collection_name: str, ids: list[str]) -> bool:
        """Validate none of the IDs exist in the collection"""
        retrieved_ids = [
            point.id for point in self.client.retrieve(collection_name, ids=ids, with_payload=False, with_vectors=False)
        ]

        if existing_ids := set(ids) & set(retrieved_ids):
            logger.log(f"Existing IDs: {existing_ids}.", level=logging.WARN)
            return False

        return True
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import os
from collections.abc import Callable
from typing import Any

from ....import_utils import optional_import_block, require_optional_import
from .base import Document, ItemID, QueryResults, VectorDB
from .utils import chroma_results_to_query_results, filter_results_by_distance, get_logger

with optional_import_block() as result:
    import chromadb
    import chromadb.errors
    import chromadb.utils.embedding_functions as ef
    from chromadb.api.models.Collection import Collection

if result.is_successful and chromadb.__version__ < "0.4.15":
    raise ImportError("Please upgrade chromadb to version 0.4.15 or later.")


CHROMADB_MAX_BATCH_SIZE = os.environ.get("CHROMADB_MAX_BATCH_SIZE", 40000)
logger = get_logger(__name__)


@require_optional_import("chromadb", "retrievechat")
class ChromaVectorDB(VectorDB):
    """A vector database that uses ChromaDB as the backend."""

    def __init__(
        self, *, client=None, path: str = "tmp/db", embedding_function: Callable = None, metadata: dict = None, **kwargs
    ) -> None:
        """Initialize the vector database.

        Args:
            client: chromadb.Client | The client object of the vector database. Default is None.
                If provided, it will use the client object directly and ignore other arguments.
            path: str | The path to the vector database. Default is `tmp/db`. The default was `None` for version `<=0.2.24`.
            embedding_function: Callable | The embedding function used to generate the vector representation
                of the documents. Default is None, SentenceTransformerEmbeddingFunction("all-MiniLM-L6-v2") will be used.
            metadata: dict | The metadata of the vector database. Default is None. If None, it will use this
                setting: `{"hnsw:space": "ip", "hnsw:construction_ef": 30, "hnsw:M": 32}`. For more details of
                the metadata, please refer to [distances](https://github.com/nmslib/hnswlib#supported-distances),
                [hnsw](https://github.com/chroma-core/chroma/blob/566bc80f6c8ee29f7d99b6322654f32183c368c4/chromadb/segment/impl/vector/local_hnsw.py#L184),
                and [ALGO_PARAMS](https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md).
            kwargs: dict | Additional keyword arguments.

        Returns:
            None
        """
        self.client = client
        self.path = path
        self.embedding_function = (
            ef.SentenceTransformerEmbeddingFunction("all-MiniLM-L6-v2")
            if embedding_function is None
            else embedding_function
        )
        self.metadata = metadata if metadata else {"hnsw:space": "ip", "hnsw:construction_ef": 30, "hnsw:M": 32}
        if not self.client:
            if self.path is not None:
                self.client = chromadb.PersistentClient(path=self.path, **kwargs)
            else:
                self.client = chromadb.Client(**kwargs)
        self.active_collection = None
        self.type = "chroma"

    def create_collection(
        self, collection_name: str, overwrite: bool = False, get_or_create: bool = True
    ) -> "Collection":
        """Create a collection in the vector database.
        Case 1. if the collection does not exist, create the collection.
        Case 2. the collection exists, if overwrite is True, it will overwrite the collection.
        Case 3. the collection exists and overwrite is False, if get_or_create is True, it will get the collection,
            otherwise it raise a ValueError.

        Args:
            collection_name: str | The name of the collection.
            overwrite: bool | Whether to overwrite the collection if it exists. Default is False.
            get_or_create: bool | Whether to get the collection if it exists. Default is True.

        Returns:
            Collection | The collection object.
        """
        try:
            if self.active_collection and self.active_collection.name == collection_name:
                collection = self.active_collection
            else:
                collection = self.client.get_collection(collection_name, embedding_function=self.embedding_function)
        except (ValueError, chromadb.errors.ChromaError):
            collection = None
        if collection is None:
            return self.client.create_collection(
                collection_name,
                embedding_function=self.embedding_function,
                get_or_create=get_or_create,
                metadata=self.metadata,
            )
        elif overwrite:
            self.client.delete_collection(collection_name)
            return self.client.create_collection(
                collection_name,
                embedding_function=self.embedding_function,
                get_or_create=get_or_create,
                metadata=self.metadata,
            )
        elif get_or_create:
            return collection
        else:
            raise ValueError(f"Collection {collection_name} already exists.")

    def get_collection(self, collection_name: str = None) -> "Collection":
        """Get the collection from the vector database.

        Args:
            collection_name: str | The name of the collection. Default is None. If None, return the
                current active collection.

        Returns:
            Collection | The collection object.
        """
        if collection_name is None:
            if self.active_collection is None:
                raise ValueError("No collection is specified.")
            else:
                logger.info(
                    f"No collection is specified. Using current active collection {self.active_collection.name}."
                )
        else:
            if not (self.active_collection and self.active_collection.name == collection_name):
                self.active_collection = self.client.get_collection(
                    name=collection_name, embedding_function=self.embedding_function
                )
        return self.active_collection

    def delete_collection(self, collection_name: str) -> None:
        """Delete the collection from the vector database.

        Args:
            collection_name: str | The name of the collection.

        Returns:
            None
        """
        self.client.delete_collection(collection_name)
        if self.active_collection and self.active_collection.name == collection_name:
            self.active_collection = None

    def _batch_insert(
        self, collection: "Collection", embeddings=None, ids=None, metadatas=None, documents=None, upsert=False
    ) -> None:
        batch_size = int(CHROMADB_MAX_BATCH_SIZE)
        for i in range(0, len(documents), min(batch_size, len(documents))):
            end_idx = i + min(batch_size, len(documents) - i)
            collection_kwargs = {
                "documents": documents[i:end_idx],
                "ids": ids[i:end_idx],
                "metadatas": metadatas[i:end_idx] if metadatas else None,
                "embeddings": embeddings[i:end_idx] if embeddings else None,
            }
            if upsert:
                collection.upsert(**collection_kwargs)
            else:
                collection.add(**collection_kwargs)

    def insert_docs(self, docs: list[Document], collection_name: str = None, upsert: bool = False) -> None:
        """Insert documents into the collection of the vector database.

        Args:
            docs: List[Document] | A list of documents. Each document is a TypedDict `Document`.
            collection_name: str | The name of the collection. Default is None.
            upsert: bool | Whether to update the document if it exists. Default is False.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        if not docs:
            return
        if docs[0].get("content") is None:
            raise ValueError("The document content is required.")
        if docs[0].get("id") is None:
            raise ValueError("The document id is required.")
        documents = [doc.get("content") for doc in docs]
        ids = [doc.get("id") for doc in docs]
        collection = self.get_collection(collection_name)
        if docs[0].get("embedding") is None:
            logger.info(
                "No content embedding is provided. Will use the VectorDB's embedding function to generate the content embedding."
            )
            embeddings = None
        else:
            embeddings = [doc.get("embedding") for doc in docs]
        metadatas = None if docs[0].get("metadata") is None else [doc.get("metadata") for doc in docs]
        self._batch_insert(collection, embeddings, ids, metadatas, documents, upsert)

    def update_docs(self, docs: list[Document], collection_name: str = None) -> None:
        """Update documents in the collection of the vector database.

        Args:
            docs: List[Document] | A list of documents.
            collection_name: str | The name of the collection. Default is None.

        Returns:
            None
        """
        self.insert_docs(docs, collection_name, upsert=True)

    def delete_docs(self, ids: list[ItemID], collection_name: str = None, **kwargs) -> None:
        """Delete documents from the collection of the vector database.

        Args:
            ids: List[ItemID] | A list of document ids. Each id is a typed `ItemID`.
            collection_name: str | The name of the collection. Default is None.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        collection = self.get_collection(collection_name)
        collection.delete(ids, **kwargs)

    def retrieve_docs(
        self,
        queries: list[str],
        collection_name: str = None,
        n_results: int = 10,
        distance_threshold: float = -1,
        **kwargs: Any,
    ) -> QueryResults:
        """Retrieve documents from the collection of the vector database based on the queries.

        Args:
            queries: List[str] | A list of queries. Each query is a string.
            collection_name: str | The name of the collection. Default is None.
            n_results: int | The number of relevant documents to return. Default is 10.
            distance_threshold: float | The threshold for the distance score, only distance smaller than it will be
                returned. Don't filter with it if `< 0`. Default is -1.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            QueryResults | The query results. Each query result is a list of list of tuples containing the document and
                the distance.
        """
        collection = self.get_collection(collection_name)
        if isinstance(queries, str):
            queries = [queries]
        results = collection.query(
            query_texts=queries,
            n_results=n_results,
            **kwargs,
        )
        results["contents"] = results.pop("documents")
        results = chroma_results_to_query_results(results)
        results = filter_results_by_distance(results, distance_threshold)
        return results

    @staticmethod
    def _chroma_get_results_to_list_documents(data_dict) -> list[Document]:
        """Converts a dictionary with list values to a list of Document.

        Args:
            data_dict: A dictionary where keys map to lists or None.

        Returns:
            List[Document] | The list of Document.

        Example:
            ```python
            data_dict = {
                "key1s": [1, 2, 3],
                "key2s": ["a", "b", "c"],
                "key3s": None,
                "key4s": ["x", "y", "z"],
            }

            results = [
                {"key1": 1, "key2": "a", "key4": "x"},
                {"key1": 2, "key2": "b", "key4": "y"},
                {"key1": 3, "key2": "c", "key4": "z"},
            ]
            ```
        """
        results = []
        keys = [key for key in data_dict if data_dict[key] is not None]

        for i in range(len(data_dict[keys[0]])):
            sub_dict = {}
            for key in data_dict:
                if data_dict[key] is not None and len(data_dict[key]) > i:
                    sub_dict[key[:-1]] = data_dict[key][i]
            results.append(sub_dict)
        return results

    def get_docs_by_ids(
        self, ids: list[ItemID] = None, collection_name: str = None, include=None, **kwargs
    ) -> list[Document]:
        """Retrieve documents from the collection of the vector database based on the ids.

        Args:
            ids: List[ItemID] | A list of document ids. If None, will return all the documents. Default is None.
            collection_name: str | The name of the collection. Default is None.
            include: List[str] | The fields to include. Default is None.
                If None, will include ["metadatas", "documents"], ids will always be included.
            kwargs: dict | Additional keyword arguments.

        Returns:
            List[Document] | The results.
        """
        collection = self.get_collection(collection_name)
        include = include if include else ["metadatas", "documents"]
        results = collection.get(ids, include=include, **kwargs)
        results = self._chroma_get_results_to_list_documents(results)
        return results
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from collections.abc import Callable, Iterable, Mapping
from copy import deepcopy
from time import monotonic, sleep
from typing import Any, Literal

from ....import_utils import optional_import_block, require_optional_import
from .base import Document, ItemID, QueryResults, VectorDB
from .utils import get_logger

with optional_import_block():
    import numpy as np
    from pymongo import MongoClient, UpdateOne, errors
    from pymongo.collection import Collection
    from pymongo.driver_info import DriverInfo
    from pymongo.operations import SearchIndexModel
    from sentence_transformers import SentenceTransformer

logger = get_logger(__name__)

DEFAULT_INSERT_BATCH_SIZE = 100_000
_SAMPLE_SENTENCE = ["The weather is lovely today in paradise."]
_DELAY = 0.5


def with_id_rename(docs: Iterable) -> list[dict[str, Any]]:
    """Utility changes _id field from Collection into id for Document."""
    return [{**{k: v for k, v in d.items() if k != "_id"}, "id": d["_id"]} for d in docs]


@require_optional_import(["pymongo", "sentence_transformers", "numpy"], "retrievechat-mongodb")
class MongoDBAtlasVectorDB(VectorDB):
    """A Collection object for MongoDB."""

    def __init__(
        self,
        connection_string: str = "",
        database_name: str = "vector_db",
        embedding_function: Callable[..., Any] | None = None,
        collection_name: str = None,
        index_name: str = "vector_index",
        overwrite: bool = False,
        wait_until_index_ready: float | None = None,
        wait_until_document_ready: float | None = None,
    ):
        """Initialize the vector database.

        Args:
            connection_string: str | The MongoDB connection string to connect to. Default is ''.
            database_name: str | The name of the database. Default is 'vector_db'.
            embedding_function: Callable | The embedding function used to generate the vector representation.
            collection_name: str | The name of the collection to create for this vector database
                Defaults to None
            index_name: str | Index name for the vector database, defaults to 'vector_index'
            overwrite: bool = False
            wait_until_index_ready: Optional[float] | Blocking call to wait until the
                database indexes are ready. None, the default, means no wait.
            wait_until_document_ready: Optional[float] | Blocking call to wait until the
                database indexes are ready. None, the default, means no wait.
        """
        self.embedding_function = embedding_function or SentenceTransformer("all-MiniLM-L6-v2").encode
        self.index_name = index_name
        self._wait_until_index_ready = wait_until_index_ready
        self._wait_until_document_ready = wait_until_document_ready

        # This will get the model dimension size by computing the embeddings dimensions
        self.dimensions = self._get_embedding_size()

        try:
            self.client = MongoClient(connection_string, driver=DriverInfo(name="autogen"))
            self.client.admin.command("ping")
            logger.debug("Successfully created MongoClient")
        except errors.ServerSelectionTimeoutError as err:
            raise ConnectionError("Could not connect to MongoDB server") from err

        self.db = self.client[database_name]
        logger.debug(f"Atlas Database name: {self.db.name}")
        if collection_name:
            self.active_collection = self.create_collection(collection_name, overwrite)
        else:
            self.active_collection = None

    def _is_index_ready(self, collection: "Collection", index_name: str):
        """Check for the index name in the list of available search indexes to see if the
        specified index is of status READY

        Args:
            collection (Collection): MongoDB Collection to for the search indexes
            index_name (str): Vector Search Index name

        Returns:
            bool : True if the index is present and READY false otherwise
        """
        for index in collection.list_search_indexes(index_name):
            if index["type"] == "vectorSearch" and index["status"] == "READY":
                return True
        return False

    def _wait_for_index(self, collection: "Collection", index_name: str, action: str = "create"):
        """Waits for the index action to be completed. Otherwise throws a TimeoutError.

        Timeout set on instantiation.
        action: "create" or "delete"
        """
        assert action in ["create", "delete"], f"{action=} must be create or delete."
        start = monotonic()
        while monotonic() - start < self._wait_until_index_ready:
            if (action == "create" and self._is_index_ready(collection, index_name)) or (
                action == "delete" and len(list(collection.list_search_indexes())) == 0
            ):
                return
            sleep(_DELAY)

        raise TimeoutError(f"Index {self.index_name} is not ready!")

    def _wait_for_document(self, collection: "Collection", index_name: str, doc: Document):
        start = monotonic()
        while monotonic() - start < self._wait_until_document_ready:
            query_result = _vector_search(
                embedding_vector=np.array(self.embedding_function(doc["content"])).tolist(),
                n_results=1,
                collection=collection,
                index_name=index_name,
            )
            if query_result and query_result[0][0]["_id"] == doc["id"]:
                return
            sleep(_DELAY)

        raise TimeoutError(f"Document {self.index_name} is not ready!")

    def _get_embedding_size(self):
        return len(self.embedding_function(_SAMPLE_SENTENCE)[0])

    def list_collections(self):
        """List the collections in the vector database.

        Returns:
            List[str] | The list of collections.
        """
        return self.db.list_collection_names()

    def create_collection(
        self,
        collection_name: str,
        overwrite: bool = False,
        get_or_create: bool = True,
    ) -> "Collection":
        """Create a collection in the vector database and create a vector search index in the collection.

        Args:
            collection_name: str | The name of the collection.
            overwrite: bool | Whether to overwrite the collection if it exists. Default is False.
            get_or_create: bool | Whether to get or create the collection. Default is True
        """
        if overwrite:
            self.delete_collection(collection_name)

        if collection_name not in self.db.list_collection_names():
            # Create a new collection
            coll = self.db.create_collection(collection_name)
            self.create_index_if_not_exists(index_name=self.index_name, collection=coll)
            return coll

        if get_or_create:
            # The collection already exists, return it.
            coll = self.db[collection_name]
            self.create_index_if_not_exists(index_name=self.index_name, collection=coll)
            return coll
        else:
            # get_or_create is False and the collection already exists, raise an error.
            raise ValueError(f"Collection {collection_name} already exists.")

    def create_index_if_not_exists(self, index_name: str = "vector_index", collection: "Collection" = None) -> None:
        """Creates a vector search index on the specified collection in MongoDB.

        Args:
            index_name (str, optional): The name of the vector search index to create. Defaults to "vector_search_index".
            collection (Collection, optional): The MongoDB collection to create the index on. Defaults to None.
        """
        if not self._is_index_ready(collection, index_name):
            self.create_vector_search_index(collection, index_name)

    def get_collection(self, collection_name: str = None) -> "Collection":
        """Get the collection from the vector database.

        Args:
            collection_name: str | The name of the collection. Default is None. If None, return the
                current active collection.

        Returns:
            Collection | The collection object.
        """
        if collection_name is None:
            if self.active_collection is None:
                raise ValueError("No collection is specified.")
            else:
                logger.debug(
                    f"No collection is specified. Using current active collection {self.active_collection.name}."
                )
        else:
            self.active_collection = self.db[collection_name]

        return self.active_collection

    def delete_collection(self, collection_name: str) -> None:
        """Delete the collection from the vector database.

        Args:
            collection_name: str | The name of the collection.
        """
        for index in self.db[collection_name].list_search_indexes():
            self.db[collection_name].drop_search_index(index["name"])
            if self._wait_until_index_ready:
                self._wait_for_index(self.db[collection_name], index["name"], "delete")
        return self.db[collection_name].drop()

    def create_vector_search_index(
        self,
        collection: "Collection",
        index_name: str | None = "vector_index",
        similarity: Literal["euclidean", "cosine", "dotProduct"] = "cosine",
    ) -> None:
        """Create a vector search index in the collection.

        Args:
            collection: An existing Collection in the Atlas Database.
            index_name: Vector Search Index name.
            similarity: Algorithm used for measuring vector similarity.
            kwargs: Additional keyword arguments.

        Returns:
            None
        """
        search_index_model = SearchIndexModel(
            definition={
                "fields": [
                    {
                        "type": "vector",
                        "numDimensions": self.dimensions,
                        "path": "embedding",
                        "similarity": similarity,
                    },
                ]
            },
            name=index_name,
            type="vectorSearch",
        )
        # Create the search index
        try:
            collection.create_search_index(model=search_index_model)
            if self._wait_until_index_ready:
                self._wait_for_index(collection, index_name, "create")
            logger.debug(f"Search index {index_name} created successfully.")
        except Exception as e:
            logger.error(
                f"Error creating search index: {e}. \n"
                f"Your client must be connected to an Atlas cluster. "
                f"You may have to manually create a Collection and Search Index "
                f"if you are on a free/shared cluster."
            )
            raise e

    def insert_docs(
        self,
        docs: list[Document],
        collection_name: str = None,
        upsert: bool = False,
        batch_size: int = DEFAULT_INSERT_BATCH_SIZE,
        **kwargs: Any,
    ) -> None:
        """Insert Documents and Vector Embeddings into the collection of the vector database.

        For large numbers of Documents, insertion is performed in batches.

        Args:
            docs: A list of documents. Each document is a TypedDict `Document`.
            collection_name: The name of the collection. Default is None.
            upsert: Whether to update the document if it exists. Default is False.
            batch_size: Number of documents to be inserted in each batch
            **kwargs: Additional keyword arguments.
        """
        if not docs:
            logger.info("No documents to insert.")
            return

        collection = self.get_collection(collection_name)
        if upsert:
            self.update_docs(docs, collection.name, upsert=True)
        else:
            # Sanity checking the first document
            if docs[0].get("content") is None:
                raise ValueError("The document content is required.")
            if docs[0].get("id") is None:
                raise ValueError("The document id is required.")

            input_ids = set()
            result_ids = set()
            id_batch = []
            text_batch = []
            metadata_batch = []
            size = 0
            i = 0
            for doc in docs:
                id = doc["id"]
                text = doc["content"]
                metadata = doc.get("metadata", {})
                id_batch.append(id)
                text_batch.append(text)
                metadata_batch.append(metadata)
                id_size = 1 if isinstance(id, int) else len(id)
                size += len(text) + len(metadata) + id_size
                if (i + 1) % batch_size == 0 or size >= 47_000_000:
                    result_ids.update(self._insert_batch(collection, text_batch, metadata_batch, id_batch))
                    input_ids.update(id_batch)
                    id_batch = []
                    text_batch = []
                    metadata_batch = []
                    size = 0
                i += 1  # noqa: SIM113
            if text_batch:
                result_ids.update(self._insert_batch(collection, text_batch, metadata_batch, id_batch))
                input_ids.update(id_batch)

            if result_ids != input_ids:
                logger.warning(
                    "Possible data corruption. "
                    f"input_ids not in result_ids: {input_ids.difference(result_ids)}.\n"
                    f"result_ids not in input_ids: {result_ids.difference(input_ids)}"
                )
            if self._wait_until_document_ready and docs:
                self._wait_for_document(collection, self.index_name, docs[-1])

    def _insert_batch(
        self, collection: "Collection", texts: list[str], metadatas: list[Mapping[str, Any]], ids: list[ItemID]
    ) -> set[ItemID]:
        """Compute embeddings for and insert a batch of Documents into the Collection.

        For performance reasons, we chose to call self.embedding_function just once,
        with the hopefully small tradeoff of having recreating Document dicts.

        Args:
            collection: MongoDB Collection
            texts: List of the main contents of each document
            metadatas: List of metadata mappings
            ids: List of ids. Note that these are stored as _id in Collection.

        Returns:
            List of ids inserted.
        """
        n_texts = len(texts)
        if n_texts == 0:
            return []
        # Embed and create the documents
        embeddings = self.embedding_function(texts).tolist()
        assert len(embeddings) == n_texts, (
            f"The number of embeddings produced by self.embedding_function ({len(embeddings)} does not match the number of texts provided to it ({n_texts})."
        )
        to_insert = [
            {"_id": i, "content": t, "metadata": m, "embedding": e}
            for i, t, m, e in zip(ids, texts, metadatas, embeddings)
        ]
        # insert the documents in MongoDB Atlas
        insert_result = collection.insert_many(to_insert)  # type: ignore[union-attr]
        return insert_result.inserted_ids  # TODO Remove this. Replace by log like update_docs

    def update_docs(self, docs: list[Document], collection_name: str = None, **kwargs: Any) -> None:
        """Update documents, including their embeddings, in the Collection.

        Optionally allow upsert as kwarg.

        Uses deepcopy to avoid changing docs.

        Args:
            docs: List[Document] | A list of documents.
            collection_name: str | The name of the collection. Default is None.
            kwargs: Any | Use upsert=True` to insert documents whose ids are not present in collection.
        """
        n_docs = len(docs)
        logger.info(f"Preparing to embed and update {n_docs=}")
        # Compute the embeddings
        embeddings: list[list[float]] = self.embedding_function([doc["content"] for doc in docs]).tolist()
        # Prepare the updates
        all_updates = []
        for i in range(n_docs):
            doc = deepcopy(docs[i])
            doc["embedding"] = embeddings[i]
            doc["_id"] = doc.pop("id")

            all_updates.append(UpdateOne({"_id": doc["_id"]}, {"$set": doc}, upsert=kwargs.get("upsert", False)))
        # Perform update in bulk
        collection = self.get_collection(collection_name)
        result = collection.bulk_write(all_updates)

        if self._wait_until_document_ready and docs:
            self._wait_for_document(collection, self.index_name, docs[-1])

        # Log a result summary
        logger.info(
            "Matched: %s, Modified: %s, Upserted: %s",
            result.matched_count,
            result.modified_count,
            result.upserted_count,
        )

    def delete_docs(self, ids: list[ItemID], collection_name: str = None, **kwargs):
        """Delete documents from the collection of the vector database.

        Args:
            ids: A list of document ids. Each id is a typed `ItemID`.
            collection_name: The name of the collection. Default is None.
            **kwargs: Additional keyword arguments.
        """
        collection = self.get_collection(collection_name)
        return collection.delete_many({"_id": {"$in": ids}})

    def get_docs_by_ids(
        self, ids: list[ItemID] = None, collection_name: str = None, include: list[str] = None, **kwargs
    ) -> list[Document]:
        """Retrieve documents from the collection of the vector database based on the ids.

        Args:
            ids: List[ItemID] | A list of document ids. If None, will return all the documents. Default is None.
            collection_name: str | The name of the collection. Default is None.
            include: List[str] | The fields to include.
                If None, will include ["metadata", "content"], ids will always be included.
                Basically, use include to choose whether to include embedding and metadata
            kwargs: dict | Additional keyword arguments.

        Returns:
            List[Document] | The results.
        """
        if include is None:
            include_fields = {"_id": 1, "content": 1, "metadata": 1}
        else:
            include_fields = dict.fromkeys(set(include).union({"_id"}), 1)
        collection = self.get_collection(collection_name)
        if ids is not None:
            docs = collection.find({"_id": {"$in": ids}}, include_fields)
            # Return with _id field from Collection into id for Document
            return with_id_rename(docs)
        else:
            docs = collection.find({}, include_fields)
            # Return with _id field from Collection into id for Document
            return with_id_rename(docs)

    def retrieve_docs(
        self,
        queries: list[str],
        collection_name: str = None,
        n_results: int = 10,
        distance_threshold: float = -1,
        **kwargs: Any,
    ) -> QueryResults:
        """Retrieve documents from the collection of the vector database based on the queries.

        Args:
            queries: List[str] | A list of queries. Each query is a string.
            collection_name: str | The name of the collection. Default is None.
            n_results: int | The number of relevant documents to return. Default is 10.
            distance_threshold: float | The threshold for the distance score, only distance smaller than it will be
                returned. Don't filter with it if < 0. Default is -1.
            kwargs: Dict | Additional keyword arguments. Ones of importance follow:
                oversampling_factor: int | This times n_results is 'ef' in the HNSW algorithm.
                It determines the number of nearest neighbor candidates to consider during the search phase.
                A higher value leads to more accuracy, but is slower. Default is 10

        Returns:
            QueryResults | For each query string, a list of nearest documents and their scores.
        """
        collection = self.get_collection(collection_name)
        # Trivial case of an empty collection
        if collection.count_documents({}) == 0:
            return []

        logger.debug(f"Using index: {self.index_name}")
        results = []
        for query_text in queries:
            # Compute embedding vector from semantic query
            logger.debug(f"Query: {query_text}")
            query_vector = np.array(self.embedding_function([query_text])).tolist()[0]
            # Find documents with similar vectors using the specified index
            query_result = _vector_search(
                query_vector,
                n_results,
                collection,
                self.index_name,
                distance_threshold,
                **kwargs,
                oversampling_factor=kwargs.get("oversampling_factor", 10),
            )
            # Change each _id key to id. with_id_rename, but with (doc, score) tuples
            results.append([
                ({**{k: v for k, v in d[0].items() if k != "_id"}, "id": d[0]["_id"]}, d[1]) for d in query_result
            ])
        return results


def _vector_search(
    embedding_vector: list[float],
    n_results: int,
    collection: "Collection",
    index_name: str,
    distance_threshold: float = -1.0,
    oversampling_factor=10,
    include_embedding=False,
) -> list[tuple[dict[str, Any], float]]:
    """Core $vectorSearch Aggregation pipeline.

    Args:
        embedding_vector: Embedding vector of semantic query
        n_results: Number of documents to return. Defaults to 4.
        collection: MongoDB Collection with vector index
        index_name: Name of the vector index
        distance_threshold: Only distance measures smaller than this will be returned.
            Don't filter with it if 1 < x < 0. Default is -1.
        oversampling_factor: This times n_results is 'ef' in the HNSW algorithm.
            It determines the number of nearest neighbor candidates to consider during the search phase.
            A higher value leads to more accuracy, but is slower. Default = 10
        include_embedding: Whether to include the embedding in the results. Default is False.

    Returns:
        List of tuples of length n_results from Collection.
        Each tuple contains a document dict and a score.
    """
    pipeline = [
        {
            "$vectorSearch": {
                "index": index_name,
                "limit": n_results,
                "numCandidates": n_results * oversampling_factor,
                "queryVector": embedding_vector,
                "path": "embedding",
            }
        },
        {"$set": {"score": {"$meta": "vectorSearchScore"}}},
    ]
    if distance_threshold >= 0.0:
        similarity_threshold = 1.0 - distance_threshold
        pipeline.append({"$match": {"score": {"$gte": similarity_threshold}}})

    if not include_embedding:
        pipeline.append({"$project": {"embedding": 0}})

    logger.debug("pipeline: %s", pipeline)
    agg = collection.aggregate(pipeline)
    return [(doc, doc.pop("score")) for doc in agg]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

__all__: list[str] = []
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import logging
from typing import Any

from termcolor import colored

from .base import QueryResults


class ColoredLogger(logging.Logger):
    def __init__(self, name, level=logging.NOTSET):
        super().__init__(name, level)

    def debug(self, msg, *args, color=None, **kwargs):
        super().debug(colored(msg, color), *args, **kwargs)

    def info(self, msg, *args, color=None, **kwargs):
        super().info(colored(msg, color), *args, **kwargs)

    def warning(self, msg, *args, color="yellow", **kwargs):
        super().warning(colored(msg, color), *args, **kwargs)

    def error(self, msg, *args, color="light_red", **kwargs):
        super().error(colored(msg, color), *args, **kwargs)

    def critical(self, msg, *args, color="red", **kwargs):
        super().critical(colored(msg, color), *args, **kwargs)

    def fatal(self, msg, *args, color="red", **kwargs):
        super().fatal(colored(msg, color), *args, **kwargs)


def get_logger(name: str, level: int = logging.INFO) -> ColoredLogger:
    logger = ColoredLogger(name, level)
    console_handler = logging.StreamHandler()
    logger.addHandler(console_handler)
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    logger.handlers[0].setFormatter(formatter)
    return logger


logger = get_logger(__name__)


def filter_results_by_distance(results: QueryResults, distance_threshold: float = -1) -> QueryResults:
    """Filters results based on a distance threshold.

    Args:
        results: QueryResults | The query results. List[List[Tuple[Document, float]]]
        distance_threshold: The maximum distance allowed for results.

    Returns:
        QueryResults | A filtered results containing only distances smaller than the threshold.
    """
    if distance_threshold > 0:
        results = [[(key, value) for key, value in data if value < distance_threshold] for data in results]

    return results


def chroma_results_to_query_results(data_dict: dict[str, list[list[Any]]], special_key="distances") -> QueryResults:
    """Converts a dictionary with list-of-list values to a list of tuples.

    Args:
        data_dict: A dictionary where keys map to lists of lists or None.
        special_key: The key in the dictionary containing the special values
                    for each tuple.

    Returns:
        A list of tuples, where each tuple contains a sub-dictionary with
        some keys from the original dictionary and the value from the
        special_key.

    Example:
        ```python
        data_dict = {
            "key1s": [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
            "key2s": [["a", "b", "c"], ["c", "d", "e"], ["e", "f", "g"]],
            "key3s": None,
            "key4s": [["x", "y", "z"], ["1", "2", "3"], ["4", "5", "6"]],
            "distances": [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]],
        }

        results = [
            [
                ({"key1": 1, "key2": "a", "key4": "x"}, 0.1),
                ({"key1": 2, "key2": "b", "key4": "y"}, 0.2),
                ({"key1": 3, "key2": "c", "key4": "z"}, 0.3),
            ],
            [
                ({"key1": 4, "key2": "c", "key4": "1"}, 0.4),
                ({"key1": 5, "key2": "d", "key4": "2"}, 0.5),
                ({"key1": 6, "key2": "e", "key4": "3"}, 0.6),
            ],
            [
                ({"key1": 7, "key2": "e", "key4": "4"}, 0.7),
                ({"key1": 8, "key2": "f", "key4": "5"}, 0.8),
                ({"key1": 9, "key2": "g", "key4": "6"}, 0.9),
            ],
        ]
        ```
    """
    keys = [
        key
        for key in data_dict
        if key != special_key and data_dict[key] is not None and isinstance(data_dict[key][0], list)
    ]
    result = []
    data_special_key = data_dict[special_key]

    for i in range(len(data_special_key)):
        sub_result = []
        for j, distance in enumerate(data_special_key[i]):
            sub_dict = {}
            for key in keys:
                if len(data_dict[key]) > i:
                    sub_dict[key[:-1]] = data_dict[key][i][j]  # remove 's' in the end from key
            sub_result.append((sub_dict, distance))
        result.append(sub_result)

    return result
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from collections.abc import Callable, Mapping, Sequence
from typing import Any, Protocol, TypedDict, runtime_checkable

Metadata = Mapping[str, Any] | None
Vector = Sequence[float] | Sequence[int]
ItemID = str | int  # chromadb doesn't support int ids, VikingDB does


class Document(TypedDict):
    """A Document is a record in the vector database.

    id: ItemID | the unique identifier of the document.
    content: str | the text content of the chunk.
    metadata: Metadata, Optional | contains additional information about the document such as source, date, etc.
    embedding: Vector, Optional | the vector representation of the content.
    """

    id: ItemID
    content: str
    metadata: Metadata | None
    embedding: Vector | None


"""QueryResults is the response from the vector database for a query/queries.
A query is a list containing one string while queries is a list containing multiple strings.
The response is a list of query results, each query result is a list of tuples containing the document and the distance.
"""
QueryResults = list[list[tuple[Document, float]]]


@runtime_checkable
class VectorDB(Protocol):
    """Abstract class for vector database. A vector database is responsible for storing and retrieving documents.

    Attributes:
        active_collection: Any | The active collection in the vector database. Make get_collection faster. Default is None.
        type: str | The type of the vector database, chroma, pgvector, etc. Default is "".

    Methods:
        create_collection: Callable[[str, bool, bool], Any] | Create a collection in the vector database.
        get_collection: Callable[[str], Any] | Get the collection from the vector database.
        delete_collection: Callable[[str], Any] | Delete the collection from the vector database.
        insert_docs: Callable[[List[Document], str, bool], None] | Insert documents into the collection of the vector database.
        update_docs: Callable[[List[Document], str], None] | Update documents in the collection of the vector database.
        delete_docs: Callable[[List[ItemID], str], None] | Delete documents from the collection of the vector database.
        retrieve_docs: Callable[[List[str], str, int, float], QueryResults] | Retrieve documents from the collection of the vector database based on the queries.
        get_docs_by_ids: Callable[[List[ItemID], str], List[Document]] | Retrieve documents from the collection of the vector database based on the ids.
    """

    active_collection: Any = None
    type: str = ""
    embedding_function: Callable[[list[str]], list[list[float]]] | None = (
        None  # embeddings = embedding_function(sentences)
    )

    def create_collection(self, collection_name: str, overwrite: bool = False, get_or_create: bool = True) -> Any:
        """Create a collection in the vector database.
        Case 1. if the collection does not exist, create the collection.
        Case 2. the collection exists, if overwrite is True, it will overwrite the collection.
        Case 3. the collection exists and overwrite is False, if get_or_create is True, it will get the collection,
            otherwise it raise a ValueError.

        Args:
            collection_name: str | The name of the collection.
            overwrite: bool | Whether to overwrite the collection if it exists. Default is False.
            get_or_create: bool | Whether to get the collection if it exists. Default is True.

        Returns:
            Any | The collection object.
        """
        ...

    def get_collection(self, collection_name: str = None) -> Any:
        """Get the collection from the vector database.

        Args:
            collection_name: str | The name of the collection. Default is None. If None, return the
                current active collection.

        Returns:
            Any | The collection object.
        """
        ...

    def delete_collection(self, collection_name: str) -> Any:
        """Delete the collection from the vector database.

        Args:
            collection_name: str | The name of the collection.

        Returns:
            Any
        """
        ...

    def insert_docs(self, docs: list[Document], collection_name: str = None, upsert: bool = False, **kwargs) -> None:
        """Insert documents into the collection of the vector database.

        Args:
            docs: List[Document] | A list of documents. Each document is a TypedDict `Document`.
            collection_name: str | The name of the collection. Default is None.
            upsert: bool | Whether to update the document if it exists. Default is False.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        ...

    def update_docs(self, docs: list[Document], collection_name: str = None, **kwargs) -> None:
        """Update documents in the collection of the vector database.

        Args:
            docs: List[Document] | A list of documents.
            collection_name: str | The name of the collection. Default is None.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        ...

    def delete_docs(self, ids: list[ItemID], collection_name: str = None, **kwargs) -> None:
        """Delete documents from the collection of the vector database.

        Args:
            ids: List[ItemID] | A list of document ids. Each id is a typed `ItemID`.
            collection_name: str | The name of the collection. Default is None.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            None
        """
        ...

    def retrieve_docs(
        self,
        queries: list[str],
        collection_name: str = None,
        n_results: int = 10,
        distance_threshold: float = -1,
        **kwargs: Any,
    ) -> QueryResults:
        """Retrieve documents from the collection of the vector database based on the queries.

        Args:
            queries: List[str] | A list of queries. Each query is a string.
            collection_name: str | The name of the collection. Default is None.
            n_results: int | The number of relevant documents to return. Default is 10.
            distance_threshold: float | The threshold for the distance score, only distance smaller than it will be
                returned. Don't filter with it if < 0. Default is -1.
            kwargs: Dict | Additional keyword arguments.

        Returns:
            QueryResults | The query results. Each query result is a list of list of tuples containing the document and
                the distance.
        """
        ...

    def get_docs_by_ids(
        self, ids: list[ItemID] = None, collection_name: str = None, include: list[str] | None = None, **kwargs: Any
    ) -> list[Document]:
        """Retrieve documents from the collection of the vector database based on the ids.

        Args:
            ids: List[ItemID] | A list of document ids. If None, will return all the documents. Default is None.
            collection_name: str | The name of the collection. Default is None.
            include: List[str] | The fields to include. Default is None.
                If None, will include ["metadatas", "documents"], ids will always be included. This may differ
                depending on the implementation.
            kwargs: dict | Additional keyword arguments.

        Returns:
            List[Document] | The results.
        """
        ...


class VectorDBFactory:
    """Factory class for creating vector databases."""

    PREDEFINED_VECTOR_DB = ["chroma", "pgvector", "mongodb", "qdrant", "couchbase"]

    @staticmethod
    def create_vector_db(db_type: str, **kwargs) -> VectorDB:
        """Create a vector database.

        Args:
            db_type: str | The type of the vector database.
            kwargs: Dict | The keyword arguments for initializing the vector database.

        Returns:
            VectorDB | The vector database.
        """
        if db_type.lower() in ["chroma", "chromadb"]:
            from .chromadb import ChromaVectorDB

            return ChromaVectorDB(**kwargs)
        if db_type.lower() in ["pgvector", "pgvectordb"]:
            from .pgvectordb import PGVectorDB

            return PGVectorDB(**kwargs)
        if db_type.lower() in ["mdb", "mongodb", "atlas"]:
            from .mongodb import MongoDBAtlasVectorDB

            return MongoDBAtlasVectorDB(**kwargs)
        if db_type.lower() in ["qdrant", "qdrantdb"]:
            from .qdrant import QdrantVectorDB

            return QdrantVectorDB(**kwargs)
        if db_type.lower() in ["couchbase", "couchbasedb", "capella"]:
            from .couchbase import CouchbaseVectorDB

            return CouchbaseVectorDB(**kwargs)
        else:
            raise ValueError(
                f"Unsupported vector database type: {db_type}. Valid types are {VectorDBFactory.PREDEFINED_VECTOR_DB}."
            )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

import json
import time
from collections.abc import Callable
from datetime import timedelta
from typing import Any, Literal, Optional

from ....import_utils import optional_import_block, require_optional_import
from .base import Document, ItemID, QueryResults, VectorDB
from .utils import get_logger

with optional_import_block():
    import numpy as np
    from couchbase import search
    from couchbase.auth import PasswordAuthenticator
    from couchbase.cluster import Cluster, ClusterOptions
    from couchbase.collection import Collection
    from couchbase.management.search import SearchIndex
    from couchbase.options import SearchOptions
    from couchbase.vector_search import VectorQuery, VectorSearch
    from sentence_transformers import SentenceTransformer


logger = get_logger(__name__)

DEFAULT_BATCH_SIZE = 1000
_SAMPLE_SENTENCE = ["The weather is lovely today in paradise."]
TEXT_KEY = "content"
EMBEDDING_KEY = "embedding"


@require_optional_import(["couchbase", "sentence_transformers"], "retrievechat-couchbase")
class CouchbaseVectorDB(VectorDB):
    """A vector database implementation that uses Couchbase as the backend."""

    def __init__(
        self,
        connection_string: str = "couchbase://localhost",
        username: str = "Administrator",
        password: str = "password",
        bucket_name: str = "vector_db",
        embedding_function: Callable = None,
        scope_name: str = "_default",
        collection_name: str = "_default",
        index_name: str = None,
    ):
        """Initialize the vector database.

        Args:
            connection_string (str): The Couchbase connection string to connect to. Default is 'couchbase://localhost'.
            username (str): The username for Couchbase authentication. Default is 'Administrator'.
            password (str): The password for Couchbase authentication. Default is 'password'.
            bucket_name (str): The name of the bucket. Default is 'vector_db'.
            embedding_function (Callable): The embedding function used to generate the vector representation. Default is SentenceTransformer("all-MiniLM-L6-v2").encode.
            scope_name (str): The name of the scope. Default is '_default'.
            collection_name (str): The name of the collection to create for this vector database. Default is '_default'.
            index_name (str): Index name for the vector database. Default is None.
            overwrite (bool): Whether to overwrite existing data. Default is False.
            wait_until_index_ready (float or None): Blocking call to wait until the database indexes are ready. None means no wait. Default is None.
            wait_until_document_ready (float or None): Blocking call to wait until the database documents are ready. None means no wait. Default is None.
        """
        if embedding_function is None:
            embedding_function = SentenceTransformer("all-MiniLM-L6-v2").encode
        self.embedding_function = embedding_function
        self.index_name = index_name

        # This will get the model dimension size by computing the embeddings dimensions
        self.dimensions = self._get_embedding_size()

        try:
            auth = PasswordAuthenticator(username, password)
            cluster = Cluster(connection_string, ClusterOptions(auth))
            cluster.wait_until_ready(timedelta(seconds=5))
            self.cluster = cluster

            self.bucket = cluster.bucket(bucket_name)
            self.scope = self.bucket.scope(scope_name)
            self.collection = self.scope.collection(collection_name)
            self.active_collection = self.collection

            logger.debug("Successfully connected to Couchbase")
        except Exception as err:
            raise ConnectionError("Could not connect to Couchbase server") from err

    def search_index_exists(self, index_name: str):
        """Check if the specified index is ready"""
        try:
            search_index_mgr = self.scope.search_indexes()
            index = search_index_mgr.get_index(index_name)
            return index.is_valid()
        except Exception:
            return False

    def _get_embedding_size(self):
        return len(self.embedding_function(_SAMPLE_SENTENCE)[0])

    def create_collection(
        self,
        collection_name: str,
        overwrite: bool = False,
        get_or_create: bool = True,
    ) -> "Collection":
        """Create a collection in the vector database and create a vector search index in the collection.

        Args:
            collection_name (str): The name of the collection.
            overwrite (bool): Whether to overwrite the collection if it exists. Default is False.
            get_or_create (bool): Whether to get or create the collection. Default is True
        """
        if overwrite:
            self.delete_collection(collection_name)

        try:
            collection_mgr = self.bucket.collections()
            collection_mgr.create_collection(self.scope.name, collection_name)
            self.cluster.query(f"CREATE PRIMARY INDEX ON {self.bucket.name}.{self.scope.name}.{collection_name}")

        except Exception:
            if not get_or_create:
                raise ValueError(f"Collection {collection_name} already exists.")
            else:
                logger.debug(f"Collection {collection_name} already exists. Getting the collection.")

        collection = self.scope.collection(collection_name)
        self.create_index_if_not_exists(index_name=self.index_name, collection=collection)
        return collection

    def create_index_if_not_exists(
        self, index_name: str = "vector_index", collection: Optional["Collection"] = None
    ) -> None:
        """Creates a vector search index on the specified collection in Couchbase.

        Args:
            index_name (str, optional): The name of the vector search index to create. Defaults to "vector_search_index".
            collection (Collection, optional): The Couchbase collection to create the index on. Defaults to None.
        """
        if not self.search_index_exists(index_name):
            self.create_vector_search_index(collection, index_name)

    def get_collection(self, collection_name: str | None = None) -> "Collection":
        """Get the collection from the vector database.

        Args:
            collection_name (str): The name of the collection. Default is None. If None, return the current active collection.

        Returns:
            The collection object (Collection)
        """
        if collection_name is None:
            if self.active_collection is None:
                raise ValueError("No collection is specified.")
            else:
                logger.debug(
                    f"No collection is specified. Using current active collection {self.active_collection.name}."
                )
        else:
            self.active_collection = self.scope.collection(collection_name)

        return self.active_collection

    def delete_collection(self, collection_name: str) -> None:
        """Delete the collection from the vector database.

        Args:
            collection_name (str): The name of the collection.
        """
        try:
            collection_mgr = self.bucket.collections()
            collection_mgr.drop_collection(self.scope.name, collection_name)
        except Exception as e:
            logger.error(f"Error deleting collection: {e}")

    def create_vector_search_index(
        self,
        collection,
        index_name: str | None = "vector_index",
        similarity: Literal["l2_norm", "dot_product"] = "dot_product",
    ) -> None:
        """Create a vector search index in the collection."""
        search_index_mgr = self.scope.search_indexes()
        dims = self._get_embedding_size()
        index_definition = {
            "type": "fulltext-index",
            "name": index_name,
            "sourceType": "couchbase",
            "sourceName": self.bucket.name,
            "planParams": {"maxPartitionsPerPIndex": 1024, "indexPartitions": 1},
            "params": {
                "doc_config": {
                    "docid_prefix_delim": "",
                    "docid_regexp": "",
                    "mode": "scope.collection.type_field",
                    "type_field": "type",
                },
                "mapping": {
                    "analysis": {},
                    "default_analyzer": "standard",
                    "default_datetime_parser": "dateTimeOptional",
                    "default_field": "_all",
                    "default_mapping": {"dynamic": True, "enabled": False},
                    "default_type": "_default",
                    "docvalues_dynamic": False,
                    "index_dynamic": True,
                    "store_dynamic": True,
                    "type_field": "_type",
                    "types": {
                        f"{self.scope.name}.{collection.name}": {
                            "dynamic": False,
                            "enabled": True,
                            "properties": {
                                "embedding": {
                                    "dynamic": False,
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "dims": dims,
                                            "index": True,
                                            "name": "embedding",
                                            "similarity": similarity,
                                            "type": "vector",
                                            "vector_index_optimized_for": "recall",
                                        }
                                    ],
                                },
                                "metadata": {"dynamic": True, "enabled": True},
                                "content": {
                                    "dynamic": False,
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "include_in_all": True,
                                            "index": True,
                                            "name": "content",
                                            "store": True,
                                            "type": "text",
                                        }
                                    ],
                                },
                            },
                        }
                    },
                },
                "store": {"indexType": "scorch", "segmentVersion": 16},
            },
            "sourceParams": {},
        }

        search_index_def = SearchIndex.from_json(json.dumps(index_definition))
        max_attempts = 10
        attempt = 0
        while attempt < max_attempts:
            try:
                search_index_mgr.upsert_index(search_index_def)
                break
            except Exception as e:
                logger.debug(f"Attempt {attempt + 1}/{max_attempts}: Error creating search index: {e}")
                time.sleep(3)
                attempt += 1

        if attempt == max_attempts:
            logger.error(f"Error creating search index after {max_attempts} attempts.")
            raise RuntimeError(f"Error creating search index after {max_attempts} attempts.")

        logger.info(f"Search index {index_name} created successfully.")

    def upsert_docs(
        self, docs: list[Document], collection: "Collection", batch_size: int = DEFAULT_BATCH_SIZE, **kwargs: Any
    ) -> None:
        if docs[0].get("content") is None:
            raise ValueError("The document content is required.")
        if docs[0].get("id") is None:
            raise ValueError("The document id is required.")

        for i in range(0, len(docs), batch_size):
            batch = docs[i : i + batch_size]
            docs_to_upsert = {}
            for doc in batch:
                doc_id = doc["id"]
                embedding = self.embedding_function([
                    doc["content"]
                ]).tolist()  # Gets new embedding even in case of document update
                doc_content = {
                    TEXT_KEY: doc["content"],
                    "metadata": doc.get("metadata", {}),
                    EMBEDDING_KEY: embedding,
                    "id": doc_id,
                }
                docs_to_upsert[doc_id] = doc_content
            collection.upsert_multi(docs_to_upsert)

    def insert_docs(
        self,
        docs: list[Document],
        collection_name: str = None,
        upsert: bool = False,
        batch_size: int = DEFAULT_BATCH_SIZE,
        **kwargs: Any,
    ) -> None:
        """Insert Documents and Vector Embeddings into the collection of the vector database. Documents are upserted in all cases."""
        if not docs:
            logger.info("No documents to insert.")
            return

        collection = self.get_collection(collection_name)
        self.upsert_docs(docs, collection, batch_size=batch_size)

    def update_docs(
        self, docs: list[Document], collection_name: str = None, batch_size: int = DEFAULT_BATCH_SIZE, **kwargs: Any
    ) -> None:
        """Update documents, including their embeddings, in the Collection."""
        collection = self.get_collection(collection_name)
        self.upsert_docs(docs, collection, batch_size)

    def delete_docs(
        self, ids: list[ItemID], collection_name: str = None, batch_size: int = DEFAULT_BATCH_SIZE, **kwargs
    ):
        """Delete documents from the collection of the vector database."""
        collection = self.get_collection(collection_name)
        # based on batch size, delete the documents
        for i in range(0, len(ids), batch_size):
            batch = ids[i : i + batch_size]
            collection.remove_multi(batch)

    def get_docs_by_ids(
        self,
        ids: list[ItemID] | None = None,
        collection_name: str = None,
        include: list[str] | None = None,
        **kwargs: Any,
    ) -> list[Document]:
        """Retrieve documents from the collection of the vector database based on the ids."""
        if include is None:
            include = [TEXT_KEY, "metadata", "id"]
        elif "id" not in include:
            include.append("id")

        collection = self.get_collection(collection_name)
        if ids is not None:
            docs = [collection.get(doc_id) for doc_id in ids]
        else:
            # Get all documents using couchbase query
            include_str = ", ".join(include)
            query = f"SELECT {include_str} FROM {self.bucket.name}.{self.scope.name}.{collection.name}"
            result = self.cluster.query(query)
            docs = []
            for row in result:
                docs.append(row)

        return [{k: v for k, v in doc.items() if k in include or k == "id"} for doc in docs]

    def retrieve_docs(
        self,
        queries: list[str],
        collection_name: str = None,
        n_results: int = 10,
        distance_threshold: float = -1,
        **kwargs: Any,
    ) -> QueryResults:
        """Retrieve documents from the collection of the vector database based on the queries.
        Note: Distance threshold is not supported in Couchbase FTS.
        """
        results: QueryResults = []
        for query_text in queries:
            query_vector = np.array(self.embedding_function([query_text])).tolist()[0]
            query_result = self._vector_search(
                query_vector,
                n_results,
                **kwargs,
            )
            results.append(query_result)
        return results

    def _vector_search(
        self, embedding_vector: list[float], n_results: int = 10, **kwargs
    ) -> list[tuple[dict[str, Any], float]]:
        """Core vector search using Couchbase FTS."""
        search_req = search.SearchRequest.create(
            VectorSearch.from_vector_query(
                VectorQuery(
                    EMBEDDING_KEY,
                    embedding_vector,
                    n_results,
                )
            )
        )

        search_options = SearchOptions(limit=n_results, fields=["*"])
        result = self.scope.search(self.index_name, search_req, search_options)

        docs_with_score = []

        for row in result.rows():
            doc = row.fields
            doc["id"] = row.id
            score = row.score

            docs_with_score.append((doc, score))

        return docs_with_score
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import warnings
from collections.abc import Callable
from typing import Any, Literal, Optional

from ...import_utils import optional_import_block, require_optional_import
from ...retrieve_utils import TEXT_FORMATS, get_files_from_dir, split_files_to_chunks
from ..contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent
from ..contrib.vectordb.utils import (
    chroma_results_to_query_results,
    filter_results_by_distance,
    get_logger,
)

logger = get_logger(__name__)

with optional_import_block():
    import fastembed  # noqa: F401
    from qdrant_client import QdrantClient, models
    from qdrant_client.fastembed_common import QueryResponse


@require_optional_import(["fastembed", "qdrant_client"], "retrievechat-qdrant")
class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):
    def __init__(
        self,
        name="RetrieveChatAgent",  # default set to RetrieveChatAgent
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "ALWAYS",
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        retrieve_config: dict[str, Any] | None = None,  # config for the retrieve agent
        **kwargs: Any,
    ):
        """Args:
        name (str): name of the agent.
        human_input_mode (str): whether to ask for human inputs every time a message is received.
            Possible values are "ALWAYS", "TERMINATE", "NEVER".
            1. When "ALWAYS", the agent prompts for human input every time a message is received.
                Under this mode, the conversation stops when the human input is "exit",
                or when is_termination_msg is True and there is no human input.
            2. When "TERMINATE", the agent only prompts for human input only when a termination message is received or
                the number of auto reply reaches the max_consecutive_auto_reply.
            3. When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops
                when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
        is_termination_msg (function): a function that takes a message in the form of a dictionary
            and returns a boolean value indicating if this received message is a termination message.
            The dict can contain the following keys: "content", "role", "name", "function_call".
        retrieve_config (dict or None): config for the retrieve agent.
            To use default config, set to None. Otherwise, set to a dictionary with the following keys:
            - task (Optional, str): the task of the retrieve chat. Possible values are "code", "qa" and "default". System
                prompt will be different for different tasks. The default value is `default`, which supports both code and qa.
            - client (Optional, qdrant_client.QdrantClient(":memory:")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.
                will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.
            - docs_path (Optional, Union[str, List[str]]): the path to the docs directory. It can also be the path to a single file,
                the url to a single file or a list of directories, files and urls. Default is None, which works only if the collection is already created.
            - extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite.,
                when set to true it enables the system to assign unique IDs starting from "length+i" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection..
                By default, "extra_docs" is set to false, starting document IDs from zero. This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.
            - collection_name (Optional, str): the name of the collection.
                If key not provided, a default name `autogen-docs` will be used.
            - model (Optional, str): the model to use for the retrieve chat.
                If key not provided, a default model `gpt-4` will be used.
            - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.
                If key not provided, a default size `max_tokens * 0.4` will be used.
            - context_max_tokens (Optional, int): the context max token size for the retrieve chat.
                If key not provided, a default size `max_tokens * 0.8` will be used.
            - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are
                "multi_lines" and "one_line". If key not provided, a default mode `multi_lines` will be used.
            - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.
                If chunk_mode is "one_line", this parameter will be ignored.
            - embedding_model (Optional, str): the embedding model to use for the retrieve chat.
                If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used. All available models
                can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.
            - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.
            - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is "".
                If not "" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.
            - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.
            - custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string.
                The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).
                Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.
            - custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.
                Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.
            - custom_text_types (Optional, List[str]): a list of file types to be processed. Default is `autogen.retrieve_utils.TEXT_FORMATS`.
                This only applies to files under the directories in `docs_path`. Explicitly included files and urls will be chunked regardless of their types.
            - recursive (Optional, bool): whether to search documents recursively in the docs_path. Default is True.
            - parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.
            - on_disk (Optional, bool): Whether to store the collection on disk. Default is False.
            - quantization_config: Quantization configuration. If None, quantization will be disabled.
            - hnsw_config: HNSW configuration. If None, default configuration will be used.
              You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/#vector-index.
              API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection
            - payload_indexing: Whether to create a payload index for the document field. Default is False.
              You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/#payload-index
              API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index
         **kwargs (dict): other kwargs in [UserProxyAgent](/docs/api-reference/autogen/UserProxyAgent#userproxyagent).

        """
        warnings.warn(
            "The QdrantRetrieveUserProxyAgent is deprecated. Please use the RetrieveUserProxyAgent instead, set `vector_db` to `qdrant`.",
            DeprecationWarning,
            stacklevel=2,
        )
        super().__init__(name, human_input_mode, is_termination_msg, retrieve_config, **kwargs)
        self._client = self._retrieve_config.get("client", QdrantClient(":memory:"))
        self._embedding_model = self._retrieve_config.get("embedding_model", "BAAI/bge-small-en-v1.5")
        # Uses all available CPU cores to encode data when set to 0
        self._parallel = self._retrieve_config.get("parallel", 0)
        self._on_disk = self._retrieve_config.get("on_disk", False)
        self._quantization_config = self._retrieve_config.get("quantization_config", None)
        self._hnsw_config = self._retrieve_config.get("hnsw_config", None)
        self._payload_indexing = self._retrieve_config.get("payload_indexing", False)

    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = ""):
        """Args:
        problem (str): the problem to be solved.
        n_results (int): the number of results to be retrieved. Default is 20.
        search_string (str): only docs that contain an exact match of this string will be retrieved. Default is "".
        """
        if not self._collection:
            print("Trying to create collection.")
            create_qdrant_from_dir(
                dir_path=self._docs_path,
                max_tokens=self._chunk_token_size,
                client=self._client,
                collection_name=self._collection_name,
                chunk_mode=self._chunk_mode,
                must_break_at_empty_line=self._must_break_at_empty_line,
                embedding_model=self._embedding_model,
                custom_text_split_function=self.custom_text_split_function,
                custom_text_types=self._custom_text_types,
                recursive=self._recursive,
                extra_docs=self._extra_docs,
                parallel=self._parallel,
                on_disk=self._on_disk,
                quantization_config=self._quantization_config,
                hnsw_config=self._hnsw_config,
                payload_indexing=self._payload_indexing,
            )
            self._collection = True

        results = query_qdrant(
            query_texts=problem,
            n_results=n_results,
            search_string=search_string,
            client=self._client,
            collection_name=self._collection_name,
            embedding_model=self._embedding_model,
        )
        results["contents"] = results.pop("documents")
        results = chroma_results_to_query_results(results, "distances")
        results = filter_results_by_distance(results, self._distance_threshold)

        self._search_string = search_string
        self._results = results


@require_optional_import(["fastembed", "qdrant_client"], "retrievechat-qdrant")
def create_qdrant_from_dir(
    dir_path: str,
    max_tokens: int = 4000,
    client: "QdrantClient" = None,
    collection_name: str = "all-my-documents",
    chunk_mode: str = "multi_lines",
    must_break_at_empty_line: bool = True,
    embedding_model: str = "BAAI/bge-small-en-v1.5",
    custom_text_split_function: Callable = None,
    custom_text_types: list[str] = TEXT_FORMATS,
    recursive: bool = True,
    extra_docs: bool = False,
    parallel: int = 0,
    on_disk: bool = False,
    quantization_config: Optional["models.QuantizationConfig"] = None,
    hnsw_config: Optional["models.HnswConfigDiff"] = None,
    payload_indexing: bool = False,
    qdrant_client_options: dict[str, Any] | None = {},
):
    """Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a
      url to a single file.

    Args:
        dir_path (str): the path to the directory, file or url.
        max_tokens (Optional, int): the maximum number of tokens per chunk. Default is 4000.
        client (Optional, QdrantClient): the QdrantClient instance. Default is None.
        collection_name (Optional, str): the name of the collection. Default is "all-my-documents".
        chunk_mode (Optional, str): the chunk mode. Default is "multi_lines".
        must_break_at_empty_line (Optional, bool): Whether to break at empty line. Default is True.
        embedding_model (Optional, str): the embedding model to use. Default is "BAAI/bge-small-en-v1.5".
            The list of all the available models can be at https://qdrant.github.io/fastembed/examples/Supported_Models/.
        custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.
            Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.
        custom_text_types (Optional, List[str]): a list of file types to be processed. Default is TEXT_FORMATS.
        recursive (Optional, bool): whether to search documents recursively in the dir_path. Default is True.
        extra_docs (Optional, bool): whether to add more documents in the collection. Default is False
        parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores
        on_disk (Optional, bool): Whether to store the collection on disk. Default is False.
        quantization_config: Quantization configuration. If None, quantization will be disabled.
            Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection
        hnsw_config: HNSW configuration. If None, default configuration will be used.
            Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection
        payload_indexing: Whether to create a payload index for the document field. Default is False.
        qdrant_client_options: (Optional, dict): the options for instantiating the qdrant client.
            Ref: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.
    """
    if client is None:
        client = QdrantClient(**qdrant_client_options)
        client.set_model(embedding_model)

    if custom_text_split_function is not None:
        chunks, _ = split_files_to_chunks(
            get_files_from_dir(dir_path, custom_text_types, recursive),
            custom_text_split_function=custom_text_split_function,
        )
    else:
        chunks, _ = split_files_to_chunks(
            get_files_from_dir(dir_path, custom_text_types, recursive), max_tokens, chunk_mode, must_break_at_empty_line
        )
    logger.info(f"Found {len(chunks)} chunks.")

    collection = None
    # Check if collection by same name exists, if not, create it with custom options
    try:
        collection = client.get_collection(collection_name=collection_name)
    except Exception:
        client.create_collection(
            collection_name=collection_name,
            vectors_config=client.get_fastembed_vector_params(
                on_disk=on_disk, quantization_config=quantization_config, hnsw_config=hnsw_config
            ),
        )
        collection = client.get_collection(collection_name=collection_name)

    length = 0
    if extra_docs:
        length = len(collection.get()["ids"])

    # Upsert in batch of 100 or less if the total number of chunks is less than 100
    for i in range(0, len(chunks), min(100, len(chunks))):
        end_idx = i + min(100, len(chunks) - i)
        client.add(
            collection_name,
            documents=chunks[i:end_idx],
            ids=[(j + length) for j in range(i, end_idx)],
            parallel=parallel,
        )

    # Create a payload index for the document field
    # Enables highly efficient payload filtering. Reference: https://qdrant.tech/documentation/concepts/indexing/#indexing
    # Creating an index requires additional computational resources and memory.
    # If filtering performance is critical, we can consider creating an index.
    if payload_indexing:
        client.create_payload_index(
            collection_name=collection_name,
            field_name="document",
            field_schema=models.TextIndexParams(
                type="text",
                tokenizer=models.TokenizerType.WORD,
                min_token_len=2,
                max_token_len=15,
            ),
        )


@require_optional_import("qdrant_client", "retrievechat-qdrant")
def query_qdrant(
    query_texts: list[str],
    n_results: int = 10,
    client: "QdrantClient" = None,
    collection_name: str = "all-my-documents",
    search_string: str = "",
    embedding_model: str = "BAAI/bge-small-en-v1.5",
    qdrant_client_options: dict[str, Any] | None = {},
) -> list[list["QueryResponse"]]:
    """Perform a similarity search with filters on a Qdrant collection

    Args:
        query_texts (List[str]): the query texts.
        n_results (Optional, int): the number of results to return. Default is 10.
        client (Optional, API): the QdrantClient instance. A default in-memory client will be instantiated if None.
        collection_name (Optional, str): the name of the collection. Default is "all-my-documents".
        search_string (Optional, str): the search string. Default is "".
        embedding_model (Optional, str): the embedding model to use. Default is "all-MiniLM-L6-v2". Will be ignored if embedding_function is not None.
        qdrant_client_options: (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.

    Returns:
        list[list[QueryResponse]]: the query result. The format is:
            class QueryResponse(BaseModel, extra="forbid"):  # type: ignore
                id: Union[str, int]
                embedding: Optional[List[float]]
                metadata: Dict[str, Any]
                document: str
                score: float
    """
    if client is None:
        client = QdrantClient(**qdrant_client_options)
        client.set_model(embedding_model)

    results = client.query_batch(
        collection_name,
        query_texts,
        limit=n_results,
        query_filter=(
            models.Filter(
                must=[
                    models.FieldCondition(
                        key="document",
                        match=models.MatchText(text=search_string),
                    )
                ]
            )
            if search_string
            else None
        ),
    )

    data = {
        "ids": [[result.id for result in sublist] for sublist in results],
        "documents": [[result.document for result in sublist] for sublist in results],
        "distances": [[result.score for result in sublist] for sublist in results],
        "metadatas": [[result.metadata for result in sublist] for sublist in results],
    }
    return data
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

__all__: list[str] = []
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
import logging
from typing import Optional

from ....import_utils import optional_import_block, require_optional_import
from .document import Document, DocumentType
from .graph_query_engine import GraphStoreQueryResult

with optional_import_block():
    from neo4j import GraphDatabase
    from neo4j_graphrag.embeddings import Embedder, OpenAIEmbeddings
    from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline
    from neo4j_graphrag.generation import GraphRAG
    from neo4j_graphrag.indexes import create_vector_index
    from neo4j_graphrag.llm.openai_llm import LLMInterface, OpenAILLM
    from neo4j_graphrag.retrievers import VectorRetriever

# Set up logging
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


@require_optional_import(["neo4j", "neo4j_graphrag"], "neo4j")
class Neo4jNativeGraphQueryEngine:
    """A graph query engine implemented using the Neo4j GraphRAG SDK.
    Provides functionality to initialize a knowledge graph,
    create a vector index, and query the graph using Neo4j and LLM.
    """

    def __init__(  # type: ignore[no-any-unimported]
        self,
        host: str = "neo4j://localhost",
        port: int = 7687,
        username: str = "neo4j",
        password: str = "password",
        embeddings: Optional["Embedder"] = None,
        embedding_dimension: int | None = 3072,
        llm: Optional["LLMInterface"] = None,
        query_llm: Optional["LLMInterface"] = None,
        entities: list[str] | None = None,
        relations: list[str] | None = None,
        potential_schema: list[tuple[str, str, str]] | None = None,
    ):
        """Initialize a Neo4j graph query engine.

        Args:
            host (str): Neo4j host URL.
            port (int): Neo4j port.
            username (str): Neo4j username.
            password (str): Neo4j password.
            embeddings (Embedder): Embedding model to embed chunk data and retrieve answers.
            embedding_dimension (int): Dimension of the embeddings for the model.
            llm (LLMInterface): Language model for creating the knowledge graph (returns JSON responses).
            query_llm (LLMInterface): Language model for querying the knowledge graph.
            entities (List[str], optional): Custom entities for guiding graph construction.
            relations (List[str], optional): Custom relations for guiding graph construction.
            potential_schema (List[tuple[str, str, str]], optional): Schema (triplets, i.e., [entity] -> [relationship] -> [entity]) to guide graph construction.
        """
        self.uri = f"{host}:{port}"
        self.driver = GraphDatabase.driver(self.uri, auth=(username, password))
        self.embeddings = embeddings or OpenAIEmbeddings(model="text-embedding-3-large")
        self.embedding_dimension = embedding_dimension
        self.llm = llm or OpenAILLM(
            model_name="gpt-4o",
            model_params={"response_format": {"type": "json_object"}, "temperature": 0},
        )
        self.query_llm = query_llm or OpenAILLM(model_name="gpt-4o", model_params={"temperature": 0})
        self.entities = entities
        self.relations = relations
        self.potential_schema = potential_schema

    def init_db(self, input_doc: list[Document] | None = None) -> None:
        """Initialize the Neo4j graph database using the provided input doc.
        Currently this method only supports single document input (only reads the first doc).

        This method supports both text and PDF documents. It performs the following steps:
        1. Clears the existing database.
        2. Extracts graph nodes and relationships from the input data to build a knowledge graph.
        3. Creates a vector index for efficient retrieval.

        Args:
            input_doc (list[Document]): Input documents for building the graph.

        Raises:
            ValueError: If the input document is not provided or its type is unsupported.
        """
        if input_doc is None or len(input_doc) == 0:
            raise ValueError("Input document is required to initialize the database.")
        elif len(input_doc) > 1:
            raise ValueError("Only the first document will be used to initialize the database.")

        logger.info("Clearing the database...")
        self._clear_db()

        self._initialize_kg_builders()

        self._build_graph(input_doc)

        self.index_name = "vector-index-name"
        logger.info(f"Creating vector index '{self.index_name}'...")
        self._create_index(self.index_name)

    def add_records(self, new_records: list[Document]) -> bool:
        """Add new records to the Neo4j database.

        Args:
            new_records (list[Document]): List of new Documents to be added

        Returns:
            bool: True if records were added successfully, False otherwise.
        """
        for record in new_records:
            if not isinstance(record, Document):
                raise ValueError("Invalid record type. Expected Document.")

        self._build_graph(new_records)

        return True

    def query(self, question: str) -> GraphStoreQueryResult:
        """Query the Neo4j database using a natural language question.

        Args:
            question (str): The question to be answered by querying the graph.

        Returns:
            GraphStoreQueryResult: The result of the query.
        """
        self.retriever = VectorRetriever(
            driver=self.driver,
            index_name=self.index_name,
            embedder=self.embeddings,
        )
        rag = GraphRAG(retriever=self.retriever, llm=self.query_llm)
        result = rag.search(query_text=question, retriever_config={"top_k": 5})

        return GraphStoreQueryResult(answer=result.answer)

    def _create_index(self, name: str) -> None:
        """Create a vector index for the Neo4j knowledge graph.

        Args:
            name (str): Name of the vector index to create.
        """
        logger.info(f"Creating vector index '{name}'...")
        create_vector_index(
            self.driver,
            name=name,
            label="Chunk",
            embedding_property="embedding",
            dimensions=self.embedding_dimension,
            similarity_fn="euclidean",
        )
        logger.info(f"Vector index '{name}' created successfully.")

    def _clear_db(self) -> None:
        """Clear all nodes and relationships from the Neo4j database."""
        logger.info("Clearing all nodes and relationships in the database...")
        self.driver.execute_query("MATCH (n) DETACH DELETE n;")
        logger.info("Database cleared successfully.")

    def _initialize_kg_builders(self) -> None:
        """Initialize the knowledge graph builders"""
        logger.info("Initializing the knowledge graph builders...")
        self.text_kg_builder = SimpleKGPipeline(
            driver=self.driver,
            embedder=self.embeddings,
            llm=self.llm,
            entities=self.entities,
            relations=self.relations,
            potential_schema=self.potential_schema,
            on_error="IGNORE",
            from_pdf=False,
        )

        self.pdf_kg_builder = SimpleKGPipeline(
            driver=self.driver,
            embedder=self.embeddings,
            llm=self.llm,
            entities=self.entities,
            relations=self.relations,
            potential_schema=self.potential_schema,
            on_error="IGNORE",
            from_pdf=True,
        )

    def _build_graph(self, input_doc: list[Document]) -> None:
        """Build the knowledge graph using the provided input documents.

        Args:
            input_doc (List[Document]): List of input documents for building the graph.
        """
        logger.info("Building the knowledge graph...")
        for doc in input_doc:
            if doc.doctype == DocumentType.TEXT:
                # todo: we assume this is a path, and not URL
                with open(doc.path_or_url) as file:  # type: ignore[arg-type]
                    text = file.read()
                asyncio.run(self.text_kg_builder.run_async(text=text))
            elif doc.doctype == DocumentType.PDF:
                asyncio.run(self.pdf_kg_builder.run_async(file_path=doc.path_or_url))
            else:
                raise ValueError(f"Unsupported document type: {doc.doctype}")

        logger.info("Knowledge graph built successfully.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import Agent, ConversableAgent, UserProxyAgent
from .graph_query_engine import GraphStoreQueryResult
from .graph_rag_capability import GraphRagCapability
from .neo4j_graph_query_engine import Neo4jGraphQueryEngine


class Neo4jGraphCapability(GraphRagCapability):
    """The Neo4j graph capability integrates Neo4j Property graph into a graph rag agent.
    Ref: https://neo4j.com/labs/genai-ecosystem/llamaindex/#_property_graph_constructing_modules


    For usage, please refer to example notebook/agentchat_graph_rag_neo4j.ipynb
    """

    def __init__(self, query_engine: Neo4jGraphQueryEngine):
        """Initialize GraphRAG capability with a graph query engine"""
        self.query_engine = query_engine

    def add_to_agent(self, agent: ConversableAgent) -> None:
        """Add Neo4j GraphRAG capability to a UserProxyAgent.
        The restriction to a UserProxyAgent to make sure the returned message only contains information retrieved from the graph DB instead of any LLMs.
        """
        if not isinstance(agent, UserProxyAgent):
            raise Exception("Neo4j GraphRAG capability can only be added to a UserProxyAgent.")

        self.graph_rag_agent = agent

        # Validate the agent config
        if agent.llm_config not in (None, False):
            raise Exception(
                "Agents with GraphRAG capabilities do not use an LLM configuration. Please set your llm_config to None or False."
            )

        # Register method to generate the reply using a Neo4j query
        # All other reply methods will be removed
        agent.register_reply(
            [ConversableAgent, None], self._reply_using_neo4j_query, position=0, remove_other_reply_funcs=True
        )

    def _reply_using_neo4j_query(
        self,
        recipient: ConversableAgent,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Query neo4j and return the message. Internally, it queries the Property graph
        and returns the answer from the graph query engine.
        TODO: reply with a dictionary including both the answer and semantic source triplets.

        Args:
            recipient: The agent instance that will receive the message.
            messages: A list of messages in the conversation history with the sender.
            sender: The agent instance that sent the message.
            config: Optional configuration for message processing.

        Returns:
            A tuple containing a boolean indicating success and the assistant's reply.
        """
        if not messages:
            return False, None

        question = self._get_last_question(messages[-1])
        if not question:
            return False, None

        result: GraphStoreQueryResult = self.query_engine.query(question)  # type: ignore[arg-type]

        return True, result.answer

    def _get_last_question(self, message: dict[str, Any] | str) -> str | dict[str, Any] | None:
        """Retrieves the last message from the conversation history."""
        if isinstance(message, str):
            return message
        if isinstance(message, dict) and "content" in message:
            return message["content"]  # type: ignore[no-any-return]
        return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .document import Document, DocumentType
from .graph_query_engine import GraphQueryEngine, GraphStoreQueryResult
from .graph_rag_capability import GraphRagCapability

__all__ = ["Document", "DocumentType", "GraphQueryEngine", "GraphRagCapability", "GraphStoreQueryResult"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import os
import warnings
from typing import Any, Optional

from ....import_utils import optional_import_block, require_optional_import
from .document import Document
from .graph_query_engine import GraphStoreQueryResult

with optional_import_block():
    from falkordb import FalkorDB, Graph
    from graphrag_sdk import KnowledgeGraph, Source
    from graphrag_sdk.model_config import KnowledgeGraphModelConfig
    from graphrag_sdk.models import GenerativeModel
    from graphrag_sdk.models.openai import OpenAiGenerativeModel
    from graphrag_sdk.ontology import Ontology


@require_optional_import(["falkordb", "graphrag_sdk"], "graph-rag-falkor-db")
class FalkorGraphQueryEngine:
    """This is a wrapper for FalkorDB KnowledgeGraph."""

    def __init__(  # type: ignore[no-any-unimported]
        self,
        name: str,
        host: str = "127.0.0.1",
        port: int = 6379,
        username: str | None = None,
        password: str | None = None,
        model: Optional["GenerativeModel"] = None,
        ontology: Optional["Ontology"] = None,
    ):
        """Initialize a FalkorDB knowledge graph.
        Please also refer to https://github.com/FalkorDB/GraphRAG-SDK/blob/main/graphrag_sdk/kg.py

        TODO: Fix LLM API cost calculation for FalkorDB usages.

        Args:
            name (str): Knowledge graph name.
            host (str): FalkorDB hostname.
            port (int): FalkorDB port number.
            username (str|None): FalkorDB username.
            password (str|None): FalkorDB password.
            model (GenerativeModel): LLM model to use for FalkorDB to build and retrieve from the graph, default to use OAI gpt-4o.
            ontology: FalkorDB knowledge graph schema/ontology, https://github.com/FalkorDB/GraphRAG-SDK/blob/main/graphrag_sdk/ontology.py
                If None, FalkorDB will auto generate an ontology from the input docs.
        """
        self.name = name
        self.ontology_table_name = name + "_ontology"
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.model = model or OpenAiGenerativeModel("gpt-4o")
        self.model_config = KnowledgeGraphModelConfig.with_model(self.model)
        self.ontology = ontology
        self.knowledge_graph: KnowledgeGraph | None = None  # type: ignore[no-any-unimported]
        self.falkordb = FalkorDB(host=self.host, port=self.port, username=self.username, password=self.password)

    def connect_db(self) -> None:
        """Connect to an existing knowledge graph."""
        if self.name in self.falkordb.list_graphs():
            try:
                self.ontology = self._load_ontology_from_db()
            except Exception:
                warnings.warn("Graph Ontology is not loaded.")

            if self.ontology is None:
                raise ValueError(f"Ontology of the knowledge graph '{self.name}' can't be None.")

            self.knowledge_graph = KnowledgeGraph(
                name=self.name,
                host=self.host,
                port=self.port,
                username=self.username,
                password=self.password,
                model_config=self.model_config,
                ontology=self.ontology,
            )

            # Establishing a chat session will maintain the history
            self._chat_session = self.knowledge_graph.chat_session()
        else:
            raise ValueError(f"Knowledge graph '{self.name}' does not exist")

    def init_db(self, input_doc: list[Document]) -> None:
        """Build the knowledge graph with input documents."""
        sources = []
        for doc in input_doc:
            if doc.path_or_url and os.path.exists(doc.path_or_url):
                sources.append(Source(doc.path_or_url))

        if sources:
            # Auto generate graph ontology if not created by user.
            if self.ontology is None:
                self.ontology = Ontology.from_sources(
                    sources=sources,
                    model=self.model,
                )
            # Save Ontology to graph for future access.
            self._save_ontology_to_db(self.ontology)

            self.knowledge_graph = KnowledgeGraph(
                name=self.name,
                host=self.host,
                port=self.port,
                username=self.username,
                password=self.password,
                model_config=KnowledgeGraphModelConfig.with_model(self.model),
                ontology=self.ontology,
            )

            self.knowledge_graph.process_sources(sources)

            # Establishing a chat session will maintain the history
            self._chat_session = self.knowledge_graph.chat_session()
        else:
            raise ValueError("No input documents could be loaded.")

    def add_records(self, new_records: list[Document]) -> bool:
        raise NotImplementedError("This method is not supported by FalkorDB SDK yet.")

    def query(self, question: str, n_results: int = 1, **kwargs: Any) -> GraphStoreQueryResult:
        """Query the knowledge graph with a question and optional message history.

        Args:
        question: a human input question.
        n_results: number of returned results.
        kwargs:
            messages: a list of message history.

        Returns: FalkorGraphQueryResult
        """
        if self.knowledge_graph is None:
            raise ValueError("Knowledge graph has not been selected or created.")

        response = self._chat_session.send_message(question)

        return GraphStoreQueryResult(answer=response["response"], results=[])

    def delete(self) -> bool:
        """Delete graph and its data from database."""
        all_graphs = self.falkordb.list_graphs()
        if self.name in all_graphs:
            self.falkordb.select_graph(self.name).delete()
        if self.ontology_table_name in all_graphs:
            self.falkordb.select_graph(self.ontology_table_name).delete()
        return True

    def __get_ontology_storage_graph(self) -> "Graph":  # type: ignore[no-any-unimported]
        return self.falkordb.select_graph(self.ontology_table_name)

    def _save_ontology_to_db(self, ontology: "Ontology") -> None:  # type: ignore[no-any-unimported]
        """Save graph ontology to a separate table with {graph_name}_ontology"""
        if self.ontology_table_name in self.falkordb.list_graphs():
            raise ValueError(f"Knowledge graph {self.name} is already created.")
        graph = self.__get_ontology_storage_graph()
        ontology.save_to_graph(graph)

    def _load_ontology_from_db(self) -> "Ontology":  # type: ignore[no-any-unimported]
        if self.ontology_table_name not in self.falkordb.list_graphs():
            raise ValueError(f"Knowledge graph {self.name} has not been created.")
        graph = self.__get_ontology_storage_graph()
        return Ontology.from_schema_graph(graph)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Any

__all__ = ["Document", "DocumentType"]


class DocumentType(Enum):
    """Enum for supporting document type."""

    TEXT = auto()
    HTML = auto()
    PDF = auto()
    JSON = auto()


@dataclass
class Document:
    """A wrapper of graph store query results."""

    doctype: DocumentType
    data: Any | None = None
    path_or_url: str | None = field(default_factory=lambda: "")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from ...conversable_agent import ConversableAgent
from ..capabilities.agent_capability import AgentCapability
from .graph_query_engine import GraphQueryEngine

__all__ = ["GraphRagCapability"]


class GraphRagCapability(AgentCapability):
    """A graph-based RAG capability uses a graph query engine to give a conversable agent the graph-based RAG ability.

    An agent class with graph-based RAG capability could:\n
    1. create a graph in the underlying database with input documents.\n
    2. retrieved relevant information based on messages received by the agent.\n
    3. generate answers from retrieved information and send messages back.\n

    For example,
    ```python
    graph_query_engine = GraphQueryEngine(...)
    graph_query_engine.init_db([Document(doc1), Document(doc2), ...])

    graph_rag_agent = ConversableAgent(
        name="graph_rag_agent",
        max_consecutive_auto_reply=3,
        ...
    )
    graph_rag_capability = GraphRagCapbility(graph_query_engine)
    graph_rag_capability.add_to_agent(graph_rag_agent)

    user_proxy = UserProxyAgent(
        name="user_proxy",
        code_execution_config=False,
        is_termination_msg=lambda msg: "TERMINATE" in msg["content"],
        human_input_mode="ALWAYS",
    )
    user_proxy.initiate_chat(graph_rag_agent, message="Name a few actors who've played in 'The Matrix'")

    # ChatResult(
        # chat_id=uuid.uuid4().int,
        # chat_history=[
            # {'content': 'Name a few actors who've played in \'The Matrix\'', 'role': 'graph_rag_agent'},
            # {'content': 'A few actors who have played in The Matrix are:
            #   - Keanu Reeves
            #   - Laurence Fishburne
            #   - Carrie-Anne Moss
            #   - Hugo Weaving',
            #   'role': 'user_proxy'},
        # ...)
    ```
    """

    def __init__(self, query_engine: GraphQueryEngine) -> None:
        """Initialize graph-based RAG capability with a graph query engine"""
        ...

    def add_to_agent(self, agent: ConversableAgent) -> None:
        """Add the capability to an agent"""
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import Agent, ConversableAgent
from .graph_query_engine import GraphStoreQueryResult
from .graph_rag_capability import GraphRagCapability
from .neo4j_native_graph_query_engine import Neo4jNativeGraphQueryEngine


class Neo4jNativeGraphCapability(GraphRagCapability):
    """The Neo4j native graph capability integrates Neo4j native query engine into a graph rag agent.

    For usage, please refer to example notebook/agentchat_graph_rag_neo4j_native.ipynb
    """

    def __init__(self, query_engine: Neo4jNativeGraphQueryEngine):
        """Initialize GraphRAG capability with a neo4j native graph query engine"""
        self.query_engine = query_engine

    def add_to_agent(self, agent: ConversableAgent) -> None:
        """Add native Neo4j GraphRAG capability to a ConversableAgent.
        llm_config of the agent must be None/False (default) to make sure the returned message only contains information retrieved from the graph DB instead of any LLMs.
        """
        self.graph_rag_agent = agent

        # Validate the agent config
        if agent.llm_config not in (None, False):
            raise Exception(
                "Agents with GraphRAG capabilities do not use an LLM configuration. Please set your llm_config to None or False."
            )

        # Register method to generate the reply using a Neo4j query
        # All other reply methods will be removed
        agent.register_reply(
            [ConversableAgent, None], self._reply_using_native_neo4j_query, position=0, remove_other_reply_funcs=True
        )

    def _reply_using_native_neo4j_query(
        self,
        recipient: ConversableAgent,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Query Neo4j and return the message. Internally, it uses the Neo4jNativeGraphQueryEngine to query the graph.

        The agent's system message will be incorporated into the query, if it's not blank.

        If no results are found, a default message is returned: "I'm sorry, I don't have an answer for that."

        Args:
            recipient: The agent instance that will receive the message.
            messages: A list of messages in the conversation history with the sender.
            sender: The agent instance that sent the message.
            config: Optional configuration for message processing.

        Returns:
            A tuple containing a boolean indicating success and the assistant's reply.
        """
        # todo: fix typing, this is not correct
        question = self._messages_summary(messages, recipient.system_message)  # type: ignore[arg-type]
        result: GraphStoreQueryResult = self.query_engine.query(question)

        return True, result.answer if result.answer else "I'm sorry, I don't have an answer for that."

    def _messages_summary(self, messages: dict[str, Any] | str, system_message: str) -> str:
        """Summarize the messages in the conversation history. Excluding any message with 'tool_calls' and 'tool_responses'
        Includes the 'name' (if it exists) and the 'content', with a new line between each one, like:
        customer:
        <content>

        agent:
        <content>
        """
        if isinstance(messages, str):
            return (f"IMPORTANT: {system_message}\n" if system_message else "") + f"Context:\n\n{messages}"

        elif isinstance(messages, list):
            summary = ""
            for message in messages:
                if "content" in message and "tool_calls" not in message and "tool_responses" not in message:
                    summary += f"{message.get('name', '')}: {message.get('content', '')}\n\n"

            if system_message:
                summary = f"IMPORTANT: {system_message}\nContext:\n\n{summary}"

            return summary

        else:
            raise ValueError("Invalid messages format. Must be a list of messages or a string.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from dataclasses import dataclass, field
from typing import Any, Protocol, runtime_checkable

from .document import Document

__all__ = ["GraphQueryEngine", "GraphStoreQueryResult"]


@dataclass
class GraphStoreQueryResult:
    """A wrapper of graph store query results.

    answer: human readable answer to question/query.
    results: intermediate results to question/query, e.g. node entities.
    """

    answer: str | None = None
    results: list[Any] = field(default_factory=list)


@runtime_checkable
class GraphQueryEngine(Protocol):
    """An abstract base class that represents a graph query engine on top of a underlying graph database.

    This interface defines the basic methods for graph-based RAG.
    """

    def init_db(self, input_doc: list[Document] | None = None) -> None:
        """This method initializes graph database with the input documents or records.
        Usually, it takes the following steps,
        1. connecting to a graph database.
        2. extract graph nodes, edges based on input data, graph schema and etc.
        3. build indexes etc.

        Args:
        input_doc: a list of input documents that are used to build the graph in database.

        """
        pass

    def add_records(self, new_records: list[Any]) -> bool:
        """Add new records to the underlying database and add to the graph if required."""
        pass

    def query(self, question: str, n_results: int = 1, **kwarg: Any) -> GraphStoreQueryResult:
        """This method transform a string format question into database query and return the result."""
        pass
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import Agent, ConversableAgent
from .falkor_graph_query_engine import FalkorGraphQueryEngine
from .graph_query_engine import GraphStoreQueryResult
from .graph_rag_capability import GraphRagCapability


class FalkorGraphRagCapability(GraphRagCapability):
    """The FalkorDB GraphRAG capability integrate FalkorDB with graphrag_sdk version: 0.1.3b0.
    Ref: https://github.com/FalkorDB/GraphRAG-SDK/tree/2-move-away-from-sql-to-json-ontology-detection

    For usage, please refer to example notebook/agentchat_graph_rag_falkordb.ipynb
    """

    def __init__(self, query_engine: FalkorGraphQueryEngine):
        """Initialize GraphRAG capability with a graph query engine"""
        self.query_engine = query_engine

    def add_to_agent(self, agent: ConversableAgent) -> None:
        """Add FalkorDB GraphRAG capability to a ConversableAgent.

        Args:
            agent: The ConversableAgent instance to add the capability to.

        The restriction to a ConversableAgent to make sure the returned message does not contain information retrieved from the graph DB instead of any LLMs.

        """
        if not isinstance(agent, ConversableAgent):
            raise Exception("FalkorDB GraphRAG capability can only be added to a ConversableAgent.")

        self.graph_rag_agent = agent

        # Validate the agent config
        if agent.llm_config not in (None, False):
            raise Exception(
                "Agents with GraphRAG capabilities do not use an LLM configuration. Please set your llm_config to None or False."
            )

        # Register method to generate the reply using a FalkorDB query
        # All other reply methods will be removed
        agent.register_reply(
            [ConversableAgent, None], self._reply_using_falkordb_query, position=0, remove_other_reply_funcs=True
        )

    def _reply_using_falkordb_query(
        self,
        recipient: ConversableAgent,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Query FalkorDB and return the message. Internally, it utilises OpenAI to generate a reply based on the given messages.
        The history with FalkorDB is also logged and updated.

        The agent's system message will be incorporated into the query, if it's not blank.

        If no results are found, a default message is returned: "I'm sorry, I don't have an answer for that."

        Args:
            recipient: The agent instance that will receive the message.
            messages: A list of messages in the conversation history with the sender.
            sender: The agent instance that sent the message.
            config: Optional configuration for message processing.

        Returns:
            A tuple containing a boolean indicating success and the assistant's reply.
        """
        # todo: fix typing, this is not correct
        question = self._messages_summary(messages, recipient.system_message)  # type: ignore[arg-type]
        result: GraphStoreQueryResult = self.query_engine.query(question)

        return True, result.answer if result.answer else "I'm sorry, I don't have an answer for that."

    def _messages_summary(self, messages: dict[str, Any] | str, system_message: str) -> str:
        """Summarize the messages in the conversation history. Excluding any message with 'tool_calls' and 'tool_responses'
        Includes the 'name' (if it exists) and the 'content', with a new line between each one, like:
        customer:
        <content>

        agent:
        <content>
        """
        if isinstance(messages, str):
            return (f"IMPORTANT: {system_message}\n" if system_message else "") + f"Context:\n\n{messages}"

        elif isinstance(messages, list):
            summary = ""
            for message in messages:
                if "content" in message and "tool_calls" not in message and "tool_responses" not in message:
                    summary += f"{message.get('name', '')}: {message.get('content', '')}\n\n"

            if system_message:
                summary = f"IMPORTANT: {system_message}\nContext:\n\n{summary}"

            return summary

        else:
            raise ValueError("Invalid messages format. Must be a list of messages or a string.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import os
from typing import Any, Optional, TypeAlias

from ....import_utils import optional_import_block, require_optional_import
from .document import Document, DocumentType
from .graph_query_engine import GraphStoreQueryResult

with optional_import_block():
    from llama_index.core import PropertyGraphIndex, SimpleDirectoryReader
    from llama_index.core.base.embeddings.base import BaseEmbedding
    from llama_index.core.chat_engine.types import ChatMode
    from llama_index.core.indices.property_graph import (
        DynamicLLMPathExtractor,
        SchemaLLMPathExtractor,
    )
    from llama_index.core.indices.property_graph.transformations.schema_llm import Triple
    from llama_index.core.llms import LLM
    from llama_index.core.readers.json import JSONReader
    from llama_index.core.schema import Document as LlamaDocument
    from llama_index.core.schema import TransformComponent
    from llama_index.embeddings.openai import OpenAIEmbedding
    from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
    from llama_index.llms.openai import OpenAI


@require_optional_import("llama_index", "neo4j")
class Neo4jGraphQueryEngine:
    """
    This class serves as a wrapper for a property graph query engine backed by LlamaIndex and Neo4j,\n
    facilitating the creating, connecting, updating, and querying of LlamaIndex property graphs.\n
    \n
    It builds a property graph Index from input documents,\n
    storing and retrieving data from the property graph in the Neo4j database.\n
    \n
    It extracts triplets, i.e., [entity] -> [relationship] -> [entity] sets,\n
    from the input documents using llamIndex extractors.\n
    \n
    Users can provide custom entities, relationships, and schema to guide the extraction process.\n
    \n
    If strict is True, the engine will extract triplets following the schema\n
    of allowed relationships for each entity specified in the schema.\n
    \n
    It also leverages LlamaIndex's chat engine which has a conversation history internally to provide context-aware responses.\n
    \n
    For usage, please refer to example notebook/agentchat_graph_rag_neo4j.ipynb\n
    """

    def __init__(  # type: ignore[no-any-unimported]
        self,
        host: str = "bolt://localhost",
        port: int = 7687,
        database: str = "neo4j",
        username: str = "neo4j",
        password: str = "neo4j",
        llm: Optional["LLM"] = None,
        embedding: Optional["BaseEmbedding"] = None,
        entities: Optional["TypeAlias"] = None,
        relations: Optional["TypeAlias"] = None,
        schema: dict[str, str] | list["Triple"] | None = None,
        strict: bool | None = False,
    ):
        """Initialize a Neo4j Property graph.
        Please also refer to https://docs.llamaindex.ai/en/stable/examples/property_graph/graph_store/

        Args:
            name (str): Property graph name.
            host (str): Neo4j hostname.
            port (int): Neo4j port number.
            database (str): Neo4j database name.
            username (str): Neo4j username.
            password (str): Neo4j password.
            llm (LLM): Language model to use for extracting triplets.
            embedding (BaseEmbedding): Embedding model to use constructing index and query
            entities (Optional[TypeAlias]): Custom suggested entities to include in the graph.
            relations (Optional[TypeAlias]): Custom suggested relations to include in the graph.
            schema (Optional[Union[Dict[str, str], List[Triple]]): Custom schema to specify allowed relationships for each entity.
            strict (Optional[bool]): If false, allows for values outside of the input schema.
        """
        self.host = host
        self.port = port
        self.database = database
        self.username = username
        self.password = password
        self.llm = llm or OpenAI(model="gpt-4o", temperature=0.0)
        self.embedding = embedding or OpenAIEmbedding(model_name="text-embedding-3-small")
        self.entities = entities
        self.relations = relations
        self.schema = schema
        self.strict = strict

    def init_db(self, input_doc: list[Document] | None = None) -> None:
        """Build the knowledge graph with input documents."""
        self.documents = self._load_doc(input_doc if input_doc is not None else [])

        self.graph_store = Neo4jPropertyGraphStore(
            username=self.username,
            password=self.password,
            url=self.host + ":" + str(self.port),
            database=self.database,
        )

        # delete all entities and relationships in case a graph pre-exists
        self._clear()

        # Create knowledge graph extractors.
        self.kg_extractors = self._create_kg_extractors()

        self.index = PropertyGraphIndex.from_documents(
            self.documents,
            llm=self.llm,
            embed_model=self.embedding,
            kg_extractors=self.kg_extractors,
            property_graph_store=self.graph_store,
            show_progress=True,
        )

    def connect_db(self) -> None:
        """Connect to an existing knowledge graph database."""
        self.graph_store = Neo4jPropertyGraphStore(
            username=self.username,
            password=self.password,
            url=self.host + ":" + str(self.port),
            database=self.database,
        )

        self.kg_extractors = self._create_kg_extractors()

        self.index = PropertyGraphIndex.from_existing(
            property_graph_store=self.graph_store,
            kg_extractors=self.kg_extractors,
            llm=self.llm,
            embed_model=self.embedding,
            show_progress=True,
        )

    def add_records(self, new_records: list[Document]) -> bool:
        """Add new records to the knowledge graph. Must be local files.

        Args:
            new_records (List[Document]): List of new documents to add.

        Returns:
            bool: True if successful, False otherwise.
        """
        if self.graph_store is None:
            raise ValueError("Knowledge graph is not initialized. Please call init_db or connect_db first.")

        try:
            """
            SimpleDirectoryReader will select the best file reader based on the file extensions,
            see  _load_doc for supported file types.
            """
            new_documents = SimpleDirectoryReader(input_files=[doc.path_or_url for doc in new_records]).load_data()

            for doc in new_documents:
                self.index.insert(doc)

            return True
        except Exception as e:
            print(f"Error adding records: {e}")
            return False

    def query(self, question: str, n_results: int = 1, **kwargs: Any) -> GraphStoreQueryResult:
        """Query the property graph with a question using LlamaIndex chat engine.
        We use the condense_plus_context chat mode
        which condenses the conversation history and the user query into a standalone question,
        and then build a context for the standadlone question
        from the property graph to generate a response.

        Args:
            question: a human input question.
            n_results: number of results to return.
            **kwargs: additional keyword arguments.

        Returns:
            A GrapStoreQueryResult object containing the answer and related triplets.
        """
        if not hasattr(self, "index"):
            raise ValueError("Property graph index is not created.")

        # Initialize chat engine if not already initialized
        if not hasattr(self, "chat_engine"):
            self.chat_engine = self.index.as_chat_engine(chat_mode=ChatMode.CONDENSE_PLUS_CONTEXT, llm=self.llm)

        response = self.chat_engine.chat(question)
        return GraphStoreQueryResult(answer=str(response))

    def _clear(self) -> None:
        """Delete all entities and relationships in the graph.
        TODO: Delete all the data in the database including indexes and constraints.
        """
        with self.graph_store._driver.session() as session:
            session.run("MATCH (n) DETACH DELETE n;")

    def _load_doc(self, input_doc: list[Document]) -> list["LlamaDocument"]:  # type: ignore[no-any-unimported]
        """Load documents from the input files. Currently support the following file types:
        .csv - comma-separated values
        .docx - Microsoft Word
        .epub - EPUB ebook format
        .hwp - Hangul Word Processor
        .ipynb - Jupyter Notebook
        .jpeg, .jpg - JPEG image
        .mbox - MBOX email archive
        .md - Markdown
        .mp3, .mp4 - audio and video
        .pdf - Portable Document Format
        .png - Portable Network Graphics
        .ppt, .pptm, .pptx - Microsoft PowerPoint
        .json JSON files
        """
        for doc in input_doc:
            if not os.path.exists(doc.path_or_url):  # type: ignore[arg-type]
                raise ValueError(f"Document file not found: {doc.path_or_url}")

        common_type_input_files = []
        json_type_input_files = []
        for doc in input_doc:
            if doc.doctype is DocumentType.JSON:
                json_type_input_files.append(doc.path_or_url)
            else:
                common_type_input_files.append(doc.path_or_url)
        loaded_documents = []
        if common_type_input_files:
            loaded_documents.extend(SimpleDirectoryReader(input_files=common_type_input_files).load_data())
        for json_file in json_type_input_files:
            loaded_documents.extend(JSONReader().load_data(input_file=json_file))  # type: ignore[arg-type]

        return loaded_documents

    def _create_kg_extractors(self) -> list["TransformComponent"]:  # type: ignore[no-any-unimported]
        """If strict is True,
        extract paths following a strict schema of allowed relationships for each entity.

        If strict is False,
        auto-create relationships and schema that fit the graph

        # To add more extractors, please refer to https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#construction
        """
        #
        kg_extractors: list[TransformComponent] = [  # type: ignore[no-any-unimported]
            SchemaLLMPathExtractor(
                llm=self.llm,
                possible_entities=self.entities,
                possible_relations=self.relations,
                kg_validation_schema=self.schema,
                strict=self.strict if self.strict else False,
            ),
        ]

        # DynamicLLMPathExtractor will auto-create relationships and schema that fit the graph
        if not self.strict:
            kg_extractors.append(
                DynamicLLMPathExtractor(
                    llm=self.llm,
                    allowed_entity_types=self.entities,
                    allowed_relation_types=self.relations,
                )
            )

        return kg_extractors
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Sequence
from pathlib import Path
from typing import Any, Protocol, runtime_checkable

from ....doc_utils import export_module

__all__ = ["RAGQueryEngine"]


@export_module("autogen.agentchat.contrib.rag")
@runtime_checkable
class RAGQueryEngine(Protocol):
    """A protocol class that represents a document ingestation and query engine on top of an underlying database.

    This interface defines the basic methods for RAG.
    """

    def init_db(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> bool:
        """Initialize the database with the input documents or records.

        This method initializes database with the input documents or records.
        Usually, it takes the following steps:\n
        1. connecting to a database.\n
        2. insert records.\n
        3. build indexes etc.\n

        Args:\n
            new_doc_dir (Optional[Union[Path, str]]): A directory containing documents to be ingested.\n
            new_doc_paths_or_urls (Optional[Sequence[Union[Path, str]]]): A list of paths or URLs to documents to be ingested.\n
            *args: Any additional arguments\n
            **kwargs: Any additional keyword arguments\n
        Returns:\n
            bool: True if initialization is successful, False otherwise\n
        """
        ...

    def add_docs(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        """Add new documents to the underlying data store."""
        ...

    def connect_db(self, *args: Any, **kwargs: Any) -> bool:
        """Connect to the database.

        Args:
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments
        Returns:
            bool: True if connection is successful, False otherwise
        """
        ...

    def query(self, question: str, *args: Any, **kwargs: Any) -> str:
        """Transform a string format question into database query and return the result.

        Args:
            question: a string format question
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments
        """
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .chromadb_query_engine import ChromaDBQueryEngine
from .llamaindex_query_engine import LlamaIndexQueryEngine
from .mongodb_query_engine import MongoDBQueryEngine
from .query_engine import RAGQueryEngine

__all__ = ["ChromaDBQueryEngine", "LlamaIndexQueryEngine", "MongoDBQueryEngine", "RAGQueryEngine"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
from collections.abc import Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ..vectordb.base import VectorDBFactory

with optional_import_block():
    from chromadb import HttpClient
    from chromadb.api.types import EmbeddingFunction
    from chromadb.config import DEFAULT_DATABASE, DEFAULT_TENANT, Settings
    from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex
    from llama_index.core.llms import LLM
    from llama_index.core.schema import Document as LlamaDocument
    from llama_index.llms.openai import OpenAI
    from llama_index.vector_stores.chroma import ChromaVectorStore

__all__ = ["ChromaDBQueryEngine"]

DEFAULT_COLLECTION_NAME = "docling-parsed-docs"
EMPTY_RESPONSE_TEXT = "Empty Response"  # Indicates that the query did not return any results
EMPTY_RESPONSE_REPLY = "Sorry, I couldn't find any information on that. If you haven't ingested any documents, please try that."  # Default response for queries without results


# Set up logging
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


@require_optional_import(["chromadb", "llama_index"], "rag")
@export_module("autogen.agentchat.contrib.rag")
class ChromaDBQueryEngine:
    """This engine leverages Chromadb to persist document embeddings in a named collection
    and LlamaIndex's VectorStoreIndex to efficiently index and retrieve documents, and generate an answer in response
    to natural language queries. Collection can be regarded as an abstraction of group of documents in the database.

    It expects a Chromadb server to be running and accessible at the specified host and port.
    Refer to this [link](https://docs.trychroma.com/production/containers/docker) for running Chromadb in a Docker container.
    If the host and port are not provided, the engine will create an in-memory ChromaDB client.


    """

    def __init__(  # type: ignore[no-any-unimported]
        self,
        host: str | None = "localhost",
        port: int | None = 8000,
        settings: Optional["Settings"] = None,
        tenant: str | None = None,
        database: str | None = None,
        embedding_function: "EmbeddingFunction[Any] | None" = None,
        metadata: dict[str, Any] | None = None,
        llm: Optional["LLM"] = None,
        collection_name: str | None = None,
    ) -> None:
        """Initializes the ChromaDBQueryEngine with db_path, metadata, and embedding function and llm.

        Args:
            host: The host address of the ChromaDB server. Default is localhost.
            port: The port number of the ChromaDB server. Default is 8000.
            settings: A dictionary of settings to communicate with the chroma server. Default is None.
            tenant: The tenant to use for this client. Defaults to the default tenant.
            database: The database to use for this client. Defaults to the default database.
            embedding_function: A callable that converts text into vector embeddings. Default embedding uses Sentence Transformers model all-MiniLM-L6-v2.
                For more embeddings that ChromaDB support, please refer to [embeddings](https://docs.trychroma.com/docs/embeddings/embedding-functions)
            metadata: A dictionary containing configuration parameters for the Chromadb collection.
                This metadata is typically used to configure the HNSW indexing algorithm. Defaults to `{"hnsw:space": "ip", "hnsw:construction_ef": 30, "hnsw:M": 32}`
                For more details about the default metadata, please refer to [HNSW configuration](https://cookbook.chromadb.dev/core/configuration/#hnsw-configuration)
            llm: LLM model used by LlamaIndex for query processing.
                 You can find more supported LLMs at [LLM](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/)
            collection_name (str): The unique name for the Chromadb collection. If omitted, a constant name will be used. Populate this to reuse previous ingested data.
        """
        self.llm: LLM = llm or OpenAI(model="gpt-4o", temperature=0.0)  # type: ignore[no-any-unimported]
        if not host or not port:
            logger.warning(
                "Can't connect to remote Chroma client without host or port not. Using an ephemeral, in-memory client."
            )
            self.client = None
        else:
            try:
                self.client = HttpClient(
                    host=host,
                    port=port,
                    settings=settings,
                    tenant=tenant if tenant else DEFAULT_TENANT,  # type: ignore[arg-type, no-any-unimported]
                    database=database if database else DEFAULT_DATABASE,  # type: ignore[arg-type, no-any-unimported]
                )
            except Exception as e:
                raise ValueError(f"Failed to connect to the ChromaDB client: {e}")

        self.db_config = {"client": self.client, "embedding_function": embedding_function, "metadata": metadata}
        self.collection_name = collection_name if collection_name else DEFAULT_COLLECTION_NAME

    def init_db(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> bool:
        """Initialize the database with the input documents or records.
        It overwrites the existing collection in the database.

        It takes the following steps,
        1. Set up ChromaDB and LlamaIndex storage.
        2. insert documents and build indexes upon them.

        Args:
            new_doc_dir: a dir of input documents that are used to create the records in database.
            new_doc_paths_or_urls:
                a sequence of input documents that are used to create the records in database.
                a document can be a path to a file or a url.
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments

        Returns:
            bool: True if initialization is successful

        """
        self._set_up(overwrite=True)
        documents = self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths_or_urls)
        self.index = VectorStoreIndex.from_documents(documents=documents, storage_context=self.storage_context)
        return True

    def connect_db(self, *args: Any, **kwargs: Any) -> bool:
        """Connect to the database.
        It does not overwrite the existing collection in the database.

         It takes the following steps,
        1. Set up ChromaDB and LlamaIndex storage.
        2. Create the llamaIndex vector store index for querying or inserting docs later

        Args:
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments

        Returns:
            bool: True if connection is successful
        """
        self._set_up(overwrite=False)
        self.index = VectorStoreIndex.from_vector_store(
            vector_store=self.vector_store, storage_context=self.storage_context
        )

        return True

    def add_docs(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        """Add new documents to the underlying database and add to the index.

        Args:
            new_doc_dir: A dir of input documents that are used to create the records in database.
            new_doc_paths_or_urls: A sequence of input documents that are used to create the records in database. A document can be a path to a file or a url.
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments
        """
        self._validate_query_index()
        documents = self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths_or_urls)
        for doc in documents:
            self.index.insert(doc)

    def query(self, question: str) -> str:
        """Retrieve information from indexed documents by processing a query using the engine's LLM.

        Args:
            question: A natural language query string used to search the indexed documents.

        Returns:
            A string containing the response generated by LLM.
        """
        self._validate_query_index()
        self.query_engine = self.index.as_query_engine(llm=self.llm)
        response = self.query_engine.query(question)

        if str(response) == EMPTY_RESPONSE_TEXT:
            return EMPTY_RESPONSE_REPLY

        return str(response)

    def get_collection_name(self) -> str:
        """Get the name of the collection used by the query engine.

        Returns:
            The name of the collection.
        """
        if self.collection_name:
            return self.collection_name
        else:
            raise ValueError("Collection name not set.")

    def _validate_query_index(self) -> None:
        """Ensures an index exists"""
        if not hasattr(self, "index"):
            raise Exception("Query index is not initialized. Please call init_db or connect_db first.")

    def _set_up(self, overwrite: bool) -> None:
        """Set up ChromaDB and LlamaIndex storage by:
        1. Initialize the ChromaDB using VectorDBFactory and create a collection with the given name.
        2. Create the LlamaIndex vector store and storage context for the collection.

        Args:
            overwrite: If True, overwrite the existing collection with the same name.
        """
        self.vector_db = VectorDBFactory().create_vector_db(db_type="chroma", **self.db_config)
        self.collection = self.vector_db.create_collection(collection_name=self.collection_name, overwrite=overwrite)
        self.vector_store = ChromaVectorStore(chroma_collection=self.collection)
        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)

    def _load_doc(  # type: ignore[no-any-unimported]
        self, input_dir: Path | str | None, input_docs: Sequence[Path | str] | None
    ) -> Sequence["LlamaDocument"]:
        """Load documents from a directory and/or a sequence of file paths.

        It uses LlamaIndex's SimpleDirectoryReader that supports multiple file[formats]((https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/#supported-file-types)).

        Args:
            input_dir (Optional[Union[Path, str]]): The directory containing documents to be loaded.
                If provided, all files in the directory will be considered.
            input_docs (Optional[Sequence[Union[Path, str]]]): A sequence of individual file paths to load.
                Each path must point to an existing file.

        Returns:
            A sequence of documents loaded as LlamaDocument objects.

        Raises:
            ValueError: If the specified directory does not exist.
            ValueError: If any provided file path does not exist.
            ValueError: If neither input_dir nor input_docs is provided.
        """
        loaded_documents = []
        if input_dir:
            logger.info(f"Loading docs from directory: {input_dir}")
            if not os.path.exists(input_dir):
                raise ValueError(f"Input directory not found: {input_dir}")
            loaded_documents.extend(SimpleDirectoryReader(input_dir=input_dir).load_data())

        if input_docs:
            for doc in input_docs:
                logger.info(f"Loading input doc: {doc}")
                if not os.path.exists(doc):
                    raise ValueError(f"Document file not found: {doc}")
            loaded_documents.extend(SimpleDirectoryReader(input_files=input_docs).load_data())  # type: ignore[arg-type]

        if not input_dir and not input_docs:
            raise ValueError("No input directory or docs provided!")

        return loaded_documents


# mypy will fail if ChromaDBQueryEngine does not implement RAGQueryEngine protocol
if TYPE_CHECKING:
    from .query_engine import RAGQueryEngine

    def _check_implement_protocol(o: ChromaDBQueryEngine) -> RAGQueryEngine:
        return o
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
from collections.abc import Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import

with optional_import_block():
    from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex
    from llama_index.core.llms import LLM
    from llama_index.core.schema import Document as LlamaDocument
    from llama_index.core.vector_stores.types import BasePydanticVectorStore
    from llama_index.llms.openai import OpenAI

__all__ = ["LlamaIndexQueryEngine"]


EMPTY_RESPONSE_TEXT = "Empty Response"  # Indicates that the query did not return any results
EMPTY_RESPONSE_REPLY = "Sorry, I couldn't find any information on that. If you haven't ingested any documents, please try that."  # Default response for queries without results


# Set up logging
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


@require_optional_import("llama_index", "rag")
@export_module("autogen.agentchat.contrib.rag")
class LlamaIndexQueryEngine:
    """This engine leverages LlamaIndex's VectorStoreIndex to efficiently index and retrieve documents, and generate an answer in response
    to natural language queries. It use any LlamaIndex [vector store](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/).

    By default the engine will use OpenAI's GPT-4o model (use the `llm` parameter to change that).
    """

    def __init__(  # type: ignore[no-any-unimported]
        self,
        vector_store: "BasePydanticVectorStore",
        llm: Optional["LLM"] = None,
        file_reader_class: type["SimpleDirectoryReader"] | None = None,
    ) -> None:
        """Initializes the LlamaIndexQueryEngine with the given vector store.

        Args:
            vector_store: The vector store to use for indexing and querying documents.
            llm: LLM model used by LlamaIndex for query processing. You can find more supported LLMs at [LLM](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/).
            file_reader_class: The file reader class to use for loading documents. Only SimpleDirectoryReader is currently supported.
        """
        self.llm: LLM = llm or OpenAI(model="gpt-4o", temperature=0.0)  # type: ignore[no-any-unimported]
        self.vector_store = vector_store
        self.file_reader_class = file_reader_class if file_reader_class else SimpleDirectoryReader

    def init_db(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> bool:
        """Initialize the database with the input documents or records.

        It takes the following steps:
        1. Set up LlamaIndex storage context.
        2. insert documents and build an index upon them.

        Args:
            new_doc_dir: a dir of input documents that are used to create the records in database.
            new_doc_paths_or_urls: A sequence of input documents that are used to create the records in database. A document can be a Path to a file or a url.
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments

        Returns:
            bool: True if initialization is successful

        """
        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
        documents = self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths_or_urls)
        self.index = VectorStoreIndex.from_documents(documents=documents, storage_context=self.storage_context)
        return True

    def connect_db(self, *args: Any, **kwargs: Any) -> bool:
        """Connect to the database.
        It sets up the LlamaIndex storage and create an index from the existing vector store.

        Args:
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments

        Returns:
            bool: True if connection is successful
        """
        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
        self.index = VectorStoreIndex.from_vector_store(
            vector_store=self.vector_store, storage_context=self.storage_context
        )

        return True

    def add_docs(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        """Add new documents to the underlying database and add to the index.

        Args:
            new_doc_dir: A dir of input documents that are used to create the records in database.
            new_doc_paths_or_urls: A sequence of input documents that are used to create the records in database. A document can be a Path to a file or a url.
            *args: Any additional arguments
            **kwargs: Any additional keyword arguments
        """
        self._validate_query_index()
        documents = self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths_or_urls)
        for doc in documents:
            self.index.insert(doc)

    def query(self, question: str) -> str:
        """Retrieve information from indexed documents by processing a query using the engine's LLM.

        Args:
            question: A natural language query string used to search the indexed documents.

        Returns:
            A string containing the response generated by LLM.
        """
        self._validate_query_index()
        self.query_engine = self.index.as_query_engine(llm=self.llm)
        response = self.query_engine.query(question)

        if str(response) == EMPTY_RESPONSE_TEXT:
            return EMPTY_RESPONSE_REPLY

        return str(response)

    def _validate_query_index(self) -> None:
        """Ensures an index exists"""
        if not hasattr(self, "index"):
            raise Exception("Query index is not initialized. Please call init_db or connect_db first.")

    def _load_doc(  # type: ignore[no-any-unimported]
        self, input_dir: Path | str | None, input_docs: Sequence[Path | str] | None
    ) -> Sequence["LlamaDocument"]:
        """Load documents from a directory and/or a sequence of file paths.

        Default to uses LlamaIndex's SimpleDirectoryReader that supports multiple file[formats](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/#supported-file-types).

        Args:
            input_dir (Optional[Union[Path, str]]): The directory containing documents to be loaded.
                If provided, all files in the directory will be considered.
            input_docs (Optional[Sequence[Union[Path, str]]]): A sequence of individual file paths to load.
                Each path must point to an existing file.

        Returns:
            A sequence of documents loaded as LlamaDocument objects.

        Raises:
            ValueError: If the specified directory does not exist.
            ValueError: If any provided file path does not exist.
            ValueError: If neither input_dir nor input_docs is provided.
        """
        loaded_documents: list[LlamaDocument] = []  # type: ignore[no-any-unimported]
        if input_dir:
            logger.info(f"Loading docs from directory: {input_dir}")
            if not os.path.exists(input_dir):
                raise ValueError(f"Input directory not found: {input_dir}")
            loaded_documents.extend(self.file_reader_class(input_dir=input_dir).load_data())  # type: ignore[operator]

        if input_docs:
            for doc in input_docs:
                logger.info(f"Loading input doc: {doc}")
                if not os.path.exists(doc):
                    raise ValueError(f"Document file not found: {doc}")
            loaded_documents.extend(self.file_reader_class(input_files=input_docs).load_data())  # type: ignore[operator, arg-type]

        if not input_dir and not input_docs:
            raise ValueError("No input directory or docs provided!")

        return loaded_documents


# mypy will fail if LlamaIndexQueryEngine does not implement RAGQueryEngine protocol
if TYPE_CHECKING:
    from .query_engine import RAGQueryEngine

    def _check_implement_protocol(o: LlamaIndexQueryEngine) -> RAGQueryEngine:
        return o
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
from collections.abc import Callable, Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional, Union

from autogen.agentchat.contrib.vectordb.base import VectorDBFactory
from autogen.agentchat.contrib.vectordb.mongodb import MongoDBAtlasVectorDB
from autogen.doc_utils import export_module
from autogen.import_utils import optional_import_block, require_optional_import

with optional_import_block():
    from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex
    from llama_index.core.embeddings import BaseEmbedding
    from llama_index.core.schema import Document as LlamaDocument
    from llama_index.llms.langchain.base import LLM  # type: ignore[attr-defined]
    from llama_index.llms.openai import OpenAI
    from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch
    from pymongo import MongoClient
    from sentence_transformers import SentenceTransformer

__all__ = ["MongoDBQueryEngine"]

DEFAULT_COLLECTION_NAME = "docling-parsed-docs"
EMPTY_RESPONSE_TEXT = "Empty Response"  # Indicates that the query did not return any results
EMPTY_RESPONSE_REPLY = "Sorry, I couldn't find any information on that. If you haven't ingested any documents, please try that."  # Default response for queries without results

# Set up logging
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


@require_optional_import(["pymongo", "llama_index", "sentence_transformers"], "rag")
@export_module("autogen.agentchat.contrib.rag")
class MongoDBQueryEngine:
    """A query engine backed by MongoDB Atlas that supports document insertion and querying.

    This engine initializes a vector database, builds an index from input documents,
    and allows querying using the chat engine interface.

    Attributes:
        vector_db (MongoDBAtlasVectorDB): The MongoDB vector database instance.
        vector_search_engine (MongoDBAtlasVectorSearch): The vector search engine.
        storage_context (StorageContext): The storage context for the vector store.
        index (Optional[VectorStoreIndex]): The index built from the documents.
    """

    def __init__(  # type: ignore[no-any-unimported]
        self,
        connection_string: str,
        llm: Optional["LLM"] = None,
        database_name: str | None = None,
        embedding_function: Union["BaseEmbedding", Callable[..., Any]] | None = None,  # type: ignore[type-arg]
        embedding_model: Union["BaseEmbedding", str] | None = None,
        collection_name: str | None = None,
    ):
        """Initializes a MongoDBQueryEngine instance.

        Args:
            connection_string (str): Connection string used to connect to MongoDB.
            llm (Optional[LLM]): Language model for querying. Defaults to an OpenAI model if not provided.
            database_name (Optional[str]): Name of the MongoDB database.
            embedding_function (Optional[Union["BaseEmbedding", Callable[..., Any]]]): Custom embedding function. If None (default),
                defaults to SentenceTransformer encoding.
            embedding_model (Optional[Union["BaseEmbedding", str]]): Embedding model identifier or instance. If None (default),
                "local:all-MiniLM-L6-v2" will be used.
            collection_name (Optional[str]): Name of the MongoDB collection. If None (default), `DEFAULT_COLLECTION_NAME` will be used.

        Raises:
            ValueError: If no connection string is provided.
        """
        if not connection_string:
            raise ValueError("Connection string is required to connect to MongoDB.")

        self.connection_string = connection_string
        # ToDo: Is it okay if database_name is None?
        self.database_name = database_name
        self.collection_name = collection_name or DEFAULT_COLLECTION_NAME
        self.llm: LLM = llm or OpenAI(model="gpt-4o", temperature=0.0)  # type: ignore[no-any-unimported]
        self.embedding_model = embedding_model or "local:all-MiniLM-L6-v2"  # type: ignore[no-any-unimported]

        # encode is a method of SentenceTransformer, so we need to use a type ignore here.
        self.embedding_function = embedding_function or SentenceTransformer("all-MiniLM-L6-v2").encode  # type: ignore[call-overload]

        # These will be initialized later.
        self.vector_db: MongoDBAtlasVectorDB | None = None
        self.vector_search_engine: MongoDBAtlasVectorSearch | None = None  # type: ignore[no-any-unimported]
        self.storage_context: StorageContext | None = None  # type: ignore[no-any-unimported]
        self.index: VectorStoreIndex | None = None  # type: ignore[no-any-unimported]

    def _set_up(self, overwrite: bool) -> None:
        """Sets up the MongoDB vector database, search engine, and storage context.

        This method initializes the vector database using the provided connection details,
        creates a vector search engine instance, and sets the storage context for indexing.

        Args:
            overwrite (bool): Flag indicating whether to overwrite the existing collection.
        """
        logger.info("Setting up the database.")
        self.vector_db: MongoDBAtlasVectorDB = VectorDBFactory.create_vector_db(  # type: ignore[assignment, no-redef]
            db_type="mongodb",
            connection_string=self.connection_string,
            database_name=self.database_name,
            embedding_function=self.embedding_function,
            collection_name=self.collection_name,
            overwrite=overwrite,  # new parameter to control creation behavior
        )
        logger.info("Vector database created.")
        self.vector_search_engine = MongoDBAtlasVectorSearch(
            mongodb_client=self.vector_db.client,  # type: ignore[union-attr]
            db_name=self.database_name,  # type: ignore[arg-type]
            collection_name=self.collection_name,
        )
        logger.info("Vector search engine created.")
        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_search_engine)

    def _check_existing_collection(self) -> bool:
        """Checks if the specified collection exists in the MongoDB database.

        Returns:
            bool: True if the collection exists; False otherwise.
        """
        client: MongoClient[Any] = MongoClient(self.connection_string)  # type: ignore[no-any-unimported]
        db = client[self.database_name]  # type: ignore[index]
        return self.collection_name in db.list_collection_names()

    def connect_db(self, *args: Any, **kwargs: Any) -> bool:
        """Connects to the MongoDB database and initializes the query index from the existing collection.

        This method verifies the existence of the collection, sets up the database connection,
        builds the vector store index, and pings the MongoDB server.

        Returns:
            bool: True if connection is successful; False otherwise.
        """
        try:
            # Check if the target collection exists.
            if not self._check_existing_collection():
                raise ValueError(
                    f"Collection '{self.collection_name}' not found in database '{self.database_name}'. "
                    "Please run init_db to create a new collection."
                )
            # Reinitialize without overwriting the existing collection.
            self._set_up(overwrite=False)

            self.index = VectorStoreIndex.from_vector_store(
                vector_store=self.vector_search_engine,  # type: ignore[arg-type]
                storage_context=self.storage_context,
                embed_model=self.embedding_model,
            )

            self.vector_db.client.admin.command("ping")  # type: ignore[union-attr]
            logger.info("Connected to MongoDB successfully.")
            return True
        except Exception as error:
            logger.error("Failed to connect to MongoDB: %s", error)
            return False

    def init_db(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> bool:
        """Initializes the MongoDB database by creating or overwriting the collection and indexing documents.

        This method loads documents from a directory or provided file paths, sets up the database (optionally
        overwriting any existing collection), builds the vector store index, and inserts the documents.

        Args:
            new_doc_dir (Optional[Union[Path, str]]): Directory containing documents to be indexed.
            new_doc_paths_or_urls (Optional[Sequence[Union[Path, str]]]): List of file paths or URLs for documents.
            *args (Any): Additional positional arguments.
            **kwargs (Any): Additional keyword arguments.

        Returns:
            bool: True if the database is successfully initialized; False otherwise.
        """
        try:
            # Check if the collection already exists.
            if self._check_existing_collection():
                logger.warning(
                    f"Collection '{self.collection_name}' already exists in database '{self.database_name}'. "
                    "Please use connect_db to connect to the existing collection or use init_db to overwrite it."
                )
            # Set up the database with overwriting.
            self._set_up(overwrite=True)
            self.vector_db.client.admin.command("ping")  # type: ignore[union-attr]
            # Gather document paths.
            logger.info("Setting up the database with existing collection.")
            documents = self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths_or_urls)
            self.index = VectorStoreIndex.from_vector_store(
                vector_store=self.vector_search_engine,  # type: ignore[arg-type]
                storage_context=self.storage_context,
                embed_model=self.embedding_model,
            )
            for doc in documents:
                self.index.insert(doc)
            logger.info("Database initialized with %d documents.", len(documents))
            return True
        except Exception as e:
            logger.error("Failed to initialize the database: %s", e)
            return False

    def _validate_query_index(self) -> None:
        """Validates that the query index is initialized.

        Raises:
            Exception: If the query index is not initialized.
        """
        if not hasattr(self, "index"):
            raise Exception("Query index is not initialized. Please call init_db or connect_db first.")

    def _load_doc(  # type: ignore[no-any-unimported]
        self, input_dir: Path | str | None, input_docs: Sequence[Path | str] | None
    ) -> Sequence["LlamaDocument"]:
        """Loads documents from a directory or a list of file paths.

        Args:
            input_dir (Optional[Union[Path, str]]): Directory from which to load documents.
            input_docs (Optional[Sequence[Union[Path, str]]]): List of document file paths or URLs.

        Returns:
            Sequence[LlamaDocument]: A sequence of loaded LlamaDocument objects.

        Raises:
            ValueError: If the input directory or any specified document file does not exist.
        """
        loaded_documents = []
        if input_dir:
            logger.info("Loading docs from directory: %s", input_dir)
            if not os.path.exists(input_dir):
                raise ValueError(f"Input directory not found: {input_dir}")
            loaded_documents.extend(SimpleDirectoryReader(input_dir=input_dir).load_data())

        if input_docs:
            for doc in input_docs:
                logger.info("Loading input doc: %s", doc)
                if not os.path.exists(doc):
                    raise ValueError(f"Document file not found: {doc}")
            loaded_documents.extend(SimpleDirectoryReader(input_files=input_docs).load_data())  # type: ignore[arg-type]
        if not input_dir and not input_docs:
            raise ValueError("No input directory or docs provided!")

        return loaded_documents

    def add_docs(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        """Adds new documents to the existing vector store index.

        This method validates that the index exists, loads documents from the specified directory or file paths,
        and inserts them into the vector store index.

        Args:
            new_doc_dir (Optional[Union[Path, str]]): Directory containing new documents.
            new_doc_paths_or_urls (Optional[Sequence[Union[Path, str]]]): List of file paths or URLs for new documents.
            *args (Any): Additional positional arguments.
            **kwargs (Any): Additional keyword arguments.
        """
        self._validate_query_index()
        documents = self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths_or_urls)
        for doc in documents:
            self.index.insert(doc)  # type: ignore[union-attr]

    def query(self, question: str, *args: Any, **kwargs: Any) -> Any:  # type: ignore[no-any-unimported, type-arg]
        """Queries the indexed documents using the provided question.

        This method validates that the query index is initialized, creates a query engine from the vector store index,
        and executes the query. If the response is empty, a default reply is returned.

        Args:
            question (str): The query question.
            args (Any): Additional positional arguments.
            kwargs (Any): Additional keyword arguments.

        Returns:
            Any: The query response as a string, or a default reply if no results are found.
        """
        self._validate_query_index()
        self.query_engine = self.index.as_query_engine(llm=self.llm)  # type: ignore[union-attr]
        response = self.query_engine.query(question)

        if str(response) == EMPTY_RESPONSE_TEXT:
            return EMPTY_RESPONSE_REPLY

        return str(response)

    def get_collection_name(self) -> str:
        """Retrieves the name of the MongoDB collection.

        Returns:
            str: The collection name.

        Raises:
            ValueError: If the collection name is not set.
        """
        if self.collection_name:
            return self.collection_name
        else:
            raise ValueError("Collection name not set.")


if TYPE_CHECKING:
    from .query_engine import RAGQueryEngine

    def _check_implement_protocol(o: MongoDBQueryEngine) -> RAGQueryEngine:
        return o
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import os
import re
from collections.abc import Callable
from time import sleep
from typing import Any, Literal

from pydantic import BaseModel, root_validator

from ...code_utils import UNKNOWN, execute_code, extract_code, infer_lang
from ...import_utils import optional_import_block, require_optional_import
from ...math_utils import get_answer
from .. import Agent, UserProxyAgent

with optional_import_block() as result:
    import wolframalpha

PROMPTS = {
    # default
    "default": """Let's use Python to solve a math problem.

Query requirements:
You should always use the 'print' function for the output and use fractions/radical forms instead of decimals.
You can use packages like sympy to help you.
You must follow the formats below to write your code:
```python
# your code
```

First state the key idea to solve the problem. You may choose from three ways to solve the problem:
Case 1: If the problem can be solved with Python code directly, please write a program to solve it. You can enumerate all possible arrangements if needed.
Case 2: If the problem is mostly reasoning, you can solve it by yourself directly.
Case 3: If the problem cannot be handled in the above two ways, please follow this process:
1. Solve the problem step by step (do not over-divide the steps).
2. Take out any queries that can be asked through Python (for example, any calculations or equations that can be calculated).
3. Wait for me to give the results.
4. Continue if you think the result is correct. If the result is invalid or unexpected, please correct your query or reasoning.

After all the queries are run and you get the answer, put the answer in \\boxed{}.

Problem:
""",
    # select python or wolfram
    "two_tools": """Let's use two tools (Python and Wolfram alpha) to solve a math problem.

Query requirements:
You must follow the formats below to write your query:
For Wolfram Alpha:
```wolfram
# one wolfram query
```
For Python:
```python
# your code
```
When using Python, you should always use the 'print' function for the output and use fractions/radical forms instead of decimals. You can use packages like sympy to help you.
When using wolfram, give one query in each code block.

Please follow this process:
1. Solve the problem step by step (do not over-divide the steps).
2. Take out any queries that can be asked through Python or Wolfram Alpha, select the most suitable tool to be used (for example, any calculations or equations that can be calculated).
3. Wait for me to give the results.
4. Continue if you think the result is correct. If the result is invalid or unexpected, please correct your query or reasoning.

After all the queries are run and you get the answer, put the final answer in \\boxed{}.

Problem: """,
    # use python step by step
    "python": """Let's use Python to solve a math problem.

Query requirements:
You should always use the 'print' function for the output and use fractions/radical forms instead of decimals.
You can use packages like sympy to help you.
You must follow the formats below to write your code:
```python
# your code
```

Please follow this process:
1. Solve the problem step by step (do not over-divide the steps).
2. Take out any queries that can be asked through Python (for example, any calculations or equations that can be calculated).
3. Wait for me to give the results.
4. Continue if you think the result is correct. If the result is invalid or unexpected, please correct your query or reasoning.

After all the queries are run and you get the answer, put the answer in \\boxed{}.

Problem: """,
}


def _is_termination_msg_mathchat(message):
    """Check if a message is a termination message."""
    if isinstance(message, dict):
        message = message.get("content")
        if message is None:
            return False
    cb = extract_code(message)
    contain_code = False
    for c in cb:
        if c[0] == "python" or c[0] == "wolfram":
            contain_code = True
            break
    return not contain_code and get_answer(message) is not None and get_answer(message) != ""


def _add_print_to_last_line(code):
    """Add print() to the last line of a string."""
    # 1. check if there is already a print statement
    if "print(" in code:
        return code
    # 2. extract the last line, enclose it in print() and return the new string
    lines = code.splitlines()
    last_line = lines[-1]
    if "\t" in last_line or "=" in last_line:
        return code
    if "=" in last_line:
        last_line = "print(" + last_line.split(" = ")[0] + ")"
        lines.append(last_line)
    else:
        lines[-1] = "print(" + last_line + ")"
    # 3. join the lines back together
    return "\n".join(lines)


def _remove_print(code):
    """Remove all print statements from a string."""
    lines = code.splitlines()
    lines = [line for line in lines if not line.startswith("print(")]
    return "\n".join(lines)


class MathUserProxyAgent(UserProxyAgent):
    """(Experimental) A MathChat agent that can handle math problems."""

    MAX_CONSECUTIVE_AUTO_REPLY = 15  # maximum number of consecutive auto replies (subject to future change)
    DEFAULT_REPLY = "Continue. Please keep solving the problem until you need to query. (If you get to the answer, put it in \\boxed{}.)"

    def __init__(
        self,
        name: str | None = "MathChatAgent",  # default set to MathChatAgent
        is_termination_msg: Callable[[dict[str, Any]], bool]
        | None = _is_termination_msg_mathchat,  # terminate if \boxed{} in message
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "NEVER",  # Fully automated
        default_auto_reply: str | dict[str, Any] | None = DEFAULT_REPLY,
        max_invalid_q_per_step=3,  # a parameter needed in MathChat
        **kwargs: Any,
    ):
        """Args:
        name (str): name of the agent
        is_termination_msg (function): a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.
            The dict can contain the following keys: "content", "role", "name", "function_call".
        human_input_mode (str): whether to ask for human inputs every time a message is received.
            Possible values are "ALWAYS", "TERMINATE", "NEVER".
            (1) When "ALWAYS", the agent prompts for human input every time a message is received.
                Under this mode, the conversation stops when the human input is "exit",
                or when is_termination_msg is True and there is no human input.
            (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or
                the number of auto reply reaches the max_consecutive_auto_reply.
            (3) (Default) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops
                when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
        default_auto_reply (str or dict or None): the default auto reply message when no code execution or llm based reply is generated.
        max_invalid_q_per_step (int): (ADDED) the maximum number of invalid queries per step.
        **kwargs (dict): other kwargs in [UserProxyAgent](/docs/api-reference/autogen/UserProxyAgent#userproxyagent).
        """
        super().__init__(
            name=name,
            is_termination_msg=is_termination_msg,
            human_input_mode=human_input_mode,
            default_auto_reply=default_auto_reply,
            **kwargs,
        )
        self.register_reply([Agent, None], MathUserProxyAgent._generate_math_reply, position=2)
        # fixed var
        self._max_invalid_q_per_step = max_invalid_q_per_step

        # mutable
        self._valid_q_count = 0
        self._total_q_count = 0
        self._accum_invalid_q_per_step = 0
        self._previous_code = ""
        self.last_reply = None

    @staticmethod
    def message_generator(sender, recipient, context):
        """Generate a prompt for the assistant agent with the given problem and prompt.

        Args:
            sender (Agent): the sender of the message.
            recipient (Agent): the recipient of the message.
            context (dict): a dictionary with the following fields:
                problem (str): the problem to be solved.
                prompt_type (str, Optional): the type of the prompt. Possible values are "default", "python", "wolfram".
                    (1) "default": the prompt that allows the agent to choose between 3 ways to solve a problem:
                        1. write a python program to solve it directly.
                        2. solve it directly without python.
                        3. solve it step by step with python.
                    (2) "python":
                        a simplified prompt from the third way of the "default" prompt, that asks the assistant
                        to solve the problem step by step with python.
                    (3) "two_tools":
                        a simplified prompt similar to the "python" prompt, but allows the model to choose between
                        Python and Wolfram Alpha to solve the problem.
                customized_prompt (str, Optional): a customized prompt to be used. If it is not None, the prompt_type will be ignored.

        Returns:
            str: the generated prompt ready to be sent to the assistant agent.
        """
        sender._reset()
        problem = context.get("problem")
        prompt_type = context.get("prompt_type", "default")
        customized_prompt = context.get("customized_prompt", None)
        if customized_prompt is not None:
            return customized_prompt + problem
        return PROMPTS[prompt_type] + problem

    def _reset(self):
        # super().reset()
        self._valid_q_count = 0
        self._total_q_count = 0
        self._accum_invalid_q_per_step = 0
        self._previous_code = ""
        self.last_reply = None

    def execute_one_python_code(self, pycode):
        """Execute python code blocks.

        Previous python code will be saved and executed together with the new code.
        the "print" function will also be added to the last line of the code if needed
        """
        # Need to replace all "; " with "\n" to avoid syntax error when adding `print` to the last line
        pycode = pycode.replace("; ", "\n").replace(";", "\n")
        pycode = self._previous_code + _add_print_to_last_line(pycode)

        return_code, output, _ = execute_code(pycode, **self._code_execution_config, timeout=5)
        is_success = return_code == 0

        if not is_success:
            # Remove the file information from the error string
            pattern = r'File "/[^"]+\.py", line \d+, in .+\n'
            if isinstance(output, str):
                output = re.sub(pattern, "", output)
            output = "Error: " + output
        elif output == "":
            # Check if there is any print statement
            if "print" not in pycode:
                output = "No output found. Make sure you print the results."
                is_success = False
            else:
                output = "No output found."
                is_success = True

        if len(output) > 2000:
            output = "Your requested query response is too long. You might have made a mistake. Please revise your reasoning and query."
            is_success = False

        if is_success:
            # remove print and check if it still works
            tmp = self._previous_code + "\n" + _remove_print(pycode) + "\n"
            rcode, _, _ = execute_code(tmp, **self._code_execution_config)
        else:
            # only add imports and check if it works
            tmp = self._previous_code + "\n"
            for line in pycode.split("\n"):
                if "import" in line:
                    tmp += line + "\n"
            rcode, _, _ = execute_code(tmp, **self._code_execution_config)

        if rcode == 0:
            self._previous_code = tmp
        return output, is_success

    def execute_one_wolfram_query(self, query: str):
        """Run one wolfram query and return the output.

        Args:
            query: string of the query.

        Returns:
            output: string with the output of the query.
            is_success: boolean indicating whether the query was successful.
        """
        # wolfram query handler
        wolfram = WolframAlphaAPIWrapper()
        output, is_success = wolfram.run(query)
        if output == "":
            output = "Error: The wolfram query is invalid."
            is_success = False
        return output, is_success

    def _generate_math_reply(
        self,
        messages: list[dict] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ):
        """Generate an auto reply."""
        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        message = message.get("content", "")
        code_blocks = extract_code(message)

        if len(code_blocks) == 1 and code_blocks[0][0] == UNKNOWN:
            # no code block is found, lang should be `UNKNOWN``
            return True, self._default_auto_reply
        is_success, all_success = True, True
        reply = ""
        for code_block in code_blocks:
            lang, code = code_block
            if not lang:
                lang = infer_lang(code)
            if lang == "python":
                output, is_success = self.execute_one_python_code(code)
            elif lang == "wolfram":
                output, is_success = self.execute_one_wolfram_query(code)
            else:
                output = "Error: Unknown language."
                is_success = False

            reply += output + "\n"
            if not is_success:
                all_success = False
                self._valid_q_count -= 1  # count invalid queries

        reply = reply.strip()

        if self.last_reply == reply:
            return True, reply + "\nYour query or result is same from the last, please try a new approach."
        self.last_reply = reply

        if not all_success:
            self._accum_invalid_q_per_step += 1
            if self._accum_invalid_q_per_step > self._max_invalid_q_per_step:
                self._accum_invalid_q_per_step = 0
                reply = "Please revisit the problem statement and your reasoning. If you think this step is correct, solve it yourself and continue the next step. Otherwise, correct this step."

        return True, reply


# Modified based on langchain. Langchain is licensed under MIT License:
# The MIT License

# Copyright (c) Harrison Chase

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.


def get_from_dict_or_env(data: dict[str, Any], key: str, env_key: str, default: str | None = None) -> str:
    """Get a value from a dictionary or an environment variable."""
    if data.get(key):
        return data[key]
    elif os.environ.get(env_key):
        return os.environ[env_key]
    elif default is not None:
        return default
    else:
        raise ValueError(
            f"Did not find {key}, please add an environment variable"
            f" `{env_key}` which contains it, or pass"
            f"  `{key}` as a named parameter."
        )


class WolframAlphaAPIWrapper(BaseModel):
    """Wrapper for Wolfram Alpha.

    Docs for using:

    1. Go to wolfram alpha and sign up for a developer account
    2. Create an app and get your APP ID
    3. Save your APP ID into WOLFRAM_ALPHA_APPID env variable
    4. pip install wolframalpha

    """

    wolfram_client: Any  #: :meta private:
    wolfram_alpha_appid: str | None = None

    @root_validator(skip_on_failure=True)
    @classmethod
    @require_optional_import("wolframalpha", "mathchat")
    def validate_environment(cls, values: dict) -> dict:
        """Validate that api key and python package exists in environment."""
        wolfram_alpha_appid = get_from_dict_or_env(values, "wolfram_alpha_appid", "WOLFRAM_ALPHA_APPID")
        values["wolfram_alpha_appid"] = wolfram_alpha_appid

        client = wolframalpha.Client(wolfram_alpha_appid)
        values["wolfram_client"] = client

        return values

    def run(self, query: str) -> tuple[str, bool]:
        """Run query through WolframAlpha and parse result."""
        from urllib.error import HTTPError

        is_success = False  # added
        res = None
        for _ in range(20):
            try:
                res = self.wolfram_client.query(query)
                break
            except HTTPError:
                sleep(1)
            except Exception:
                return (
                    "Wolfram Alpha wasn't able to answer it. Please try a new query for wolfram or use python.",
                    is_success,
                )
        if res is None:
            return (
                "Wolfram Alpha wasn't able to answer it (may due to web error), you can try again or use python.",
                is_success,
            )

        try:
            if not res["@success"]:
                return (
                    "Your Wolfram query is invalid. Please try a new query for wolfram or use python.",
                    is_success,
                )
            assumption = next(res.pods).text
            answer = ""
            for result in res["pod"]:
                if result["@title"] == "Solution":
                    answer = result["subpod"]["plaintext"]
                if result["@title"] == "Results" or result["@title"] == "Solutions":
                    for i, sub in enumerate(result["subpod"]):
                        answer += f"ans {i}: " + sub["plaintext"] + "\n"
                    break
            if answer == "":
                answer = next(res.results).text

        except Exception:
            return (
                "Wolfram Alpha wasn't able to answer it. Please try a new query for wolfram or use python.",
                is_success,
            )

        if answer is None or answer == "":
            # We don't want to return the assumption alone if answer is empty
            return "No good Wolfram Alpha Result was found", is_success
        is_success = True
        return f"Assumption: {assumption} \nAnswer: {answer}", is_success
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any, Literal

from ...llm_config import LLMConfig
from ..agent import Agent
from ..assistant_agent import ConversableAgent

system_message = """You are an expert in text analysis.
The user will give you TEXT to analyze.
The user will give you analysis INSTRUCTIONS copied twice, at both the beginning and the end.
You will follow these INSTRUCTIONS in analyzing the TEXT, then give the results of your expert analysis in the format requested."""


class TextAnalyzerAgent(ConversableAgent):
    """(Experimental) Text Analysis agent, a subclass of ConversableAgent designed to analyze text as instructed."""

    def __init__(
        self,
        name="analyzer",
        system_message: str | None = system_message,
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "NEVER",
        llm_config: LLMConfig | dict[str, Any] | bool | None = None,
        **kwargs: Any,
    ):
        """Args:
        name (str): name of the agent.
        system_message (str): system message for the ChatCompletion inference.
        human_input_mode (str): This agent should NEVER prompt the human for input.
        llm_config (LLMConfig or dict or False): llm inference configuration.
            Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create)
            for available options.
            To disable llm-based auto reply, set to False.
        **kwargs (dict): other kwargs in [ConversableAgent](/docs/api-reference/autogen/ConversableAgent#conversableagent).
        """
        super().__init__(
            name=name,
            system_message=system_message,
            human_input_mode=human_input_mode,
            llm_config=llm_config,
            **kwargs,
        )
        self.register_reply(Agent, TextAnalyzerAgent._analyze_in_reply, position=2)

    def _analyze_in_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Analyzes the given text as instructed, and returns the analysis as a message.
        Assumes exactly two messages containing the text to analyze and the analysis instructions.
        See Teachability.analyze for an example of how to use this method.
        """
        if self.llm_config is False:
            raise ValueError("TextAnalyzerAgent requires self.llm_config to be set in its base class.")
        if messages is None:
            messages = self._oai_messages[sender]  # In case of a direct call.
        assert len(messages) == 2

        # Delegate to the analysis method.
        return True, self.analyze_text(messages[0]["content"], messages[1]["content"])

    def analyze_text(self, text_to_analyze, analysis_instructions):
        """Analyzes the given text as instructed, and returns the analysis."""
        # Assemble the message.
        text_to_analyze = "# TEXT\n" + text_to_analyze + "\n"
        analysis_instructions = "# INSTRUCTIONS\n" + analysis_instructions + "\n"
        msg_text = "\n".join([
            analysis_instructions,
            text_to_analyze,
            analysis_instructions,
        ])  # Repeat the instructions.
        # Generate and return the analysis string.
        return self.generate_oai_reply([{"role": "user", "content": msg_text}], None, None)[1]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def detect_outlier_iqr(csv_file: str, column_name: str):
    """Detect outliers in a specified column of a CSV file using the IQR method.

    Args:
    csv_file (str): The path to the CSV file.
    column_name (str): The name of the column to detect outliers in.

    Returns:
    list: A list of row indices that correspond to the outliers.
    """
    import pandas as pd

    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(csv_file)

    # Calculate the quartiles and IQR for the specified column
    q1 = df[column_name].quantile(0.25)
    q3 = df[column_name].quantile(0.75)
    iqr = q3 - q1

    # Find the outliers based on the defined criteria
    outliers = df[(df[column_name] < q1 - 1.5 * iqr) | (df[column_name] > q3 + 1.5 * iqr)]

    # Return the row indices of the outliers
    return outliers.index.tolist()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def explore_csv(file_path, num_lines=5):
    """Reads a CSV file and prints the column names, shape, data types, and the first few lines of data.

    Args:
        file_path (str): The path to the CSV file.
        num_lines (int, optional): The number of lines to print. Defaults to 5.
    """
    import pandas as pd

    df = pd.read_csv(file_path)
    header = df.columns
    print("Columns:")
    print(", ".join(header))
    print("Shape:", df.shape)
    print("Data Types:")
    print(df.dtypes)
    print("First", num_lines, "lines:")
    print(df.head(num_lines))
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def calculate_correlation(csv_path: str, column1: str, column2: str, method: str = "pearson") -> float:
    """Calculate the correlation between two columns in a CSV file.

    Args:
    csv_path (str): The path to the CSV file.
    column1 (str): The name of the first column.
    column2 (str): The name of the second column.
    method (str or callable, optional): The method used to calculate the correlation.
        - 'pearson' (default): Pearson correlation coefficient.
        - 'kendall': Kendall Tau correlation coefficient.
        - 'spearman': Spearman rank correlation coefficient.
        - callable: A custom correlation function that takes two arrays and returns a scalar.

    Returns:
    float: The correlation coefficient between the two columns.
    """
    import pandas as pd

    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(csv_path)

    # Select the specified columns
    selected_columns = df[[column1, column2]]

    # Calculate the correlation based on the specified method
    if method == "pearson":
        correlation = selected_columns.corr().iloc[0, 1]
    elif method == "kendall":
        correlation = selected_columns.corr(method="kendall").iloc[0, 1]
    elif method == "spearman":
        correlation = selected_columns.corr(method="spearman").iloc[0, 1]
    elif callable(method):
        correlation = selected_columns.corr(method=method).iloc[0, 1]
    else:
        raise ValueError("Invalid correlation method. Please choose 'pearson', 'kendall', 'spearman', or a callable.")

    return correlation
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["pandas", "scipy"])
def shapiro_wilk_test(csv_file, column_name):
    """Perform the Shapiro-Wilk test on a specified column of a CSV file.

    Args:
    csv_file (str): The path to the CSV file.
    column_name (str): The name of the column to perform the test on.

    Returns:
    float: The p-value resulting from the Shapiro-Wilk test.
    """
    import pandas as pd
    from scipy.stats import shapiro

    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(csv_file)

    # Extract the specified column as a numpy array
    column_data = df[column_name].values

    # Perform the Shapiro-Wilk test
    _, p_value = shapiro(column_data)

    return p_value
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def calculate_skewness_and_kurtosis(csv_file: str, column_name: str) -> tuple:
    """Calculate the skewness and kurtosis of a specified column in a CSV file. The kurtosis is calculated using the Fisher definition.
    The two metrics are computed using scipy.stats functions.

    Args:
    csv_file (str): The path to the CSV file.
    column_name (str): The name of the column to calculate skewness and kurtosis for.

    Returns:
    tuple: (skewness, kurtosis)
    """
    import pandas as pd
    from scipy.stats import kurtosis, skew

    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(csv_file)

    # Extract the specified column
    column = df[column_name]

    # Calculate the skewness and kurtosis
    skewness = skew(column)
    kurt = kurtosis(column)

    return skewness, kurt
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def detect_outlier_zscore(csv_file, column_name, threshold=3):
    """Detect outliers in a CSV file based on a specified column. The outliers are determined by calculating the z-score of the data points in the column.

    Args:
    csv_file (str): The path to the CSV file.
    column_name (str): The name of the column to calculate z-scores for.
    threshold (float, optional): The threshold value for determining outliers. By default set to 3.

    Returns:
    list: A list of row indices where the z-score is above the threshold.
    """
    import numpy as np
    import pandas as pd

    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(csv_file)

    # Calculate the z-score for the specified column
    z_scores = np.abs((df[column_name] - df[column_name].mean()) / df[column_name].std())

    # Find the row indices where the z-score is above the threshold
    outlier_indices = np.where(z_scores > threshold)[0]

    # Return the row indices of the outliers
    return outlier_indices
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

__all__: list[str] = []
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["sympy"])
def calculate_circle_area_from_diameter(diameter):
    """Calculate the area of a circle given its diameter.

    Args:
    diameter (float): The diameter of the circle.

    Returns:
    float: The area of the circle.
    """
    from sympy import pi

    radius = diameter / 2
    area = pi * radius**2
    return area
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def calculate_fraction_sum(
    fraction1_numerator: int, fraction1_denominator: int, fraction2_numerator: int, fraction2_denominator: int
):
    """Calculates the sum of two fractions and returns the result as a mixed number.

    Args:
        fraction1_numerator: The numerator of the first fraction.
        fraction1_denominator: The denominator of the first fraction.
        fraction2_numerator: The numerator of the second fraction.
        fraction2_denominator: The denominator of the second fraction.

    Returns:
        str: The sum of the two fractions as a mixed number in the format 'a b/c'
    """
    from fractions import Fraction

    fraction1 = Fraction(fraction1_numerator, fraction1_denominator)
    fraction2 = Fraction(fraction2_numerator, fraction2_denominator)
    result = fraction1 + fraction2
    mixed_number = result.numerator // result.denominator
    mixed_fraction_numerator = result.numerator % result.denominator
    if mixed_fraction_numerator > 0:
        return f"{mixed_number} {Fraction(mixed_fraction_numerator, result.denominator)}"
    else:
        return str(mixed_number)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def evaluate_expression(expression):
    """Evaluates a mathematical expression with support for floor function notation and power notation.

    Args:
        expression (str): The mathematical expression to evaluate. It can only contain one symbol 'x'.

    Returns:
        Union[sympy.Expr, str]: The evaluated result as a sympy expression if successful,
        otherwise an error message as a string.

    """
    from sympy import symbols, sympify

    # Replace power with ** for sympy
    expression = expression.replace("^", "**")
    # Replace the floor function notation
    expression = expression.replace("\\lfloor", "floor(").replace("\\rfloor", ")")
    try:
        # Create a symbol 'x' for use in case it is in the expression
        symbols("x")
        # Evaluate the expression
        result = sympify(expression)
        return result
    except Exception as e:
        return str(e)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def calculate_reflected_point(point):
    """Calculates the reflection point of a given point about the line y=x.

    Args:
        point (dict): A dictionary representing the coordinates of the point.
            The dictionary should have keys 'x' and 'y' representing the x and y coordinates respectively.

    Returns:
        dict: A dictionary representing the coordinates of the reflected point. Its keys are 'x' and 'y'.
    """
    # Swap x and y for reflection about y=x
    reflected_point = {"x": point["y"], "y": point["x"]}
    return reflected_point
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from contextlib import suppress


def modular_inverse_sum(expressions, modulus):
    """Calculates the sum of modular inverses of the given expressions modulo the specified modulus.

    Args:
        expressions (list): A list of numbers for which the modular inverses need to be calculated.
        modulus (int): The modulus value.

    Returns:
        int: The sum of modular inverses modulo the specified modulus.
    """
    from sympy import mod_inverse

    mod_sum = 0
    for number in expressions:
        with suppress(ValueError):
            mod_sum += mod_inverse(number, modulus)
    return mod_sum % modulus
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def calculate_day_of_the_week(total_days: int, starting_day: str):
    """Calculates the day of the week after a given number of days starting from a specified day.

    Args:
        total_days: The number of days to calculate.
        starting_day: The starting day of the week, should be one of 'Monday', 'Tuesday', 'Wednesday', etc.

    Returns:
        str: The day of the week after the specified number of days.
    """
    days_of_week = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]

    start_index = days_of_week.index(starting_day)
    end_index = (start_index + total_days) % 7
    return days_of_week[end_index]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["sympy"])
def calculate_matrix_power(matrix, power):
    """Calculate the power of a given matrix.

    Args:
        matrix (list): An array of numbers that represents the matrix.
        power (int): The power to which the matrix is raised.

    Returns:
        Matrix: The resulting matrix after raising to power.

    Raises:
        ValueError: If the power is negative and the matrix is not invertible.
    """
    from sympy import Matrix, eye

    m = Matrix(matrix)
    if power == 0:
        return eye(m.shape[0])
    elif power < 0:
        if not m.is_invertible():
            raise ValueError("Matrix is not invertible.")
        return m.inverse() ** (-power)
    elif power > 0:
        return m**power
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["sympy"])
def complex_numbers_product(complex_numbers):
    """Calculates the product of a list of complex numbers.

    Args:
        complex_numbers (list): A list of dictionaries representing complex numbers.
            Each dictionary should have 'real' and 'imag' keys representing the real
            and imaginary parts of the complex number.

    Returns:
        complex: The simplified product of the complex numbers.

    """
    from sympy import I, simplify

    result = 1
    for c in complex_numbers:
        result *= c["real"] + I * c["imag"]
    return simplify(result)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def fraction_to_mixed_numbers(numerator, denominator):
    """Simplifies a fraction to its lowest terms and returns it as a mixed number.

    Args:
        numerator (int): The numerator of the fraction.
        denominator (int): The denominator of the fraction.

    Returns:
        str: The simplified fraction as a string. If the fraction is already an integer, it returns the integer as a string.
             If the fraction is a proper fraction, it returns the mixed number representation as a string.
             If the numerator or denominator is not an integer, it returns an error message.
             If the denominator is zero, it returns an error message.
    """
    from sympy import Rational

    # Ensure that numerator and denominator are integers
    if not isinstance(numerator, int) or not isinstance(denominator, int):
        return "Error: Numerator and denominator must be integers."

    # Handle the case where the denominator is zero
    if denominator == 0:
        return "Error: Denominator cannot be zero."

    # Simplify the fraction to its lowest terms
    result = Rational(numerator, denominator)
    # Return the result as a mixed number if needed
    if result.is_integer:
        return str(int(result))
    else:
        # Result as a mixed number
        integer_part = int(result)
        fractional_part = result - integer_part
        if fractional_part != 0:
            return f"{integer_part} {fractional_part}"
        else:
            return str(integer_part)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def simplify_mixed_numbers(numerator1, denominator1, numerator2, denominator2, whole_number1, whole_number2):
    """Simplifies the sum of two mixed numbers and returns the result as a string in the format 'a b/c'.

    Args:
        numerator1 (int): The numerator of the first fraction.
        denominator1 (int): The denominator of the first fraction.
        numerator2 (int): The numerator of the second fraction.
        denominator2 (int): The denominator of the second fraction.
        whole_number1 (int): The whole number part of the first mixed number.
        whole_number2 (int): The whole number part of the second mixed number.

    Returns:
        str: The simplified sum of the two mixed numbers as a string in the format 'a b/c'.
    """
    from fractions import Fraction

    # Convert mixed numbers to improper fractions
    fraction1 = whole_number1 * denominator1 + numerator1
    fraction2 = whole_number2 * denominator2 + numerator2
    # Create Fraction objects
    frac1 = Fraction(fraction1, denominator1)
    frac2 = Fraction(fraction2, denominator2)
    # Calculate the sum
    result = frac1 + frac2
    # Convert to mixed number
    mixed_number = result.numerator // result.denominator
    mixed_fraction_numerator = result.numerator % result.denominator
    mixed_fraction = Fraction(mixed_fraction_numerator, result.denominator)
    # Return as a string in the format 'a b/c'
    if mixed_fraction_numerator > 0:
        return f"{mixed_number} {mixed_fraction}"
    else:
        return str(mixed_number)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["sympy"])
def compute_currency_conversion(amount, exchange_rate):
    """Compute the currency conversion of the given amount using the provided exchange rate.

    Args:
    amount (float): The amount to be converted.
    exchange_rate (float): The exchange rate to use for the conversion, represented as the amount of second currency equivalent to one unit of the first currency.

    Returns:
    float: The converted amount.

    """
    from sympy import Rational

    # Calculate the converted amount using the given exchange rate
    converted_amount = Rational(amount, exchange_rate)
    return float(converted_amount)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def sum_of_primes_below(threshold):
    """Calculates the sum of all prime numbers below a given threshold.

    Args:
        threshold (int): The maximum number (exclusive) up to which primes are summed.

    Returns:
        int: The sum of all prime numbers below the threshold.
    """
    from sympy import primerange

    return sum(primerange(2, threshold))
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def sum_of_digit_factorials(number):
    """Calculates the sum of the factorial of each digit in a number, often used in problems involving curious numbers like 145.

    Args:
        number (int): The number for which to calculate the sum of digit factorials.

    Returns:
        int: The sum of the factorials of the digits in the given number.
    """
    from math import factorial

    return sum(factorial(int(digit)) for digit in str(number))
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def find_continuity_point(f_leq, f_gt, x_value):
    """Find the value 'a' that ensures the continuity of a piecewise function at a given point.

    Args:
        f_leq (str): The function expression for f(x) when x is less than or equal to the continuity point, in the form of a string.
        f_gt (str): The function expression for f(x) when x is greater than the continuity point, in the form of a string.
        x_value (float): The x-value at which continuity is to be ensured.

    Returns:
        float or None: The value of 'a' that satisfies the continuity condition,
        or None if no such value exists.
    """
    from sympy import Eq, solve, symbols, sympify

    x, a = symbols("x a")

    # Convert string to sympy expression
    f_leq_expr = sympify(f_leq)
    f_gt_expr = sympify(f_gt)

    # Evaluate the expressions at the given x_value
    f_leq_value = f_leq_expr.subs(x, x_value)
    f_gt_value = f_gt_expr.subs(x, x_value)

    # Set up the equation for a
    equation = Eq(f_leq_value, f_gt_value)

    # Solve the equation
    a_value = solve(equation, a)

    return a_value[0] if a_value else None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def count_distinct_permutations(sequence):
    """Counts the number of distinct permutations of a sequence where items may be indistinguishable.

    Args:
        sequence (iterable): The sequence for which to count the distinct permutations.

    Returns:
        int: The number of distinct permutations.

    Example:
        >>> count_distinct_permutations("aab")
        3
        >>> count_distinct_permutations([1, 2, 2])
        3
    """
    from collections import Counter
    from math import factorial

    counts = Counter(sequence)
    total_length = sum(counts.values())
    permutations = factorial(total_length)
    for count in counts.values():
        permutations //= factorial(count)
    return permutations
# Introduction

This directory contains a library of manually created python tools. These tools have three categories: math, data_analysis and information_retrieval.

# Directory Layout
```
tools
 README.md
 data_analysis
    calculate_correlation.py
    ...
 information_retrieval
    arxiv_download.py
    arxiv_search.py
    ...
 math
    calculate_circle_area_from_diameter.py
    ...
 tool_description.tsv
```

Tools can be imported from `tools/{category}/{tool_name}.py` with exactly the same function name.

`tool_description.tsv` contains descriptions of tools for retrieval.

# How to use
Some tools require Bing Search API key and RapidAPI key. For Bing API, you can read more about how to get an API on the [Bing Web Search API](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api) page. For RapidAPI, you can [sign up](https://rapidapi.com/auth/sign-up) and subscribe to these two links([link1](https://rapidapi.com/solid-api-solid-api-default/api/youtube-transcript3), [link2](https://rapidapi.com/420vijay47/api/youtube-mp3-downloader2)). These apis have free billing options and there is no need to worry about extra costs.

To install the requirements for running tools, use pip install.
```bash
pip install -r autogen/agentchat/contrib/captainagent/tools/requirements.txt
```

Whenever you run the tool-related code, remember to export the api keys to system variables.
```bash
export BING_API_KEY=""
export RAPID_API_KEY=""
```
or
```python
import os
os.environ["BING_API_KEY"] = ""
os.environ["RAPID_API_KEY"] = ""
```
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def get_wikipedia_text(title):
    """Retrieves the text content of a Wikipedia page. It does not support tables and other complex formatting.

    Args:
        title (str): The title of the Wikipedia page.

    Returns:
        str or None: The text content of the Wikipedia page if it exists, None otherwise.
    """
    import wikipediaapi

    wiki_wiki = wikipediaapi.Wikipedia("Mozilla/5.0 (merlin@example.com)", "en")
    page = wiki_wiki.page(title)

    if page.exists():
        return page.text
    else:
        return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["PyMuPDF"])
def extract_pdf_text(pdf_path, page_number=None):
    """Extracts text from a specified page or the entire PDF file.

    Args:
        pdf_path (str): The path to the PDF file.
        page_number (int, optional): The page number to extract (starting from 0). If not provided,
            the function will extract text from the entire PDF file.

    Returns:
        str: The extracted text.
    """
    import fitz

    # Open the PDF file
    doc = fitz.open(pdf_path)

    # Extract text from the entire PDF file or a specific page
    text = ""
    if page_number is None:
        # Extract content from the entire PDF file
        for page in doc:
            text += page.get_text()
    else:
        # Extract content from a specific page
        page = doc[page_number]
        text = page.get_text()

    # Close the PDF file
    doc.close()

    return text
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import os

from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["transformers", "torch", "pillow"], ["transformers", "torch", "os"])
def image_qa(image, question, ckpt="Salesforce/blip-vqa-base"):
    """Perform question answering on an image using a pre-trained VQA model.

    Args:
        image (Union[str, Image.Image]): The image to perform question answering on. It can be either file path to the image or a PIL Image object.
        question: The question to ask about the image.
        ckpt: The checkpoint name to use. Default is "Salesforce/blip-vqa-base".

    Returns:
        dict: The generated answer text.
    """
    import torch
    from PIL import Image
    from transformers import BlipForQuestionAnswering, BlipProcessor

    def image_processing(img):
        if isinstance(img, Image.Image):
            return img.convert("RGB")
        elif isinstance(img, str):
            if os.path.exists(img):
                return Image.open(img).convert("RGB")
            else:
                full_path = img
                if os.path.exists(full_path):
                    return Image.open(full_path).convert("RGB")
                else:
                    raise FileNotFoundError

    def text_processing(file_path):
        # Check the file extension
        if file_path.endswith(".txt"):
            with open(file_path) as file:
                content = file.read()
        else:
            # if the file is not .txt, then it is a string, directly return the string
            return file_path
        return content

    image = image_processing(image)
    question = text_processing(question)

    processor = BlipProcessor.from_pretrained(ckpt)
    model = BlipForQuestionAnswering.from_pretrained(ckpt, torch_dtype=torch.float16).to("cuda")

    raw_image = image

    inputs = processor(raw_image, question, return_tensors="pt").to("cuda", torch.float16)
    out = model.generate(**inputs)
    result_formatted = processor.decode(out[0], skip_special_tokens=True)

    return result_formatted
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
# alternative api: https://rapidapi.com/omarmhaimdat/api/youtube-v2


def get_youtube_caption(video_id: str) -> str:
    """Retrieves the captions for a YouTube video.

    Args:
        video_id (str): The ID of the YouTube video.

    Returns:
        str: The captions of the YouTube video in text format.

    Raises:
        KeyError: If the RAPID_API_KEY environment variable is not set.
    """
    import os

    import requests

    rapid_api_key = os.environ["RAPID_API_KEY"]
    video_url = f"https://www.youtube.com/watch?v={video_id}"
    url = "https://youtube-transcript3.p.rapidapi.com/api/transcript-with-url"

    querystring = {"url": video_url, "lang": "en", "flat_text": "true"}

    headers = {"X-RapidAPI-Key": rapid_api_key, "X-RapidAPI-Host": "youtube-transcript3.p.rapidapi.com"}

    response = requests.get(url, headers=headers, params=querystring)
    response = response.json()
    print(response)
    return response["transcript"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import os

from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["PyMuPDF"], ["os"])
def extract_pdf_image(pdf_path: str, output_dir: str, page_number=None):
    """Extracts images from a PDF file and saves them to the specified output directory.

    Args:
        pdf_path (str): The path to the PDF file.
        output_dir (str): The directory to save the extracted images.
        page_number (int, optional): The page number to extract images from. If not provided, extract images from all pages.
    """
    import fitz  # PyMuPDF library

    # Open the PDF file
    doc = fitz.open(pdf_path)

    # Create the output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Extract images from the PDF file
    images = []
    if page_number is not None:
        page = doc[page_number - 1]  # Adjust page number to 0-based index
        for img in page.get_images():
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            images.append(image_bytes)
    else:
        for page in doc:
            for img in page.get_images():
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                images.append(image_bytes)

    # Save the extracted images
    for i, image_bytes in enumerate(images):
        image_path = os.path.join(output_dir, f"image_{i}.png")
        with open(image_path, "wb") as f:
            f.write(image_bytes)

    # Print the total number of images saved
    print(f"Saved a total of {len(images)} images")

    # Close the PDF file
    doc.close()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def youtube_download(url: str):
    """Downloads a YouTube video and returns the download link.

    Args:
        url: The URL of the YouTube video.

    Returns:
        str: The download link for the audio.
    """
    import os

    import requests

    endpoint = "https://youtube-mp3-downloader2.p.rapidapi.com/ytmp3/ytmp3/"

    querystring = {"url": url}

    headers = {
        "X-RapidAPI-Key": os.environ.get("RAPIDAPI_KEY"),
        "X-RapidAPI-Host": "youtube-mp3-downloader2.p.rapidapi.com",
    }

    response = requests.get(endpoint, headers=headers, params=querystring)
    response = response.json()

    if "link" in response:
        return response["link"]
    else:
        print("Error: Unable to retrieve download link.")
        print(response)
        # or you can return an error message
        # return "Error: Unable to retrieve download link."
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["arxiv"])
def arxiv_search(query, max_results=10, sortby="relevance"):
    """Search for articles on arXiv based on the given query.

    Args:
        query (str): The search query.
        max_results (int, optional): The maximum number of results to retrieve. Defaults to 10.
        sortby (str, optional): The sorting criterion for the search results. Can be 'relevance' or 'submittedDate'. Defaults to 'relevance'.

    Returns:
        list: A list of dictionaries containing information about the search results. Each dictionary contains the following keys:
            - 'title': The title of the article.
            - 'authors': The authors of the article.
            - 'summary': The summary of the article.
            - 'entry_id': The entry ID of the article.
            - 'doi': The DOI of the article (If applicable).
            - 'published': The publication date of the article in the format 'Y-M'.
    """
    import arxiv

    def get_author(r):
        return ", ".join(a.name for a in r.authors)

    criterion = {"relevance": arxiv.SortCriterion.Relevance, "submittedDate": arxiv.SortCriterion.SubmittedDate}[sortby]

    client = arxiv.Client()
    search = arxiv.Search(query=query, max_results=max_results, sort_by=criterion)
    res = []
    results = client.results(search)
    for r in results:
        print("Entry id:", r.entry_id)
        print("Title:", r.title)
        print("Authors:", get_author(r))
        print("DOI:", r.doi)
        print("Published:", r.published.strftime("%Y-%m"))
        # print("Summary:", r.summary)
        res.append({
            "title": r.title,
            "authors": get_author(r),
            "summary": r.summary,
            "entry_id": r.entry_id,
            "doi": r.doi,
            "published": r.published.strftime("%Y-%m"),
        })
    return res
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["arxiv"])
def arxiv_download(id_list: list[str], download_dir="./") -> list[str]:
    """Downloads PDF files from ArXiv based on a list of arxiv paper IDs.

    Args:
        id_list (list): A list of paper IDs to download. e.g. [2302.00006v1]
        download_dir (str, optional): The directory to save the downloaded PDF files. Defaults to './'.

    Returns:
        list: A list of paths to the downloaded PDF files.
    """
    import arxiv

    paths = []
    for paper in arxiv.Client().results(arxiv.Search(id_list=id_list)):
        path = paper.download_pdf(download_dir, filename=paper.get_short_id() + ".pdf")
        paths.append(path)
        print("Paper id:", paper.get_short_id(), "Downloaded to:", path)
    return paths
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["openai-whisper"])
def transcribe_audio_file(file_path):
    """Transcribes the audio file located at the given file path.

    Args:
        file_path (str): The path to the audio file.

    Returns:
        str: The transcribed text from the audio file.
    """
    import whisper

    model = whisper.load_model("base")
    result = model.transcribe(file_path)
    return result["text"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import os

from autogen.coding.func_with_reqs import with_requirements


@with_requirements(["easyocr"], ["os"])
def optical_character_recognition(image):
    """Perform optical character recognition (OCR) on the given image.

    Args:
        image (Union[str, Image.Image]): The image to perform OCR on. It can be either a file path or an Image object.

    Returns:
        str: The extracted text from the image.

    Raises:
        FileNotFoundError: If the image file path does not exist.
    """
    import io

    import easyocr
    from PIL import Image

    def image_processing(img):
        if isinstance(img, Image.Image):
            return img.convert("RGB")
        elif isinstance(img, str):
            if os.path.exists(img):
                return Image.open(img).convert("RGB")
            else:
                full_path = img
                if os.path.exists(full_path):
                    return Image.open(full_path).convert("RGB")
                else:
                    raise FileNotFoundError

    reader = easyocr.Reader(["en"])  # Load the OCR model into memory

    if isinstance(image, str):
        # If image is a path, use it directly
        if not os.path.exists(image):
            raise FileNotFoundError
        image_path_or_bytes = image
    else:
        # If image is an Image object, convert it to a bytes stream
        buffer = io.BytesIO()
        image = image_processing(image)  # Process the image if needed
        image.save(buffer, format="JPEG")
        buffer.seek(0)
        image_path_or_bytes = buffer

    # Read text from the image or image path
    result = reader.readtext(image_path_or_bytes)

    # Extract only the text from the result
    result_text = [text for _, text, _ in result]

    return ", ".join(result_text)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def perform_web_search(query, count=10, offset=0):
    """Perform a web search using Bing API.

    Args:
        query (str): The search query.
        count (int, optional): Number of search results to retrieve. Defaults to 10.
        offset (int, optional): Offset of the first search result. Defaults to 0.

    Returns:
        The name, URL and snippet of each search result.
    """
    import os

    import requests

    # Get the Bing API key from the environment variable
    bing_api_key = os.getenv("BING_API_KEY")

    # Check if the API key is available
    if not bing_api_key:
        raise ValueError("Bing API key not found in environment variable")

    # Set up the API request
    url = "https://api.bing.microsoft.com/v7.0/search"
    headers = {
        "Ocp-Apim-Subscription-Key": bing_api_key,
    }
    params = {
        "q": query,
        "count": count,  # Number of search results to retrieve
        "offset": offset,  # Offset of the first search result
    }

    # Send the API request
    response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()

    # Process the search results
    search_results = response.json()
    for index, result in enumerate(search_results["webPages"]["value"]):
        print(f"Search Result {index + 1}:")
        print(result["name"])
        print(result["url"])
        print(result["snippet"])
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
def scrape_wikipedia_tables(url: str, header_keyword: str):
    """Scrapes Wikipedia tables based on a given URL and header keyword.

    Args:
        url: The URL of the Wikipedia page to scrape.
        header_keyword: The keyword to search for in the headers of the page.

    Returns:
        list: A list of lists representing the scraped table data. Each inner list represents a row in the table,
              with each element representing a cell value.
    """
    import requests
    from bs4 import BeautifulSoup

    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, "html.parser")
    headers = soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6"])
    data = []
    for header in headers:
        if header_keyword.lower() in header.text.lower():
            table = header.find_next_sibling("table", class_="wikitable")
            if table:
                rows = table.find_all("tr")
                for row in rows:
                    cols = row.find_all(["th", "td"])
                    cols = [ele.text.strip() for ele in cols]
                    data.append([ele for ele in cols if ele])
                break
    return data
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import hashlib
import json
import os
from collections.abc import Callable
from typing import Any, Literal

from termcolor import colored

from .... import GroupChat, GroupChatManager, UserProxyAgent
from ....doc_utils import export_module
from ....llm_config import LLMConfig
from ...conversable_agent import ConversableAgent
from .agent_builder import AgentBuilder
from .tool_retriever import ToolBuilder, format_ag2_tool, get_full_tool_description


@export_module("autogen.agentchat.contrib.captainagent")
class CaptainAgent(ConversableAgent):
    """(In preview) Captain agent, designed to solve a task with an agent or a group of agents."""

    DEFAULT_NESTED_CONFIG = {
        "autobuild_init_config": {
            "config_file_or_env": "OAI_CONFIG_LIST",
            "builder_model": "gpt-4o",
            "agent_model": "gpt-4o",
        },
        "autobuild_build_config": {
            "default_llm_config": {"temperature": 1, "top_p": 0.95, "max_tokens": 2048},
            "code_execution_config": {
                "timeout": 300,
                "work_dir": "groupchat",
                "last_n_messages": 1,
                "use_docker": False,
            },
            "coding": True,
        },
        "group_chat_config": {"max_round": 10},
        "group_chat_llm_config": None,
        "max_turns": 5,
    }

    AUTOBUILD_TOOL = {
        "type": "function",
        "function": {
            "name": "seek_experts_help",
            "description": """Build a group of experts and let them chat with each other in a group chat.""",
            "parameters": {
                "type": "object",
                "properties": {
                    "group_name": {"type": "string", "description": "[REQUIRED] Name of the group."},
                    "building_task": {
                        "type": "string",
                        "description": """Instructions that help a build manager to build a group of experts.""",
                    },
                    "execution_task": {
                        "type": "string",
                        "description": """[REQUIRED] The task that needs the experts to solve by conversation.""",
                    },
                },
            },
        },
    }

    AUTOBUILD_SYSTEM_MESSAGE = """# Your role
You are a perfect manager of a group of advanced experts.

# How to solve the task
When a task is assigned to you:
1. Analysis of its constraints and conditions for completion.
2. Respond with a specific plan of how to solve the task.

After that, you can solve the task in two ways:
- Delegate the resolution of tasks to other experts created by seeking a group of experts for help and derive conclusive insights from their conversation summarization.
- Analysis and solve the task with your coding and language skills.

# How to seek experts help
The tool "seek_experts_help" can build a group of experts according to the building_task and let them chat with each other in a group chat to solve the execution_task you provided.
- This tool will summarize the essence of the experts' conversation and the derived conclusions.
- You should not modify any task information from meta_user_proxy, including code blocks, but you can provide extra information.
- Within a single response, you are limited to initiating one group of experts.

## building_task
This task helps a build manager to build a group of experts for your task.
You should suggest less then three roles (including a checker for verification) with the following format.

### Format
- [Detailed description for role 1]
- [Detailed description for role 2]
- [Detailed description for checker]

## execution_task
This is the task that needs the experts to solve by conversation.
You should Provide the following information in markdown format.

### Format
## Task description
...
## Plan for solving the task
...
## Output format
...
## Constraints and conditions for completion
...
## [Optional] results (including code blocks) and reason from last response
...

# After seek_experts_help
You will receive a comprehensive conclusion from the conversation, including the task information, results, reason for the results, conversation contradiction or issues, and additional information.
You **must** conduct a thorough verification for the result and reason's logical compliance by leveraging the step-by-step backward reasoning with the same group of experts (with the same group name) when:
- The conversation has contradictions or issues (need double-check marked as yes), or
- The result is different from the previous results.

Note that the previous experts will forget everything after you obtain the response from them. You should provide the results (including code blocks) you collected from the previous experts' response and put it in the new execution_task.

# Some useful instructions
- You only have one tool called "seek_experts_help".
- Provide a answer yourself after "seek_experts_help".
- You should suggest python code in a python coding block (```python...```). If you need to get the value of a variable, you must use the print statement.
- When using code, you must indicate the script type in the code block.
- Do not suggest incomplete code which requires users to modify.
- Be clear about which step uses code, which step uses your language skill, and which step to build a group chat.
- If the code's result indicates there is an error, fix the error and output the whole code again.
- If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
- Include verifiable evidence in your response if possible.
- After completing all tasks and verifications, you should conclude the operation and reply "TERMINATE"
"""

    DEFAULT_DESCRIPTION = "A helpful AI assistant that can build a group of agents at a proper time to solve a task."

    # This is used to prompt the LLM to summarize the conversation history between CaptainAgent's tool execution history
    DEFAULT_SUMMARY_PROMPT = "Read the following conversation history between an expert and a group of agent experts, summarize the conversation history. Your summarization should include the initial task, the experts' plan and the attempt, finally the results of the conversation. If the experts arrived at a conclusion, state it as it is without any modification."

    def __init__(
        self,
        name: str,
        system_message: str | None = None,
        llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None,
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        max_consecutive_auto_reply: int | None = None,
        human_input_mode: str | None = "NEVER",
        code_execution_config: dict[str, Any] | Literal[False] | None = False,
        nested_config: dict[str, Any] | None = None,
        agent_lib: str | None = None,
        tool_lib: str | None = None,
        agent_config_save_path: str | None = None,
        description: str | None = DEFAULT_DESCRIPTION,
        **kwargs: Any,
    ):
        """
        Args:\n
        name (str): agent name.\n
        system_message (str): system message for the ChatCompletion inference.\n
            Please override this attribute if you want to reprogram the agent.\n
        llm_config (LLMConfig or dict or False): llm inference configuration.\n
            Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create) for available options.\n
        is_termination_msg (function): a function that takes a message in the form of a dictionary\n
            and returns a boolean value indicating if this received message is a termination message.\n
            The dict can contain the following keys: "content", "role", "name", "function_call".\n
        max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.\n
            default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n
            The limit only plays a role when human_input_mode is not "ALWAYS".\n
        agent_lib (str): the path or a JSON file of the agent library for retrieving the nested chat instantiated by CaptainAgent.\n
        tool_lib (str): the path to the tool library for retrieving the tools used in the nested chat instantiated by CaptainAgent.\n
        nested_config (dict): the configuration for the nested chat instantiated by CaptainAgent.\n
            A full list of keys and their functionalities can be found in [docs](https://docs.ag2.ai/latest/docs/user-guide/reference-agents/captainagent).\n
        agent_config_save_path (str): the path to save the generated or retrieved agent configuration.\n
        **kwargs (dict): Please refer to other kwargs in\n
            [ConversableAgent](https://github.com/ag2ai/ag2/blob/main/autogen/agentchat/conversable_agent.py#L74).\n
        """
        super().__init__(
            name,
            is_termination_msg=is_termination_msg,
            max_consecutive_auto_reply=max_consecutive_auto_reply,
            human_input_mode=human_input_mode,
            code_execution_config=code_execution_config,
            llm_config=llm_config,
            description=description,
            **kwargs,
        )

        llm_config = LLMConfig.get_current_llm_config(llm_config)

        if system_message is None:
            system_message = self.AUTOBUILD_SYSTEM_MESSAGE
        nested_config = self._update_config(self.DEFAULT_NESTED_CONFIG, nested_config)
        if (
            "llm_config" not in nested_config["autobuild_init_config"]
            or nested_config["autobuild_init_config"]["llm_config"] is None
        ):
            nested_config["autobuild_init_config"]["llm_config"] = llm_config.copy()
        if nested_config["group_chat_llm_config"] is None:
            nested_config["group_chat_llm_config"] = llm_config.copy()
        if agent_lib:
            nested_config["autobuild_build_config"]["library_path_or_json"] = agent_lib
        if tool_lib:
            if "autobuild_tool_config" not in nested_config:
                nested_config["autobuild_tool_config"] = {}
            nested_config["autobuild_tool_config"]["tool_root"] = tool_lib

        self.assistant = ConversableAgent(name="CaptainAgent", system_message=system_message, llm_config=llm_config)
        self.assistant.update_tool_signature(self.AUTOBUILD_TOOL, is_remove=False)

        self.executor = CaptainUserProxyAgent(
            name="Expert_summoner",
            nested_config=nested_config,
            agent_config_save_path=agent_config_save_path,
            is_termination_msg=lambda x: x.get("content", "") and "terminate" in x.get("content", "").lower(),
            code_execution_config=code_execution_config,
            human_input_mode="NEVER",
        )

        self.register_nested_chats(
            [
                {
                    "sender": self.executor,
                    "recipient": self.assistant,
                    "max_turns": nested_config["max_turns"],
                    "summary_method": "reflection_with_llm",
                    "summary_args": {
                        "summary_prompt": self.DEFAULT_SUMMARY_PROMPT,
                    },
                }
            ],
            trigger=UserProxyAgent,
            position=0,
        )

    @staticmethod
    def _update_config(default_dict: dict[str, Any], update_dict: dict[str, Any] | None) -> dict[str, Any]:
        """Recursively updates the default_dict with values from update_dict."""
        if update_dict is None:
            return default_dict

        for key, value in update_dict.items():
            default_value = default_dict.get(key)
            if isinstance(default_value, dict) and isinstance(value, dict):
                # Recursively update nested dictionaries
                default_dict[key] = CaptainAgent._update_config(default_value, value)
            else:
                # Update the value or add new key
                default_dict[key] = value

        return default_dict


class CaptainUserProxyAgent(ConversableAgent):
    """(In preview) A proxy agent for the captain agent, that can execute code and provide feedback to the other agents."""

    CONVERSATION_REVIEW_PROMPT = """# Your task
Briefly summarize the conversation history derived from an experts' group chat by following the answer format.
If you found non-trivial errors or issues in the conversation, point it out with a detailed reason, if you think it is worth further verification, mark the "Need double-check" as "Yes"
If you find the conversation ends with TERMINATE and the task is solved, this is normal situation, you can mark the "Need double-check" as "No".

# Conversation history:
{chat_history}

# Answer format
## Task
...

## Results
...

## Reason for the results
...

## Errors or issues in the conversation
...

### Need to double-check?
[Yes or No]

## Additional information (file path, code blocks, url, etc.)
...
"""

    AUTOBUILD_TASK_DESC = """You are given: (1) a task and advises from your manager with a specific plan and (2) a general task.
Collect information from the general task, follow the suggestions from manager to solve the task.

# General Task
{general_task}

# Task and suggestions from manager
{manager_task} """

    DEFAULT_AUTO_REPLY = "I'm a proxy and I can only execute your tool or end the conversation. If you think the problem is solved, please reply me only with 'TERMINATE'."

    DEFAULT_USER_PROXY_AGENT_DESCRIPTIONS = {
        "ALWAYS": "An attentive HUMAN user who can answer questions about the task, and can perform tasks such as running Python code or inputting command line commands at a Linux terminal and reporting back the execution results.",
        "TERMINATE": "A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.",
        "NEVER": "A computer terminal that can running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks), or the conversation history and result of a group of agents",
    }

    def __init__(
        self,
        name: str,
        nested_config: dict[str, Any],
        agent_config_save_path: str = None,
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        max_consecutive_auto_reply: int | None = None,
        human_input_mode: str | None = "NEVER",
        code_execution_config: dict[str, Any] | Literal[False] | None = None,
        default_auto_reply: str | dict[str, Any] | None = DEFAULT_AUTO_REPLY,
        llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = False,
        system_message: str | list | None = "",
        description: str | None = None,
    ):
        """Args:
        name (str): name of the agent.
        nested_config (dict): the configuration for the nested chat instantiated by CaptainAgent.
        is_termination_msg (function): a function that takes a message in the form of a dictionary
            and returns a boolean value indicating if this received message is a termination message.
            The dict can contain the following keys: "content", "role", "name", "function_call".
        max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.
            default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).
            The limit only plays a role when human_input_mode is not "ALWAYS".
        human_input_mode (str): whether to ask for human inputs every time a message is received.
            Possible values are "ALWAYS", "TERMINATE", "NEVER".
            (1) When "ALWAYS", the agent prompts for human input every time a message is received.
                Under this mode, the conversation stops when the human input is "exit",
                or when is_termination_msg is True and there is no human input.
            (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or
                the number of auto reply reaches the max_consecutive_auto_reply.
            (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops
                when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
        code_execution_config (dict or False): config for the code execution.
            To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:
            - work_dir (Optional, str): The working directory for the code execution.
                If None, a default working directory will be used.
                The default working directory is the "extensions" directory under
                "path_to_autogen".
            - use_docker (Optional, list, str or bool): The docker image to use for code execution.
                Default is True, which means the code will be executed in a docker container. A default list of images will be used.
                If a list or a str of image name(s) is provided, the code will be executed in a docker container
                with the first image successfully pulled.
                If False, the code will be executed in the current environment.
                We strongly recommend using docker for code execution.
            - timeout (Optional, int): The maximum execution time in seconds.
            - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.
        default_auto_reply (str or dict or None): the default auto reply message when no code execution or llm based reply is generated.
        llm_config (LLMConfig or dict or False): llm inference configuration.
            Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create)
            for available options.
            Default to false, which disables llm-based auto reply.
        system_message (str or List): system message for ChatCompletion inference.
            Only used when llm_config is not False. Use it to reprogram the agent.
        description (str): a short description of the agent. This description is used by other agents
            (e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)
        """
        description = (
            description if description is not None else self.DEFAULT_USER_PROXY_AGENT_DESCRIPTIONS[human_input_mode]
        )
        super().__init__(
            name=name,
            system_message=system_message,
            is_termination_msg=is_termination_msg,
            max_consecutive_auto_reply=max_consecutive_auto_reply,
            human_input_mode=human_input_mode,
            code_execution_config=code_execution_config,
            llm_config=llm_config,
            default_auto_reply=default_auto_reply,
            description=description,
        )
        self.register_function(
            function_map={
                "seek_experts_help": lambda **args: self._run_autobuild(**args),
            }
        )
        self._agent_config_save_path = agent_config_save_path
        self._nested_config = nested_config.copy()
        self._code_execution_config = code_execution_config
        self.build_history = {}
        self.tool_history = {}
        self.build_times = 0

    def _run_autobuild(self, group_name: str, execution_task: str, building_task: str = "") -> str:
        """Build a group of agents by AutoBuild to solve the task.
        This function requires the nested_config to contain the autobuild_init_config, autobuild_llm_config, group_chat_llm_config.
        """
        print("==> Running AutoBuild...", flush=True)
        print("\n==> Building task: ", building_task, flush=True)
        print("\n==> Execution task: ", execution_task, flush=True)

        builder = AgentBuilder(**self._nested_config["autobuild_init_config"])
        # if the group is already built, load from history
        if group_name in self.build_history:
            agent_list, agent_configs = builder.load(config_json=json.dumps(self.build_history[group_name]))
            if self._nested_config.get("autobuild_tool_config", None) and agent_configs["coding"] is True:
                # tool library is enabled, reload tools and bind them to the agents
                tool_root_dir = self.tool_root_dir
                tool_builder = ToolBuilder(
                    corpus_root=tool_root_dir,
                    retriever=self._nested_config["autobuild_tool_config"].get("retriever", "all-mpnet-base-v2"),
                    type=self.tool_type,
                )
                for idx, agent in enumerate(agent_list):
                    if idx == len(self.tool_history[group_name]):
                        break
                    tool_builder.bind(agent, "\n\n".join(self.tool_history[group_name][idx]))
                agent_list[-1] = tool_builder.bind_user_proxy(agent_list[-1], tool_root_dir)
        else:
            if self._nested_config["autobuild_build_config"].get("library_path_or_json", None):
                # Build from retrieval
                agent_list, agent_configs = builder.build_from_library(
                    building_task, **self._nested_config["autobuild_build_config"]
                )
                self.build_history[group_name] = agent_configs.copy()

                if self._nested_config.get("autobuild_tool_config", None) and agent_configs["coding"] is True:
                    skills = building_task.split("\n")
                    if len(skills) == 0:
                        skills = [building_task]

                    tool_type = "default"
                    if self._nested_config["autobuild_tool_config"].get("tool_root", "default") == "default":
                        print(colored("==> Retrieving tools...", "green"), flush=True)
                        cur_path = os.path.dirname(os.path.abspath(__file__))
                        tool_root_dir = os.path.join(cur_path, "tools")
                    elif isinstance(self._nested_config["autobuild_tool_config"].get("tool_root", "default"), list):
                        # We get a list, in this case, we assume it contains several tools for the agents
                        tool_root_dir = self._nested_config["autobuild_tool_config"]["tool_root"]
                        tool_type = "user_defined"
                    else:
                        tool_root_dir = self._nested_config["autobuild_tool_config"]["tool_root"]
                    self.tool_root_dir = tool_root_dir
                    self.tool_type = tool_type

                    # Retrieve and build tools based on the smilarities between the skills and the tool description
                    tool_builder = ToolBuilder(
                        corpus_root=tool_root_dir,
                        retriever=self._nested_config["autobuild_tool_config"].get("retriever", "all-mpnet-base-v2"),
                        type=tool_type,
                    )
                    if tool_type == "default":
                        for idx, skill in enumerate(skills):
                            tools = tool_builder.retrieve(skill)
                            docstrings = []
                            for tool in tools:
                                category, tool_name = tool.split(" ")[0], tool.split(" ")[1]
                                tool_path = os.path.join(tool_root_dir, category, f"{tool_name}.py")
                                docstring = get_full_tool_description(tool_path)
                                docstrings.append(docstring)
                            tool_builder.bind(agent_list[idx], "\n\n".join(docstrings))
                        # the last agent is the user proxy agent, we need special treatment
                        agent_list[-1] = tool_builder.bind_user_proxy(agent_list[-1], tool_root_dir)
                    else:
                        # a list containing all the tools that the agents share
                        docstrings = []
                        for tool in tool_root_dir:
                            docstrings.append(format_ag2_tool(tool))
                        for idx, agent in enumerate(agent_list):
                            if idx == len(agent_list) - 1:
                                break
                            tool_builder.bind(agent, "\n\n".join(docstrings))
                        agent_list[-1] = tool_builder.bind_user_proxy(agent_list[-1], tool_root_dir)

                    # log tools
                    tool_history = self.tool_history.get(group_name, [])
                    tool_history.append(docstrings)
                    self.tool_history[group_name] = tool_history
            else:
                # Build agents from scratch
                agent_list, agent_configs = builder.build(
                    building_task, **self._nested_config["autobuild_build_config"]
                )
                self.build_history[group_name] = agent_configs.copy()

        if self._agent_config_save_path is not None:
            building_task_md5 = hashlib.md5(building_task.encode("utf-8")).hexdigest()
            with open(f"{self._agent_config_save_path}/build_history_{building_task_md5}.json", "w") as f:
                json.dump(self.build_history, f)

        self.build_times += 1
        # start nested chat
        nested_group_chat = GroupChat(
            agents=agent_list,
            messages=[],
            allow_repeat_speaker=agent_list[:-1] if agent_configs["coding"] is True else agent_list,
            **self._nested_config["group_chat_config"],
        )
        manager = GroupChatManager(
            groupchat=nested_group_chat,
            llm_config=self._nested_config["group_chat_llm_config"],
        )
        key = list(self.chat_messages.keys())[0]
        general_task = self.chat_messages[key][0]["content"]
        agent_list[0].initiate_chat(
            manager, message=self.AUTOBUILD_TASK_DESC.format(general_task=general_task, manager_task=execution_task)
        )
        chat_history = []
        key = list(agent_list[0].chat_messages.keys())[0]
        chat_messages = agent_list[0].chat_messages[key]
        for item in chat_messages:
            chat_history.append(item)

        # Review the group chat history
        summary_model = builder.builder_model
        summarized_history = (
            summary_model.create(
                messages=[
                    {
                        "role": "user",
                        "content": self.CONVERSATION_REVIEW_PROMPT.format(chat_history=chat_history),
                    }
                ]
            )
            .choices[0]
            .message.content
        )

        return f"# Response from seek_agent_help: \n{summarized_history}"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .agent_builder import AgentBuilder
from .captainagent import CaptainAgent
from .tool_retriever import ToolBuilder, format_ag2_tool, get_full_tool_description

__all__ = ["AgentBuilder", "CaptainAgent", "ToolBuilder", "format_ag2_tool", "get_full_tool_description"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import hashlib
import importlib
import json
import logging
import re
import subprocess as sp
import time
from typing import Any

from termcolor import colored

from .... import AssistantAgent, ConversableAgent, OpenAIWrapper, UserProxyAgent
from ....code_utils import CODE_BLOCK_PATTERN
from ....doc_utils import export_module
from ....llm_config import LLMConfig

__all__ = ["AgentBuilder"]

logger = logging.getLogger(__name__)


def _config_check(config: dict):
    # check config loading
    assert config.get("coding") is not None, 'Missing "coding" in your config.'
    assert config.get("default_llm_config") is not None, 'Missing "default_llm_config" in your config.'
    assert config.get("code_execution_config") is not None, 'Missing "code_execution_config" in your config.'

    for agent_config in config["agent_configs"]:
        assert agent_config.get("name", None) is not None, 'Missing agent "name" in your agent_configs.'
        assert agent_config.get("system_message", None) is not None, (
            'Missing agent "system_message" in your agent_configs.'
        )
        assert agent_config.get("description", None) is not None, 'Missing agent "description" in your agent_configs.'


def _retrieve_json(text):
    match = re.findall(CODE_BLOCK_PATTERN, text, flags=re.DOTALL)
    if not match:
        return text
    code_blocks = []
    for _, code in match:
        code_blocks.append(code)
    return code_blocks[0]


@export_module("autogen.agentchat.contrib.captainagent")
class AgentBuilder:
    """AgentBuilder can help user build an automatic task solving process powered by multi-agent system.
    Specifically, our building pipeline includes initialize and build.
    """

    online_server_name = "online"

    DEFAULT_PROXY_AUTO_REPLY = 'There is no code from the last 1 message for me to execute. Group chat manager should let other participants to continue the conversation. If the group chat manager want to end the conversation, you should let other participant reply me only with "TERMINATE"'

    GROUP_CHAT_DESCRIPTION = """ # Group chat instruction
You are now working in a group chat with different expert and a group chat manager.
You should refer to the previous message from other participant members or yourself, follow their topic and reply to them.

**Your role is**: {name}
Group chat members: {members}{user_proxy_desc}

When the task is complete and the result has been carefully verified, after obtaining agreement from the other members, you can end the conversation by replying only with "TERMINATE".

# Your profile
{sys_msg}
"""

    DEFAULT_DESCRIPTION = """## Your role
[Complete this part with expert's name and skill description]

## Task and skill instructions
- [Complete this part with task description]
- [Complete this part with skill description]
- [(Optional) Complete this part with other information]
"""

    CODING_AND_TASK_SKILL_INSTRUCTION = """## Useful instructions for task-solving
- Solve the task step by step if you need to.
- When you find an answer, verify the answer carefully. Include verifiable evidence with possible test case in your response if possible.
- All your reply should be based on the provided facts.

## How to verify?
**You have to keep believing that everyone else's answers are wrong until they provide clear enough evidence.**
- Verifying with step-by-step backward reasoning.
- Write test cases according to the general task.

## How to use code?
- Suggest python code (in a python coding block) or shell script (in a sh coding block) for the Computer_terminal to execute.
- If missing python packages, you can install the package by suggesting a `pip install` code in the ```sh ... ``` block.
- When using code, you must indicate the script type in the coding block.
- Do not the coding block which requires users to modify.
- Do not suggest a coding block if it's not intended to be executed by the Computer_terminal.
- The Computer_terminal cannot modify your code.
- **Use 'print' function for the output when relevant**.
- Check the execution result returned by the Computer_terminal.
- Do not ask Computer_terminal to copy and paste the result.
- If the result indicates there is an error, fix the error and output the code again. """

    CODING_PROMPT = """Does the following task need programming (i.e., access external API or tool by coding) to solve,
or coding may help the following task become easier?

TASK: {task}

Answer only YES or NO.
"""

    AGENT_NAME_PROMPT = """# Your task
Suggest no more than {max_agents} experts with their name according to the following user requirement.

## User requirement
{task}

# Task requirement
- Expert's name should follow the format: [skill]_Expert.
- Only reply the names of the experts, separated by ",".
- If coding skills are required, they should be limited to Python and Shell.
For example: Python_Expert, Math_Expert, ... """

    AGENT_SYS_MSG_PROMPT = """# Your goal
- According to the task and expert name, write a high-quality description for the expert by filling the given template.
- Ensure that your description are clear and unambiguous, and include all necessary information.

# Task
{task}

# Expert name
{position}

# Template
{default_sys_msg}
"""

    AGENT_DESCRIPTION_PROMPT = """# Your goal
Summarize the following expert's description in a sentence.

# Expert name
{position}

# Expert's description
{sys_msg}
"""

    AGENT_SEARCHING_PROMPT = """# Your goal
Considering the following task, what experts should be involved to the task?

# TASK
{task}

# EXPERT LIST
{agent_list}

# Requirement
- You should consider if the experts' name and profile match the task.
- Considering the effort, you should select less then {max_agents} experts; less is better.
- Separate expert names by commas and use "_" instead of space. For example, Product_manager,Programmer
- Only return the list of expert names.
"""

    AGENT_SELECTION_PROMPT = """# Your goal
Match roles in the role set to each expert in expert set.

# Skill set
{skills}

# Expert pool (formatting with name: description)
{expert_pool}

# Answer format
```json
{{
    "skill_1 description": "expert_name: expert_description", // if there exists an expert that suitable for skill_1
    "skill_2 description": "None", // if there is no experts that suitable for skill_2
    ...
}}
```
"""

    def __init__(
        self,
        config_file_or_env: str | None = "OAI_CONFIG_LIST",
        config_file_location: str | None = "",
        llm_config: LLMConfig | dict[str, Any] | None = None,
        builder_model: str | list | None = [],
        agent_model: str | list | None = [],
        builder_model_tags: list | None = [],
        agent_model_tags: list | None = [],
        max_agents: int | None = 5,
    ):
        """(These APIs are experimental and may change in the future.)

        Args:
            config_file_or_env (Optional[str], optional): Path to the config file or name of the environment
                variable containing the OpenAI API configurations. Defaults to "OAI_CONFIG_LIST".
            config_file_location (Optional[str], optional): Location of the config file if not in the
                current directory. Defaults to "".
            llm_config (Optional[Union[LLMConfig, dict[str, Any]]], optional): Specific configs for LLM
            builder_model (Optional[Union[str, list]], optional): Model identifier(s) to use as the
                builder/manager model that coordinates agent creation. Can be a string or list of strings.
                Filters the config list to match these models. Defaults to [].
            agent_model (Optional[Union[str, list]], optional): Model identifier(s) to use for the
                generated participant agents. Can be a string or list of strings. Defaults to [].
            builder_model_tags (Optional[list], optional): Tags to filter which models from the config
                can be used as builder models. Defaults to [].
            agent_model_tags (Optional[list], optional): Tags to filter which models from the config
                can be used as agent models. Defaults to [].
            max_agents (Optional[int], optional): Maximum number of agents to create for each task.
                Defaults to 5.
        """
        builder_model = builder_model if isinstance(builder_model, list) else [builder_model]
        builder_filter_dict = {}
        if len(builder_model) != 0:
            builder_filter_dict.update({"model": builder_model})
        if len(builder_model_tags) != 0:
            builder_filter_dict.update({"tags": builder_model_tags})

        llm_config = (
            LLMConfig.from_json(env=config_file_or_env, file_location=config_file_location).where(**builder_filter_dict)
            if llm_config is None
            else llm_config
        )
        builder_config_list = llm_config.config_list

        if len(builder_config_list) == 0:
            raise RuntimeError(
                f"Fail to initialize build manager: {builder_model}{builder_model_tags} does not exist in {config_file_or_env}. "
                f'If you want to change this model, please specify the "builder_model" in the constructor.'
            )
        self.builder_model = OpenAIWrapper(config_list=builder_config_list)

        self.agent_model = agent_model if isinstance(agent_model, list) else [agent_model]
        self.agent_model_tags = agent_model_tags
        self.config_file_or_env = config_file_or_env
        self.config_file_location = config_file_location
        self.llm_config = llm_config

        self.building_task: str = None
        self.agent_configs: list[dict[str, Any]] = []
        self.open_ports: list[str] = []
        self.agent_procs: dict[str, tuple[sp.Popen, str]] = {}
        self.agent_procs_assign: dict[str, tuple[ConversableAgent, str]] = {}
        self.cached_configs: dict = {}

        self.max_agents = max_agents

    def set_builder_model(self, model: str):
        self.builder_model = model

    def set_agent_model(self, model: str):
        self.agent_model = model

    def _create_agent(
        self,
        agent_config: dict[str, Any],
        member_name: list[str],
        llm_config: LLMConfig | dict[str, Any],
        use_oai_assistant: bool | None = False,
    ) -> AssistantAgent:
        """Create a group chat participant agent.

        If the agent rely on an open-source model, this function will automatically set up an endpoint for that agent.
        The API address of that endpoint will be "localhost:{free port}".

        Args:
            agent_config: agent's config. It should include the following information:
                1. model_name: backbone model of an agent, e.g., gpt-4-1106-preview, meta/Llama-2-70b-chat
                2. agent_name: use to identify an agent in the group chat.
                3. system_message: including persona, task solving instruction, etc.
                4. description: brief description of an agent that help group chat manager to pick the speaker.
            member_name: a list of agent names in the group chat.
            llm_config: specific configs for LLM (e.g., config_list, seed, temperature, ...).
            use_oai_assistant: use OpenAI assistant api instead of self-constructed agent.

        Returns:
            agent: a set-up agent.
        """
        model_name_or_hf_repo = agent_config.get("model", [])
        model_name_or_hf_repo = (
            model_name_or_hf_repo if isinstance(model_name_or_hf_repo, list) else [model_name_or_hf_repo]
        )
        model_tags = agent_config.get("tags", [])
        agent_name = agent_config["name"]
        system_message = agent_config["system_message"]
        description = agent_config["description"]

        # Path to the customize **ConversableAgent** class.
        agent_path = agent_config.get("agent_path")
        filter_dict = {}
        if len(model_name_or_hf_repo) > 0:
            filter_dict.update({"model": model_name_or_hf_repo})
        if len(model_tags) > 0:
            filter_dict.update({"tags": model_tags})
        config_list = (
            LLMConfig.from_json(env=self.config_file_or_env, file_location=self.config_file_location)
            .where(**filter_dict)
            .config_list
            if self.llm_config is None
            else self.llm_config.config_list
        )
        if len(config_list) == 0:
            raise RuntimeError(
                f"Fail to initialize agent {agent_name}: {model_name_or_hf_repo}{model_tags} does not exist in {self.config_file_or_env}.\n"
                f'If you would like to change this model, please specify the "agent_model" in the constructor.\n'
                f"If you load configs from json, make sure the model in agent_configs is in the {self.config_file_or_env}."
            )
        server_id = self.online_server_name
        current_config = llm_config.copy()
        current_config.update({"config_list": config_list})
        if use_oai_assistant:
            from ..gpt_assistant_agent import GPTAssistantAgent

            agent = GPTAssistantAgent(
                name=agent_name,
                llm_config={**current_config, "assistant_id": None},
                instructions=system_message,
                overwrite_instructions=False,
            )
        else:
            user_proxy_desc = ""
            if self.cached_configs["coding"] is True:
                user_proxy_desc = (
                    "\nThe group also include a Computer_terminal to help you run the python and shell code."
                )

            model_class = AssistantAgent
            if agent_path:
                module_path, model_class_name = agent_path.replace("/", ".").rsplit(".", 1)
                module = importlib.import_module(module_path)
                model_class = getattr(module, model_class_name)
                if not issubclass(model_class, ConversableAgent):
                    logger.error(f"{model_class} is not a ConversableAgent. Use AssistantAgent as default")
                    model_class = AssistantAgent

            additional_config = {
                k: v
                for k, v in agent_config.items()
                if k not in ["model", "name", "system_message", "description", "agent_path", "tags"]
            }
            agent = model_class(
                name=agent_name, llm_config=current_config.copy(), description=description, **additional_config
            )
            if system_message == "":
                system_message = agent.system_message
            else:
                system_message = f"{system_message}\n\n{self.CODING_AND_TASK_SKILL_INSTRUCTION}"

            enhanced_sys_msg = self.GROUP_CHAT_DESCRIPTION.format(
                name=agent_name, members=member_name, user_proxy_desc=user_proxy_desc, sys_msg=system_message
            )
            agent.update_system_message(enhanced_sys_msg)
        self.agent_procs_assign[agent_name] = (agent, server_id)
        return agent

    def clear_agent(self, agent_name: str, recycle_endpoint: bool | None = True):
        """Clear a specific agent by name.

        Args:
            agent_name: the name of agent.
            recycle_endpoint: trigger for recycle the endpoint server. If true, the endpoint will be recycled
                when there is no agent depending on.
        """
        _, server_id = self.agent_procs_assign[agent_name]
        del self.agent_procs_assign[agent_name]
        if recycle_endpoint:
            if server_id == self.online_server_name:
                return
            else:
                for _, iter_sid in self.agent_procs_assign.values():
                    if server_id == iter_sid:
                        return
                self.agent_procs[server_id][0].terminate()
                self.open_ports.append(server_id.split("_")[-1])
        print(colored(f"Agent {agent_name} has been cleared.", "yellow"), flush=True)

    def clear_all_agents(self, recycle_endpoint: bool | None = True):
        """Clear all cached agents."""
        for agent_name in list(self.agent_procs_assign):
            self.clear_agent(agent_name, recycle_endpoint)
        print(colored("All agents have been cleared.", "yellow"), flush=True)

    def build(
        self,
        building_task: str,
        default_llm_config: LLMConfig | dict[str, Any],
        coding: bool | None = None,
        code_execution_config: dict[str, Any] | None = None,
        use_oai_assistant: bool | None = False,
        user_proxy: ConversableAgent | None = None,
        max_agents: int | None = None,
        **kwargs: Any,
    ) -> tuple[list[ConversableAgent], dict[str, Any]]:
        """Auto build agents based on the building task.

        Args:
            building_task: instruction that helps build manager (gpt-4) to decide what agent should be built.
            default_llm_config: specific configs for LLM (e.g., config_list, seed, temperature, ...).
            coding: use to identify if the user proxy (a code interpreter) should be added.
            code_execution_config: specific configs for user proxy (e.g., last_n_messages, work_dir, ...).
            use_oai_assistant: use OpenAI assistant api instead of self-constructed agent.
            user_proxy: user proxy's class that can be used to replace the default user proxy.
            max_agents (Optional[int], default=None): Maximum number of agents to create for the task. If None, uses the value from self.max_agents.
            **kwargs (Any): Additional arguments to pass to _build_agents.
                - agent_configs: Optional list of predefined agent configurations to use.

        Returns:
            agent_list: a list of agents.
            cached_configs: cached configs.
        """
        if code_execution_config is None:
            code_execution_config = {
                "last_n_messages": 1,
                "work_dir": "groupchat",
                "use_docker": False,
                "timeout": 10,
            }

        if max_agents is None:
            max_agents = self.max_agents

        agent_configs = kwargs.get("agent_configs", [])
        self.building_task = building_task

        print(colored("==> Generating agents...", "green"), flush=True)
        resp_agent_name = (
            self.builder_model.create(
                messages=[
                    {
                        "role": "user",
                        "content": self.AGENT_NAME_PROMPT.format(task=building_task, max_agents=max_agents),
                    }
                ]
            )
            .choices[0]
            .message.content
        )
        agent_name_list = [agent_name.strip().replace(" ", "_") for agent_name in resp_agent_name.split(",")]
        print(f"{agent_name_list} are generated.", flush=True)

        print(colored("==> Generating system message...", "green"), flush=True)
        agent_sys_msg_list = []
        for name in agent_name_list:
            print(f"Preparing system message for {name}", flush=True)
            resp_agent_sys_msg = (
                self.builder_model.create(
                    messages=[
                        {
                            "role": "user",
                            "content": self.AGENT_SYS_MSG_PROMPT.format(
                                task=building_task,
                                position=name,
                                default_sys_msg=self.DEFAULT_DESCRIPTION,
                            ),
                        }
                    ]
                )
                .choices[0]
                .message.content
            )
            agent_sys_msg_list.append(resp_agent_sys_msg)

        print(colored("==> Generating description...", "green"), flush=True)
        agent_description_list = []
        for name, sys_msg in list(zip(agent_name_list, agent_sys_msg_list)):
            print(f"Preparing description for {name}", flush=True)
            resp_agent_description = (
                self.builder_model.create(
                    messages=[
                        {
                            "role": "user",
                            "content": self.AGENT_DESCRIPTION_PROMPT.format(position=name, sys_msg=sys_msg),
                        }
                    ]
                )
                .choices[0]
                .message.content
            )
            agent_description_list.append(resp_agent_description)

        for name, sys_msg, description in list(zip(agent_name_list, agent_sys_msg_list, agent_description_list)):
            agent_configs.append({
                "name": name,
                "model": self.agent_model,
                "tags": self.agent_model_tags,
                "system_message": sys_msg,
                "description": description,
            })

        if coding is None:
            resp = (
                self.builder_model.create(
                    messages=[{"role": "user", "content": self.CODING_PROMPT.format(task=building_task)}]
                )
                .choices[0]
                .message.content
            )
            coding = resp == "YES"

        self.cached_configs.update({
            "building_task": building_task,
            "agent_configs": agent_configs,
            "coding": coding,
            "default_llm_config": default_llm_config,
            "code_execution_config": code_execution_config,
        })
        _config_check(self.cached_configs)
        return self._build_agents(use_oai_assistant, user_proxy=user_proxy, **kwargs)

    def build_from_library(
        self,
        building_task: str,
        library_path_or_json: str,
        default_llm_config: LLMConfig | dict[str, Any],
        top_k: int = 3,
        coding: bool | None = None,
        code_execution_config: dict[str, Any] | None = None,
        use_oai_assistant: bool | None = False,
        embedding_model: str | None = "all-mpnet-base-v2",
        user_proxy: ConversableAgent | None = None,
        **kwargs: Any,
    ) -> tuple[list[ConversableAgent], dict[str, Any]]:
        """Build agents from a library.
        The library is a list of agent configs, which contains the name and system_message for each agent.
        We use a build manager to decide what agent in that library should be involved to the task.

        Args:
            building_task: instruction that helps build manager (gpt-4) to decide what agent should be built.
            library_path_or_json: path or JSON string config of agent library.
            default_llm_config: specific configs for LLM (e.g., config_list, seed, temperature, ...).
            top_k: number of results to return.
            coding: use to identify if the user proxy (a code interpreter) should be added.
            code_execution_config: specific configs for user proxy (e.g., last_n_messages, work_dir, ...).
            use_oai_assistant: use OpenAI assistant api instead of self-constructed agent.
            embedding_model: a Sentence-Transformers model use for embedding similarity to select agents from library.
                As reference, chromadb use "all-mpnet-base-v2" as default.
            user_proxy: user proxy's class that can be used to replace the default user proxy.
            **kwargs: Additional arguments to pass to _build_agents.

        Returns:
            agent_list: a list of agents.
            cached_configs: cached configs.
        """
        import chromadb
        from chromadb.utils import embedding_functions

        if code_execution_config is None:
            code_execution_config = {
                "last_n_messages": 1,
                "work_dir": "groupchat",
                "use_docker": False,
                "timeout": 120,
            }

        try:
            agent_library = json.loads(library_path_or_json)
        except json.decoder.JSONDecodeError:
            with open(library_path_or_json) as f:
                agent_library = json.load(f)
        except Exception as e:
            raise e

        print(colored("==> Looking for suitable agents in the library...", "green"), flush=True)
        skills = building_task.replace(":", " ").split("\n")
        # skills = [line.split("-", 1)[1].strip() if line.startswith("-") else line for line in lines]
        if len(skills) == 0:
            skills = [building_task]

        chroma_client = chromadb.Client()
        collection = chroma_client.create_collection(
            name="agent_list",
            embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name=embedding_model),
        )
        collection.add(
            documents=[agent["description"] for agent in agent_library],
            metadatas=[{"source": "agent_profile"} for _ in range(len(agent_library))],
            ids=[f"agent_{i}" for i in range(len(agent_library))],
        )
        agent_desc_list = set()
        for skill in skills:
            recall = set(collection.query(query_texts=[skill], n_results=top_k)["documents"][0])
            agent_desc_list = agent_desc_list.union(recall)

        agent_config_list = []
        for description in list(agent_desc_list):
            for agent in agent_library:
                if agent["description"] == description:
                    agent_config_list.append(agent.copy())
                    break
        chroma_client.delete_collection(collection.name)

        # double recall from the searching result
        expert_pool = [f"{agent['name']}: {agent['description']}" for agent in agent_config_list]
        while True:
            skill_agent_pair_json = (
                self.builder_model.create(
                    messages=[
                        {
                            "role": "user",
                            "content": self.AGENT_SELECTION_PROMPT.format(
                                skills=building_task, expert_pool=expert_pool, max_agents=self.max_agents
                            ),
                        }
                    ]
                )
                .choices[0]
                .message.content
            )
            try:
                skill_agent_pair_json = _retrieve_json(skill_agent_pair_json)
                skill_agent_pair = json.loads(skill_agent_pair_json)
                break
            except Exception as e:
                print(e, flush=True)
                time.sleep(5)
                continue

        recalled_agent_config_list = []
        recalled_name_desc = []
        for skill, agent_profile in skill_agent_pair.items():
            # If no suitable agent, generate an agent
            if agent_profile == "None":
                _, agent_config_temp = self.build(
                    building_task=skill,
                    default_llm_config=default_llm_config.copy(),
                    coding=False,
                    use_oai_assistant=use_oai_assistant,
                    max_agents=1,
                )
                self.clear_agent(agent_config_temp["agent_configs"][0]["name"])
                recalled_agent_config_list.append(agent_config_temp["agent_configs"][0])
            else:
                if agent_profile in recalled_name_desc:
                    # prevent identical agents
                    continue
                recalled_name_desc.append(agent_profile)
                name = agent_profile.split(":")[0].strip()
                desc = agent_profile.split(":")[1].strip()
                for agent in agent_config_list:
                    if name == agent["name"] and desc == agent["description"]:
                        recalled_agent_config_list.append(agent.copy())

        print(f"{[agent['name'] for agent in recalled_agent_config_list]} are selected.", flush=True)

        if coding is None:
            resp = (
                self.builder_model.create(
                    messages=[{"role": "user", "content": self.CODING_PROMPT.format(task=building_task)}]
                )
                .choices[0]
                .message.content
            )
            coding = resp == "YES"

        self.cached_configs.update({
            "building_task": building_task,
            "agent_configs": recalled_agent_config_list,
            "coding": coding,
            "default_llm_config": default_llm_config,
            "code_execution_config": code_execution_config,
        })
        _config_check(self.cached_configs)

        return self._build_agents(use_oai_assistant, user_proxy=user_proxy, **kwargs)

    def _build_agents(
        self, use_oai_assistant: bool | None = False, user_proxy: ConversableAgent | None = None, **kwargs
    ) -> tuple[list[ConversableAgent], dict[str, Any]]:
        """Build agents with generated configs.

        Args:
            use_oai_assistant: use OpenAI assistant api instead of self-constructed agent.
            user_proxy: user proxy's class that can be used to replace the default user proxy.
            **kwargs: Additional keyword arguments.

        Returns:
            agent_list: a list of agents.
            cached_configs: cached configs.
        """
        agent_configs = self.cached_configs["agent_configs"]
        default_llm_config = self.cached_configs["default_llm_config"]
        coding = self.cached_configs["coding"]
        code_execution_config = self.cached_configs["code_execution_config"]

        print(colored("==> Creating agents...", "green"), flush=True)
        for config in agent_configs:
            print(f"Creating agent {config['name']}...", flush=True)
            self._create_agent(
                agent_config=config.copy(),
                member_name=[agent["name"] for agent in agent_configs],
                llm_config=default_llm_config,
                use_oai_assistant=use_oai_assistant,
            )
        agent_list = [agent_config[0] for agent_config in self.agent_procs_assign.values()]

        if coding is True:
            print("Adding user console proxy...", flush=True)
            if user_proxy is None:
                user_proxy = UserProxyAgent(
                    name="Computer_terminal",
                    is_termination_msg=lambda x: x == "TERMINATE" or x == "TERMINATE.",
                    code_execution_config=code_execution_config,
                    human_input_mode="NEVER",
                    default_auto_reply=self.DEFAULT_PROXY_AUTO_REPLY,
                )
            agent_list = agent_list + [user_proxy]

        return agent_list, self.cached_configs.copy()

    def save(self, filepath: str | None = None) -> str:
        """Save building configs. If the filepath is not specific, this function will create a filename by encrypt the
        building_task string by md5 with "save_config_" prefix, and save config to the local path.

        Args:
            filepath: save path.

        Return:
            filepath: path save.
        """
        if filepath is None:
            filepath = f"./save_config_{hashlib.md5(self.building_task.encode('utf-8')).hexdigest()}.json"
        with open(filepath, "w") as save_file:
            json.dump(self.cached_configs, save_file, indent=4)
        print(colored(f"Building config saved to {filepath}", "green"), flush=True)

        return filepath

    def load(
        self,
        filepath: str | None = None,
        config_json: str | None = None,
        use_oai_assistant: bool | None = False,
        **kwargs: Any,
    ) -> tuple[list[ConversableAgent], dict[str, Any]]:
        """Load building configs and call the build function to complete building without calling online LLMs' api.

        Args:
            filepath: filepath or JSON string for the save config.
            config_json: JSON string for the save config.
            use_oai_assistant: use OpenAI assistant api instead of self-constructed agent.
            **kwargs (Any): Additional arguments to pass to _build_agents:
                - code_execution_config (Optional[dict[str, Any]]): If provided, overrides the
                code execution configuration from the loaded config.

        Returns:
            agent_list: a list of agents.
            cached_configs: cached configs.
        """
        # load json string.
        if config_json is not None:
            print(colored("Loading config from JSON...", "green"), flush=True)
            cached_configs = json.loads(config_json)

        # load from path.
        if filepath is not None:
            print(colored(f"Loading config from {filepath}", "green"), flush=True)
            with open(filepath) as f:
                cached_configs = json.load(f)

        _config_check(cached_configs)

        agent_configs = cached_configs["agent_configs"]
        default_llm_config = cached_configs["default_llm_config"]
        coding = cached_configs["coding"]

        if kwargs.get("code_execution_config") is not None:
            # for test
            self.cached_configs.update({
                "building_task": cached_configs["building_task"],
                "agent_configs": agent_configs,
                "coding": coding,
                "default_llm_config": default_llm_config,
                "code_execution_config": kwargs["code_execution_config"],
            })
            del kwargs["code_execution_config"]
            return self._build_agents(use_oai_assistant, **kwargs)
        else:
            code_execution_config = cached_configs["code_execution_config"]
            self.cached_configs.update({
                "building_task": cached_configs["building_task"],
                "agent_configs": agent_configs,
                "coding": coding,
                "default_llm_config": default_llm_config,
                "code_execution_config": code_execution_config,
            })
            return self._build_agents(use_oai_assistant, **kwargs)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import contextlib
import functools
import importlib.util
import inspect
import io
import os
import traceback
from hashlib import md5
from pathlib import Path
from textwrap import dedent, indent

from .... import AssistantAgent, UserProxyAgent
from ....coding import CodeExecutor, CodeExtractor, LocalCommandLineCodeExecutor, MarkdownCodeExtractor
from ....coding.base import CodeBlock, CodeResult
from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from ....tools import Tool, get_function_schema, load_basemodels_if_needed

with optional_import_block():
    import pandas as pd
    from sentence_transformers import SentenceTransformer, util


@require_optional_import(["pandas", "sentence_transformers"], "retrievechat")
@export_module("autogen.agentchat.contrib.captainagent")
class ToolBuilder:
    TOOL_PROMPT_DEFAULT = """\n## Functions
You have access to the following functions. They can be accessed from the module called 'functions' by their function names.
For example, if there is a function called `foo` you could import it by writing `from functions import foo`
{functions}
"""
    TOOL_PROMPT_USER_DEFINED = """\n## Functions
You have access to the following functions. You can write python code to call these functions directly without importing them.
{functions}
"""

    def __init__(self, corpus_root, retriever="all-mpnet-base-v2", type="default"):
        if type == "default":
            corpus_path = os.path.join(corpus_root, "tool_description.tsv")
            self.df = pd.read_csv(corpus_path, sep="\t")
            document_list = self.df["document_content"].tolist()
            self.TOOL_PROMPT = self.TOOL_PROMPT_DEFAULT
        else:
            self.TOOL_PROMPT = self.TOOL_PROMPT_USER_DEFINED
            # user defined tools, retrieve is actually not needed, just for consistency
            document_list = []
            for tool in corpus_root:
                document_list.append(tool.description)

        self.model = SentenceTransformer(retriever)
        self.embeddings = self.model.encode(document_list)
        self.type = type

    def retrieve(self, query, top_k=3):
        # Encode the query using the Sentence Transformer model
        query_embedding = self.model.encode([query])

        hits = util.semantic_search(query_embedding, self.embeddings, top_k=top_k)

        results = []
        for hit in hits[0]:
            results.append(self.df.iloc[hit["corpus_id"], 1])
        return results

    def bind(self, agent: AssistantAgent, functions: str):
        """Binds the function to the agent so that agent is aware of it."""
        sys_message = agent.system_message
        sys_message += self.TOOL_PROMPT.format(functions=functions)
        agent.update_system_message(sys_message)
        return

    def bind_user_proxy(self, agent: UserProxyAgent, tool_root: str | list):
        """Updates user proxy agent with a executor so that code executor can successfully execute function-related code.
        Returns an updated user proxy.
        """
        if isinstance(tool_root, str):
            # Find all the functions in the tool root
            functions = find_callables(tool_root)

            code_execution_config = agent._code_execution_config
            executor = LocalCommandLineCodeExecutor(
                timeout=code_execution_config.get("timeout", 180),
                work_dir=code_execution_config.get("work_dir", "coding"),
                functions=functions,
            )
            code_execution_config = {
                "executor": executor,
                "last_n_messages": code_execution_config.get("last_n_messages", 1),
            }
            updated_user_proxy = UserProxyAgent(
                name=agent.name,
                is_termination_msg=agent._is_termination_msg,
                code_execution_config=code_execution_config,
                human_input_mode="NEVER",
                default_auto_reply=agent._default_auto_reply,
            )
            return updated_user_proxy
        else:
            # second case: user defined tools
            code_execution_config = agent._code_execution_config
            executor = LocalExecutorWithTools(
                tools=tool_root,
                work_dir=code_execution_config.get("work_dir", "coding"),
            )
            code_execution_config = {
                "executor": executor,
                "last_n_messages": code_execution_config.get("last_n_messages", 1),
            }
            updated_user_proxy = UserProxyAgent(
                name=agent.name,
                is_termination_msg=agent._is_termination_msg,
                code_execution_config=code_execution_config,
                human_input_mode="NEVER",
                default_auto_reply=agent._default_auto_reply,
            )
            return updated_user_proxy


class LocalExecutorWithTools(CodeExecutor):
    """An executor that executes code blocks with injected tools. In this executor, the func within the tools can be called directly without declaring in the code block.

    For example, for a tool converted from langchain, the relevant functions can be called directly.
    ```python
    from langchain_community.tools import WikipediaQueryRun
    from langchain_community.utilities import WikipediaAPIWrapper
    from autogen.interop import Interoperability

    api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=3000)
    langchain_tool = WikipediaQueryRun(api_wrapper=api_wrapper)
    interop = Interoperability()
    ag2_tool = interop.convert_tool(tool=langchain_tool, type="langchain")

    # `ag2_tool.name` is wikipedia
    local_executor = LocalExecutorWithTools(tools=[ag2_tool], work_dir="./")

    code = '''
    result = wikipedia(tool_input={"query":"Christmas"})
    print(result)
    '''
    print(
        local_executor.execute_code_blocks(
            code_blocks=[
                CodeBlock(language="python", code=code),
            ]
        )
    )
    ```
    In this case, the `wikipedia` function can be called directly in the code block. This hides the complexity of the tool.

    Args:
        tools: The tools to inject into the code execution environment. Default is an empty list.
        work_dir: The working directory for the code execution. Default is the current directory.
    """

    @property
    def code_extractor(self) -> CodeExtractor:
        """(Experimental) Export a code extractor that can be used by an agent."""
        return MarkdownCodeExtractor()

    def __init__(self, tools: list[Tool] | None = None, work_dir: Path | str = Path()):
        self.tools = tools if tools is not None else []
        self.work_dir = work_dir
        if not os.path.exists(work_dir):
            os.makedirs(work_dir, exist_ok=True)

    def execute_code_blocks(self, code_blocks: list[CodeBlock]) -> CodeResult:
        """Execute code blocks and return the result.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.

        Returns:
            CodeResult: The result of the code execution.
        """
        logs_all = ""
        exit_code = 0  # Success code
        code_file = None  # Path to the first saved codeblock content

        for idx, code_block in enumerate(code_blocks):
            code = code_block.code
            code_hash = md5(code.encode()).hexdigest()
            filename = f"tmp_code_{code_hash}.py"
            filepath = os.path.join(self.work_dir, filename)
            # Save code content to file
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(code)

            if idx == 0:
                code_file = filepath

            # Create a new execution environment
            execution_env = {}
            # Inject the tools
            for tool in self.tools:
                execution_env[tool.name] = _wrap_function(tool.func)

            # Prepare to capture stdout and stderr
            stdout = io.StringIO()
            stderr = io.StringIO()

            # Execute the code block
            try:
                # Redirect stdout and stderr
                with contextlib.redirect_stdout(stdout), contextlib.redirect_stderr(stderr):
                    # Exec the code in the execution environment
                    exec(code, execution_env)
            except Exception:
                # Capture exception traceback
                tb = traceback.format_exc()
                stderr.write(tb)
                exit_code = 1  # Non-zero exit code indicates failure

            # Collect outputs
            stdout_content = stdout.getvalue()
            stderr_content = stderr.getvalue()
            logs_all += stdout_content + stderr_content

        return CodeResult(exit_code=exit_code, output=logs_all, code_file=code_file)

    def restart(self):
        """Restart the code executor. Since this executor is stateless, no action is needed."""
        pass


@export_module("autogen.agentchat.contrib.captainagent")
def format_ag2_tool(tool: Tool):
    # get the args first
    schema = get_function_schema(tool.func, description=tool.description)

    arg_name = list(inspect.signature(tool.func).parameters.keys())[0]
    arg_info = schema["function"]["parameters"]["properties"][arg_name]["properties"]

    content = f'def {tool.name}({arg_name}):\n    """\n'
    content += indent(tool.description, "    ") + "\n"
    content += (
        indent(
            f"You must format all the arguments into a dictionary and pass them as **kwargs to {arg_name}. You should use print function to get the results.",
            "    ",
        )
        + "\n"
        + indent(f"For example:\n\tresult = {tool.name}({arg_name}={{'arg1': 'value1' }})", "    ")
        + "\n"
    )
    content += indent(f"Arguments passed in {arg_name}:\n", "    ")
    for arg, info in arg_info.items():
        content += indent(f"{arg} ({info['type']}): {info['description']}\n", "    " * 2)
    content += '    """\n'
    return content


def _wrap_function(func):
    """Wrap the function to dump the return value to json.

    Handles both sync and async functions.

    Args:
        func: the function to be wrapped.

    Returns:
        The wrapped function.
    """

    @load_basemodels_if_needed
    @functools.wraps(func)
    def _wrapped_func(*args, **kwargs):
        return func(*args, **kwargs)

    return _wrapped_func


@export_module("autogen.agentchat.contrib.captainagent")
def get_full_tool_description(py_file):
    """Retrieves the function signature for a given Python file."""
    with open(py_file) as f:
        code = f.read()
        exec(code)
        function_name = os.path.splitext(os.path.basename(py_file))[0]
        if function_name in locals():
            func = locals()[function_name]
            content = f"def {func.__name__}{inspect.signature(func)}:\n"
            docstring = func.__doc__

            if docstring:
                docstring = dedent(docstring)
                docstring = '"""' + docstring + '"""'
                docstring = indent(docstring, "    ")
                content += docstring + "\n"
            return content
        else:
            raise ValueError(f"Function {function_name} not found in {py_file}")


def _wrap_function(func):
    """Wrap the function to dump the return value to json.

    Handles both sync and async functions.

    Args:
        func: the function to be wrapped.

    Returns:
        The wrapped function.
    """

    @load_basemodels_if_needed
    @functools.wraps(func)
    def _wrapped_func(*args, **kwargs):
        return func(*args, **kwargs)

    return _wrapped_func


def find_callables(directory):
    """Find all callable objects defined in Python files within the specified directory."""
    callables = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith(".py"):
                module_name = os.path.splitext(file)[0]
                module_path = os.path.join(root, file)
                spec = importlib.util.spec_from_file_location(module_name, module_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                for name, value in module.__dict__.items():
                    if callable(value) and name == module_name:
                        callables.append(value)
                        break
    return callables
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any

from ... import OpenAIWrapper
from ...import_utils import optional_import_block, require_optional_import
from .. import Agent, ConversableAgent
from .vectordb.utils import get_logger

logger = get_logger(__name__)

with optional_import_block():
    from llama_index.core.agent.runner.base import AgentRunner
    from llama_index.core.base.llms.types import ChatMessage
    from llama_index.core.chat_engine.types import AgentChatResponse
    from pydantic import BaseModel, ConfigDict

    Config = ConfigDict(arbitrary_types_allowed=True)

    # Add Pydantic configuration to allow arbitrary types
    # Added to mitigate PydanticSchemaGenerationError
    BaseModel.model_config = Config


@require_optional_import("llama_index", "neo4j")
class LLamaIndexConversableAgent(ConversableAgent):
    def __init__(
        self,
        name: str,
        llama_index_agent: "AgentRunner",
        description: str | None = None,
        **kwargs: Any,
    ):
        """Args:
        name (str): agent name.
        llama_index_agent (AgentRunner): llama index agent.
            Please override this attribute if you want to reprogram the agent.
        description (str): a short description of the agent. This description is used by other agents
            (e.g. the GroupChatManager) to decide when to call upon this agent.
        **kwargs (dict): Please refer to other kwargs in
            [ConversableAgent](/docs/api-reference/autogen/ConversableAgent#conversableagent).
        """
        if llama_index_agent is None:
            raise ValueError("llama_index_agent must be provided")

        if not description or description.strip() == "":
            raise ValueError("description must be provided")

        super().__init__(
            name,
            description=description,
            **kwargs,
        )

        self._llama_index_agent = llama_index_agent

        # Override the `generate_oai_reply`
        self.replace_reply_func(ConversableAgent.generate_oai_reply, LLamaIndexConversableAgent._generate_oai_reply)

        self.replace_reply_func(ConversableAgent.a_generate_oai_reply, LLamaIndexConversableAgent._a_generate_oai_reply)

    def _generate_oai_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Generate a reply using autogen.oai."""
        user_message, history = self._extract_message_and_history(messages=messages, sender=sender)

        chat_response: AgentChatResponse = self._llama_index_agent.chat(message=user_message, chat_history=history)

        extracted_response = chat_response.response

        return (True, extracted_response)

    async def _a_generate_oai_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Generate a reply using autogen.oai."""
        user_message, history = self._extract_message_and_history(messages=messages, sender=sender)

        chat_response: AgentChatResponse = await self._llama_index_agent.achat(
            message=user_message, chat_history=history
        )

        extracted_response = chat_response.response

        return (True, extracted_response)

    def _extract_message_and_history(
        self, messages: list[dict[str, Any]] | None = None, sender: Agent | None = None
    ) -> tuple[str, list["ChatMessage"]]:
        """Extract the message and history from the messages."""
        if not messages:
            messages = self._oai_messages[sender]

        if not messages:
            return "", []

        message = messages[-1].get("content", "")

        history = messages[:-1]
        history_messages: list[ChatMessage] = []
        for history_message in history:
            content = history_message.get("content", "")
            role = history_message.get("role", "user")
            if role and (role == "user" or role == "assistant"):
                history_messages.append(ChatMessage(content=content, role=role, additional_kwargs={}))
        return message, history_messages
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
import json
import logging
import time
from collections import defaultdict
from typing import Any

from ... import OpenAIWrapper
from ...llm_config import LLMConfig
from ...oai.openai_utils import create_gpt_assistant, retrieve_assistants_by_name, update_gpt_assistant
from ...runtime_logging import log_new_agent, logging_enabled
from ..agent import Agent
from ..assistant_agent import AssistantAgent, ConversableAgent

logger = logging.getLogger(__name__)


class GPTAssistantAgent(ConversableAgent):
    """An experimental AG2 agent class that leverages the OpenAI Assistant API for conversational capabilities.
    This agent is unique in its reliance on the OpenAI Assistant for state management, differing from other agents like ConversableAgent.
    """

    DEFAULT_MODEL_NAME = "gpt-4-0125-preview"

    def __init__(
        self,
        name="GPT Assistant",
        instructions: str | None = None,
        llm_config: LLMConfig | dict[str, Any] | bool | None = None,
        assistant_config: dict[str, Any] | None = None,
        overwrite_instructions: bool = False,
        overwrite_tools: bool = False,
        **kwargs: Any,
    ):
        """Args:
        name (str): name of the agent. It will be used to find the existing assistant by name. Please remember to delete an old assistant with the same name if you intend to create a new assistant with the same name.
        instructions (str): instructions for the OpenAI assistant configuration.
        When instructions is not None, the system message of the agent will be
        set to the provided instructions and used in the assistant run, irrespective
        of the overwrite_instructions flag. But when instructions is None,
        and the assistant does not exist, the system message will be set to
        AssistantAgent.DEFAULT_SYSTEM_MESSAGE. If the assistant exists, the
        system message will be set to the existing assistant instructions.
        llm_config (LLMConfig or dict or False): llm inference configuration.
            - model: Model to use for the assistant (gpt-4-1106-preview, gpt-3.5-turbo-1106).
        assistant_config
            - assistant_id: ID of the assistant to use. If None, a new assistant will be created.
            - check_every_ms: check thread run status interval
            - tools: Give Assistants access to OpenAI-hosted tools like Code Interpreter and Knowledge Retrieval,
                    or build your own tools using Function calling. ref https://platform.openai.com/docs/assistants/tools
            - file_ids: (Deprecated) files used by retrieval in run. It is Deprecated, use tool_resources instead. https://platform.openai.com/docs/assistants/migration/what-has-changed.
            - tool_resources: A set of resources that are used by the assistant's tools. The resources are specific to the type of tool.
        overwrite_instructions (bool): whether to overwrite the instructions of an existing assistant. This parameter is in effect only when assistant_id is specified in llm_config.
        overwrite_tools (bool): whether to overwrite the tools of an existing assistant. This parameter is in effect only when assistant_id is specified in llm_config.
        kwargs (dict): Additional configuration options for the agent.
            - verbose (bool): If set to True, enables more detailed output from the assistant thread.
            - Other kwargs: Except verbose, others are passed directly to ConversableAgent.
        """
        self._verbose = kwargs.pop("verbose", False)
        openai_client_cfg, openai_assistant_cfg = self._process_assistant_config(llm_config, assistant_config)

        super().__init__(
            name=name, system_message=instructions, human_input_mode="NEVER", llm_config=openai_client_cfg, **kwargs
        )
        if logging_enabled():
            log_new_agent(self, locals())

        # GPTAssistantAgent's azure_deployment param may cause NotFoundError (404) in client.beta.assistants.list()
        # See: https://github.com/microsoft/autogen/pull/1721
        model_name = self.DEFAULT_MODEL_NAME
        if openai_client_cfg.get("config_list") is not None and len(openai_client_cfg["config_list"]) > 0:
            model_name = openai_client_cfg["config_list"][0].pop("model", self.DEFAULT_MODEL_NAME)
        else:
            model_name = openai_client_cfg.pop("model", self.DEFAULT_MODEL_NAME)

        logger.warning("OpenAI client config of GPTAssistantAgent(%s) - model: %s", name, model_name)

        oai_wrapper = OpenAIWrapper(**openai_client_cfg)
        if len(oai_wrapper._clients) > 1:
            logger.warning("GPT Assistant only supports one OpenAI client. Using the first client in the list.")

        self._openai_client = oai_wrapper._clients[0]._oai_client
        openai_assistant_id = openai_assistant_cfg.get("assistant_id", None)
        if openai_assistant_id is None:
            # try to find assistant by name first
            candidate_assistants = retrieve_assistants_by_name(self._openai_client, name)
            if len(candidate_assistants) > 0:
                # Filter out candidates with the same name but different instructions, file IDs, and function names.
                candidate_assistants = self.find_matching_assistant(
                    candidate_assistants,
                    instructions,
                    openai_assistant_cfg.get("tools", []),
                )

            if len(candidate_assistants) == 0:
                logger.warning("No matching assistant found, creating a new assistant")
                # create a new assistant
                if instructions is None:
                    logger.warning(
                        "No instructions were provided for new assistant. Using default instructions from AssistantAgent.DEFAULT_SYSTEM_MESSAGE."
                    )
                    instructions = AssistantAgent.DEFAULT_SYSTEM_MESSAGE
                self._openai_assistant = create_gpt_assistant(
                    self._openai_client,
                    name=name,
                    instructions=instructions,
                    model=model_name,
                    assistant_config=openai_assistant_cfg,
                )
            else:
                logger.warning(
                    "Matching assistant found, using the first matching assistant: %s",
                    candidate_assistants[0].__dict__,
                )
                self._openai_assistant = candidate_assistants[0]
        else:
            # retrieve an existing assistant
            self._openai_assistant = self._openai_client.beta.assistants.retrieve(openai_assistant_id)
            # if no instructions are provided, set the instructions to the existing instructions
            if instructions is None:
                logger.warning(
                    "No instructions were provided for given assistant. Using existing instructions from assistant API."
                )
                instructions = self.get_assistant_instructions()
            elif overwrite_instructions is True:
                logger.warning(
                    "overwrite_instructions is True. Provided instructions will be used and will modify the assistant in the API"
                )
                self._openai_assistant = update_gpt_assistant(
                    self._openai_client,
                    assistant_id=openai_assistant_id,
                    assistant_config={
                        "instructions": instructions,
                    },
                )
            else:
                logger.warning(
                    "overwrite_instructions is False. Provided instructions will be used without permanently modifying the assistant in the API."
                )

            # Check if tools are specified in assistant_config
            specified_tools = openai_assistant_cfg.get("tools", None)

            if specified_tools is None:
                # Check if the current assistant has tools defined
                if self._openai_assistant.tools:
                    logger.warning(
                        "No tools were provided for given assistant. Using existing tools from assistant API."
                    )
                else:
                    logger.info(
                        "No tools were provided for the assistant, and the assistant currently has no tools set."
                    )
            elif overwrite_tools is True:
                # Tools are specified and overwrite_tools is True; update the assistant's tools
                logger.warning(
                    "overwrite_tools is True. Provided tools will be used and will modify the assistant in the API"
                )
                self._openai_assistant = update_gpt_assistant(
                    self._openai_client,
                    assistant_id=openai_assistant_id,
                    assistant_config={
                        "tools": specified_tools,
                        "tool_resources": openai_assistant_cfg.get("tool_resources", None),
                    },
                )
            else:
                # Tools are specified but overwrite_tools is False; do not update the assistant's tools
                logger.warning("overwrite_tools is False. Using existing tools from assistant API.")

        self.update_system_message(self._openai_assistant.instructions)
        # lazily create threads
        self._openai_threads = {}
        self._unread_index = defaultdict(int)
        self.register_reply([Agent, None], GPTAssistantAgent._invoke_assistant, position=2)

    def _invoke_assistant(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Invokes the OpenAI assistant to generate a reply based on the given messages.

        Args:
            messages: A list of messages in the conversation history with the sender.
            sender: The agent instance that sent the message.
            config: Optional configuration for message processing.

        Returns:
            A tuple containing a boolean indicating success and the assistant's reply.
        """
        if messages is None:
            messages = self._oai_messages[sender]
        unread_index = self._unread_index[sender] or 0
        pending_messages = messages[unread_index:]

        # Check and initiate a new thread if necessary
        if self._openai_threads.get(sender, None) is None:
            self._openai_threads[sender] = self._openai_client.beta.threads.create(
                messages=[],
            )
        assistant_thread = self._openai_threads[sender]
        # Process each unread message
        for message in pending_messages:
            if message["content"].strip() == "":
                continue
            # Convert message roles to 'user' or 'assistant', by calling _map_role_for_api, to comply with OpenAI API spec
            api_role = self._map_role_for_api(message["role"])
            self._openai_client.beta.threads.messages.create(
                thread_id=assistant_thread.id,
                content=message["content"],
                role=api_role,
            )

        # Create a new run to get responses from the assistant
        run = self._openai_client.beta.threads.runs.create(
            thread_id=assistant_thread.id,
            assistant_id=self._openai_assistant.id,
            # pass the latest system message as instructions
            instructions=self.system_message,
        )

        run_response_messages = self._get_run_response(assistant_thread, run)
        assert len(run_response_messages) > 0, "No response from the assistant."

        response = {
            "role": run_response_messages[-1]["role"],
            "content": "",
        }
        for message in run_response_messages:
            # just logging or do something with the intermediate messages?
            # if current response is not empty and there is more, append new lines
            if len(response["content"]) > 0:
                response["content"] += "\n\n"
            response["content"] += message["content"]

        self._unread_index[sender] = len(self._oai_messages[sender]) + 1
        return True, response

    def _map_role_for_api(self, role: str) -> str:
        """Maps internal message roles to the roles expected by the OpenAI Assistant API.

        Args:
            role (str): The role from the internal message.

        Returns:
            str: The mapped role suitable for the API.
        """
        if role in ["function", "tool"]:
            return "assistant"
        elif role == "system":
            return "system"
        elif role == "user":
            return "user"
        elif role == "assistant":
            return "assistant"
        else:
            # Default to 'assistant' for any other roles not recognized by the API
            return "assistant"

    def _get_run_response(self, thread, run):
        """Waits for and processes the response of a run from the OpenAI assistant.

        Args:
            thread: The thread object initiated with the OpenAI assistant.
            run: The run object initiated with the OpenAI assistant.

        Returns:
            Updated run object, status of the run, and response messages.
        """
        while True:
            run = self._wait_for_run(run.id, thread.id)
            if run.status == "completed":
                response_messages = self._openai_client.beta.threads.messages.list(thread.id, order="asc")

                new_messages = []
                for msg in response_messages:
                    if msg.run_id == run.id:
                        for content in msg.content:
                            if content.type == "text":
                                new_messages.append({
                                    "role": msg.role,
                                    "content": self._format_assistant_message(content.text),
                                })
                            elif content.type == "image_file":
                                new_messages.append({
                                    "role": msg.role,
                                    "content": f"Received file id={content.image_file.file_id}",
                                })
                return new_messages
            elif run.status == "requires_action":
                actions = []
                for tool_call in run.required_action.submit_tool_outputs.tool_calls:
                    function = tool_call.function
                    tool_call_id = tool_call.id
                    is_exec_success, tool_response = self.execute_function(
                        function.dict(), call_id=tool_call_id, verbose=self._verbose
                    )
                    tool_response["metadata"] = {
                        "tool_call_id": tool_call.id,
                        "run_id": run.id,
                        "thread_id": thread.id,
                    }

                    logger.info(
                        "Intermediate executing(%s, Success: %s) : %s",
                        tool_response["name"],
                        is_exec_success,
                        tool_response["content"],
                    )
                    actions.append(tool_response)

                submit_tool_outputs = {
                    "tool_outputs": [
                        {"output": action["content"], "tool_call_id": action["metadata"]["tool_call_id"]}
                        for action in actions
                    ],
                    "run_id": run.id,
                    "thread_id": thread.id,
                }

                run = self._openai_client.beta.threads.runs.submit_tool_outputs(**submit_tool_outputs)
            else:
                run_info = json.dumps(run.dict(), indent=2)
                raise ValueError(f"Unexpected run status: {run.status}. Full run info:\n\n{run_info})")

    def _wait_for_run(self, run_id: str, thread_id: str) -> Any:
        """Waits for a run to complete or reach a final state.

        Args:
            run_id: The ID of the run.
            thread_id: The ID of the thread associated with the run.

        Returns:
            The updated run object after completion or reaching a final state.
        """
        in_progress = True
        while in_progress:
            run = self._openai_client.beta.threads.runs.retrieve(run_id, thread_id=thread_id)
            in_progress = run.status in ("in_progress", "queued")
            if in_progress:
                time.sleep(self.llm_config.get("check_every_ms", 1000) / 1000)
        return run

    def _format_assistant_message(self, message_content):
        """Formats the assistant's message to include annotations and citations."""
        annotations = message_content.annotations
        citations = []

        # Iterate over the annotations and add footnotes
        for index, annotation in enumerate(annotations):
            # Replace the text with a footnote
            message_content.value = message_content.value.replace(annotation.text, f" [{index}]")

            # Gather citations based on annotation attributes
            if file_citation := getattr(annotation, "file_citation", None):
                try:
                    cited_file = self._openai_client.files.retrieve(file_citation.file_id)
                    citations.append(f"[{index}] {cited_file.filename}: {file_citation.quote}")
                except Exception as e:
                    logger.error(f"Error retrieving file citation: {e}")
            elif file_path := getattr(annotation, "file_path", None):
                try:
                    cited_file = self._openai_client.files.retrieve(file_path.file_id)
                    citations.append(f"[{index}] Click <here> to download {cited_file.filename}")
                except Exception as e:
                    logger.error(f"Error retrieving file citation: {e}")
                # Note: File download functionality not implemented above for brevity

        # Add footnotes to the end of the message before displaying to user
        message_content.value += "\n" + "\n".join(citations)
        return message_content.value

    def can_execute_function(self, name: str) -> bool:
        """Whether the agent can execute the function."""
        return False

    def reset(self):
        """Resets the agent, clearing any existing conversation thread and unread message indices."""
        super().reset()
        for thread in self._openai_threads.values():
            # Delete the existing thread to start fresh in the next conversation
            self._openai_client.beta.threads.delete(thread.id)
        self._openai_threads = {}
        # Clear the record of unread messages
        self._unread_index.clear()

    def clear_history(self, agent: Agent | None = None):
        """Clear the chat history of the agent.

        Args:
            agent: the agent with whom the chat history to clear. If None, clear the chat history with all agents.
        """
        super().clear_history(agent)
        if self._openai_threads.get(agent, None) is not None:
            # Delete the existing thread to start fresh in the next conversation
            thread = self._openai_threads[agent]
            logger.info("Clearing thread %s", thread.id)
            self._openai_client.beta.threads.delete(thread.id)
            self._openai_threads.pop(agent)
            self._unread_index[agent] = 0

    def pretty_print_thread(self, thread):
        """Pretty print the thread."""
        if thread is None:
            print("No thread to print")
            return
        # NOTE: that list may not be in order, sorting by created_at is important
        messages = self._openai_client.beta.threads.messages.list(
            thread_id=thread.id,
        )
        messages = sorted(messages.data, key=lambda x: x.created_at)
        print("~~~~~~~THREAD CONTENTS~~~~~~~")
        for message in messages:
            content_types = [content.type for content in message.content]
            print(f"[{message.created_at}]", message.role, ": [", ", ".join(content_types), "]")
            for content in message.content:
                content_type = content.type
                if content_type == "text":
                    print(content.type, ": ", content.text.value)
                elif content_type == "image_file":
                    print(content.type, ": ", content.image_file.file_id)
                else:
                    print(content.type, ": ", content)
        print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

    @property
    def oai_threads(self) -> dict[Agent, Any]:
        """Return the threads of the agent."""
        return self._openai_threads

    @property
    def assistant_id(self):
        """Return the assistant id"""
        return self._openai_assistant.id

    @property
    def openai_client(self):
        return self._openai_client

    @property
    def openai_assistant(self):
        return self._openai_assistant

    def get_assistant_instructions(self):
        """Return the assistant instructions from OAI assistant API"""
        return self._openai_assistant.instructions

    def delete_assistant(self):
        """Delete the assistant from OAI assistant API"""
        logger.warning("Permanently deleting assistant...")
        self._openai_client.beta.assistants.delete(self.assistant_id)

    def find_matching_assistant(self, candidate_assistants, instructions, tools):
        """Find the matching assistant from a list of candidate assistants.
        Filter out candidates with the same name but different instructions, and function names.
        """
        matching_assistants = []

        # Preprocess the required tools for faster comparison
        required_tool_types = {
            "file_search" if tool.get("type") in ["retrieval", "file_search"] else tool.get("type") for tool in tools
        }

        required_function_names = {
            tool.get("function", {}).get("name")
            for tool in tools
            if tool.get("type") not in ["code_interpreter", "retrieval", "file_search"]
        }

        for assistant in candidate_assistants:
            # Check if instructions are similar
            if instructions and instructions != getattr(assistant, "instructions", None):
                logger.warning(
                    "instructions not match, skip assistant(%s): %s",
                    assistant.id,
                    getattr(assistant, "instructions", None),
                )
                continue

            # Preprocess the assistant's tools
            assistant_tool_types = {
                "file_search" if tool.type in ["retrieval", "file_search"] else tool.type for tool in assistant.tools
            }
            assistant_function_names = {tool.function.name for tool in assistant.tools if hasattr(tool, "function")}

            # Check if the tool types, function names match
            if required_tool_types != assistant_tool_types or required_function_names != assistant_function_names:
                logger.warning(
                    "tools not match, skip assistant(%s): tools %s, functions %s",
                    assistant.id,
                    assistant_tool_types,
                    assistant_function_names,
                )
                continue

            # Append assistant to matching list if all conditions are met
            matching_assistants.append(assistant)

        return matching_assistants

    def _process_assistant_config(self, llm_config, assistant_config):
        """Process the llm_config and assistant_config to extract the model name and assistant related configurations."""
        if llm_config is False:
            raise ValueError("llm_config=False is not supported for GPTAssistantAgent.")

        openai_client_cfg = {} if llm_config is None else copy.deepcopy(llm_config)

        openai_assistant_cfg = {} if assistant_config is None else copy.deepcopy(assistant_config)

        # Move the assistant related configurations to assistant_config
        # It's important to keep forward compatibility
        assistant_config_items = ["assistant_id", "tools", "file_ids", "tool_resources", "check_every_ms"]
        for item in assistant_config_items:
            if openai_client_cfg.get(item) is not None and openai_assistant_cfg.get(item) is None:
                openai_assistant_cfg[item] = openai_client_cfg[item]
            openai_client_cfg.pop(item, None)

        return openai_client_cfg, openai_assistant_cfg
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import copy
import logging
import re
from collections.abc import Callable
from datetime import datetime
from typing import Annotated, Any, Literal

from ... import Agent, AssistantAgent, ConversableAgent, OpenAIWrapper, UserProxyAgent
from ...browser_utils import SimpleTextBrowser
from ...llm_config import LLMConfig
from ...oai.openai_utils import filter_config
from ...token_count_utils import count_token, get_max_token_limit

logger = logging.getLogger(__name__)


class WebSurferAgent(ConversableAgent):
    """(In preview) An agent that acts as a basic web surfer that can search the web and visit web pages."""

    DEFAULT_PROMPT = (
        "You are a helpful AI assistant with access to a web browser (via the provided functions). In fact, YOU ARE THE ONLY MEMBER OF YOUR PARTY WITH ACCESS TO A WEB BROWSER, so please help out where you can by performing web searches, navigating pages, and reporting what you find. Today's date is "
        + datetime.now().date().isoformat()
    )

    DEFAULT_DESCRIPTION = "A helpful assistant with access to a web browser. Ask them to perform web searches, open pages, navigate to Wikipedia, answer questions from pages, and or generate summaries."

    def __init__(
        self,
        name: str,
        system_message: str | list[str] | None = DEFAULT_PROMPT,
        description: str | None = DEFAULT_DESCRIPTION,
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        max_consecutive_auto_reply: int | None = None,
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "TERMINATE",
        function_map: dict[str, Callable[..., Any]] | None = None,
        code_execution_config: dict[str, Any] | Literal[False] = False,
        llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None,
        summarizer_llm_config: LLMConfig | dict[str, Any] | Literal[False] | None = None,
        default_auto_reply: str | dict[str, Any] | None = "",
        browser_config: dict[str, Any] | None = None,
        **kwargs: Any,
    ):
        super().__init__(
            name=name,
            system_message=system_message,
            description=description,
            is_termination_msg=is_termination_msg,
            max_consecutive_auto_reply=max_consecutive_auto_reply,
            human_input_mode=human_input_mode,
            function_map=function_map,
            code_execution_config=code_execution_config,
            llm_config=llm_config,
            default_auto_reply=default_auto_reply,
            **kwargs,
        )

        self._create_summarizer_client(summarizer_llm_config, llm_config)

        # Create the browser
        self.browser = SimpleTextBrowser(**(browser_config if browser_config else {}))

        inner_llm_config = copy.deepcopy(llm_config)

        # Set up the inner monologue
        self._assistant = AssistantAgent(
            self.name + "_inner_assistant",
            system_message=system_message,  # type: ignore[arg-type]
            llm_config=inner_llm_config,
            is_termination_msg=lambda m: False,
        )

        self._user_proxy = UserProxyAgent(
            self.name + "_inner_user_proxy",
            human_input_mode="NEVER",
            code_execution_config=False,
            default_auto_reply="",
            is_termination_msg=lambda m: False,
        )

        if inner_llm_config not in [None, False]:
            self._register_functions()

        self.register_reply([Agent, None], WebSurferAgent.generate_surfer_reply, remove_other_reply_funcs=True)
        self.register_reply([Agent, None], ConversableAgent.generate_code_execution_reply)
        self.register_reply([Agent, None], ConversableAgent.generate_function_call_reply)
        self.register_reply([Agent, None], ConversableAgent.check_termination_and_human_reply)

    def _create_summarizer_client(
        self, summarizer_llm_config: LLMConfig | dict[str, Any], llm_config: LLMConfig | dict[str, Any]
    ) -> None:
        # If the summarizer_llm_config is None, we copy it from the llm_config
        if summarizer_llm_config is None:
            if llm_config is None:  # Nothing to copy
                self.summarizer_llm_config = None
            elif llm_config is False:  # LLMs disabled
                self.summarizer_llm_config = False
            else:  # Create a suitable config
                self.summarizer_llm_config = copy.deepcopy(llm_config)  # type: ignore[assignment]
                if "config_list" in self.summarizer_llm_config:  # type: ignore[operator]
                    preferred_models = filter_config(  # type: ignore[no-untyped-call]
                        self.summarizer_llm_config["config_list"],  # type: ignore[index]
                        {"model": ["gpt-3.5-turbo-1106", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo-16k"]},
                    )
                    if len(preferred_models) == 0:
                        logger.warning(
                            "The summarizer did not find the preferred model (gpt-3.5-turbo-16k) in the config list. "
                            "Semantic operations on webpages (summarization or Q&A) might be costly or ineffective."
                        )
                    else:
                        self.summarizer_llm_config["config_list"] = preferred_models  # type: ignore[index]
        else:
            self.summarizer_llm_config = summarizer_llm_config  # type: ignore[assignment]

        # Create the summarizer client
        self.summarization_client = (
            None if self.summarizer_llm_config is False else OpenAIWrapper(**self.summarizer_llm_config)
        )  # type: ignore[arg-type]

    def _register_functions(self) -> None:
        """Register the functions for the inner assistant and user proxy."""

        # Helper functions
        def _browser_state() -> tuple[str, str]:
            header = f"Address: {self.browser.address}\n"
            if self.browser.page_title is not None:
                header += f"Title: {self.browser.page_title}\n"

            current_page = self.browser.viewport_current_page
            total_pages = len(self.browser.viewport_pages)

            header += f"Viewport position: Showing page {current_page + 1} of {total_pages}.\n"
            return (header, self.browser.viewport)

        @self._user_proxy.register_for_execution()
        @self._assistant.register_for_llm(
            name="informational_web_search",
            description="Perform an INFORMATIONAL web search query then return the search results.",
        )
        def _informational_search(query: Annotated[str, "The informational web search query to perform."]) -> str:
            self.browser.visit_page(f"bing: {query}")
            header, content = _browser_state()
            return header.strip() + "\n=======================\n" + content

        @self._user_proxy.register_for_execution()
        @self._assistant.register_for_llm(
            name="navigational_web_search",
            description="Perform a NAVIGATIONAL web search query then immediately navigate to the top result. Useful, for example, to navigate to a particular Wikipedia article or other known destination. Equivalent to Google's \"I'm Feeling Lucky\" button.",
        )
        def _navigational_search(query: Annotated[str, "The navigational web search query to perform."]) -> str:
            self.browser.visit_page(f"bing: {query}")

            # Extract the first linl
            m = re.search(r"\[.*?\]\((http.*?)\)", self.browser.page_content)
            if m:
                self.browser.visit_page(m.group(1))

            # Return where we ended up
            header, content = _browser_state()
            return header.strip() + "\n=======================\n" + content

        @self._user_proxy.register_for_execution()
        @self._assistant.register_for_llm(
            name="visit_page", description="Visit a webpage at a given URL and return its text."
        )
        def _visit_page(url: Annotated[str, "The relative or absolute url of the webpage to visit."]) -> str:
            self.browser.visit_page(url)
            header, content = _browser_state()
            return header.strip() + "\n=======================\n" + content

        @self._user_proxy.register_for_execution()
        @self._assistant.register_for_llm(
            name="page_up",
            description="Scroll the viewport UP one page-length in the current webpage and return the new viewport content.",
        )
        def _page_up() -> str:
            self.browser.page_up()
            header, content = _browser_state()
            return header.strip() + "\n=======================\n" + content

        @self._user_proxy.register_for_execution()
        @self._assistant.register_for_llm(
            name="page_down",
            description="Scroll the viewport DOWN one page-length in the current webpage and return the new viewport content.",
        )
        def _page_down() -> str:
            self.browser.page_down()
            header, content = _browser_state()
            return header.strip() + "\n=======================\n" + content

        if self.summarization_client is not None:

            @self._user_proxy.register_for_execution()
            @self._assistant.register_for_llm(
                name="answer_from_page",
                description="Uses AI to read the page and directly answer a given question based on the content.",
            )
            def _answer_from_page(
                question: Annotated[str | None, "The question to directly answer."],
                url: Annotated[str | None, "[Optional] The url of the page. (Defaults to the current page)"] = None,
            ) -> str:
                if url is not None and url != self.browser.address:
                    self.browser.visit_page(url)

                # We are likely going to need to fix this later, but summarize only as many tokens that fit in the buffer
                limit = 4096
                try:
                    limit = get_max_token_limit(self.summarizer_llm_config["config_list"][0]["model"])  # type: ignore[index]
                except ValueError:
                    pass  # limit is unknown
                except TypeError:
                    pass  # limit is unknown

                if limit < 16000:
                    logger.warning(
                        f"The token limit ({limit}) of the WebSurferAgent.summarizer_llm_config, is below the recommended 16k."
                    )

                buffer = ""
                for line in re.split(r"([\r\n]+)", self.browser.page_content):
                    tokens = count_token(buffer + line)
                    if tokens + 1024 > limit:  # Leave room for our summary
                        break
                    buffer += line

                buffer = buffer.strip()
                if len(buffer) == 0:
                    return "Nothing to summarize."

                messages = [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that can summarize long documents to answer question.",
                    }
                ]

                prompt = f"Please summarize the following into one or two paragraph:\n\n{buffer}"
                if question is not None:
                    prompt = f"Please summarize the following into one or two paragraphs with respect to '{question}':\n\n{buffer}"

                messages.append(
                    {"role": "user", "content": prompt},
                )

                response = self.summarization_client.create(context=None, messages=messages)  # type: ignore[union-attr]
                extracted_response = self.summarization_client.extract_text_or_completion_object(response)[0]  # type: ignore[union-attr]
                return str(extracted_response)

            @self._user_proxy.register_for_execution()
            @self._assistant.register_for_llm(
                name="summarize_page",
                description="Uses AI to summarize the content found at a given url. If the url is not provided, the current page is summarized.",
            )
            def _summarize_page(
                url: Annotated[
                    str | None, "[Optional] The url of the page to summarize. (Defaults to current page)"
                ] = None,
            ) -> str:
                return _answer_from_page(url=url, question=None)

    def generate_surfer_reply(
        self,
        messages: list[dict[str, str]] | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, str | dict[str, str] | None]:
        """Generate a reply using autogen.oai."""
        if messages is None:
            messages = self._oai_messages[sender]

        self._user_proxy.reset()  # type: ignore[no-untyped-call]
        self._assistant.reset()  # type: ignore[no-untyped-call]

        # Clone the messages to give context
        self._assistant.chat_messages[self._user_proxy] = []
        history = messages[0 : len(messages) - 1]
        for message in history:
            self._assistant.chat_messages[self._user_proxy].append(message)

        # Remind the agent where it is
        self._user_proxy.send(
            f"Your browser is currently open to the page '{self.browser.page_title}' at the address '{self.browser.address}'.",
            self._assistant,
            request_reply=False,
            silent=True,
        )

        self._user_proxy.send(messages[-1]["content"], self._assistant, request_reply=True, silent=True)
        agent_reply = self._user_proxy.chat_messages[self._assistant][-1]
        # print("Agent Reply: " + str(agent_reply))
        proxy_reply = self._user_proxy.generate_reply(
            messages=self._user_proxy.chat_messages[self._assistant], sender=self._assistant
        )
        # print("Proxy Reply: " + str(proxy_reply))

        if proxy_reply == "":  # Was the default reply
            return True, None if agent_reply is None else agent_reply["content"]
        else:
            return True, None if proxy_reply is None else proxy_reply["content"]  # type: ignore[index]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import hashlib
import os
import re
import uuid
from collections.abc import Callable
from typing import Any, Literal

from ...code_utils import extract_code
from ...formatting_utils import colored
from ...import_utils import optional_import_block, require_optional_import
from ...retrieve_utils import (
    TEXT_FORMATS,
    create_vector_db_from_dir,
    get_files_from_dir,
    query_vector_db,
    split_files_to_chunks,
)
from ...token_count_utils import count_token
from .. import UserProxyAgent
from ..agent import Agent
from ..contrib.vectordb.base import Document, QueryResults, VectorDB, VectorDBFactory
from ..contrib.vectordb.utils import (
    chroma_results_to_query_results,
    filter_results_by_distance,
    get_logger,
)

__all__ = ["RetrieveUserProxyAgent"]

with optional_import_block():
    import chromadb
    from IPython import get_ipython

logger = get_logger(__name__)

PROMPT_DEFAULT = """You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the
context provided by the user. You should follow the following steps to answer a question:
Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or
a question answering task.
Step 2, you reply based on the intent.
If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.
If user's intent is code generation, you must obey the following rules:
Rule 1. You MUST NOT install any packages because all the packages needed are already installed.
Rule 2. You must follow the formats below to write your code:
```language
# your code
```

If user's intent is question answering, you must give as short an answer as possible.

User's question is: {input_question}

Context is: {input_context}

The source of the context is: {input_sources}

If you can answer the question, in the end of your answer, add the source of the context in the format of `Sources: source1, source2, ...`.
"""

PROMPT_CODE = """You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the
context provided by the user.
If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.
For code generation, you must obey the following rules:
Rule 1. You MUST NOT install any packages because all the packages needed are already installed.
Rule 2. You must follow the formats below to write your code:
```language
# your code
```

User's question is: {input_question}

Context is: {input_context}
"""

PROMPT_QA = """You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the
context provided by the user.
If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.
You must give as short an answer as possible.

User's question is: {input_question}

Context is: {input_context}
"""

HASH_LENGTH = int(os.environ.get("HASH_LENGTH", 8))
UPDATE_CONTEXT_IN_PROMPT = "you should reply exactly `UPDATE CONTEXT`"


@require_optional_import(["chromadb", "IPython"], "retrievechat")
class RetrieveUserProxyAgent(UserProxyAgent):
    """(In preview) The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding
    similarity, and sends them along with the question to the Retrieval-Augmented Assistant
    """

    def __init__(
        self,
        name="RetrieveChatAgent",  # default set to RetrieveChatAgent
        human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "ALWAYS",
        is_termination_msg: Callable[[dict[str, Any]], bool] | None = None,
        retrieve_config: dict[str, Any] | None = None,  # config for the retrieve agent
        **kwargs: Any,
    ):
        r"""Args:
            name (str): name of the agent.

            human_input_mode (str): whether to ask for human inputs every time a message is received.
                Possible values are "ALWAYS", "TERMINATE", "NEVER".
                1. When "ALWAYS", the agent prompts for human input every time a message is received.
                    Under this mode, the conversation stops when the human input is "exit",
                    or when is_termination_msg is True and there is no human input.
                2. When "TERMINATE", the agent only prompts for human input only when a termination
                    message is received or the number of auto reply reaches
                    the max_consecutive_auto_reply.
                3. When "NEVER", the agent will never prompt for human input. Under this mode, the
                    conversation stops when the number of auto reply reaches the
                    max_consecutive_auto_reply or when is_termination_msg is True.

            is_termination_msg (function): a function that takes a message in the form of a dictionary
                and returns a boolean value indicating if this received message is a termination message.
                The dict can contain the following keys: "content", "role", "name", "function_call".

            retrieve_config (dict or None): config for the retrieve agent.

                To use default config, set to None. Otherwise, set to a dictionary with the
                following keys:
                - `task` (Optional, str) - the task of the retrieve chat. Possible values are
                    "code", "qa" and "default". System prompt will be different for different tasks.
                     The default value is `default`, which supports both code and qa, and provides
                     source information in the end of the response.
                - `vector_db` (Optional, Union[str, VectorDB]) - the vector db for the retrieve chat.
                    If it's a string, it should be the type of the vector db, such as "chroma"; otherwise,
                    it should be an instance of the VectorDB protocol. Default is "chroma".
                    Set `None` to use the deprecated `client`.
                - `db_config` (Optional, Dict) - the config for the vector db. Default is `{}`. Please make
                    sure you understand the config for the vector db you are using, otherwise, leave it as `{}`.
                    Only valid when `vector_db` is a string.
                - `client` (Optional, chromadb.Client) - the chromadb client. If key not provided, a
                     default client `chromadb.Client()` will be used. If you want to use other
                     vector db, extend this class and override the `retrieve_docs` function.
                     *[Deprecated]* use `vector_db` instead.
                - `docs_path` (Optional, Union[str, List[str]]) - the path to the docs directory. It
                     can also be the path to a single file, the url to a single file or a list
                     of directories, files and urls. Default is None, which works only if the
                     collection is already created.
                - `extra_docs` (Optional, bool) - when true, allows adding documents with unique IDs
                    without overwriting existing ones; when false, it replaces existing documents
                    using default IDs, risking collection overwrite., when set to true it enables
                    the system to assign unique IDs starting from "length+i" for new document
                    chunks, preventing the replacement of existing documents and facilitating the
                    addition of more content to the collection..
                    By default, "extra_docs" is set to false, starting document IDs from zero.
                    This poses a risk as new documents might overwrite existing ones, potentially
                    causing unintended loss or alteration of data in the collection.
                    *[Deprecated]* use `new_docs` when use `vector_db` instead of `client`.
                - `new_docs` (Optional, bool) - when True, only adds new documents to the collection;
                    when False, updates existing documents and adds new ones. Default is True.
                    Document id is used to determine if a document is new or existing. By default, the
                    id is the hash value of the content.
                - `model` (Optional, str) - the model to use for the retrieve chat.
                    If key not provided, a default model `gpt-4` will be used.
                - `chunk_token_size` (Optional, int) - the chunk token size for the retrieve chat.
                    If key not provided, a default size `max_tokens * 0.4` will be used.
                - `context_max_tokens` (Optional, int) - the context max token size for the
                    retrieve chat.
                    If key not provided, a default size `max_tokens * 0.8` will be used.
                - `chunk_mode` (Optional, str) - the chunk mode for the retrieve chat. Possible values
                    are "multi_lines" and "one_line". If key not provided, a default mode
                    `multi_lines` will be used.
                - `must_break_at_empty_line` (Optional, bool) - chunk will only break at empty line
                    if True. Default is True.
                    If chunk_mode is "one_line", this parameter will be ignored.
                - `embedding_model` (Optional, str) - the embedding model to use for the retrieve chat.
                    If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available
                    models can be found at `https://www.sbert.net/docs/sentence_transformer/pretrained_models.html`.
                    The default model is a fast model. If you want to use a high performance model,
                    `all-mpnet-base-v2` is recommended.
                    *[Deprecated]* no need when use `vector_db` instead of `client`.
                - `embedding_function` (Optional, Callable) - the embedding function for creating the
                    vector db. Default is None, SentenceTransformer with the given `embedding_model`
                    will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding
                    functions, you can pass it here,
                    follow the examples in `https://docs.trychroma.com/embeddings`.
                - `customized_prompt` (Optional, str) - the customized prompt for the retrieve chat.
                    Default is None.
                - `customized_answer_prefix` (Optional, str) - the customized answer prefix for the
                    retrieve chat. Default is "".
                    If not "" and the customized_answer_prefix is not in the answer,
                    `Update Context` will be triggered.
                - `update_context` (Optional, bool) - if False, will not apply `Update Context` for
                    interactive retrieval. Default is True.
                - `collection_name` (Optional, str) - the name of the collection.
                    If key not provided, a default name `ag2-docs` will be used.
                - `get_or_create` (Optional, bool) - Whether to get the collection if it exists. Default is False.
                - `overwrite` (Optional, bool) - Whether to overwrite the collection if it exists. Default is False.
                    Case 1. if the collection does not exist, create the collection.
                    Case 2. the collection exists, if overwrite is True, it will overwrite the collection.
                    Case 3. the collection exists and overwrite is False, if get_or_create is True, it will get the collection,
                        otherwise it raise a ValueError.
                - `custom_token_count_function` (Optional, Callable) - a custom function to count the
                    number of tokens in a string.
                    The function should take (text:str, model:str) as input and return the
                    token_count(int). the retrieve_config["model"] will be passed in the function.
                    Default is autogen.token_count_utils.count_token that uses tiktoken, which may
                    not be accurate for non-OpenAI models.
                - `custom_text_split_function` (Optional, Callable) - a custom function to split a
                    string into a list of strings.
                    Default is None, will use the default function in
                    `autogen.retrieve_utils.split_text_to_chunks`.
                - `custom_text_types` (Optional, List[str]) - a list of file types to be processed.
                    Default is `autogen.retrieve_utils.TEXT_FORMATS`.
                    This only applies to files under the directories in `docs_path`. Explicitly
                    included files and urls will be chunked regardless of their types.
                - `recursive` (Optional, bool) - whether to search documents recursively in the
                    docs_path. Default is True.
                - `distance_threshold` (Optional, float) - the threshold for the distance score, only
                    distance smaller than it will be returned. Will be ignored if < 0. Default is -1.

            `**kwargs` (dict): other kwargs in [UserProxyAgent](https://docs.ag2.ai/latest/docs/api-reference/autogen/UserProxyAgent).

        Example:
        Example of overriding retrieve_docs - If you have set up a customized vector db, and it's
        not compatible with chromadb, you can easily plug in it with below code.
        *[Deprecated]* use `vector_db` instead. You can extend VectorDB and pass it to the agent.
        ```python
        class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):
            def query_vector_db(
                self,
                query_texts: List[str],
                n_results: int = 10,
                search_string: str = "",
                **kwargs: Any,
            ) -> Dict[str, Union[List[str], List[List[str]]]]:
                # define your own query function here
                pass

            def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):
                results = self.query_vector_db(
                    query_texts=[problem],
                    n_results=n_results,
                    search_string=search_string,
                    **kwargs: Any,
                )

                self._results = results
                print("doc_ids: ", results["ids"])
        ```
        """
        super().__init__(
            name=name,
            human_input_mode=human_input_mode,
            **kwargs,
        )

        self._retrieve_config = {} if retrieve_config is None else retrieve_config
        self._task = self._retrieve_config.get("task", "default")
        self._vector_db = self._retrieve_config.get("vector_db", "chroma")
        self._db_config = self._retrieve_config.get("db_config", {})
        self._docs_path = self._retrieve_config.get("docs_path", None)
        self._extra_docs = self._retrieve_config.get("extra_docs", False)
        self._new_docs = self._retrieve_config.get("new_docs", True)
        self._collection_name = self._retrieve_config.get("collection_name", "ag2-docs")
        if "docs_path" not in self._retrieve_config:
            logger.warning(
                "docs_path is not provided in retrieve_config. "
                f"Will raise ValueError if the collection `{self._collection_name}` doesn't exist. "
                "Set docs_path to None to suppress this warning."
            )
        self._model = self._retrieve_config.get("model", "gpt-4")
        self._max_tokens = self.get_max_tokens(self._model)
        self._chunk_token_size = int(self._retrieve_config.get("chunk_token_size", self._max_tokens * 0.4))
        self._chunk_mode = self._retrieve_config.get("chunk_mode", "multi_lines")
        self._must_break_at_empty_line = self._retrieve_config.get("must_break_at_empty_line", True)
        self._embedding_model = self._retrieve_config.get("embedding_model", "all-MiniLM-L6-v2")
        self._embedding_function = self._retrieve_config.get("embedding_function", None)
        self.customized_prompt = self._retrieve_config.get("customized_prompt", None)
        self.customized_answer_prefix = self._retrieve_config.get("customized_answer_prefix", "").upper()
        self.update_context = self._retrieve_config.get("update_context", True)
        self._get_or_create = self._retrieve_config.get("get_or_create", False) if self._docs_path is not None else True
        self._overwrite = self._retrieve_config.get("overwrite", False)
        self.custom_token_count_function = self._retrieve_config.get("custom_token_count_function", count_token)
        self.custom_text_split_function = self._retrieve_config.get("custom_text_split_function", None)
        self._custom_text_types = self._retrieve_config.get("custom_text_types", TEXT_FORMATS)
        self._recursive = self._retrieve_config.get("recursive", True)
        self._context_max_tokens = self._retrieve_config.get("context_max_tokens", self._max_tokens * 0.8)
        self._collection = self._docs_path is None  # whether the collection is created
        self._ipython = get_ipython()
        self._doc_idx = -1  # the index of the current used doc
        self._results = []  # the results of the current query
        self._intermediate_answers = set()  # the intermediate answers
        self._doc_contents = []  # the contents of the current used doc
        self._doc_ids = []  # the ids of the current used doc
        self._current_docs_in_context = []  # the ids of the current context sources
        self._search_string = ""  # the search string used in the current query
        self._distance_threshold = self._retrieve_config.get("distance_threshold", -1)
        # update the termination message function
        self._is_termination_msg = (
            self._is_termination_msg_retrievechat if is_termination_msg is None else is_termination_msg
        )
        if isinstance(self._vector_db, str):
            if not isinstance(self._db_config, dict):
                raise ValueError("`db_config` should be a dictionary.")
            if "embedding_function" in self._retrieve_config:
                self._db_config["embedding_function"] = self._embedding_function
            self._vector_db = VectorDBFactory.create_vector_db(db_type=self._vector_db, **self._db_config)
        self._client = self._retrieve_config.get("client", None)
        if self._client is None and hasattr(self._vector_db, "client"):
            # Since the client arg is deprecated, let's check
            # if the `vector_db` instance has a 'client' attribute.
            self._client = getattr(self._vector_db, "client", None)
        if self._client is None:
            self._client = chromadb.Client()
        self.register_reply(Agent, RetrieveUserProxyAgent._generate_retrieve_user_reply, position=2)
        self.register_hook(
            hookable_method="process_message_before_send",
            hook=self._check_update_context_before_send,
        )

    def _init_db(self):
        if not self._vector_db:
            return

        is_to_chunk = False  # whether to chunk the raw files
        if self._new_docs:
            is_to_chunk = True
        if not self._docs_path:
            try:
                self._vector_db.get_collection(self._collection_name)
                logger.warning(f"`docs_path` is not provided. Use the existing collection `{self._collection_name}`.")
                self._overwrite = False
                self._get_or_create = True
                is_to_chunk = False
            except ValueError:
                raise ValueError(
                    "`docs_path` is not provided. "
                    f"The collection `{self._collection_name}` doesn't exist either. "
                    "Please provide `docs_path` or create the collection first."
                )
        elif self._get_or_create and not self._overwrite:
            try:
                self._vector_db.get_collection(self._collection_name)
                logger.info(f"Use the existing collection `{self._collection_name}`.", color="green")
            except ValueError:
                is_to_chunk = True
        else:
            is_to_chunk = True

        self._vector_db.active_collection = self._vector_db.create_collection(
            self._collection_name, overwrite=self._overwrite, get_or_create=self._get_or_create
        )

        docs = None
        if is_to_chunk:
            if self.custom_text_split_function is not None:
                chunks, sources = split_files_to_chunks(
                    get_files_from_dir(self._docs_path, self._custom_text_types, self._recursive),
                    custom_text_split_function=self.custom_text_split_function,
                )
            else:
                chunks, sources = split_files_to_chunks(
                    get_files_from_dir(self._docs_path, self._custom_text_types, self._recursive),
                    self._chunk_token_size,
                    self._chunk_mode,
                    self._must_break_at_empty_line,
                )
            logger.info(f"Found {len(chunks)} chunks.")

            if self._new_docs:
                all_docs_ids = {
                    doc["id"]
                    for doc in self._vector_db.get_docs_by_ids(ids=None, collection_name=self._collection_name)
                }
            else:
                all_docs_ids = set()

            chunk_ids = (
                [hashlib.blake2b(chunk.encode("utf-8")).hexdigest()[:HASH_LENGTH] for chunk in chunks]
                if self._vector_db.type != "qdrant"
                else [str(uuid.UUID(hex=hashlib.md5(chunk.encode("utf-8")).hexdigest())) for chunk in chunks]
            )
            chunk_ids_set = set(chunk_ids)
            chunk_ids_set_idx = [chunk_ids.index(hash_value) for hash_value in chunk_ids_set]
            docs = [
                Document(id=chunk_ids[idx], content=chunks[idx], metadata=sources[idx])
                for idx in chunk_ids_set_idx
                if chunk_ids[idx] not in all_docs_ids
            ]

        self._vector_db.insert_docs(docs=docs, collection_name=self._collection_name, upsert=True)

    def _is_termination_msg_retrievechat(self, message):
        """Check if a message is a termination message.
        For code generation, terminate when no code block is detected. Currently only detect python code blocks.
        For question answering, terminate when don't update context, i.e., answer is given.
        """
        if isinstance(message, dict):
            message = message.get("content")
            if message is None:
                return False
        cb = extract_code(message)
        contain_code = False
        for c in cb:
            # todo: support more languages
            if c[0] == "python":
                contain_code = True
                break
        update_context_case1, update_context_case2 = self._check_update_context(message)
        return not (contain_code or update_context_case1 or update_context_case2)

    def _check_update_context_before_send(self, sender, message, recipient, silent):
        if not isinstance(message, (str, dict)):
            return message
        elif isinstance(message, dict):
            msg_text = message.get("content", message)
        else:
            msg_text = message

        if msg_text.strip().upper() == "UPDATE CONTEXT":
            doc_contents = self._get_context(self._results)

            # Always use self.problem as the query text to retrieve docs, but each time we replace the context with the
            # next similar docs in the retrieved doc results.
            if not doc_contents:
                for _tmp_retrieve_count in range(1, 5):
                    self._reset(intermediate=True)
                    self.retrieve_docs(
                        self.problem, self.n_results * (2 * _tmp_retrieve_count + 1), self._search_string
                    )
                    doc_contents = self._get_context(self._results)
                    if doc_contents or self.n_results * (2 * _tmp_retrieve_count + 1) >= len(self._results[0]):
                        break
            msg_text = self._generate_message(doc_contents, task=self._task)

        if isinstance(message, dict):
            message["content"] = msg_text
        return message

    @staticmethod
    def get_max_tokens(model="gpt-3.5-turbo"):
        if "32k" in model:
            return 32000
        elif "16k" in model:
            return 16000
        elif "gpt-4" in model:
            return 8000
        else:
            return 4000

    def _reset(self, intermediate=False):
        self._doc_idx = -1  # the index of the current used doc
        self._results = []  # the results of the current query
        if not intermediate:
            self._intermediate_answers = set()  # the intermediate answers
            self._doc_contents = []  # the contents of the current used doc
            self._doc_ids = []  # the ids of the current used doc

    def _get_context(self, results: QueryResults):
        doc_contents = ""
        self._current_docs_in_context = []
        current_tokens = 0
        _doc_idx = self._doc_idx
        _tmp_retrieve_count = 0
        for idx, doc in enumerate(results[0]):
            doc = doc[0]
            if idx <= _doc_idx:
                continue
            if doc["id"] in self._doc_ids:
                continue
            _doc_tokens = self.custom_token_count_function(doc["content"], self._model)
            if _doc_tokens > self._context_max_tokens:
                func_print = f"Skip doc_id {doc['id']} as it is too long to fit in the context."
                print(colored(func_print, "green"), flush=True)
                self._doc_idx = idx
                continue
            if current_tokens + _doc_tokens > self._context_max_tokens:
                break
            func_print = f"Adding content of doc {doc['id']} to context."
            print(colored(func_print, "green"), flush=True)
            current_tokens += _doc_tokens
            doc_contents += doc["content"] + "\n"
            _metadata = doc.get("metadata")
            if isinstance(_metadata, dict):
                self._current_docs_in_context.append(_metadata.get("source", ""))
            self._doc_idx = idx
            self._doc_ids.append(doc["id"])
            self._doc_contents.append(doc["content"])
            _tmp_retrieve_count += 1
            if _tmp_retrieve_count >= self.n_results:
                break
        return doc_contents

    def _generate_message(self, doc_contents, task="default"):
        if not doc_contents:
            print(colored("No more context, will terminate.", "green"), flush=True)
            return "TERMINATE"
        if self.customized_prompt:
            message = self.customized_prompt.format(input_question=self.problem, input_context=doc_contents)
        elif task.upper() == "CODE":
            message = PROMPT_CODE.format(input_question=self.problem, input_context=doc_contents)
        elif task.upper() == "QA":
            message = PROMPT_QA.format(input_question=self.problem, input_context=doc_contents)
        elif task.upper() == "DEFAULT":
            message = PROMPT_DEFAULT.format(
                input_question=self.problem, input_context=doc_contents, input_sources=self._current_docs_in_context
            )
        else:
            raise NotImplementedError(f"task {task} is not implemented.")
        return message

    def _check_update_context(self, message):
        if isinstance(message, dict):
            message = message.get("content", "")
        elif not isinstance(message, str):
            message = ""
        update_context_case1 = "UPDATE CONTEXT" in message.upper() and UPDATE_CONTEXT_IN_PROMPT not in message
        update_context_case2 = self.customized_answer_prefix and self.customized_answer_prefix not in message.upper()
        return update_context_case1, update_context_case2

    def _generate_retrieve_user_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """In this function, we will update the context and reset the conversation based on different conditions.
        We'll update the context and reset the conversation if update_context is True and either of the following:
        (1) the last message contains "UPDATE CONTEXT",
        (2) the last message doesn't contain "UPDATE CONTEXT" and the customized_answer_prefix is not in the message.
        """
        if config is None:
            config = self
        if messages is None:
            messages = self._oai_messages[sender]
        message = messages[-1]
        update_context_case1, update_context_case2 = self._check_update_context(message)
        if (update_context_case1 or update_context_case2) and self.update_context:
            print(colored("Updating context and resetting conversation.", "green"), flush=True)
            # extract the first sentence in the response as the intermediate answer
            _message = message.get("content", "").split("\n")[0].strip()
            _intermediate_info = re.split(r"(?<=[.!?])\s+", _message)
            self._intermediate_answers.add(_intermediate_info[0])

            if update_context_case1:
                # try to get more context from the current retrieved doc results because the results may be too long to fit
                # in the LLM context.
                doc_contents = self._get_context(self._results)

                # Always use self.problem as the query text to retrieve docs, but each time we replace the context with the
                # next similar docs in the retrieved doc results.
                if not doc_contents:
                    for _tmp_retrieve_count in range(1, 5):
                        self._reset(intermediate=True)
                        self.retrieve_docs(
                            self.problem, self.n_results * (2 * _tmp_retrieve_count + 1), self._search_string
                        )
                        doc_contents = self._get_context(self._results)
                        if doc_contents or self.n_results * (2 * _tmp_retrieve_count + 1) >= len(self._results[0]):
                            break
            elif update_context_case2:
                # Use the current intermediate info as the query text to retrieve docs, and each time we append the top similar
                # docs in the retrieved doc results to the context.
                for _tmp_retrieve_count in range(5):
                    self._reset(intermediate=True)
                    self.retrieve_docs(
                        _intermediate_info[0], self.n_results * (2 * _tmp_retrieve_count + 1), self._search_string
                    )
                    self._get_context(self._results)
                    doc_contents = "\n".join(self._doc_contents)  # + "\n" + "\n".join(self._intermediate_answers)
                    if doc_contents or self.n_results * (2 * _tmp_retrieve_count + 1) >= len(self._results[0]):
                        break

            self.clear_history()
            sender.clear_history()
            return True, self._generate_message(doc_contents, task=self._task)
        else:
            return False, None

    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = ""):
        """Retrieve docs based on the given problem and assign the results to the class property `_results`.
        The retrieved docs should be type of `QueryResults` which is a list of tuples containing the document and
        the distance.

        Args:
            problem (str): the problem to be solved.
            n_results (int): the number of results to be retrieved. Default is 20.
            search_string (str): only docs that contain an exact match of this string will be retrieved. Default is "".
                Not used if the vector_db doesn't support it.

        Returns:
            None.
        """
        if isinstance(self._vector_db, VectorDB):
            if not self._collection or not self._get_or_create:
                print("Trying to create collection.")
                self._init_db()
                self._collection = True
                self._get_or_create = True

            kwargs = {}
            if hasattr(self._vector_db, "type") and self._vector_db.type == "chroma":
                kwargs["where_document"] = {"$contains": search_string} if search_string else None
            results = self._vector_db.retrieve_docs(
                queries=[problem],
                n_results=n_results,
                collection_name=self._collection_name,
                distance_threshold=self._distance_threshold,
                **kwargs,
            )
            self._search_string = search_string
            self._results = results
            print("VectorDB returns doc_ids: ", [[r[0]["id"] for r in rr] for rr in results])
            return

        if not self._collection or not self._get_or_create:
            print("Trying to create collection.")
            self._client = create_vector_db_from_dir(
                dir_path=self._docs_path,
                max_tokens=self._chunk_token_size,
                client=self._client,
                collection_name=self._collection_name,
                chunk_mode=self._chunk_mode,
                must_break_at_empty_line=self._must_break_at_empty_line,
                embedding_model=self._embedding_model,
                get_or_create=self._get_or_create,
                embedding_function=self._embedding_function,
                custom_text_split_function=self.custom_text_split_function,
                custom_text_types=self._custom_text_types,
                recursive=self._recursive,
                extra_docs=self._extra_docs,
            )
            self._collection = True
            self._get_or_create = True

        results = query_vector_db(
            query_texts=[problem],
            n_results=n_results,
            search_string=search_string,
            client=self._client,
            collection_name=self._collection_name,
            embedding_model=self._embedding_model,
            embedding_function=self._embedding_function,
        )
        results["contents"] = results.pop("documents")
        results = chroma_results_to_query_results(results, "distances")
        results = filter_results_by_distance(results, self._distance_threshold)

        self._search_string = search_string
        self._results = results
        print("doc_ids: ", [[r[0]["id"] for r in rr] for rr in results])

    @staticmethod
    def message_generator(sender, recipient, context):
        """Generate an initial message with the given context for the RetrieveUserProxyAgent.

        Args:
            sender (Agent): the sender agent. It should be the instance of RetrieveUserProxyAgent.
            recipient (Agent): the recipient agent. Usually it's the assistant agent.
            context (dict): the context for the message generation. It should contain the following keys:
                - `problem` (str) - the problem to be solved.
                - `n_results` (int) - the number of results to be retrieved. Default is 20.
                - `search_string` (str) - only docs that contain an exact match of this string will be retrieved. Default is "".

        Returns:
            str: the generated message ready to be sent to the recipient agent.
        """
        sender._reset()

        problem = context.get("problem", "")
        n_results = context.get("n_results", 20)
        search_string = context.get("search_string", "")

        sender.retrieve_docs(problem, n_results, search_string)
        sender.problem = problem
        sender.n_results = n_results
        doc_contents = sender._get_context(sender._results)
        message = sender._generate_message(doc_contents, sender._task)
        return message

    def run_code(self, code, **kwargs):
        lang = kwargs.get("lang")
        if code.startswith("!") or code.startswith("pip") or lang in ["bash", "shell", "sh"]:
            return (
                0,
                "You MUST NOT install any packages because all the packages needed are already installed.",
                None,
            )
        if self._ipython is None or lang != "python":
            return super().run_code(code, **kwargs)
        else:
            result = self._ipython.run_cell(code)
            log = str(result.result)
            exitcode = 0 if result.success else 1
            if result.error_before_exec is not None:
                log += f"\n{result.error_before_exec}"
                exitcode = 1
            if result.error_in_exec is not None:
                log += f"\n{result.error_in_exec}"
                exitcode = 1
            return exitcode, log, None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import base64
import copy
import os
import re
from io import BytesIO
from math import ceil
from typing import Any, Union

import requests

from ...import_utils import optional_import_block, require_optional_import
from .. import utils

with optional_import_block():
    from PIL import Image


# Parameters for token counting for images for different models
MODEL_PARAMS = {
    "gpt-4-vision": {
        "max_edge": 2048,
        "min_edge": 768,
        "tile_size": 512,
        "base_token_count": 85,
        "token_multiplier": 170,
    },
    "gpt-4o-mini": {
        "max_edge": 2048,
        "min_edge": 768,
        "tile_size": 512,
        "base_token_count": 2833,
        "token_multiplier": 5667,
    },
    "gpt-4o": {"max_edge": 2048, "min_edge": 768, "tile_size": 512, "base_token_count": 85, "token_multiplier": 170},
}


@require_optional_import("PIL", "unknown")
def get_pil_image(image_file: Union[str, "Image.Image"]) -> "Image.Image":
    """Loads an image from a file and returns a PIL Image object.

    Parameters:
        image_file (str, or Image): The filename, URL, URI, or base64 string of the image file.

    Returns:
        Image.Image: The PIL Image object.
    """
    if isinstance(image_file, Image.Image):
        # Already a PIL Image object
        return image_file

    # Remove quotes if existed
    if image_file.startswith('"') and image_file.endswith('"'):
        image_file = image_file[1:-1]
    if image_file.startswith("'") and image_file.endswith("'"):
        image_file = image_file[1:-1]

    if image_file.startswith("http://") or image_file.startswith("https://"):
        # A URL file
        response = requests.get(image_file)
        content = BytesIO(response.content)
        image = Image.open(content)
    # Match base64-encoded image URIs for supported formats: jpg, jpeg, png, gif, bmp, webp
    elif re.match(r"data:image/(?:jpg|jpeg|png|gif|bmp|webp);base64,", image_file):
        # A URI. Remove the prefix and decode the base64 string.
        base64_data = re.sub(r"data:image/(?:jpg|jpeg|png|gif|bmp|webp);base64,", "", image_file)
        image = _to_pil(base64_data)
    elif os.path.exists(image_file):
        # A local file
        image = Image.open(image_file)
    else:
        # base64 encoded string
        image = _to_pil(image_file)

    return image.convert("RGB")


@require_optional_import("PIL", "unknown")
def get_image_data(image_file: Union[str, "Image.Image"], use_b64=True) -> bytes:
    """Loads an image and returns its data either as raw bytes or in base64-encoded format.

    This function first loads an image from the specified file, URL, or base64 string using
    the `get_pil_image` function. It then saves this image in memory in PNG format and
    retrieves its binary content. Depending on the `use_b64` flag, this binary content is
    either returned directly or as a base64-encoded string.

    Parameters:
        image_file (str, or Image): The path to the image file, a URL to an image, or a base64-encoded
                          string of the image.
        use_b64 (bool): If True, the function returns a base64-encoded string of the image data.
                        If False, it returns the raw byte data of the image. Defaults to True.

    Returns:
        bytes: The image data in raw bytes if `use_b64` is False, or a base64-encoded string
               if `use_b64` is True.
    """
    image = get_pil_image(image_file)

    buffered = BytesIO()
    image.save(buffered, format="PNG")
    content = buffered.getvalue()

    if use_b64:
        return base64.b64encode(content).decode("utf-8")
    else:
        return content


@require_optional_import("PIL", "unknown")
def llava_formatter(prompt: str, order_image_tokens: bool = False) -> tuple[str, list[str]]:
    """Formats the input prompt by replacing image tags and returns the new prompt along with image locations.

    Parameters:
        - prompt (str): The input string that may contain image tags like `<img ...>`.
        - order_image_tokens (bool, optional): Whether to order the image tokens with numbers.
            It will be useful for GPT-4V. Defaults to False.

    Returns:
        - Tuple[str, List[str]]: A tuple containing the formatted string and a list of images (loaded in b64 format).
    """
    # Initialize variables
    new_prompt = prompt
    image_locations = []
    images = []
    image_count = 0

    # Regular expression pattern for matching <img ...> tags
    img_tag_pattern = re.compile(r"<img ([^>]+)>")

    # Find all image tags
    for match in img_tag_pattern.finditer(prompt):
        image_location = match.group(1)

        try:
            img_data = get_image_data(image_location)
        except Exception as e:
            # Remove the token
            print(f"Warning! Unable to load image from {image_location}, because of {e}")
            new_prompt = new_prompt.replace(match.group(0), "", 1)
            continue

        image_locations.append(image_location)
        images.append(img_data)

        # Increment the image count and replace the tag in the prompt
        new_token = f"<image {image_count}>" if order_image_tokens else "<image>"

        new_prompt = new_prompt.replace(match.group(0), new_token, 1)
        image_count += 1

    return new_prompt, images


@require_optional_import("PIL", "unknown")
def pil_to_data_uri(image: "Image.Image") -> str:
    """Converts a PIL Image object to a data URI.

    Parameters:
        image (Image.Image): The PIL Image object.

    Returns:
        str: The data URI string.
    """
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    content = buffered.getvalue()
    return convert_base64_to_data_uri(base64.b64encode(content).decode("utf-8"))


def convert_base64_to_data_uri(base64_image):
    def _get_mime_type_from_data_uri(base64_image):
        # Decode the base64 string
        image_data = base64.b64decode(base64_image)
        # Check the first few bytes for known signatures
        if image_data.startswith(b"\xff\xd8\xff"):
            return "image/jpeg"
        elif image_data.startswith(b"\x89PNG\r\n\x1a\n"):
            return "image/png"
        elif image_data.startswith(b"GIF87a") or image_data.startswith(b"GIF89a"):
            return "image/gif"
        elif image_data.startswith(b"RIFF") and image_data[8:12] == b"WEBP":
            return "image/webp"
        return "image/jpeg"  # use jpeg for unknown formats, best guess.

    mime_type = _get_mime_type_from_data_uri(base64_image)
    data_uri = f"data:{mime_type};base64,{base64_image}"
    return data_uri


@require_optional_import("PIL", "unknown")
def gpt4v_formatter(prompt: str, img_format: str = "uri") -> list[str | dict[str, Any]]:
    """Formats the input prompt by replacing image tags and returns a list of text and images.

    Args:
        prompt (str): The input string that may contain image tags like `<img ...>`.
        img_format (str): what image format should be used. One of "uri", "url", "pil".

    Returns:
        List[Union[str, dict[str, Any]]]: A list of alternating text and image dictionary items.
    """
    assert img_format in ["uri", "url", "pil"]

    output = []
    last_index = 0
    image_count = 0

    # Find all image tags
    for parsed_tag in utils.parse_tags_from_content("img", prompt):
        image_location = parsed_tag["attr"]["src"]
        try:
            if img_format == "pil":
                img_data = get_pil_image(image_location)
            elif img_format == "uri":
                img_data = get_image_data(image_location)
                img_data = convert_base64_to_data_uri(img_data)
            elif img_format == "url":
                img_data = image_location
            else:
                raise ValueError(f"Unknown image format {img_format}")
        except Exception as e:
            # Warning and skip this token
            print(f"Warning! Unable to load image from {image_location}, because {e}")
            continue

        # Add text before this image tag to output list
        output.append({"type": "text", "text": prompt[last_index : parsed_tag["match"].start()]})

        # Add image data to output list
        output.append({"type": "image_url", "image_url": {"url": img_data}})

        last_index = parsed_tag["match"].end()
        image_count += 1

    # Add remaining text to output list
    output.append({"type": "text", "text": prompt[last_index:]})
    return output


def extract_img_paths(paragraph: str) -> list:
    """Extract image paths (URLs or local paths) from a text paragraph.

    Parameters:
        paragraph (str): The input text paragraph.

    Returns:
        list: A list of extracted image paths.
    """
    # Regular expression to match image URLs and file paths.
    # This regex detects URLs and file paths with common image extensions, including support for the webp format.
    img_path_pattern = re.compile(
        r"\b(?:http[s]?://\S+\.(?:jpg|jpeg|png|gif|bmp|webp)|\S+\.(?:jpg|jpeg|png|gif|bmp|webp))\b", re.IGNORECASE
    )

    # Find all matches in the paragraph
    img_paths = re.findall(img_path_pattern, paragraph)
    return img_paths


@require_optional_import("PIL", "unknown")
def _to_pil(data: str) -> "Image.Image":
    """Converts a base64 encoded image data string to a PIL Image object.

    This function first decodes the base64 encoded string to bytes, then creates a BytesIO object from the bytes,
    and finally creates and returns a PIL Image object from the BytesIO object.

    Parameters:
        data (str): The encoded image data string.

    Returns:
        Image.Image: The PIL Image object created from the input data.
    """
    return Image.open(BytesIO(base64.b64decode(data)))


@require_optional_import("PIL", "unknown")
def message_formatter_pil_to_b64(messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Converts the PIL image URLs in the messages to base64 encoded data URIs.

    This function iterates over a list of message dictionaries. For each message,
    if it contains a 'content' key with a list of items, it looks for items
    with an 'image_url' key. The function then converts the PIL image URL
    (pointed to by 'image_url') to a base64 encoded data URI.

    Parameters:
        messages (List[Dict]): A list of message dictionaries. Each dictionary
                               may contain a 'content' key with a list of items,
                               some of which might be image URLs.

    Returns:
        List[Dict]: A new list of message dictionaries with PIL image URLs in the
                    'image_url' key converted to base64 encoded data URIs.

    Example Input:
        ```python
        [
            {'content': [{'type': 'text', 'text': 'You are a helpful AI assistant.'}], 'role': 'system'},
            {'content': [
                {'type': 'text', 'text': "What's the breed of this dog here?"},
                {'type': 'image_url', 'image_url': {'url': a PIL.Image.Image}},
                {'type': 'text', 'text': '.'}],
            'role': 'user'}
        ]
        ```

    Example Output:
        ```python
        [
            {'content': [{'type': 'text', 'text': 'You are a helpful AI assistant.'}], 'role': 'system'},
            {'content': [
                {'type': 'text', 'text': "What's the breed of this dog here?"},
                {'type': 'image_url', 'image_url': {'url': a B64 Image}},
                {'type': 'text', 'text': '.'}],
            'role': 'user'}
        ]
        ```
    """
    new_messages = []
    for message in messages:
        # Handle the new GPT messages format.
        if isinstance(message, dict) and "content" in message and isinstance(message["content"], list):
            message = copy.deepcopy(message)
            for item in message["content"]:
                if isinstance(item, dict) and "image_url" in item:
                    item["image_url"]["url"] = pil_to_data_uri(item["image_url"]["url"])

        new_messages.append(message)

    return new_messages


@require_optional_import("PIL", "unknown")
def num_tokens_from_gpt_image(
    image_data: Union[str, "Image.Image"], model: str = "gpt-4-vision", low_quality: bool = False
) -> int:
    """Calculate the number of tokens required to process an image based on its dimensions
    after scaling for different GPT models. Supports "gpt-4-vision", "gpt-4o", and "gpt-4o-mini".
    This function scales the image so that its longest edge is at most 2048 pixels and its shortest
    edge is at most 768 pixels (for "gpt-4-vision"). It then calculates the number of 512x512 tiles
    needed to cover the scaled image and computes the total tokens based on the number of these tiles.

    Reference: https://openai.com/api/pricing/

    Args:
        image_data : Union[str, Image.Image]: The image data which can either be a base64 encoded string, a URL, a file path, or a PIL Image object.
        model: str: The model being used for image processing. Can be "gpt-4-vision", "gpt-4o", or "gpt-4o-mini".
        low_quality: bool: Whether to use low-quality processing. Defaults to False.

    Returns:
        int: The total number of tokens required for processing the image.

    Examples:
    --------
    >>> from PIL import Image
    >>> img = Image.new("RGB", (2500, 2500), color="red")
    >>> num_tokens_from_gpt_image(img, model="gpt-4-vision")
    765
    """
    image = get_pil_image(image_data)  # PIL Image
    width, height = image.size

    # Determine model parameters
    if "gpt-4-vision" in model or "gpt-4-turbo" in model or "gpt-4v" in model or "gpt-4-v" in model:
        params = MODEL_PARAMS["gpt-4-vision"]
    elif "gpt-4o-mini" in model:
        params = MODEL_PARAMS["gpt-4o-mini"]
    elif "gpt-4o" in model:
        params = MODEL_PARAMS["gpt-4o"]
    else:
        raise ValueError(
            f"Model {model} is not supported. Choose 'gpt-4-vision', 'gpt-4-turbo', 'gpt-4v', 'gpt-4-v', 'gpt-4o', or 'gpt-4o-mini'."
        )

    if low_quality:
        return params["base_token_count"]

    # 1. Constrain the longest edge
    if max(width, height) > params["max_edge"]:
        scale_factor = params["max_edge"] / max(width, height)
        width, height = int(width * scale_factor), int(height * scale_factor)

    # 2. Further constrain the shortest edge
    if min(width, height) > params["min_edge"]:
        scale_factor = params["min_edge"] / min(width, height)
        width, height = int(width * scale_factor), int(height * scale_factor)

    # 3. Count how many tiles are needed to cover the image
    tiles_width = ceil(width / params["tile_size"])
    tiles_height = ceil(height / params["tile_size"])
    total_tokens = params["base_token_count"] + params["token_multiplier"] * (tiles_width * tiles_height)

    return total_tokens
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import json
from collections.abc import AsyncGenerator, Callable
from contextlib import asynccontextmanager
from logging import Logger, getLogger
from typing import TYPE_CHECKING, Any

from ......doc_utils import export_module
from ......import_utils import optional_import_block, require_optional_import
from ......llm_config import LLMConfig
from ...realtime_events import AudioDelta, FunctionCall, RealtimeEvent, SessionCreated
from ..realtime_client import RealtimeClientBase, Role, register_realtime_client

with optional_import_block():
    from websockets.asyncio.client import connect


if TYPE_CHECKING:
    from websockets.asyncio.client import ClientConnection

    from ..realtime_client import RealtimeClientProtocol

__all__ = ["GeminiRealtimeClient"]

global_logger = getLogger(__name__)


HOST = "generativelanguage.googleapis.com"
API_VERSION = "v1alpha"


@register_realtime_client()
@require_optional_import("websockets", "gemini", except_for=["get_factory", "__init__"])
@export_module("autogen.agentchat.realtime.experimental.clients")
class GeminiRealtimeClient(RealtimeClientBase):
    """(Experimental) Client for Gemini Realtime API."""

    def __init__(
        self,
        *,
        llm_config: LLMConfig | dict[str, Any],
        logger: Logger | None = None,
    ) -> None:
        """(Experimental) Client for Gemini Realtime API.

        Args:
            llm_config: The config for the client.
            logger: The logger for the client.
        """
        super().__init__()
        self._llm_config = llm_config
        self._logger = logger

        self._connection: ClientConnection | None = None
        config = llm_config["config_list"][0]

        self._model: str = config["model"]
        self._voice = config.get("voice", "charon")
        self._temperature: float = config.get("temperature", 0.8)  # type: ignore[union-attr]

        self._response_modality = "AUDIO"

        self._api_key = config.get("api_key", None)
        # todo: add test with base_url just to make sure it works
        self._base_url: str = config.get(
            "base_url",
            f"wss://{HOST}/ws/google.ai.generativelanguage.{API_VERSION}.GenerativeService.BidiGenerateContent?key={self._api_key}",
        )
        self._final_config: dict[str, Any] = {}
        self._pending_session_updates: dict[str, Any] = {}
        self._is_reading_events = False

    @property
    def logger(self) -> Logger:
        """Get the logger for the Gemini Realtime API."""
        return self._logger or global_logger

    @property
    def connection(self) -> "ClientConnection":
        """Get the Gemini WebSocket connection."""
        if self._connection is None:
            raise RuntimeError("Gemini WebSocket is not initialized")
        return self._connection

    async def send_function_result(self, call_id: str, result: str) -> None:
        """Send the result of a function call to the Gemini Realtime API.

        Args:
            call_id (str): The ID of the function call.
            result (str): The result of the function call.
        """
        msg = {
            "tool_response": {"function_responses": [{"id": call_id, "response": {"result": {"string_value": result}}}]}
        }
        if self._is_reading_events:
            await self.connection.send(json.dumps(msg))

    async def send_text(self, *, role: Role, text: str, turn_complete: bool = True) -> None:
        """Send a text message to the Gemini Realtime API.

        Args:
            role: The role of the message.
            text: The text of the message.
            turn_complete: A flag indicating if the turn is complete.
        """
        msg = {
            "client_content": {
                "turn_complete": turn_complete,
                "turns": [{"role": role, "parts": [{"text": text}]}],
            }
        }
        if self._is_reading_events:
            await self.connection.send(json.dumps(msg))

    async def send_audio(self, audio: str) -> None:
        """Send audio to the Gemini Realtime API.

        Args:
            audio (str): The audio to send.
        """
        msg = {
            "realtime_input": {
                "media_chunks": [
                    {
                        "data": audio,
                        "mime_type": "audio/pcm",
                    }
                ]
            }
        }
        await self.queue_input_audio_buffer_delta(audio)
        if self._is_reading_events:
            await self.connection.send(json.dumps(msg))

    async def truncate_audio(self, audio_end_ms: int, content_index: int, item_id: str) -> None:
        self.logger.info("This is not natively supported by Gemini Realtime API.")
        pass

    async def _initialize_session(self) -> None:
        """Initialize the session with the Gemini Realtime API."""
        session_config = {
            "setup": {
                "system_instruction": {
                    "role": "system",
                    "parts": [{"text": self._pending_session_updates.get("instructions", "")}],
                },
                "model": f"models/{self._model}",
                "tools": [
                    {
                        "function_declarations": [
                            {
                                "name": tool_schema["name"],
                                "description": tool_schema["description"],
                                "parameters": tool_schema["parameters"],
                            }
                            for tool_schema in self._pending_session_updates.get("tools", [])
                        ]
                    },
                ],
                "generation_config": {
                    "response_modalities": [self._response_modality],
                    "speech_config": {"voiceConfig": {"prebuiltVoiceConfig": {"voiceName": self._voice}}},
                    "temperature": self._temperature,
                },
            }
        }

        self.logger.info(f"Sending session update: {session_config}")
        await self.connection.send(json.dumps(session_config))

    async def session_update(self, session_options: dict[str, Any]) -> None:
        """Record session updates to be applied when the connection is established.

        Args:
            session_options (dict[str, Any]): The session options to update.
        """
        if self._is_reading_events:
            self.logger.warning("Is reading events. Session update will be ignored.")
        else:
            self._pending_session_updates.update(session_options)

    @asynccontextmanager
    async def connect(self) -> AsyncGenerator[None, None]:
        """Connect to the Gemini Realtime API."""
        try:
            async with connect(
                self._base_url, additional_headers={"Content-Type": "application/json"}
            ) as self._connection:
                yield
        finally:
            self._connection = None

    async def read_events(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read Events from the Gemini Realtime Client"""
        if self._connection is None:
            raise RuntimeError("Client is not connected, call connect() first.")
        await self._initialize_session()

        self._is_reading_events = True

        async for event in self._read_events():
            yield event

    async def _read_from_connection(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read messages from the Gemini Realtime connection."""
        async for raw_message in self.connection:
            message = raw_message.decode("ascii") if isinstance(raw_message, bytes) else raw_message
            events = self._parse_message(json.loads(message))
            for event in events:
                yield event

    def _parse_message(self, response: dict[str, Any]) -> list[RealtimeEvent]:
        """Parse a message from the Gemini Realtime API.

        Args:
            response (dict[str, Any]): The response to parse.

        Returns:
            list[RealtimeEvent]: The parsed events.
        """
        if "serverContent" in response and "modelTurn" in response["serverContent"]:
            try:
                b64data = response["serverContent"]["modelTurn"]["parts"][0]["inlineData"].pop("data")
                return [
                    AudioDelta(
                        delta=b64data,
                        item_id=None,
                        raw_message=response,
                    )
                ]
            except KeyError:
                return []
        elif "toolCall" in response:
            return [
                FunctionCall(
                    raw_message=response,
                    call_id=call["id"],
                    name=call["name"],
                    arguments=call["args"],
                )
                for call in response["toolCall"]["functionCalls"]
            ]
        elif "setupComplete" in response:
            return [
                SessionCreated(raw_message=response),
            ]
        else:
            return [RealtimeEvent(raw_message=response)]

    @classmethod
    def get_factory(
        cls, llm_config: LLMConfig | dict[str, Any], logger: Logger, **kwargs: Any
    ) -> Callable[[], "RealtimeClientProtocol"] | None:
        """Create a Realtime API client.

        Args:
            llm_config: The LLM config for the client.
            logger: The logger for the client.
            **kwargs: Additional arguments.

        Returns:
            RealtimeClientProtocol: The Realtime API client is returned if the model matches the pattern
        """
        if llm_config["config_list"][0].get("api_type") == "google" and list(kwargs.keys()) == []:
            return lambda: GeminiRealtimeClient(llm_config=llm_config, logger=logger, **kwargs)
        return None


# needed for mypy to check if GeminiRealtimeClient implements RealtimeClientProtocol
if TYPE_CHECKING:
    _client: RealtimeClientProtocol = GeminiRealtimeClient(llm_config={})
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .client import GeminiRealtimeClient

__all__ = ["GeminiRealtimeClient"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .gemini.client import GeminiRealtimeClient
from .oai.base_client import OpenAIRealtimeClient
from .realtime_client import RealtimeClientProtocol, Role, get_client

__all__ = [
    "GeminiRealtimeClient",
    "OpenAIRealtimeClient",
    "RealtimeClientProtocol",
    "Role",
    "get_client",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import json
from collections.abc import AsyncGenerator, Callable
from contextlib import asynccontextmanager
from logging import Logger, getLogger
from typing import TYPE_CHECKING, Any

from autogen.import_utils import optional_import_block, require_optional_import

from ......doc_utils import export_module
from ......llm_config import LLMConfig
from ...realtime_events import RealtimeEvent
from ..realtime_client import RealtimeClientBase, Role, register_realtime_client
from .utils import parse_oai_message

if TYPE_CHECKING:
    from ...websockets import WebSocketProtocol as WebSocket
    from ..realtime_client import RealtimeClientProtocol

with optional_import_block():
    import httpx

__all__ = ["OpenAIRealtimeWebRTCClient"]

global_logger = getLogger(__name__)


@register_realtime_client()
@require_optional_import("httpx", "openai-realtime", except_for="get_factory")
@export_module("autogen.agentchat.realtime.experimental.clients.oai")
class OpenAIRealtimeWebRTCClient(RealtimeClientBase):
    """(Experimental) Client for OpenAI Realtime API that uses WebRTC protocol."""

    def __init__(
        self,
        *,
        llm_config: LLMConfig | dict[str, Any],
        websocket: "WebSocket",
        logger: Logger | None = None,
    ) -> None:
        """(Experimental) Client for OpenAI Realtime API.

        Args:
            llm_config: The config for the client.
            websocket: the websocket to use for the connection
            logger: the logger to use for logging events
        """
        super().__init__()
        self._llm_config = llm_config
        self._logger = logger
        self._websocket = websocket

        config = llm_config["config_list"][0]
        self._model: str = config["model"]
        self._voice: str = config.get("voice", "alloy")
        self._temperature: float = llm_config.get("temperature", 0.8)  # type: ignore[union-attr]
        self._config = config
        self._base_url = config.get("base_url", "https://api.openai.com/v1/realtime/sessions")

    @property
    def logger(self) -> Logger:
        """Get the logger for the OpenAI Realtime API."""
        return self._logger or global_logger

    async def send_function_result(self, call_id: str, result: str) -> None:
        """Send the result of a function call to the OpenAI Realtime API.

        Args:
            call_id (str): The ID of the function call.
            result (str): The result of the function call.
        """
        await self._websocket.send_json({
            "type": "conversation.item.create",
            "item": {
                "type": "function_call_output",
                "call_id": call_id,
                "output": result,
            },
        })
        await self._websocket.send_json({"type": "response.create"})

    async def send_text(self, *, role: Role, text: str) -> None:
        """Send a text message to the OpenAI Realtime API.

        Args:
            role (str): The role of the message.
            text (str): The text of the message.
        """
        # await self.connection.response.cancel() #why is this here?
        await self._websocket.send_json({
            "type": "response.cancel",
        })
        await self._websocket.send_json({
            "type": "conversation.item.create",
            "item": {"type": "message", "role": role, "content": [{"type": "input_text", "text": text}]},
        })
        # await self.connection.response.create()
        await self._websocket.send_json({"type": "response.create"})

    async def send_audio(self, audio: str) -> None:
        """Send audio to the OpenAI Realtime API.
        in case of WebRTC, audio is already sent by js client, so we just queue it in order to be logged.

        Args:
            audio (str): The audio to send.
        """
        await self.queue_input_audio_buffer_delta(audio)

    async def truncate_audio(self, audio_end_ms: int, content_index: int, item_id: str) -> None:
        """Truncate audio in the OpenAI Realtime API.

        Args:
            audio_end_ms (int): The end of the audio to truncate.
            content_index (int): The index of the content to truncate.
            item_id (str): The ID of the item to truncate.
        """
        await self._websocket.send_json({
            "type": "conversation.item.truncate",
            "content_index": content_index,
            "item_id": item_id,
            "audio_end_ms": audio_end_ms,
        })

    async def session_update(self, session_options: dict[str, Any]) -> None:
        """Send a session update to the OpenAI Realtime API.

        In the case of WebRTC we can not send it directly, but we can send it
        to the javascript over the websocket, and rely on it to send session
        update to OpenAI

        Args:
            session_options (dict[str, Any]): The session options to update.
        """
        logger = self.logger
        logger.info(f"Sending session update: {session_options}")
        # await self.connection.session.update(session=session_options)  # type: ignore[arg-type]
        await self._websocket.send_json({"type": "session.update", "session": session_options})
        logger.info("Sending session update finished")

    def session_init_data(self) -> list[dict[str, Any]]:
        """Control initial session with OpenAI."""
        session_update = {
            "turn_detection": {"type": "server_vad"},
            "voice": self._voice,
            "modalities": ["audio", "text"],
            "temperature": self._temperature,
        }
        return [{"type": "session.update", "session": session_update}]

    async def _initialize_session(self) -> None: ...

    @asynccontextmanager
    async def connect(self) -> AsyncGenerator[None, None]:
        """Connect to the OpenAI Realtime API.

        In the case of WebRTC, we pass connection information over the
        websocket, so that javascript on the other end of websocket open
        actual connection to OpenAI
        """
        try:
            base_url = self._base_url
            api_key = self._config.get("api_key", None)
            headers = {
                "Authorization": f"Bearer {api_key}",  # Use os.getenv to get from environment
                "Content-Type": "application/json",
            }
            data = {
                # "model": "gpt-4o-realtime-preview-2024-12-17",
                "model": self._model,
                "voice": self._voice,
            }
            async with httpx.AsyncClient() as client:
                response = await client.post(base_url, headers=headers, json=data)
                response.raise_for_status()
                json_data = response.json()
                json_data["model"] = self._model
            if self._websocket is not None:
                session_init = self.session_init_data()
                await self._websocket.send_json({"type": "ag2.init", "config": json_data, "init": session_init})
            yield
        finally:
            pass

    async def read_events(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read events from the OpenAI Realtime API."""
        async for event in self._read_events():
            yield event

    async def _read_from_connection(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read messages from the OpenAI Realtime API connection.
        Again, in case of WebRTC, we do not read OpenAI messages directly since we
        do not hold connection to OpenAI. Instead we read messages from the websocket, and javascript
        client on the other side of the websocket that is connected to OpenAI is relaying events to us.
        """
        while True:
            try:
                message_json = await self._websocket.receive_text()
                message = json.loads(message_json)
                for event in self._parse_message(message):
                    yield event
            except Exception as e:
                self.logger.exception(f"Error reading from connection {e}")
                break

    def _parse_message(self, message: dict[str, Any]) -> list[RealtimeEvent]:
        """Parse a message from the OpenAI Realtime API.

        Args:
            message (dict[str, Any]): The message to parse.

        Returns:
            RealtimeEvent: The parsed event.
        """
        return [parse_oai_message(message)]

    @classmethod
    def get_factory(
        cls, llm_config: LLMConfig | dict[str, Any], logger: Logger, **kwargs: Any
    ) -> Callable[[], "RealtimeClientProtocol"] | None:
        """Create a Realtime API client.

        Args:
            llm_config: The config for the client.
            logger: The logger to use for logging events.
            **kwargs: Additional arguments.

        Returns:
            RealtimeClientProtocol: The Realtime API client is returned if the model matches the pattern
        """
        if llm_config["config_list"][0].get("api_type", "openai") == "openai" and list(kwargs.keys()) == ["websocket"]:
            return lambda: OpenAIRealtimeWebRTCClient(llm_config=llm_config, logger=logger, **kwargs)

        return None


# needed for mypy to check if OpenAIRealtimeWebRTCClient implements RealtimeClientProtocol
if TYPE_CHECKING:

    def _rtc_client(websocket: "WebSocket") -> RealtimeClientProtocol:
        return OpenAIRealtimeWebRTCClient(llm_config={}, websocket=websocket)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import AsyncGenerator, Callable
from contextlib import asynccontextmanager
from logging import Logger, getLogger
from typing import TYPE_CHECKING, Any

from ......doc_utils import export_module
from ......import_utils import optional_import_block, require_optional_import
from ......llm_config import LLMConfig
from ...realtime_events import RealtimeEvent
from ..realtime_client import RealtimeClientBase, Role, register_realtime_client
from .utils import parse_oai_message

with optional_import_block():
    from openai import DEFAULT_MAX_RETRIES, NOT_GIVEN, AsyncOpenAI
    from openai.resources.beta.realtime.realtime import AsyncRealtimeConnection


if TYPE_CHECKING:
    from ..realtime_client import RealtimeClientProtocol

__all__ = ["OpenAIRealtimeClient"]

global_logger = getLogger(__name__)


@register_realtime_client()
@require_optional_import("openai>=1.66.2", "openai-realtime", except_for=["get_factory", "__init__"])
@export_module("autogen.agentchat.realtime.experimental.clients")
class OpenAIRealtimeClient(RealtimeClientBase):
    """(Experimental) Client for OpenAI Realtime API."""

    def __init__(
        self,
        *,
        llm_config: LLMConfig | dict[str, Any],
        logger: Logger | None = None,
    ) -> None:
        """(Experimental) Client for OpenAI Realtime API.

        Args:
            llm_config: The config for the client.
            logger: the logger to use for logging events
        """
        super().__init__()
        self._llm_config = llm_config
        self._logger = logger

        self._connection: AsyncRealtimeConnection | None = None

        self.config = llm_config["config_list"][0]
        # model is passed to self._client.beta.realtime.connect function later
        self._model: str = self.config["model"]
        self._voice: str = self.config.get("voice", "alloy")
        self._temperature: float = llm_config.get("temperature", 0.8)  # type: ignore[union-attr]

        self._client: AsyncOpenAI | None = None

    @property
    def logger(self) -> Logger:
        """Get the logger for the OpenAI Realtime API."""
        return self._logger or global_logger

    @property
    def connection(self) -> "AsyncRealtimeConnection":
        """Get the OpenAI WebSocket connection."""
        if self._connection is None:
            raise RuntimeError("OpenAI WebSocket is not initialized")
        return self._connection

    async def send_function_result(self, call_id: str, result: str) -> None:
        """Send the result of a function call to the OpenAI Realtime API.

        Args:
            call_id (str): The ID of the function call.
            result (str): The result of the function call.
        """
        await self.connection.conversation.item.create(
            item={
                "type": "function_call_output",
                "call_id": call_id,
                "output": result,
            },
        )

        await self.connection.response.create()

    async def send_text(self, *, role: Role, text: str) -> None:
        """Send a text message to the OpenAI Realtime API.

        Args:
            role (str): The role of the message.
            text (str): The text of the message.
        """
        await self.connection.response.cancel()
        await self.connection.conversation.item.create(
            item={"type": "message", "role": role, "content": [{"type": "input_text", "text": text}]}
        )
        await self.connection.response.create()

    async def send_audio(self, audio: str) -> None:
        """Send audio to the OpenAI Realtime API.

        Args:
            audio (str): The audio to send.
        """
        await self.queue_input_audio_buffer_delta(audio)
        await self.connection.input_audio_buffer.append(audio=audio)

    async def truncate_audio(self, audio_end_ms: int, content_index: int, item_id: str) -> None:
        """Truncate audio in the OpenAI Realtime API.

        Args:
            audio_end_ms (int): The end of the audio to truncate.
            content_index (int): The index of the content to truncate.
            item_id (str): The ID of the item to truncate.
        """
        await self.connection.conversation.item.truncate(
            audio_end_ms=audio_end_ms, content_index=content_index, item_id=item_id
        )

    async def _initialize_session(self) -> None:
        """Control initial session with OpenAI."""
        session_update = {
            "turn_detection": {"type": "server_vad"},
            "voice": self._voice,
            "modalities": ["audio", "text"],
            "temperature": self._temperature,
        }
        await self.session_update(session_options=session_update)

    async def session_update(self, session_options: dict[str, Any]) -> None:
        """Send a session update to the OpenAI Realtime API.

        Args:
            session_options (dict[str, Any]): The session options to update.
        """
        logger = self.logger
        logger.info(f"Sending session update: {session_options}")
        await self.connection.session.update(session=session_options)  # type: ignore[arg-type]
        logger.info("Sending session update finished")

    @asynccontextmanager
    async def connect(self) -> AsyncGenerator[None, None]:
        """Connect to the OpenAI Realtime API."""
        try:
            if not self._client:
                self._client = AsyncOpenAI(
                    api_key=self.config.get("api_key", None),
                    organization=self.config.get("organization", None),
                    project=self.config.get("project", None),
                    base_url=self.config.get("base_url", None),
                    websocket_base_url=self.config.get("websocket_base_url", None),
                    timeout=self.config.get("timeout", NOT_GIVEN),
                    max_retries=self.config.get("max_retries", DEFAULT_MAX_RETRIES),
                    default_headers=self.config.get("default_headers", None),
                    default_query=self.config.get("default_query", None),
                )
            async with self._client.beta.realtime.connect(
                model=self._model,
            ) as self._connection:
                await self._initialize_session()
                yield
        finally:
            self._connection = None

    async def read_events(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read messages from the OpenAI Realtime API."""
        if self._connection is None:
            raise RuntimeError("Client is not connected, call connect() first.")

        try:
            async for event in self._read_events():
                yield event

        finally:
            self._connection = None

    async def _read_from_connection(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read messages from the OpenAI Realtime API."""
        async for message in self._connection:
            for event in self._parse_message(message.model_dump()):
                yield event

    def _parse_message(self, message: dict[str, Any]) -> list[RealtimeEvent]:
        """Parse a message from the OpenAI Realtime API.

        Args:
            message (dict[str, Any]): The message to parse.

        Returns:
            RealtimeEvent: The parsed event.
        """
        return [parse_oai_message(message)]

    @classmethod
    def get_factory(
        cls, llm_config: LLMConfig | dict[str, Any], logger: Logger, **kwargs: Any
    ) -> Callable[[], "RealtimeClientProtocol"] | None:
        """Create a Realtime API client.

        Args:
            llm_config: The config for the client.
            logger: The logger to use for logging events.
            kwargs: Additional arguments.

        Returns:
            RealtimeClientProtocol: The Realtime API client is returned if the model matches the pattern
        """
        if llm_config["config_list"][0].get("api_type", "openai") == "openai" and list(kwargs.keys()) == []:
            return lambda: OpenAIRealtimeClient(llm_config=llm_config, logger=logger, **kwargs)
        return None


# needed for mypy to check if OpenAIRealtimeWebRTCClient implements RealtimeClientProtocol
if TYPE_CHECKING:
    _client: RealtimeClientProtocol = OpenAIRealtimeClient(llm_config={})
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .base_client import OpenAIRealtimeClient
from .rtc_client import OpenAIRealtimeWebRTCClient

__all__ = ["OpenAIRealtimeClient", "OpenAIRealtimeWebRTCClient"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import json
from typing import Any

from ...realtime_events import (
    AudioDelta,
    FunctionCall,
    InputAudioBufferDelta,
    RealtimeEvent,
    SessionCreated,
    SessionUpdated,
    SpeechStarted,
)

__all__ = ["parse_oai_message"]


def parse_oai_message(message: dict[str, Any]) -> RealtimeEvent:
    """Parse a message from the OpenAI Realtime API.

    Args:
        message (dict[str, Any]): The message to parse.

    Returns:
        RealtimeEvent: The parsed event.
    """
    if message.get("type") == "session.created":
        return SessionCreated(raw_message=message)
    elif message.get("type") == "session.updated":
        return SessionUpdated(raw_message=message)
    elif message.get("type") == "response.audio.delta":
        return AudioDelta(raw_message=message, delta=message["delta"], item_id=message["item_id"])
    elif message.get("type") == "input_audio_buffer.speech_started":
        return SpeechStarted(raw_message=message)
    elif message.get("type") == "input_audio_buffer.delta":
        return InputAudioBufferDelta(delta=message["delta"], item_id=None, raw_message=message)
    elif message.get("type") == "response.function_call_arguments.done":
        return FunctionCall(
            raw_message=message,
            call_id=message["call_id"],
            name=message["name"],
            arguments=json.loads(message["arguments"]),
        )
    else:
        return RealtimeEvent(raw_message=message)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
from collections.abc import AsyncGenerator, Callable
from contextlib import AbstractAsyncContextManager
from logging import Logger
from typing import Any, Literal, Protocol, TypeVar, runtime_checkable

from anyio import create_task_group

from .....doc_utils import export_module
from .....llm_config import LLMConfig
from ..realtime_events import InputAudioBufferDelta, RealtimeEvent

__all__ = ["RealtimeClientProtocol", "Role", "get_client", "register_realtime_client"]

# define role literal type for typing
Role = Literal["user", "assistant", "system"]


@runtime_checkable
@export_module("autogen.agentchat.realtime.experimental.clients")
class RealtimeClientProtocol(Protocol):
    async def send_function_result(self, call_id: str, result: str) -> None:
        """Send the result of a function call to a Realtime API.

        Args:
            call_id (str): The ID of the function call.
            result (str): The result of the function call.
        """
        ...

    async def send_text(self, *, role: Role, text: str) -> None:
        """Send a text message to a Realtime API.

        Args:
            role (str): The role of the message.
            text (str): The text of the message.
        """
        ...

    async def send_audio(self, audio: str) -> None:
        """Send audio to a Realtime API.

        Args:
            audio (str): The audio to send.
        """
        ...

    async def truncate_audio(self, audio_end_ms: int, content_index: int, item_id: str) -> None:
        """Truncate audio in a Realtime API.

        Args:
            audio_end_ms (int): The end of the audio to truncate.
            content_index (int): The index of the content to truncate.
            item_id (str): The ID of the item to truncate.
        """
        ...

    async def session_update(self, session_options: dict[str, Any]) -> None:
        """Send a session update to a Realtime API.

        Args:
            session_options (dict[str, Any]): The session options to update.
        """
        ...

    def connect(self) -> AbstractAsyncContextManager[None]: ...

    def read_events(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read events from a Realtime Client."""
        ...

    async def _read_from_connection(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read events from a Realtime connection."""
        ...

    def _parse_message(self, message: dict[str, Any]) -> list[RealtimeEvent]:
        """Parse a message from a Realtime API.

        Args:
            message (dict[str, Any]): The message to parse.

        Returns:
            list[RealtimeEvent]: The parsed events.
        """
        ...

    @classmethod
    def get_factory(
        cls, llm_config: LLMConfig | dict[str, Any], logger: Logger, **kwargs: Any
    ) -> Callable[[], "RealtimeClientProtocol"] | None:
        """Create a Realtime API client.

        Args:
            llm_config: The config for the client.
            logger: The logger to use for logging events.
            **kwargs: Additional arguments.

        Returns:
            RealtimeClientProtocol: The Realtime API client is returned if the model matches the pattern
        """
        ...


class RealtimeClientBase:
    def __init__(self):
        self._eventQueue = asyncio.Queue()

    async def add_event(self, event: RealtimeEvent | None):
        await self._eventQueue.put(event)

    async def get_event(self) -> RealtimeEvent | None:
        return await self._eventQueue.get()

    async def _read_from_connection_task(self):
        async for event in self._read_from_connection():
            await self.add_event(event)
        await self.add_event(None)

    async def _read_events(self) -> AsyncGenerator[RealtimeEvent, None]:
        """Read events from a Realtime Client."""
        async with create_task_group() as tg:
            tg.start_soon(self._read_from_connection_task)
            while True:
                try:
                    event = await self._eventQueue.get()
                    if event is not None:
                        yield event
                    else:
                        break
                except Exception:
                    break

    async def queue_input_audio_buffer_delta(self, audio: str) -> None:
        """Queue InputAudioBufferDelta.

        Args:
            audio (str): The audio.
        """
        await self.add_event(InputAudioBufferDelta(delta=audio, item_id=None, raw_message={}))


_realtime_client_classes: dict[str, type[RealtimeClientProtocol]] = {}

T = TypeVar("T", bound=RealtimeClientProtocol)


def register_realtime_client() -> Callable[[type[T]], type[T]]:
    """Register a Realtime API client.

    Returns:
        Callable[[type[T]], type[T]]: The decorator to register the Realtime API client
    """

    def decorator(client_cls: type[T]) -> type[T]:
        """Register a Realtime API client.

        Args:
            client_cls: The client to register.
        """
        global _realtime_client_classes
        fqn = f"{client_cls.__module__}.{client_cls.__name__}"
        _realtime_client_classes[fqn] = client_cls

        return client_cls

    return decorator


@export_module("autogen.agentchat.realtime.experimental.clients")
def get_client(llm_config: LLMConfig | dict[str, Any], logger: Logger, **kwargs: Any) -> "RealtimeClientProtocol":
    """Get a registered Realtime API client.

    Args:
        llm_config: The config for the client.
        logger: The logger to use for logging events.
        **kwargs: Additional arguments.

    Returns:
        RealtimeClientProtocol: The Realtime API client.
    """
    global _realtime_client_classes
    for _, client_cls in _realtime_client_classes.items():
        factory = client_cls.get_factory(llm_config=llm_config, logger=logger, **kwargs)
        if factory:
            return factory()

    raise ValueError("Realtime API client not found.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import AsyncIterator
from typing import Any, Protocol, runtime_checkable

__all__ = ["WebSocketProtocol"]


@runtime_checkable
class WebSocketProtocol(Protocol):
    """WebSocket protocol for sending and receiving JSON data modelled after FastAPI's WebSocket."""

    async def send_json(self, data: Any, mode: str = "text") -> None: ...

    async def receive_json(self, mode: str = "text") -> Any: ...

    async def receive_text(self) -> str: ...

    def iter_text(self) -> AsyncIterator[str]: ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from abc import ABC, abstractmethod
from logging import Logger, getLogger
from typing import TYPE_CHECKING

from anyio import Event

from ....doc_utils import export_module
from .clients.realtime_client import RealtimeClientProtocol
from .realtime_events import RealtimeEvent

if TYPE_CHECKING:
    from .realtime_agent import RealtimeAgent

__all__ = ["RealtimeObserver"]

global_logger = getLogger(__name__)


@export_module("autogen.agentchat.realtime.experimental")
class RealtimeObserver(ABC):
    """Observer for the OpenAI Realtime API."""

    def __init__(self, *, logger: Logger | None = None) -> None:
        """Observer for the OpenAI Realtime API.

        Args:
            logger (Logger): The logger for the observer.
        """
        self._ready_event = Event()
        self._agent: RealtimeAgent | None = None
        self._logger = logger

    @property
    def logger(self) -> Logger:
        return self._logger or global_logger

    @property
    def agent(self) -> "RealtimeAgent":
        if self._agent is None:
            raise RuntimeError("Agent has not been set.")
        return self._agent

    @property
    def realtime_client(self) -> RealtimeClientProtocol:
        if self._agent is None:
            raise RuntimeError("Agent has not been set.")
        if self._agent.realtime_client is None:
            raise RuntimeError("Realtime client has not been set.")

        return self._agent.realtime_client

    async def run(self, agent: "RealtimeAgent") -> None:
        """Run the observer with the agent.

        When implementing, be sure to call `self._ready_event.set()` when the observer is ready to process events.

        Args:
            agent (RealtimeAgent): The realtime agent attached to the observer.
        """
        self._agent = agent
        await self.initialize_session()
        self._ready_event.set()

        await self.run_loop()

    @abstractmethod
    async def run_loop(self) -> None:
        """Run the loop if needed.

        This method is called after the observer is ready to process events.
        Events will be processed by the on_event method, this is just a hook for additional processing.
        Use initialize_session to set up the session.
        """
        ...

    @abstractmethod
    async def initialize_session(self) -> None:
        """Initialize the session for the observer."""
        ...

    async def wait_for_ready(self) -> None:
        """Get the event that is set when the observer is ready."""
        await self._ready_event.wait()

    @abstractmethod
    async def on_event(self, event: RealtimeEvent) -> None:
        """Handle an event from the OpenAI Realtime API.

        Args:
            event (RealtimeServerEvent): The event from the OpenAI Realtime API.
        """
        ...

    async def on_close(self) -> None:
        """Handle close of RealtimeClient."""
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .audio_adapters import TwilioAudioAdapter, WebSocketAudioAdapter
from .audio_observer import AudioObserver
from .function_observer import FunctionObserver
from .realtime_agent import RealtimeAgent
from .realtime_observer import RealtimeObserver
from .realtime_swarm import register_swarm

__all__ = [
    "AudioObserver",
    "FunctionObserver",
    "RealtimeAgent",
    "RealtimeObserver",
    "TwilioAudioAdapter",
    "WebSocketAudioAdapter",
    "register_swarm",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import uuid
import warnings
from collections import defaultdict
from collections.abc import Callable
from functools import partial
from typing import TYPE_CHECKING, Any, Optional, TypeVar

import anyio
from anyio import create_task_group, from_thread

from ....agentchat.contrib.swarm_agent import AfterWorkOption, initiate_swarm_chat
from ....cache import AbstractCache
from ....code_utils import content_str
from ....doc_utils import export_module
from ....fast_depends.utils import asyncify
from ... import Agent, ChatResult, ConversableAgent, LLMAgent
from ...utils import consolidate_chat_info, gather_usage_summary

if TYPE_CHECKING:
    from .clients import Role
    from .realtime_agent import RealtimeAgent

__all__ = ["register_swarm"]

SWARM_SYSTEM_MESSAGE = (
    "You are a helpful voice assistant. Your task is to listen to user and to coordinate the tasks based on his/her inputs."
    "You can and will communicate using audio output only."
)

QUESTION_ROLE: "Role" = "user"
QUESTION_MESSAGE = (
    "I have a question/information for myself. DO NOT ANSWER YOURSELF, GET THE ANSWER FROM ME. "
    "repeat the question to me **WITH AUDIO OUTPUT** and AFTER YOU GET THE ANSWER FROM ME call 'answer_task_question' with the answer in first person\n\n"
    "IMPORTANT: repeat just the question, without any additional information or context\n\n"
    "The question is: '{}'\n\n"
)
QUESTION_TIMEOUT_SECONDS = 20

logger = logging.getLogger(__name__)

F = TypeVar("F", bound=Callable[..., Any])


def message_to_dict(message: dict[str, Any] | str) -> dict[str, Any]:
    if isinstance(message, str):
        return {"content": message}
    elif isinstance(message, dict):
        return message
    else:
        return dict(message)


def parse_oai_message(message: dict[str, Any] | str, role: str, adressee: Agent) -> dict[str, Any]:
    """Parse a message into an OpenAI-compatible message format.

    Args:
        message: The message to parse.
        role: The role associated with the message.
        adressee: The agent that will receive the message.

    Returns:
        The parsed message in OpenAI-compatible format.

    Raises:
        ValueError: If the message lacks required fields like 'content', 'function_call', or 'tool_calls'.
    """
    message = message_to_dict(message)

    # Extract relevant fields while ensuring none are None
    oai_message = {
        key: message[key]
        for key in ("content", "function_call", "tool_calls", "tool_responses", "tool_call_id", "name", "context")
        if key in message and message[key] is not None
    }

    # Validate or set the content field
    if "content" not in oai_message:
        if "function_call" in oai_message or "tool_calls" in oai_message:
            oai_message["content"] = None
        else:
            raise ValueError("Message must have either 'content', 'function_call', or 'tool_calls' field.")

    # Determine and assign the role
    if message.get("role") in ["function", "tool"]:
        oai_message["role"] = message["role"]
        # Ensure all tool responses have string content
        for tool_response in oai_message.get("tool_responses", []):
            tool_response["content"] = str(tool_response["content"])
    elif "override_role" in message:
        oai_message["role"] = message["override_role"]
    else:
        oai_message["role"] = role

    # Enforce specific role requirements for assistant messages
    if oai_message.get("function_call") or oai_message.get("tool_calls"):
        oai_message["role"] = "assistant"

    # Add a name field if missing
    if "name" not in oai_message:
        oai_message["name"] = adressee.name

    return oai_message


class SwarmableAgent(Agent):
    """A class for an agent that can participate in a swarm chat."""

    def __init__(
        self,
        name: str,
        system_message: str = "You are a helpful AI Assistant.",
        is_termination_msg: Callable[..., bool] | None = None,
        description: str | None = None,
        silent: bool | None = None,
    ):
        self._oai_messages: dict[Agent, Any] = defaultdict(list)

        self._system_message = system_message
        self._description = description if description is not None else system_message
        self._is_termination_msg = (
            is_termination_msg
            if is_termination_msg is not None
            else (lambda x: content_str(x.get("content")) == "TERMINATE")
        )
        self.silent = silent

        self._name = name

        # Initialize standalone client cache object.
        self.client_cache = None
        self.previous_cache = None

        self.reply_at_receive: dict[Agent, bool] = defaultdict(bool)

    @property
    def system_message(self) -> str:
        return self._system_message

    def update_system_message(self, system_message: str) -> None:
        """Update this agent's system message.

        Args:
            system_message (str): system message for inference.
        """
        self._system_message = system_message

    @property
    def name(self) -> str:
        return self._name

    @property
    def description(self) -> str:
        return self._description

    def send(
        self,
        message: dict[str, Any] | str,
        recipient: Agent,
        request_reply: bool | None = None,
        silent: bool | None = False,
    ) -> None:
        self._oai_messages[recipient].append(parse_oai_message(message, "assistant", recipient))
        recipient.receive(message, self, request_reply)

    def receive(
        self,
        message: dict[str, Any] | str,
        sender: Agent,
        request_reply: bool | None = None,
        silent: bool | None = False,
    ) -> None:
        self._oai_messages[sender].append(parse_oai_message(message, "user", self))
        if request_reply is False or (request_reply is None and self.reply_at_receive[sender] is False):
            return
        reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)
        if reply is not None:
            self.send(reply, sender, silent=silent)

    def generate_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Optional["Agent"] = None,
        **kwargs: Any,
    ) -> str | dict[str, Any] | None:
        if messages is None:
            if sender is None:
                raise ValueError("Either messages or sender must be provided.")
            messages = self._oai_messages[sender]

        _, reply = self.check_termination_and_human_reply(messages=messages, sender=sender, config=None)

        return reply

    def check_termination_and_human_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | None]:
        raise NotImplementedError

    def initiate_chat(
        self,
        recipient: ConversableAgent,
        message: dict[str, Any] | str,
        clear_history: bool = True,
        silent: bool | None = False,
        cache: AbstractCache | None = None,
        summary_args: dict[str, Any] | None = {},
        **kwargs: dict[str, Any],
    ) -> ChatResult:
        chat_id = uuid.uuid4().int
        _chat_info = locals().copy()
        _chat_info["sender"] = self
        consolidate_chat_info(_chat_info, uniform_sender=self)
        recipient._raise_exception_on_async_reply_functions()
        recipient.previous_cache = recipient.client_cache  # type: ignore[attr-defined]
        recipient.client_cache = cache  # type: ignore[attr-defined, assignment]

        self._prepare_chat(recipient, clear_history)
        self.send(message, recipient, silent=silent)
        summary = self._last_msg_as_summary(self, recipient, summary_args)

        recipient.client_cache = recipient.previous_cache  # type: ignore[attr-defined]
        recipient.previous_cache = None  # type: ignore[attr-defined]

        chat_result = ChatResult(
            chat_id=chat_id,
            chat_history=self.chat_messages[recipient],
            summary=summary,
            cost=gather_usage_summary([self, recipient]),  # type: ignore[arg-type]
            human_input=[],
        )
        return chat_result

    async def a_generate_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Optional["Agent"] = None,
        **kwargs: Any,
    ) -> str | dict[str, Any] | None:
        return self.generate_reply(messages=messages, sender=sender, **kwargs)

    async def a_receive(
        self,
        message: dict[str, Any] | str,
        sender: "Agent",
        request_reply: bool | None = None,
    ) -> None:
        self.receive(message, sender, request_reply)

    async def a_send(
        self,
        message: dict[str, Any] | str,
        recipient: "Agent",
        request_reply: bool | None = None,
    ) -> None:
        self.send(message, recipient, request_reply)

    @property
    def chat_messages(self) -> dict[Agent, list[dict[str, Any]]]:
        """A dictionary of conversations from agent to list of messages."""
        return self._oai_messages

    def last_message(self, agent: Agent | None = None) -> dict[str, Any] | None:
        if agent is None:
            n_conversations = len(self._oai_messages)
            if n_conversations == 0:
                return None
            if n_conversations == 1:
                for conversation in self._oai_messages.values():
                    return conversation[-1]  # type: ignore[no-any-return]
            raise ValueError("More than one conversation is found. Please specify the sender to get the last message.")
        if agent not in self._oai_messages():
            raise KeyError(
                f"The agent '{agent.name}' is not present in any conversation. No history available for this agent."
            )
        return self._oai_messages[agent][-1]  # type: ignore[no-any-return]

    def _prepare_chat(
        self,
        recipient: ConversableAgent,
        clear_history: bool,
        prepare_recipient: bool = True,
        reply_at_receive: bool = True,
    ) -> None:
        self.reply_at_receive[recipient] = reply_at_receive
        if clear_history:
            self._oai_messages[recipient].clear()
        if prepare_recipient:
            recipient._prepare_chat(self, clear_history, False, reply_at_receive)  # type: ignore[arg-type]

    def _raise_exception_on_async_reply_functions(self) -> None:
        pass

    def set_ui_tools(self, tools: list | None = None) -> None:
        """Set UI tools for the agent."""
        pass

    def unset_ui_tools(self) -> None:
        """Unset UI tools for the agent."""
        pass

    @staticmethod
    def _last_msg_as_summary(sender: Agent, recipient: Agent, summary_args: dict[str, Any] | None) -> str:
        """Get a chat summary from the last message of the recipient."""
        summary = ""
        try:
            content = recipient.last_message(sender)["content"]  # type: ignore[attr-defined]
            if isinstance(content, str):
                summary = content.replace("TERMINATE", "")
            elif isinstance(content, list):
                summary = "\n".join(
                    x["text"].replace("TERMINATE", "") for x in content if isinstance(x, dict) and "text" in x
                )
        except (IndexError, AttributeError) as e:
            warnings.warn(f"Cannot extract summary using last_msg: {e}. Using an empty str as summary.", UserWarning)
        return summary


# check that the SwarmableAgent class is implementing LLMAgent protocol
if TYPE_CHECKING:

    def _create_swarmable_agent(
        name: str,
        system_message: str,
        is_termination_msg: Callable[..., bool] | None,
        description: str | None,
        silent: bool | None,
    ) -> LLMAgent:
        return SwarmableAgent(
            name=name,
            system_message=system_message,
            is_termination_msg=is_termination_msg,
            description=description,
            silent=silent,
        )


class SwarmableRealtimeAgent(SwarmableAgent):
    def __init__(
        self,
        realtime_agent: "RealtimeAgent",
        initial_agent: ConversableAgent,
        agents: list[ConversableAgent],
        question_message: str | None = None,
    ) -> None:
        self._initial_agent = initial_agent
        self._agents = agents
        self._realtime_agent = realtime_agent

        self._answer_event = anyio.Event()
        self._answer: str = ""
        self.question_message = question_message or QUESTION_MESSAGE

        super().__init__(
            name=realtime_agent._name,
            is_termination_msg=None,
            description=None,
            silent=None,
        )
        self.input_guardrails = []
        self.output_guardrails = []

    def reset_answer(self) -> None:
        """Reset the answer event."""
        self._answer_event = anyio.Event()

    def set_answer(self, answer: str) -> str:
        """Set the answer to the question."""
        self._answer = answer
        self._answer_event.set()
        return "Answer set successfully."

    async def get_answer(self) -> str:
        """Get the answer to the question."""
        await self._answer_event.wait()
        return self._answer

    async def ask_question(self, question: str, question_timeout: int) -> None:
        """Send a question for the user to the agent and wait for the answer.
        If the answer is not received within the timeout, the question is repeated.

        Args:
            question: The question to ask the user.
            question_timeout: The time in seconds to wait for the answer.
        """
        self.reset_answer()
        realtime_client = self._realtime_agent._realtime_client
        await realtime_client.send_text(role=QUESTION_ROLE, text=question)

        async def _check_event_set(timeout: int = question_timeout) -> bool:
            for _ in range(timeout):
                if self._answer_event.is_set():
                    return True
                await anyio.sleep(1)
            return False

        while not await _check_event_set():
            await realtime_client.send_text(role=QUESTION_ROLE, text=question)

    def check_termination_and_human_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: Any | None = None,
    ) -> tuple[bool, str | None]:
        """Check if the conversation should be terminated and if the agent should reply.

        Called when its agents turn in the chat conversation.

        Args:
            messages (list[dict[str, Any]]): The messages in the conversation.
            sender (Agent): The agent that sent the message.
            config (Optional[Any]): The configuration for the agent.
        """
        if not messages:
            return False, None

        async def get_input() -> None:
            async with create_task_group() as tg:
                tg.start_soon(
                    self.ask_question,
                    self.question_message.format(messages[-1]["content"]),
                    question_timeout=QUESTION_TIMEOUT_SECONDS,
                )

        from_thread.run_sync(get_input)

        return True, {"role": "user", "content": self._answer}  # type: ignore[return-value]

    def start_chat(self) -> None:
        raise NotImplementedError

    def configure_realtime_agent(self, system_message: str | None) -> None:
        realtime_agent = self._realtime_agent

        logger = realtime_agent.logger
        if not system_message:
            if realtime_agent.system_message != "You are a helpful AI Assistant.":
                logger.warning(
                    "Overriding system message set up in `__init__`, please use `system_message` parameter of the `register_swarm` function instead."
                )
            system_message = SWARM_SYSTEM_MESSAGE

        realtime_agent._system_message = system_message

        realtime_agent.register_realtime_function(
            name="answer_task_question", description="Answer question from the task"
        )(self.set_answer)

        async def on_observers_ready() -> None:
            self._realtime_agent._tg.start_soon(
                asyncify(
                    partial(
                        initiate_swarm_chat,
                        initial_agent=self._initial_agent,
                        agents=self._agents,
                        user_agent=self,  # type: ignore[arg-type]
                        messages="Find out what the user wants.",
                        after_work=AfterWorkOption.REVERT_TO_USER,
                    )
                )
            )

        self._realtime_agent.callbacks.on_observers_ready = on_observers_ready


@export_module("autogen.agentchat.realtime.experimental")
def register_swarm(
    *,
    realtime_agent: "RealtimeAgent",
    initial_agent: ConversableAgent,
    agents: list[ConversableAgent],
    system_message: str | None = None,
    question_message: str | None = None,
) -> None:
    """Create a SwarmableRealtimeAgent.

    Args:
        realtime_agent (RealtimeAgent): The RealtimeAgent to create the SwarmableRealtimeAgent from.
        initial_agent (ConversableAgent): The initial agent.
        agents (list[ConversableAgent]): The agents in the swarm.
        system_message (Optional[str]): The system message to set for the agent. If None, the default system message is used.
        question_message (Optional[str]): The question message to set for the agent. If None, the default QUESTION_MESSAGE is used.
    """
    swarmable_agent = SwarmableRealtimeAgent(
        realtime_agent=realtime_agent, initial_agent=initial_agent, agents=agents, question_message=question_message
    )

    swarmable_agent.configure_realtime_agent(system_message=system_message)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import base64
import json
from logging import Logger
from typing import TYPE_CHECKING

from .....doc_utils import export_module
from ..realtime_events import AudioDelta, RealtimeEvent, SpeechStarted
from ..realtime_observer import RealtimeObserver

if TYPE_CHECKING:
    from ..websockets import WebSocketProtocol as WebSocket


LOG_EVENT_TYPES = [
    "error",
    "response.content.done",
    "rate_limits.updated",
    "response.done",
    "input_audio_buffer.committed",
    "input_audio_buffer.speech_stopped",
    "input_audio_buffer.speech_started",
    "session.created",
]
SHOW_TIMING_MATH = False


@export_module("autogen.agentchat.realtime.experimental")
class TwilioAudioAdapter(RealtimeObserver):
    """Adapter for streaming audio from Twilio to OpenAI Realtime API and vice versa."""

    def __init__(self, websocket: "WebSocket", *, logger: Logger | None = None):
        """Adapter for streaming audio from Twilio to OpenAI Realtime API and vice versa.

        Args:
            websocket: the websocket connection to the Twilio service
            logger: the logger to use for logging events
        """
        super().__init__(logger=logger)
        self.websocket = websocket

        # Connection specific state
        self.stream_sid = None
        self.latest_media_timestamp = 0
        self.last_assistant_item: str | None = None
        self.mark_queue: list[str] = []
        self.response_start_timestamp_twilio: int | None = None

    async def on_event(self, event: RealtimeEvent) -> None:
        """Receive events from the OpenAI Realtime API, send audio back to Twilio."""
        logger = self.logger

        if isinstance(event, AudioDelta):
            audio_payload = base64.b64encode(base64.b64decode(event.delta)).decode("utf-8")
            audio_delta = {"event": "media", "streamSid": self.stream_sid, "media": {"payload": audio_payload}}
            await self.websocket.send_json(audio_delta)

            if self.response_start_timestamp_twilio is None:
                self.response_start_timestamp_twilio = self.latest_media_timestamp
                if SHOW_TIMING_MATH:
                    logger.info(f"Setting start timestamp for new response: {self.response_start_timestamp_twilio}ms")

            # Update last_assistant_item safely
            if event.item_id:
                self.last_assistant_item = event.item_id

            await self.send_mark()

        # Trigger an interruption. Your use case might work better using `input_audio_buffer.speech_stopped`, or combining the two.
        if isinstance(event, SpeechStarted):
            logger.info("Speech start detected.")
            if self.last_assistant_item:
                logger.info(f"Interrupting response with id: {self.last_assistant_item}")
                await self.handle_speech_started_event()

    async def handle_speech_started_event(self) -> None:
        """Handle interruption when the caller's speech starts."""
        logger = self.logger

        logger.info("Handling speech started event.")
        if self.mark_queue and self.response_start_timestamp_twilio is not None:
            elapsed_time = self.latest_media_timestamp - self.response_start_timestamp_twilio
            if SHOW_TIMING_MATH:
                logger.info(
                    f"Calculating elapsed time for truncation: {self.latest_media_timestamp} - {self.response_start_timestamp_twilio} = {elapsed_time}ms"
                )

            if self.last_assistant_item:
                if SHOW_TIMING_MATH:
                    logger.info(f"Truncating item with ID: {self.last_assistant_item}, Truncated at: {elapsed_time}ms")

                await self.realtime_client.truncate_audio(
                    audio_end_ms=elapsed_time,
                    content_index=0,
                    item_id=self.last_assistant_item,
                )

            await self.websocket.send_json({"event": "clear", "streamSid": self.stream_sid})

            self.mark_queue.clear()
            self.last_assistant_item = None
            self.response_start_timestamp_twilio = None

    async def send_mark(self) -> None:
        """Send a mark of audio interruption to the Twilio websocket."""
        if self.stream_sid:
            mark_event = {"event": "mark", "streamSid": self.stream_sid, "mark": {"name": "responsePart"}}
            await self.websocket.send_json(mark_event)
            self.mark_queue.append("responsePart")

    async def run_loop(self) -> None:
        """Run the adapter loop."""
        logger = self.logger

        async for message in self.websocket.iter_text():
            try:
                data = json.loads(message)
                if data["event"] == "media":
                    self.latest_media_timestamp = int(data["media"]["timestamp"])
                    await self.realtime_client.send_audio(audio=data["media"]["payload"])
                elif data["event"] == "start":
                    self.stream_sid = data["start"]["streamSid"]
                    logger.info(f"Incoming stream has started {self.stream_sid}")
                    self.response_start_timestamp_twilio = None
                    self.latest_media_timestamp = 0
                    self.last_assistant_item = None
                elif data["event"] == "mark":
                    if self.mark_queue:
                        self.mark_queue.pop(0)
            except Exception as e:
                logger.warning(f"Error processing Twilio message: {e}", stack_info=True)

    async def initialize_session(self) -> None:
        """Control initial session with OpenAI."""
        session_update = {
            "input_audio_format": "g711_ulaw",
            "output_audio_format": "g711_ulaw",
        }
        await self.realtime_client.session_update(session_update)


if TYPE_CHECKING:

    def twilio_audio_adapter(websocket: "WebSocket") -> RealtimeObserver:
        return TwilioAudioAdapter(websocket)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .twilio_audio_adapter import TwilioAudioAdapter
from .websocket_audio_adapter import WebSocketAudioAdapter

__all__ = ["TwilioAudioAdapter", "WebSocketAudioAdapter"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import base64
import json
from logging import Logger
from typing import TYPE_CHECKING

from .....doc_utils import export_module
from ..realtime_events import AudioDelta, RealtimeEvent, SpeechStarted
from ..realtime_observer import RealtimeObserver

if TYPE_CHECKING:
    from ..websockets import WebSocketProtocol as WebSocket

LOG_EVENT_TYPES = [
    "error",
    "response.content.done",
    "rate_limits.updated",
    "response.done",
    "input_audio_buffer.committed",
    "input_audio_buffer.speech_stopped",
    "input_audio_buffer.speech_started",
    "session.created",
]
SHOW_TIMING_MATH = False


@export_module("autogen.agentchat.realtime.experimental")
class WebSocketAudioAdapter(RealtimeObserver):
    def __init__(self, websocket: "WebSocket", *, logger: Logger | None = None) -> None:
        """Observer for handling function calls from the OpenAI Realtime API.

        Args:
            websocket (WebSocket): The websocket connection.
            logger (Logger): The logger for the observer.
        """
        super().__init__(logger=logger)
        self.websocket = websocket

        # Connection specific state
        self.stream_sid = None
        self.latest_media_timestamp = 0
        self.last_assistant_item: str | None = None
        self.mark_queue: list[str] = []
        self.response_start_timestamp_socket: int | None = None

    async def on_event(self, event: RealtimeEvent) -> None:
        """Receive events from the OpenAI Realtime API, send audio back to websocket."""
        logger = self.logger

        if isinstance(event, AudioDelta):
            audio_payload = base64.b64encode(base64.b64decode(event.delta)).decode("utf-8")
            audio_delta = {"event": "media", "streamSid": self.stream_sid, "media": {"payload": audio_payload}}
            await self.websocket.send_json(audio_delta)

            if self.response_start_timestamp_socket is None:
                self.response_start_timestamp_socket = self.latest_media_timestamp
                if SHOW_TIMING_MATH:
                    logger.info(f"Setting start timestamp for new response: {self.response_start_timestamp_socket}ms")

            # Update last_assistant_item safely
            if event.item_id:
                self.last_assistant_item = event.item_id

            await self.send_mark()

        # Trigger an interruption. Your use case might work better using `input_audio_buffer.speech_stopped`, or combining the two.
        if isinstance(event, SpeechStarted):
            logger.info("Speech start detected.")
            if self.last_assistant_item:
                logger.info(f"Interrupting response with id: {self.last_assistant_item}")
                await self.handle_speech_started_event()

    async def handle_speech_started_event(self) -> None:
        """Handle interruption when the caller's speech starts."""
        logger = self.logger
        logger.info("Handling speech started event.")
        if self.mark_queue and self.response_start_timestamp_socket is not None:
            elapsed_time = self.latest_media_timestamp - self.response_start_timestamp_socket
            if SHOW_TIMING_MATH:
                logger.info(
                    f"Calculating elapsed time for truncation: {self.latest_media_timestamp} - {self.response_start_timestamp_socket} = {elapsed_time}ms"
                )

            if self.last_assistant_item:
                if SHOW_TIMING_MATH:
                    logger.info(f"Truncating item with ID: {self.last_assistant_item}, Truncated at: {elapsed_time}ms")

                await self.realtime_client.truncate_audio(
                    audio_end_ms=elapsed_time,
                    content_index=0,
                    item_id=self.last_assistant_item,
                )

            await self.websocket.send_json({"event": "clear", "streamSid": self.stream_sid})

            self.mark_queue.clear()
            self.last_assistant_item = None
            self.response_start_timestamp_socket = None

    async def send_mark(self) -> None:
        if self.stream_sid:
            mark_event = {"event": "mark", "streamSid": self.stream_sid, "mark": {"name": "responsePart"}}
            await self.websocket.send_json(mark_event)
            self.mark_queue.append("responsePart")

    async def initialize_session(self) -> None:
        """Control initial session with OpenAI."""
        session_update = {"input_audio_format": "pcm16", "output_audio_format": "pcm16"}
        await self.realtime_client.session_update(session_update)

    async def run_loop(self) -> None:
        """Reads data from websocket and sends it to the RealtimeClient."""
        logger = self.logger
        async for message in self.websocket.iter_text():
            try:
                data = json.loads(message)
                if data["event"] == "media":
                    self.latest_media_timestamp = int(data["media"]["timestamp"])
                    await self.realtime_client.send_audio(audio=data["media"]["payload"])
                elif data["event"] == "start":
                    self.stream_sid = data["start"]["streamSid"]
                    logger.info(f"Incoming stream has started {self.stream_sid}")
                    self.response_start_timestamp_socket = None
                    self.latest_media_timestamp = 0
                    self.last_assistant_item = None
                elif data["event"] == "mark":
                    if self.mark_queue:
                        self.mark_queue.pop(0)
            except Exception as e:
                logger.warning(f"Failed to process message: {e}", stack_info=True)


if TYPE_CHECKING:

    def websocket_audio_adapter(websocket: "WebSocket") -> RealtimeObserver:
        return WebSocketAudioAdapter(websocket)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import json
from typing import TYPE_CHECKING, Any, Optional

from pydantic import BaseModel

from ....doc_utils import export_module
from ....fast_depends.utils import asyncify
from .realtime_events import FunctionCall, RealtimeEvent
from .realtime_observer import RealtimeObserver

if TYPE_CHECKING:
    from logging import Logger


@export_module("autogen.agentchat.realtime.experimental")
class FunctionObserver(RealtimeObserver):
    """Observer for handling function calls from the OpenAI Realtime API."""

    def __init__(self, *, logger: Optional["Logger"] = None) -> None:
        """Observer for handling function calls from the OpenAI Realtime API."""
        super().__init__(logger=logger)

    async def on_event(self, event: RealtimeEvent) -> None:
        """Handle function call events from the OpenAI Realtime API.

        Args:
            event (dict[str, Any]): The event from the OpenAI Realtime API.
        """
        if isinstance(event, FunctionCall):
            self.logger.info("Received function call event")
            await self.call_function(
                call_id=event.call_id,
                name=event.name,
                kwargs=event.arguments,
            )

    async def call_function(self, call_id: str, name: str, kwargs: dict[str, Any]) -> None:
        """Call a function registered with the agent.

        Args:
            call_id (str): The ID of the function call.
            name (str): The name of the function to call.
            kwargs (Any[str, Any]): The arguments to pass to the function.
        """
        if name in self.agent.registered_realtime_tools:
            func = self.agent.registered_realtime_tools[name].func
            func = asyncify(func)
            try:
                result = await func(**kwargs)
            except Exception:
                result = "Function call failed"
                self.logger.info(f"Function call failed: {name=}, {kwargs=}", stack_info=True)

            if isinstance(result, BaseModel):
                result = result.model_dump_json()
            elif not isinstance(result, str):
                try:
                    result = json.dumps(result)
                except Exception:
                    result = str(result)

            await self.realtime_client.send_function_result(call_id, result)
        else:
            self.logger.warning(f"Function {name} called, but is not registered with the realtime agent.")

    async def initialize_session(self) -> None:
        """Add registered tools to OpenAI with a session update."""
        session_update = {
            "tools": [tool.realtime_tool_schema for tool in self.agent.registered_realtime_tools.values()],
            "tool_choice": "auto",
        }
        await self.realtime_client.session_update(session_update)

    async def run_loop(self) -> None:
        """Run the observer loop."""
        pass


if TYPE_CHECKING:
    function_observer: RealtimeObserver = FunctionObserver()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any, Literal

from pydantic import BaseModel


class RealtimeEvent(BaseModel):
    raw_message: dict[str, Any]


class SessionCreated(RealtimeEvent):
    type: Literal["session.created"] = "session.created"


class SessionUpdated(RealtimeEvent):
    type: Literal["session.updated"] = "session.updated"


class AudioDelta(RealtimeEvent):
    type: Literal["response.audio.delta"] = "response.audio.delta"
    delta: str
    item_id: Any


class InputAudioBufferDelta(RealtimeEvent):
    type: Literal["input_audio_buffer.delta"] = "input_audio_buffer.delta"
    delta: str
    item_id: Any


class SpeechStarted(RealtimeEvent):
    type: Literal["input_audio_buffer.speech_started"] = "input_audio_buffer.speech_started"


class FunctionCall(RealtimeEvent):
    type: Literal["response.function_call_arguments.done"] = "response.function_call_arguments.done"
    name: str
    arguments: dict[str, Any]
    call_id: str
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Callable
from dataclasses import dataclass
from logging import Logger, getLogger
from typing import Any, TypeVar

from anyio import create_task_group, lowlevel

from ....doc_utils import export_module
from ....llm_config import LLMConfig
from ....tools import Tool
from .clients.realtime_client import RealtimeClientProtocol, get_client
from .function_observer import FunctionObserver
from .realtime_observer import RealtimeObserver

F = TypeVar("F", bound=Callable[..., Any])

global_logger = getLogger(__name__)


@dataclass
class RealtimeAgentCallbacks:
    """Callbacks for the Realtime Agent."""

    # async empty placeholder function
    on_observers_ready: Callable[[], Any] = lambda: lowlevel.checkpoint()


@export_module("autogen.agentchat.realtime.experimental")
class RealtimeAgent:
    def __init__(
        self,
        *,
        name: str,
        audio_adapter: RealtimeObserver | None = None,
        system_message: str = "You are a helpful AI Assistant.",
        llm_config: LLMConfig | dict[str, Any] | None = None,
        logger: Logger | None = None,
        observers: list[RealtimeObserver] | None = None,
        **client_kwargs: Any,
    ):
        """(Experimental) Agent for interacting with the Realtime Clients.

        Args:
            name (str): The name of the agent.
            audio_adapter (Optional[RealtimeObserver] = None): The audio adapter for the agent.
            system_message (str): The system message for the agent.
            llm_config (LLMConfig, dict[str, Any], bool): The config for the agent.
            logger (Optional[Logger]): The logger for the agent.
            observers (Optional[list[RealtimeObserver]]): The additional observers for the agent.
            **client_kwargs (Any): The keyword arguments for the client.
        """
        self._logger = logger
        self._name = name
        self._system_message = system_message

        llm_config = LLMConfig.get_current_llm_config(llm_config)

        self._realtime_client: RealtimeClientProtocol = get_client(
            llm_config=llm_config, logger=self.logger, **client_kwargs
        )

        self._registered_realtime_tools: dict[str, Tool] = {}
        self._observers: list[RealtimeObserver] = observers if observers else []
        self._observers.append(FunctionObserver(logger=logger))
        if audio_adapter:
            self._observers.append(audio_adapter)

        self.callbacks = RealtimeAgentCallbacks()

    @property
    def system_message(self) -> str:
        """Get the system message for the agent."""
        return self._system_message

    @property
    def logger(self) -> Logger:
        """Get the logger for the agent."""
        return self._logger or global_logger

    @property
    def realtime_client(self) -> RealtimeClientProtocol:
        """Get the OpenAI Realtime Client."""
        return self._realtime_client

    @property
    def registered_realtime_tools(self) -> dict[str, Tool]:
        """Get the registered realtime tools."""
        return self._registered_realtime_tools

    def register_observer(self, observer: RealtimeObserver) -> None:
        """Register an observer with the Realtime Agent.

        Args:
            observer (RealtimeObserver): The observer to register.
        """
        self._observers.append(observer)

    async def start_observers(self) -> None:
        for observer in self._observers:
            self._tg.start_soon(observer.run, self)

        # wait for the observers to be ready
        for observer in self._observers:
            await observer.wait_for_ready()

        await self.callbacks.on_observers_ready()

    async def run(self) -> None:
        """Run the agent."""
        # everything is run in the same task group to enable easy cancellation using self._tg.cancel_scope.cancel()
        async with create_task_group() as self._tg:  # noqa: SIM117
            # connect with the client first (establishes a connection and initializes a session)
            async with self._realtime_client.connect():
                # start the observers and wait for them to be ready
                await self.realtime_client.session_update(session_options={"instructions": self.system_message})
                await self.start_observers()

                # iterate over the events
                async for event in self.realtime_client.read_events():
                    for observer in self._observers:
                        await observer.on_event(event)

    def register_realtime_function(
        self,
        *,
        name: str | None = None,
        description: str | None = None,
    ) -> Callable[[F | Tool], Tool]:
        """Decorator for registering a function to be used by an agent.

        Args:
            name (str): The name of the function.
            description (str): The description of the function.

        Returns:
            Callable[[Union[F, Tool]], Tool]: The decorator for registering a function.
        """

        def _decorator(func_or_tool: F | Tool) -> Tool:
            """Decorator for registering a function to be used by an agent.

            Args:
                func_or_tool (Union[F, Tool]): The function or tool to register.

            Returns:
                Tool: The registered tool.
            """
            tool = Tool(func_or_tool=func_or_tool, name=name, description=description)

            self._registered_realtime_tools[tool.name] = tool

            return tool

        return _decorator
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Optional

from ....doc_utils import export_module
from .realtime_events import InputAudioBufferDelta, RealtimeEvent
from .realtime_observer import RealtimeObserver

if TYPE_CHECKING:
    from logging import Logger


@export_module("autogen.agentchat.realtime.experimental")
class AudioObserver(RealtimeObserver):
    """Observer for user voice input"""

    def __init__(self, *, logger: Optional["Logger"] = None) -> None:
        """Observer for user voice input"""
        super().__init__(logger=logger)

    async def on_event(self, event: RealtimeEvent) -> None:
        """Observe voice input events from the Realtime.

        Args:
            event (dict[str, Any]): The event from the OpenAI Realtime API.
        """
        if isinstance(event, InputAudioBufferDelta):
            self.logger.info("Received audio buffer delta")

    async def initialize_session(self) -> None:
        """No need to initialize session from this observer"""
        pass

    async def run_loop(self) -> None:
        """Run the observer loop."""
        pass


if TYPE_CHECKING:
    function_observer: RealtimeObserver = AudioObserver()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import asyncio
import datetime
import logging
import uuid
import warnings
from collections import defaultdict
from dataclasses import dataclass, field
from functools import partial
from typing import Any, TypedDict

from ..doc_utils import export_module
from ..events.agent_events import PostCarryoverProcessingEvent
from ..io.base import IOStream
from .utils import consolidate_chat_info

logger = logging.getLogger(__name__)
Prerequisite = tuple[int, int]

__all__ = ["ChatResult", "a_initiate_chats", "initiate_chats"]


class CostDict(TypedDict):
    usage_including_cached_inference: dict[str, Any]
    usage_excluding_cached_inference: dict[str, Any]


@dataclass
@export_module("autogen")
class ChatResult:
    """(Experimental) The result of a chat. Almost certain to be changed."""

    chat_id: int = field(default_factory=lambda: uuid.uuid4().int)
    """chat id"""

    chat_history: list[dict[str, Any]] = field(default_factory=list)
    """The chat history."""

    summary: str = ""
    """A summary obtained from the chat."""

    cost: CostDict = field(
        default_factory=lambda: {
            "usage_including_cached_inference": {},
            "usage_excluding_cached_inference": {},
        }
    )
    """The cost of the chat.
       The value for each usage type is a dictionary containing cost information for that specific type.
           - "usage_including_cached_inference": Cost information on the total usage, including the tokens in cached inference.
           - "usage_excluding_cached_inference": Cost information on the usage of tokens, excluding the tokens in cache. No larger than "usage_including_cached_inference".
    """

    human_input: list[str] = field(default_factory=list)
    """A list of human input solicited during the chat."""


def _validate_recipients(chat_queue: list[dict[str, Any]]) -> None:
    """Validate recipients exits and warn repetitive recipients."""
    receipts_set = set()
    for chat_info in chat_queue:
        assert "recipient" in chat_info, "recipient must be provided."
        receipts_set.add(chat_info["recipient"])
    if len(receipts_set) < len(chat_queue):
        warnings.warn(
            "Repetitive recipients detected: The chat history will be cleared by default if a recipient appears more than once. To retain the chat history, please set 'clear_history=False' in the configuration of the repeating agent.",
            UserWarning,
        )


def __create_async_prerequisites(chat_queue: list[dict[str, Any]]) -> list[Prerequisite]:
    """Create list of Prerequisite (prerequisite_chat_id, chat_id)"""
    prerequisites = []
    for chat_info in chat_queue:
        if "chat_id" not in chat_info:
            raise ValueError("Each chat must have a unique id for async multi-chat execution.")
        chat_id = chat_info["chat_id"]
        pre_chats = chat_info.get("prerequisites", [])
        for pre_chat_id in pre_chats:
            if not isinstance(pre_chat_id, int):
                raise ValueError("Prerequisite chat id is not int.")
            prerequisites.append((chat_id, pre_chat_id))
    return prerequisites


def __find_async_chat_order(chat_ids: set[int], prerequisites: list[Prerequisite]) -> list[int]:
    """Find chat order for async execution based on the prerequisite chats

    Args:
        chat_ids: A set of all chat IDs that need to be scheduled
        prerequisites: A list of tuples (chat_id, prerequisite_chat_id) where each tuple indicates that chat_id depends on prerequisite_chat_id

    Returns:
        list: a list of chat_id in order.
    """
    edges = defaultdict(set)
    indegree = defaultdict(int)
    for pair in prerequisites:
        chat, pre = pair[0], pair[1]
        if chat not in edges[pre]:
            indegree[chat] += 1
            edges[pre].add(chat)
    bfs = [i for i in chat_ids if i not in indegree]
    chat_order = []
    steps = len(indegree)
    for _ in range(steps + 1):
        if not bfs:
            break
        chat_order.extend(bfs)
        nxt = []
        for node in bfs:
            if node in edges:
                for course in edges[node]:
                    indegree[course] -= 1
                    if indegree[course] == 0:
                        nxt.append(course)
                        indegree.pop(course)
                edges.pop(node)
        bfs = nxt

    if indegree:
        return []
    return chat_order


def _post_process_carryover_item(carryover_item):
    if isinstance(carryover_item, str):
        return carryover_item
    elif isinstance(carryover_item, dict) and "content" in carryover_item:
        return str(carryover_item["content"])
    else:
        return str(carryover_item)


def __post_carryover_processing(chat_info: dict[str, Any]) -> None:
    iostream = IOStream.get_default()

    if "message" not in chat_info:
        warnings.warn(
            "message is not provided in a chat_queue entry. input() will be called to get the initial message.",
            UserWarning,
        )

    iostream.send(PostCarryoverProcessingEvent(chat_info=chat_info))


@export_module("autogen")
def initiate_chats(chat_queue: list[dict[str, Any]]) -> list[ChatResult]:
    """Initiate a list of chats.

    Args:
        chat_queue (List[Dict]): A list of dictionaries containing the information about the chats.\n
            Each dictionary should contain the input arguments for
            [`ConversableAgent.initiate_chat`](../ConversableAgent#initiate-chat).\n
            For example:\n
                - `"sender"` - the sender agent.\n
                - `"recipient"` - the recipient agent.\n
                - `"clear_history"` (bool) - whether to clear the chat history with the agent.\n
                  Default is True.\n
                - `"silent"` (bool or None) - (Experimental) whether to print the messages in this\n
                  conversation. Default is False.\n
                - `"cache"` (Cache or None) - the cache client to use for this conversation.\n
                  Default is None.\n
                - `"max_turns"` (int or None) - maximum number of turns for the chat. If None, the chat\n
                  will continue until a termination condition is met. Default is None.\n
                - `"summary_method"` (str or callable) - a string or callable specifying the method to get\n
                  a summary from the chat. Default is DEFAULT_summary_method, i.e., "last_msg".\n
                - `"summary_args"` (dict) - a dictionary of arguments to be passed to the summary_method.\n
                  Default is {}.\n
                - `"message"` (str, callable or None) - if None, input() will be called to get the\n
                  initial message.\n
                - `**context` - additional context information to be passed to the chat.\n
                - `"carryover"` - It can be used to specify the carryover information to be passed\n
                  to this chat. If provided, we will combine this carryover with the "message" content when\n
                  generating the initial chat message in `generate_init_message`.\n
                - `"finished_chat_indexes_to_exclude_from_carryover"` - It can be used by specifying a list of indexes of the finished_chats list,\n
                  from which to exclude the summaries for carryover. If 'finished_chat_indexes_to_exclude_from_carryover' is not provided or an empty list,\n
                  then summary from all the finished chats will be taken.\n

    Returns:
        (list): a list of ChatResult objects corresponding to the finished chats in the chat_queue.\n
    """
    consolidate_chat_info(chat_queue)
    _validate_recipients(chat_queue)
    current_chat_queue = chat_queue.copy()
    finished_chats = []
    while current_chat_queue:
        chat_info = current_chat_queue.pop(0)
        _chat_carryover = chat_info.get("carryover", [])
        finished_chat_indexes_to_exclude_from_carryover = chat_info.get(
            "finished_chat_indexes_to_exclude_from_carryover", []
        )

        if isinstance(_chat_carryover, str):
            _chat_carryover = [_chat_carryover]
        chat_info["carryover"] = _chat_carryover + [
            r.summary for i, r in enumerate(finished_chats) if i not in finished_chat_indexes_to_exclude_from_carryover
        ]

        if not chat_info.get("silent", False):
            __post_carryover_processing(chat_info)

        sender = chat_info["sender"]
        chat_res = sender.initiate_chat(**chat_info)
        finished_chats.append(chat_res)
    return finished_chats


def __system_now_str():
    ct = datetime.datetime.now()
    return f" System time at {ct}. "


def _on_chat_future_done(chat_future: asyncio.Future, chat_id: int):
    """Update ChatResult when async Task for Chat is completed."""
    logger.debug(f"Update chat {chat_id} result on task completion." + __system_now_str())
    chat_result = chat_future.result()
    chat_result.chat_id = chat_id


async def _dependent_chat_future(
    chat_id: int, chat_info: dict[str, Any], prerequisite_chat_futures: dict[int, asyncio.Future]
) -> asyncio.Task:
    """Create an async Task for each chat."""
    logger.debug(f"Create Task for chat {chat_id}." + __system_now_str())
    _chat_carryover = chat_info.get("carryover", [])
    finished_chat_indexes_to_exclude_from_carryover = chat_info.get(
        "finished_chat_indexes_to_exclude_from_carryover", []
    )
    finished_chats = {}
    for chat in prerequisite_chat_futures:
        chat_future = prerequisite_chat_futures[chat]
        if chat_future.cancelled():
            raise RuntimeError(f"Chat {chat} is cancelled.")

        # wait for prerequisite chat results for the new chat carryover
        finished_chats[chat] = await chat_future

    if isinstance(_chat_carryover, str):
        _chat_carryover = [_chat_carryover]
    data = [
        chat_result.summary
        for chat_id, chat_result in finished_chats.items()
        if chat_id not in finished_chat_indexes_to_exclude_from_carryover
    ]
    chat_info["carryover"] = _chat_carryover + data
    if not chat_info.get("silent", False):
        __post_carryover_processing(chat_info)

    sender = chat_info["sender"]
    chat_res_future = asyncio.create_task(sender.a_initiate_chat(**chat_info))
    call_back_with_args = partial(_on_chat_future_done, chat_id=chat_id)
    chat_res_future.add_done_callback(call_back_with_args)
    logger.debug(f"Task for chat {chat_id} created." + __system_now_str())
    return chat_res_future


async def a_initiate_chats(chat_queue: list[dict[str, Any]]) -> dict[int, ChatResult]:
    """(async) Initiate a list of chats.

    Args:
        chat_queue (List[Dict]): A list of dictionaries containing the information about the chats.

            Each dictionary should contain the input arguments for
            [`ConversableAgent.initiate_chat`](../../../ConversableAgent#initiate-chat).
            For example:
                - `"sender"` - the sender agent.
                - `"recipient"` - the recipient agent.
                - `"clear_history"` (bool) - whether to clear the chat history with the agent.
                Default is True.
                - `"silent"` (bool or None) - (Experimental) whether to print the messages in this
                conversation. Default is False.
                - `"cache"` (Cache or None) - the cache client to use for this conversation.
                Default is None.
                - `"max_turns"` (int or None) - maximum number of turns for the chat. If None, the chat
                will continue until a termination condition is met. Default is None.
                - `"summary_method"` (str or callable) - a string or callable specifying the method to get
                a summary from the chat. Default is DEFAULT_summary_method, i.e., "last_msg".
                - `"summary_args"` (dict) - a dictionary of arguments to be passed to the summary_method.
                Default is {}.
                - `"message"` (str, callable or None) - if None, input() will be called to get the
                initial message.
                - `**context` - additional context information to be passed to the chat.
                - `"carryover"` - It can be used to specify the carryover information to be passed
                to this chat. If provided, we will combine this carryover with the "message" content when
                generating the initial chat message in `generate_init_message`.
                - `"finished_chat_indexes_to_exclude_from_carryover"` - It can be used by specifying a list of indexes of the finished_chats list,
                from which to exclude the summaries for carryover. If 'finished_chat_indexes_to_exclude_from_carryover' is not provided or an empty list,
                then summary from all the finished chats will be taken.


    Returns:
        - (Dict): a dict of ChatId: ChatResult corresponding to the finished chats in the chat_queue.
    """
    consolidate_chat_info(chat_queue)
    _validate_recipients(chat_queue)
    chat_book = {chat_info["chat_id"]: chat_info for chat_info in chat_queue}
    num_chats = chat_book.keys()
    prerequisites = __create_async_prerequisites(chat_queue)
    chat_order_by_id = __find_async_chat_order(num_chats, prerequisites)
    finished_chat_futures = {}
    for chat_id in chat_order_by_id:
        chat_info = chat_book[chat_id]
        prerequisite_chat_ids = chat_info.get("prerequisites", [])
        pre_chat_futures = {}
        for pre_chat_id in prerequisite_chat_ids:
            pre_chat_future = finished_chat_futures[pre_chat_id]
            pre_chat_futures[pre_chat_id] = pre_chat_future
        current_chat_future = await _dependent_chat_future(chat_id, chat_info, pre_chat_futures)
        finished_chat_futures[chat_id] = current_chat_future
    await asyncio.gather(*list(finished_chat_futures.values()))
    finished_chats = {}
    for chat in finished_chat_futures:
        chat_result = finished_chat_futures[chat].result()
        finished_chats[chat] = chat_result
    return finished_chats
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import re
from typing import Any

from ..doc_utils import export_module
from .agent import Agent


def consolidate_chat_info(
    chat_info: dict[str, Any] | list[dict[str, Any]], uniform_sender: Agent | None = None
) -> None:
    if isinstance(chat_info, dict):
        chat_info = [chat_info]
    for c in chat_info:
        if uniform_sender is None:
            assert "sender" in c, "sender must be provided."
            sender = c["sender"]
        else:
            sender = uniform_sender
        assert "recipient" in c, "recipient must be provided."
        summary_method = c.get("summary_method")
        assert (
            summary_method is None or callable(summary_method) or summary_method in ("last_msg", "reflection_with_llm")
        ), "summary_method must be a string chosen from 'reflection_with_llm' or 'last_msg' or a callable, or None."
        if summary_method == "reflection_with_llm":
            assert sender.client is not None or c["recipient"].client is not None, (
                "llm client must be set in either the recipient or sender when summary_method is reflection_with_llm."
            )


@export_module("autogen")
def gather_usage_summary(agents: list[Agent]) -> dict[str, dict[str, Any]]:
    r"""Gather usage summary from all agents.

    Args:
        agents: (list): List of agents.

    Returns:
        dictionary: A dictionary containing two keys:
            - "usage_including_cached_inference": Cost information on the total usage, including the tokens in cached inference.
            - "usage_excluding_cached_inference": Cost information on the usage of tokens, excluding the tokens in cache. No larger than "usage_including_cached_inference".

    Example:
    ```python
    {
        "usage_including_cached_inference": {
            "total_cost": 0.0006090000000000001,
            "gpt-35-turbo": {
                "cost": 0.0006090000000000001,
                "prompt_tokens": 242,
                "completion_tokens": 123,
                "total_tokens": 365,
            },
        },
        "usage_excluding_cached_inference": {
            "total_cost": 0.0006090000000000001,
            "gpt-35-turbo": {
                "cost": 0.0006090000000000001,
                "prompt_tokens": 242,
                "completion_tokens": 123,
                "total_tokens": 365,
            },
        },
    }
    ```

    Note:
    If none of the agents incurred any cost (not having a client), then the usage_including_cached_inference and usage_excluding_cached_inference will be `{'total_cost': 0}`.
    """

    def aggregate_summary(usage_summary: dict[str, Any], agent_summary: dict[str, Any]) -> None:
        if agent_summary is None:
            return
        usage_summary["total_cost"] += agent_summary.get("total_cost", 0)
        for model, data in agent_summary.items():
            if model != "total_cost":
                if model not in usage_summary:
                    usage_summary[model] = data.copy()
                else:
                    usage_summary[model]["cost"] += data.get("cost", 0)
                    usage_summary[model]["prompt_tokens"] += data.get("prompt_tokens", 0)
                    usage_summary[model]["completion_tokens"] += data.get("completion_tokens", 0)
                    usage_summary[model]["total_tokens"] += data.get("total_tokens", 0)

    usage_including_cached_inference = {"total_cost": 0}
    usage_excluding_cached_inference = {"total_cost": 0}

    for agent in agents:
        if getattr(agent, "client", None):
            aggregate_summary(usage_including_cached_inference, agent.client.total_usage_summary)  # type: ignore[attr-defined]
            aggregate_summary(usage_excluding_cached_inference, agent.client.actual_usage_summary)  # type: ignore[attr-defined]

    return {
        "usage_including_cached_inference": usage_including_cached_inference,
        "usage_excluding_cached_inference": usage_excluding_cached_inference,
    }


def parse_tags_from_content(tag: str, content: str | list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Parses HTML style tags from message contents.

    The parsing is done by looking for patterns in the text that match the format of HTML tags. The tag to be parsed is
    specified as an argument to the function. The function looks for this tag in the text and extracts its content. The
    content of a tag is everything that is inside the tag, between the opening and closing angle brackets. The content
    can be a single string or a set of attribute-value pairs.

    Examples:
        `<img http://example.com/image.png> -> [{"tag": "img", "attr": {"src": "http://example.com/image.png"}, "match": re.Match}]`
        ```<audio text="Hello I'm a robot" prompt="whisper"> ->
                [{"tag": "audio", "attr": {"text": "Hello I'm a robot", "prompt": "whisper"}, "match": re.Match}]```

    Args:
        tag (str): The HTML style tag to be parsed.
        content (Union[str, list[dict[str, Any]]]): The message content to parse. Can be a string or a list of content
            items.

    Returns:
        list[dict[str, str]]: A list of dictionaries, where each dictionary represents a parsed tag. Each dictionary
            contains three key-value pairs: 'type' which is the tag, 'attr' which is a dictionary of the parsed attributes,
            and 'match' which is a regular expression match object.

    Raises:
        ValueError: If the content is not a string or a list.
    """
    results = []
    if isinstance(content, str):
        results.extend(_parse_tags_from_text(tag, content))
    # Handles case for multimodal messages.
    elif isinstance(content, list):
        for item in content:
            if item.get("type") == "text":
                results.extend(_parse_tags_from_text(tag, item["text"]))
    else:
        raise ValueError(f"content must be str or list, but got {type(content)}")

    return results


def _parse_tags_from_text(tag: str, text: str) -> list[dict[str, Any]]:
    pattern = re.compile(f"<{tag} (.*?)>")

    results = []
    for match in re.finditer(pattern, text):
        tag_attr = match.group(1).strip()
        attr = _parse_attributes_from_tags(tag_attr)

        results.append({"tag": tag, "attr": attr, "match": match})
    return results


def _parse_attributes_from_tags(tag_content: str) -> dict[str, str]:
    pattern = r"([^ ]+)"
    attrs = re.findall(pattern, tag_content)
    reconstructed_attrs = _reconstruct_attributes(attrs)

    def _append_src_value(content: dict[str, str], value: Any) -> None:
        if "src" in content:
            content["src"] += f" {value}"
        else:
            content["src"] = value

    content: dict[str, str] = {}
    for attr in reconstructed_attrs:
        if "=" not in attr:
            _append_src_value(content, attr)
            continue

        key, value = attr.split("=", 1)
        if value.startswith("'") or value.startswith('"'):
            content[key] = value[1:-1]  # remove quotes
        else:
            _append_src_value(content, attr)

    return content


def _reconstruct_attributes(attrs: list[str]) -> list[str]:
    """Reconstructs attributes from a list of strings where some attributes may be split across multiple elements."""

    def is_attr(attr: str) -> bool:
        if "=" in attr:
            _, value = attr.split("=", 1)
            if value.startswith("'") or value.startswith('"'):
                return True
        return False

    reconstructed = []
    found_attr = False
    for attr in attrs:
        if is_attr(attr):
            reconstructed.append(attr)
            found_attr = True
        else:
            if found_attr:
                reconstructed[-1] += f" {attr}"
                found_attr = True
            elif reconstructed:
                reconstructed[-1] += f" {attr}"
            else:
                reconstructed.append(attr)
    return reconstructed
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import TYPE_CHECKING, Any, Optional, Protocol, TypeVar, runtime_checkable

from ..doc_utils import export_module

__all__ = ["Agent", "LLMAgent", "LLMMessageType"]

Tool = TypeVar("Tool")

LLMMessageType = dict[str, Any]

DEFAULT_SUMMARY_METHOD = "last_msg"


@runtime_checkable
@export_module("autogen")
class Agent(Protocol):
    """(In preview) A protocol for Agent.

    An agent can communicate with other agents and perform actions.
    Different agents can differ in what actions they perform in the `receive` method.
    """

    @property
    def name(self) -> str:
        """The name of the agent."""
        ...

    @property
    def description(self) -> str:
        """The description of the agent. Used for the agent's introduction in
        a group chat setting.
        """
        ...

    def send(
        self,
        message: dict[str, Any] | str,
        recipient: "Agent",
        request_reply: bool | None = None,
    ) -> None:
        """Send a message to another agent.

        Args:
            message (dict or str): the message to send. If a dict, it should be
                a JSON-serializable and follows the OpenAI's ChatCompletion schema.
            recipient (Agent): the recipient of the message.
            request_reply (bool): whether to request a reply from the recipient.
        """
        ...

    async def a_send(
        self,
        message: dict[str, Any] | str,
        recipient: "Agent",
        request_reply: bool | None = None,
    ) -> None:
        """(Async) Send a message to another agent.

        Args:
            message (dict or str): the message to send. If a dict, it should be
                a JSON-serializable and follows the OpenAI's ChatCompletion schema.
            recipient (Agent): the recipient of the message.
            request_reply (bool): whether to request a reply from the recipient.
        """
        ...

    def receive(
        self,
        message: dict[str, Any] | str,
        sender: "Agent",
        request_reply: bool | None = None,
    ) -> None:
        """Receive a message from another agent.

        Args:
            message (dict or str): the message received. If a dict, it should be
                a JSON-serializable and follows the OpenAI's ChatCompletion schema.
            sender (Agent): the sender of the message.
            request_reply (bool): whether the sender requests a reply.
        """

    async def a_receive(
        self,
        message: dict[str, Any] | str,
        sender: "Agent",
        request_reply: bool | None = None,
    ) -> None:
        """(Async) Receive a message from another agent.

        Args:
            message (dict or str): the message received. If a dict, it should be
                a JSON-serializable and follows the OpenAI's ChatCompletion schema.
            sender (Agent): the sender of the message.
            request_reply (bool): whether the sender requests a reply.
        """
        ...

    def generate_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Optional["Agent"] = None,
        **kwargs: Any,
    ) -> str | dict[str, Any] | None:
        """Generate a reply based on the received messages.

        Args:
            messages (list[dict[str, Any]]): a list of messages received from other agents.
                The messages are dictionaries that are JSON-serializable and
                follows the OpenAI's ChatCompletion schema.
            sender: sender of an Agent instance.
            **kwargs: Additional keyword arguments.

        Returns:
            str or dict or None: the generated reply. If None, no reply is generated.
        """

    async def a_generate_reply(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Optional["Agent"] = None,
        **kwargs: Any,
    ) -> str | dict[str, Any] | None:
        """(Async) Generate a reply based on the received messages.

        Args:
            messages (list[dict[str, Any]]): a list of messages received from other agents.
                The messages are dictionaries that are JSON-serializable and
                follows the OpenAI's ChatCompletion schema.
            sender: sender of an Agent instance.
            **kwargs: Additional keyword arguments.

        Returns:
            str or dict or None: the generated reply. If None, no reply is generated.
        """
        ...

    def set_ui_tools(self, tools: list[Tool]) -> None:
        """Set the UI tools for the agent.

        Args:
            tools: a list of UI tools to set.
        """
        ...

    def unset_ui_tools(self, tools: list[Tool]) -> None:
        """Unset the UI tools for the agent.

        Args:
            tools: a list of UI tools to set.
        """
        ...


@runtime_checkable
@export_module("autogen")
class LLMAgent(Agent, Protocol):
    """(In preview) A protocol for an LLM agent."""

    @property
    def system_message(self) -> str:
        """The system message of this agent."""

    def update_system_message(self, system_message: str) -> None:
        """Update this agent's system message.

        Args:
            system_message (str): system message for inference.
        """


if TYPE_CHECKING:
    # mypy will fail if Conversable agent does not implement Agent protocol
    from .conversable_agent import ConversableAgent

    def _check_protocol_implementation(agent: ConversableAgent) -> Agent:
        return agent
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from ..realtime.experimental import (
    FunctionObserver,
    RealtimeAgent,
    RealtimeObserver,
    TwilioAudioAdapter,
    WebSocketAudioAdapter,
    register_swarm,
)

__all__ = [
    "FunctionObserver",
    "RealtimeAgent",
    "RealtimeObserver",
    "TwilioAudioAdapter",
    "WebSocketAudioAdapter",
    "register_swarm",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import re
import sys
from typing import Any

from ...doc_utils import export_module
from ...import_utils import optional_import_block, require_optional_import
from ...tools import Tool
from ..registry import register_interoperable_class

__all__ = ["CrewAIInteroperability"]


def _sanitize_name(s: str) -> str:
    return re.sub(r"\W|^(?=\d)", "_", s)


with optional_import_block():
    from crewai.tools import BaseTool as CrewAITool


@register_interoperable_class("crewai")
@export_module("autogen.interop")
class CrewAIInteroperability:
    """A class implementing the `Interoperable` protocol for converting CrewAI tools
    to a general `Tool` format.

    This class takes a `CrewAITool` and converts it into a standard `Tool` object.
    """

    @classmethod
    @require_optional_import("crewai", "interop-crewai")
    def convert_tool(cls, tool: Any, **kwargs: Any) -> Tool:
        """Converts a given CrewAI tool into a general `Tool` format.

        This method ensures that the provided tool is a valid `CrewAITool`, sanitizes
        the tool's name, processes its description, and prepares a function to interact
        with the tool's arguments. It then returns a standardized `Tool` object.

        Args:
            tool (Any): The tool to convert, expected to be an instance of `CrewAITool`.
            **kwargs (Any): Additional arguments, which are not supported by this method.

        Returns:
            Tool: A standardized `Tool` object converted from the CrewAI tool.

        Raises:
            ValueError: If the provided tool is not an instance of `CrewAITool`, or if
                        any additional arguments are passed.
        """
        if not isinstance(tool, CrewAITool):
            raise ValueError(f"Expected an instance of `crewai.tools.BaseTool`, got {type(tool)}")
        if kwargs:
            raise ValueError(f"The CrewAIInteroperability does not support any additional arguments, got {kwargs}")

        # needed for type checking
        crewai_tool: CrewAITool = tool  # type: ignore[no-any-unimported]

        name = _sanitize_name(crewai_tool.name)
        description = (
            crewai_tool.description.split("Tool Description: ")[-1]
            + " (IMPORTANT: When using arguments, put them all in an `args` dictionary)"
        )

        def func(args: crewai_tool.args_schema) -> Any:  # type: ignore[no-any-unimported]
            return crewai_tool.run(**args.model_dump())

        return Tool(
            name=name,
            description=description,
            func_or_tool=func,
        )

    @classmethod
    def get_unsupported_reason(cls) -> str | None:
        if sys.version_info < (3, 10) or sys.version_info >= (3, 13):
            return "This submodule is only supported for Python versions 3.10, 3.11, and 3.12"

        with optional_import_block() as result:
            import crewai.tools  # noqa: F401

        if not result.is_successful:
            return "Please install `interop-crewai` extra to use this module:\n\n\tpip install ag2[interop-crewai]"

        return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .crewai import CrewAIInteroperability

__all__ = ["CrewAIInteroperability"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from abc import ABC, abstractmethod
from collections.abc import Callable
from typing import Any, TypeVar

from ...doc_utils import export_module
from ...import_utils import optional_import_block, require_optional_import
from ...llm_config import LLMConfig
from ...oai import get_first_llm_config

with optional_import_block():
    from langchain_anthropic import ChatAnthropic
    from langchain_core.language_models import BaseChatModel
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_ollama import ChatOllama
    from langchain_openai import AzureChatOpenAI, ChatOpenAI


__all__ = ["LangChainChatModelFactory"]

T = TypeVar("T", bound="LangChainChatModelFactory")


@require_optional_import(
    ["langchain_anthropic", "langchain_google_genai", "langchain_ollama", "langchain_openai", "langchain_core"],
    "browser-use",
    except_for=["__init__", "register_factory"],
)
@export_module("autogen.interop")
class LangChainChatModelFactory(ABC):
    _factories: set["LangChainChatModelFactory"] = set()

    @classmethod
    def create_base_chat_model(cls, llm_config: LLMConfig | dict[str, Any]) -> "BaseChatModel":  # type: ignore [no-any-unimported]
        first_llm_config = get_first_llm_config(llm_config)
        for factory in LangChainChatModelFactory._factories:
            if factory.accepts(first_llm_config):
                return factory.create(first_llm_config)

        raise ValueError("Could not find a factory for the given config.")

    @classmethod
    def register_factory(cls) -> Callable[[type[T]], type[T]]:
        def decorator(factory: type[T]) -> type[T]:
            cls._factories.add(factory())
            return factory

        return decorator

    @classmethod
    def prepare_config(cls, first_llm_config: dict[str, Any]) -> dict[str, Any]:
        for pop_keys in ["api_type", "response_format"]:
            first_llm_config.pop(pop_keys, None)
        return first_llm_config

    @classmethod
    @abstractmethod
    def create(cls, first_llm_config: dict[str, Any]) -> "BaseChatModel":  # type: ignore [no-any-unimported]
        ...

    @classmethod
    @abstractmethod
    def get_api_type(cls) -> str: ...

    @classmethod
    def accepts(cls, first_llm_config: dict[str, Any]) -> bool:
        return first_llm_config.get("api_type", "openai") == cls.get_api_type()  # type: ignore [no-any-return]


@LangChainChatModelFactory.register_factory()
class ChatOpenAIFactory(LangChainChatModelFactory):
    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> "ChatOpenAI":  # type: ignore [no-any-unimported]
        first_llm_config = cls.prepare_config(first_llm_config)

        return ChatOpenAI(**first_llm_config)

    @classmethod
    def get_api_type(cls) -> str:
        return "openai"


@LangChainChatModelFactory.register_factory()
class DeepSeekFactory(ChatOpenAIFactory):
    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> "ChatOpenAI":  # type: ignore [no-any-unimported]
        if "base_url" not in first_llm_config:
            raise ValueError("base_url is required for deepseek api type.")
        return super().create(first_llm_config)

    @classmethod
    def get_api_type(cls) -> str:
        return "deepseek"


@LangChainChatModelFactory.register_factory()
class ChatAnthropicFactory(LangChainChatModelFactory):
    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> "ChatAnthropic":  # type: ignore [no-any-unimported]
        first_llm_config = cls.prepare_config(first_llm_config)

        return ChatAnthropic(**first_llm_config)

    @classmethod
    def get_api_type(cls) -> str:
        return "anthropic"


@LangChainChatModelFactory.register_factory()
class ChatGoogleGenerativeAIFactory(LangChainChatModelFactory):
    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> "ChatGoogleGenerativeAI":  # type: ignore [no-any-unimported]
        first_llm_config = cls.prepare_config(first_llm_config)

        return ChatGoogleGenerativeAI(**first_llm_config)

    @classmethod
    def get_api_type(cls) -> str:
        return "google"


@LangChainChatModelFactory.register_factory()
class AzureChatOpenAIFactory(LangChainChatModelFactory):
    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> "AzureChatOpenAI":  # type: ignore [no-any-unimported]
        first_llm_config = cls.prepare_config(first_llm_config)
        for param in ["base_url", "api_version"]:
            if param not in first_llm_config:
                raise ValueError(f"{param} is required for azure api type.")
        first_llm_config["azure_endpoint"] = first_llm_config.pop("base_url")

        return AzureChatOpenAI(**first_llm_config)

    @classmethod
    def get_api_type(cls) -> str:
        return "azure"


@LangChainChatModelFactory.register_factory()
class ChatOllamaFactory(LangChainChatModelFactory):
    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> "ChatOllama":  # type: ignore [no-any-unimported]
        first_llm_config = cls.prepare_config(first_llm_config)
        first_llm_config["base_url"] = first_llm_config.pop("client_host", None)
        if "num_ctx" not in first_llm_config:
            # In all Browser Use examples, num_ctx is set to 32000
            first_llm_config["num_ctx"] = 32000

        return ChatOllama(**first_llm_config)

    @classmethod
    def get_api_type(cls) -> str:
        return "ollama"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .langchain_chat_model_factory import LangChainChatModelFactory
from .langchain_tool import LangChainInteroperability

__all__ = ["LangChainChatModelFactory", "LangChainInteroperability"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from ...doc_utils import export_module
from ...import_utils import optional_import_block, require_optional_import
from ...tools import Tool
from ..registry import register_interoperable_class

__all__ = ["LangChainInteroperability"]

with optional_import_block():
    from langchain_core.tools import BaseTool as LangchainTool


@register_interoperable_class("langchain")
@export_module("autogen.interop")
class LangChainInteroperability:
    """A class implementing the `Interoperable` protocol for converting Langchain tools
    into a general `Tool` format.

    This class takes a `LangchainTool` and converts it into a standard `Tool` object,
    ensuring compatibility between Langchain tools and other systems that expect
    the `Tool` format.
    """

    @classmethod
    @require_optional_import("langchain_core", "interop-langchain")
    def convert_tool(cls, tool: Any, **kwargs: Any) -> Tool:
        """Converts a given Langchain tool into a general `Tool` format.

        This method verifies that the provided tool is a valid `LangchainTool`,
        processes the tool's input and description, and returns a standardized
        `Tool` object.

        Args:
            tool (Any): The tool to convert, expected to be an instance of `LangchainTool`.
            **kwargs (Any): Additional arguments, which are not supported by this method.

        Returns:
            Tool: A standardized `Tool` object converted from the Langchain tool.

        Raises:
            ValueError: If the provided tool is not an instance of `LangchainTool`, or if
                        any additional arguments are passed.
        """
        if not isinstance(tool, LangchainTool):
            raise ValueError(f"Expected an instance of `langchain_core.tools.BaseTool`, got {type(tool)}")
        if kwargs:
            raise ValueError(f"The LangchainInteroperability does not support any additional arguments, got {kwargs}")

        # needed for type checking
        langchain_tool: LangchainTool = tool  # type: ignore[no-any-unimported]

        model_type = langchain_tool.get_input_schema()

        def func(tool_input: model_type) -> Any:  # type: ignore[valid-type]
            return langchain_tool.run(tool_input.model_dump())  # type: ignore[attr-defined]

        return Tool(
            name=langchain_tool.name,
            description=langchain_tool.description,
            func_or_tool=func,
        )

    @classmethod
    def get_unsupported_reason(cls) -> str | None:
        with optional_import_block() as result:
            import langchain_core.tools  # noqa: F401

        if not result.is_successful:
            return (
                "Please install `interop-langchain` extra to use this module:\n\n\tpip install ag2[interop-langchain]"
            )

        return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Callable
from typing import TypeVar

from ..doc_utils import export_module
from .interoperable import Interoperable

__all__ = ["InteroperableRegistry", "register_interoperable_class"]

InteroperableClass = TypeVar("InteroperableClass", bound=type[Interoperable])


class InteroperableRegistry:
    def __init__(self) -> None:
        self._registry: dict[str, type[Interoperable]] = {}

    def register(self, short_name: str, cls: InteroperableClass) -> InteroperableClass:
        if short_name in self._registry:
            raise ValueError(f"Duplicate registration for {short_name}")

        self._registry[short_name] = cls

        return cls

    def get_short_names(self) -> list[str]:
        return sorted(self._registry.keys())

    def get_supported_types(self) -> list[str]:
        short_names = self.get_short_names()
        supported_types = [name for name in short_names if self._registry[name].get_unsupported_reason() is None]
        return supported_types

    def get_class(self, short_name: str) -> type[Interoperable]:
        return self._registry[short_name]

    @classmethod
    def get_instance(cls) -> "InteroperableRegistry":
        return _register


# global registry
_register = InteroperableRegistry()


# register decorator
@export_module("autogen.interop")
def register_interoperable_class(short_name: str) -> Callable[[InteroperableClass], InteroperableClass]:
    """Register an Interoperable class in the global registry.

    Returns:
        Callable[[InteroperableClass], InteroperableClass]: Decorator function

    Example:
        ```python
        @register_interoperable_class("myinterop")
        class MyInteroperability(Interoperable):
            def convert_tool(self, tool: Any) -> Tool:
                # implementation
                ...
        ```
    """

    def inner(cls: InteroperableClass) -> InteroperableClass:
        global _register
        return _register.register(short_name, cls)

    return inner
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any, Protocol, runtime_checkable

from ..doc_utils import export_module
from ..tools import Tool

__all__ = ["Interoperable"]


@runtime_checkable
@export_module("autogen.interop")
class Interoperable(Protocol):
    """A Protocol defining the interoperability interface for tool conversion.

    This protocol ensures that any class implementing it provides the method
    `convert_tool` to convert a given tool into a desired format or type.
    """

    @classmethod
    def convert_tool(cls, tool: Any, **kwargs: Any) -> Tool:
        """Converts a given tool to a desired format or type.

        This method should be implemented by any class adhering to the `Interoperable` protocol.

        Args:
            tool (Any): The tool object to be converted.
            **kwargs (Any): Additional parameters to pass during the conversion process.

        Returns:
            Tool: The converted tool in the desired format or type.
        """
        ...

    @classmethod
    def get_unsupported_reason(cls) -> str | None:
        """Returns the reason for the tool being unsupported.

        This method should be implemented by any class adhering to the `Interoperable` protocol.

        Returns:
            str: The reason for the interoperability class being unsupported.
        """
        ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .crewai import CrewAIInteroperability
from .interoperability import Interoperability
from .interoperable import Interoperable
from .langchain import LangChainChatModelFactory, LangChainInteroperability
from .litellm import LiteLLmConfigFactory
from .pydantic_ai import PydanticAIInteroperability
from .registry import register_interoperable_class

__all__ = [
    "CrewAIInteroperability",
    "Interoperability",
    "Interoperable",
    "LangChainChatModelFactory",
    "LangChainInteroperability",
    "LiteLLmConfigFactory",
    "PydanticAIInteroperability",
    "register_interoperable_class",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .pydantic_ai import PydanticAIInteroperability

__all__ = ["PydanticAIInteroperability"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import warnings
from collections.abc import Callable
from functools import wraps
from inspect import signature
from typing import Any

from ...doc_utils import export_module
from ...import_utils import optional_import_block, require_optional_import
from ...tools import Tool
from ..registry import register_interoperable_class

__all__ = ["PydanticAIInteroperability"]

with optional_import_block():
    from pydantic_ai import RunContext
    from pydantic_ai.tools import Tool as PydanticAITool
    from pydantic_ai.usage import Usage


@register_interoperable_class("pydanticai")
@export_module("autogen.interop")
class PydanticAIInteroperability:
    """A class implementing the `Interoperable` protocol for converting Pydantic AI tools
    into a general `Tool` format.

    This class takes a `PydanticAITool` and converts it into a standard `Tool` object,
    ensuring compatibility between Pydantic AI tools and other systems that expect
    the `Tool` format. It also provides a mechanism for injecting context parameters
    into the tool's function.
    """

    @staticmethod
    @require_optional_import("pydantic_ai", "interop-pydantic-ai")
    def inject_params(
        ctx: Any,
        tool: Any,
    ) -> Callable[..., Any]:
        """Wraps the tool's function to inject context parameters and handle retries.

        This method ensures that context parameters are properly passed to the tool
        when invoked and that retries are managed according to the tool's settings.

        Args:
            ctx (Optional[RunContext[Any]]): The run context, which may include dependencies and retry information.
            tool (PydanticAITool): The Pydantic AI tool whose function is to be wrapped.

        Returns:
            Callable[..., Any]: A wrapped function that includes context injection and retry handling.

        Raises:
            ValueError: If the tool fails after the maximum number of retries.
        """
        ctx_typed: RunContext[Any] | None = ctx  # type: ignore[no-any-unimported]
        tool_typed: PydanticAITool[Any] = tool  # type: ignore[no-any-unimported]

        max_retries = tool_typed.max_retries if tool_typed.max_retries is not None else 1
        f = tool_typed.function

        @wraps(f)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            current_retry = 0 if ctx_typed is None else ctx_typed.retries.get(tool_typed.name, 0)

            if current_retry >= max_retries:
                raise ValueError(f"{tool_typed.name} failed after {max_retries} retries")

            try:
                if ctx_typed is not None:
                    kwargs.pop("ctx", None)
                    ctx_typed.retry = current_retry
                    result = f(**kwargs, ctx=ctx_typed)  # type: ignore[call-arg]
                    ctx_typed.retries[tool_typed.name] = 0
                else:
                    result = f(**kwargs)  # type: ignore[call-arg]
            except Exception as e:
                if ctx_typed is not None:
                    ctx_typed.retries[tool_typed.name] += 1
                raise e

            return result

        sig = signature(f)
        if ctx_typed is not None:
            new_params = [param for name, param in sig.parameters.items() if name != "ctx"]
        else:
            new_params = list(sig.parameters.values())

        wrapper.__signature__ = sig.replace(parameters=new_params)  # type: ignore[attr-defined]

        return wrapper

    @classmethod
    @require_optional_import("pydantic_ai", "interop-pydantic-ai")
    def convert_tool(cls, tool: Any, deps: Any = None, **kwargs: Any) -> Tool:
        """Converts a given Pydantic AI tool into a general `Tool` format.

        This method verifies that the provided tool is a valid `PydanticAITool`,
        handles context dependencies if necessary, and returns a standardized `Tool` object.

        Args:
            tool (Any): The tool to convert, expected to be an instance of `PydanticAITool`.
            deps (Any, optional): The dependencies to inject into the context, required if
                                   the tool takes a context. Defaults to None.
            **kwargs (Any): Additional arguments that are not used in this method.

        Returns:
            Tool: A standardized `Tool` object converted from the Pydantic AI tool.

        Raises:
            ValueError: If the provided tool is not an instance of `PydanticAITool`, or if
                        dependencies are missing for tools that require a context.
            UserWarning: If the `deps` argument is provided for a tool that does not take a context.
        """
        if not isinstance(tool, PydanticAITool):
            raise ValueError(f"Expected an instance of `pydantic_ai.tools.Tool`, got {type(tool)}")

        # needed for type checking
        pydantic_ai_tool: PydanticAITool[Any] = tool  # type: ignore[no-any-unimported]

        if tool.takes_ctx and deps is None:
            raise ValueError("If the tool takes a context, the `deps` argument must be provided")
        if not tool.takes_ctx and deps is not None:
            warnings.warn(
                "The `deps` argument is provided but will be ignored because the tool does not take a context.",
                UserWarning,
            )

        ctx = (
            RunContext(
                model=None,  # type: ignore [arg-type]
                usage=Usage(),
                prompt="",
                deps=deps,
                retry=0,
                # All messages send to or returned by a model.
                # This is mostly used on pydantic_ai Agent level.
                messages=[],  # TODO: check in the future if this is needed on Tool level
                tool_name=pydantic_ai_tool.name,
            )
            if tool.takes_ctx
            else None
        )

        func = PydanticAIInteroperability.inject_params(
            ctx=ctx,
            tool=pydantic_ai_tool,
        )

        return Tool(
            name=pydantic_ai_tool.name,
            description=pydantic_ai_tool.description,
            func_or_tool=func,
            parameters_json_schema=pydantic_ai_tool.function_schema.json_schema,
        )

    @classmethod
    def get_unsupported_reason(cls) -> str | None:
        with optional_import_block() as result:
            import pydantic_ai.tools  # noqa: F401

        if not result.is_successful:
            return "Please install `interop-pydantic-ai` extra to use this module:\n\n\tpip install ag2[interop-pydantic-ai]"

        return None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from typing import Any

from ..doc_utils import export_module
from ..tools import Tool
from .interoperable import Interoperable
from .registry import InteroperableRegistry

__all__ = ["Interoperable"]


@export_module("autogen.interop")
class Interoperability:
    """A class to handle interoperability between different tool types.

    This class allows the conversion of tools to various interoperability classes and provides functionality
    for retrieving and registering interoperability classes.
    """

    registry = InteroperableRegistry.get_instance()

    @classmethod
    def convert_tool(cls, *, tool: Any, type: str, **kwargs: Any) -> Tool:
        """Converts a given tool to an instance of a specified interoperability type.

        Args:
            tool (Any): The tool object to be converted.
            type (str): The type of interoperability to convert the tool to.
            **kwargs (Any): Additional arguments to be passed during conversion.

        Returns:
            Tool: The converted tool.

        Raises:
            ValueError: If the interoperability class for the provided type is not found.
        """
        interop = cls.get_interoperability_class(type)
        return interop.convert_tool(tool, **kwargs)

    @classmethod
    def get_interoperability_class(cls, type: str) -> type[Interoperable]:
        """Retrieves the interoperability class corresponding to the specified type.

        Args:
            type (str): The type of the interoperability class to retrieve.

        Returns:
            type[Interoperable]: The interoperability class type.

        Raises:
            ValueError: If no interoperability class is found for the provided type.
        """
        supported_types = cls.registry.get_supported_types()
        if type not in supported_types:
            supported_types_formatted = ", ".join(["'t'" for t in supported_types])
            raise ValueError(
                f"Interoperability class {type} is not supported, supported types: {supported_types_formatted}"
            )

        return cls.registry.get_class(type)

    @classmethod
    def get_supported_types(cls) -> list[str]:
        """Returns a sorted list of all supported interoperability types.

        Returns:
            List[str]: A sorted list of strings representing the supported interoperability types.
        """
        return sorted(cls.registry.get_supported_types())
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .litellm_config_factory import LiteLLmConfigFactory

__all__ = ["LiteLLmConfigFactory"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import os
from abc import ABC, abstractmethod
from collections.abc import Callable
from typing import Any, TypeVar

from ...doc_utils import export_module
from ...llm_config import LLMConfig
from ...oai import get_first_llm_config

__all__ = ["LiteLLmConfigFactory"]

T = TypeVar("T", bound="LiteLLmConfigFactory")


def get_crawl4ai_version() -> str | None:
    """Get the installed crawl4ai version."""
    try:
        import crawl4ai

        version = getattr(crawl4ai, "__version__", None)
        return version if isinstance(version, str) else None
    except (ImportError, AttributeError):
        return None


def is_crawl4ai_v05_or_higher() -> bool:
    """Check if crawl4ai version is 0.5 or higher."""
    version = get_crawl4ai_version()
    if version is None:
        return False

    # Parse version string (e.g., "0.5.0" -> [0, 5, 0])
    try:
        version_parts = [int(x) for x in version.split(".")]
        # Check if version >= 0.5.0
        return version_parts >= [0, 5, 0]
    except (ValueError, IndexError):
        return False


@export_module("autogen.interop")
class LiteLLmConfigFactory(ABC):
    _factories: set["LiteLLmConfigFactory"] = set()

    @classmethod
    def create_lite_llm_config(cls, llm_config: LLMConfig | dict[str, Any]) -> dict[str, Any]:
        """Create a lite LLM config compatible with the installed crawl4ai version.

        For crawl4ai >=0.5: Returns config with llmConfig parameter
        For crawl4ai <0.5: Returns config with provider parameter (legacy)
        """
        first_llm_config = get_first_llm_config(llm_config)
        for factory in LiteLLmConfigFactory._factories:
            if factory.accepts(first_llm_config):
                base_config = factory.create(first_llm_config)

                # Check crawl4ai version and adapt config accordingly
                if is_crawl4ai_v05_or_higher():
                    return cls._adapt_for_crawl4ai_v05(base_config)
                else:
                    return base_config  # Use legacy format

        raise ValueError("Could not find a factory for the given config.")

    @classmethod
    def _adapt_for_crawl4ai_v05(cls, base_config: dict[str, Any]) -> dict[str, Any]:
        """Adapt the config for crawl4ai >=0.5 by moving deprecated parameters
        into an llmConfig object.
        """
        adapted_config = base_config.copy()

        # Extract deprecated parameters
        llm_config_params = {}

        if "provider" in adapted_config:
            llm_config_params["provider"] = adapted_config.pop("provider")

        if "api_token" in adapted_config:
            llm_config_params["api_token"] = adapted_config.pop("api_token")

        # Add other parameters that should be in llmConfig
        for param in ["base_url", "api_base", "api_version"]:
            if param in adapted_config:
                llm_config_params[param] = adapted_config.pop(param)

        # Create the llmConfig object if we have parameters for it
        if llm_config_params:
            adapted_config["llmConfig"] = llm_config_params

        return adapted_config

    @classmethod
    def register_factory(cls) -> Callable[[type[T]], type[T]]:
        def decorator(factory: type[T]) -> type[T]:
            cls._factories.add(factory())
            return factory

        return decorator

    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> dict[str, Any]:
        model = first_llm_config.pop("model")
        api_type = first_llm_config.pop("api_type", "openai")

        first_llm_config["provider"] = f"{api_type}/{model}"
        return first_llm_config

    @classmethod
    @abstractmethod
    def get_api_type(cls) -> str: ...

    @classmethod
    def accepts(cls, first_llm_config: dict[str, Any]) -> bool:
        return first_llm_config.get("api_type", "openai") == cls.get_api_type()  # type: ignore [no-any-return]


@LiteLLmConfigFactory.register_factory()
class DefaultLiteLLmConfigFactory(LiteLLmConfigFactory):
    @classmethod
    def get_api_type(cls) -> str:
        raise NotImplementedError("DefaultLiteLLmConfigFactory does not have an API type.")

    @classmethod
    def accepts(cls, first_llm_config: dict[str, Any]) -> bool:
        non_base_api_types = ["google", "ollama"]
        return first_llm_config.get("api_type", "openai") not in non_base_api_types

    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> dict[str, Any]:
        api_type = first_llm_config.get("api_type", "openai")
        if api_type != "openai" and "api_key" not in first_llm_config:
            raise ValueError("API key is required.")
        first_llm_config["api_token"] = first_llm_config.pop("api_key", os.getenv("OPENAI_API_KEY"))

        first_llm_config = super().create(first_llm_config)

        return first_llm_config


@LiteLLmConfigFactory.register_factory()
class GoogleLiteLLmConfigFactory(LiteLLmConfigFactory):
    @classmethod
    def get_api_type(cls) -> str:
        return "google"

    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> dict[str, Any]:
        # api type must be changed before calling super().create
        # litellm uses gemini as the api type for google
        first_llm_config["api_type"] = "gemini"
        first_llm_config["api_token"] = first_llm_config.pop("api_key")
        first_llm_config = super().create(first_llm_config)

        return first_llm_config

    @classmethod
    def accepts(cls, first_llm_config: dict[str, Any]) -> bool:
        api_type: str = first_llm_config.get("api_type", "")
        return api_type == cls.get_api_type() or api_type == "gemini"


@LiteLLmConfigFactory.register_factory()
class OllamaLiteLLmConfigFactory(LiteLLmConfigFactory):
    @classmethod
    def get_api_type(cls) -> str:
        return "ollama"

    @classmethod
    def create(cls, first_llm_config: dict[str, Any]) -> dict[str, Any]:
        first_llm_config = super().create(first_llm_config)
        if "client_host" in first_llm_config:
            first_llm_config["api_base"] = first_llm_config.pop("client_host")

        return first_llm_config
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import getpass
from typing import Any

from ..doc_utils import export_module
from ..events.base_event import BaseEvent
from ..events.print_event import PrintEvent
from .base import IOStream

__all__ = ("IOConsole",)


@export_module("autogen.io")
class IOConsole(IOStream):
    """A console input/output stream."""

    def print(self, *objects: Any, sep: str = " ", end: str = "\n", flush: bool = False) -> None:
        """Print data to the output stream.

        Args:
            objects (any): The data to print.
            sep (str, optional): The separator between objects. Defaults to " ".
            end (str, optional): The end of the output. Defaults to "\n".
            flush (bool, optional): Whether to flush the output. Defaults to False.
        """
        print_message = PrintEvent(*objects, sep=sep, end=end)
        self.send(print_message)
        # print(*objects, sep=sep, end=end, flush=flush)

    def send(self, message: BaseEvent) -> None:
        """Send a message to the output stream.

        Args:
            message (Any): The message to send.
        """
        message.print()

    def input(self, prompt: str = "", *, password: bool = False) -> str:
        """Read a line from the input stream.

        Args:
            prompt (str, optional): The prompt to display. Defaults to "".
            password (bool, optional): Whether to read a password. Defaults to False.

        Returns:
            str: The line read from the input stream.

        """
        if password:
            return getpass.getpass(prompt if prompt != "" else "Password: ")
        return input(prompt)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import logging
import ssl
import threading
from collections.abc import Callable, Iterable, Iterator
from contextlib import contextmanager
from functools import partial
from time import sleep
from typing import Any, Protocol

from ..doc_utils import export_module
from ..events.base_event import BaseEvent
from ..events.print_event import PrintEvent
from ..import_utils import optional_import_block, require_optional_import
from .base import IOStream

# Check if the websockets module is available
with optional_import_block():
    from websockets.sync.server import serve as ws_serve

__all__ = ("IOWebsockets",)


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# The following type and protocols are used to define the ServerConnection and WebSocketServer classes
# if websockets is not installed, they would be untyped
Data = str | bytes


class ServerConnection(Protocol):
    def send(self, message: Data | Iterable[Data]) -> None:
        """Send a message to the client.

        Args:
            message (Union[Data, Iterable[Data]]): The message to send.

        """
        ...  # pragma: no cover

    def recv(self, timeout: float | None = None) -> Data:
        """Receive a message from the client.

        Args:
            timeout (Optional[float], optional): The timeout for the receive operation. Defaults to None.

        Returns:
            Data: The message received from the client.

        """
        ...  # pragma: no cover

    def close(self) -> None:
        """Close the connection."""
        ...


class WebSocketServer(Protocol):
    def serve_forever(self) -> None:
        """Run the server forever."""
        ...  # pragma: no cover

    def shutdown(self) -> None:
        """Shutdown the server."""
        ...  # pragma: no cover

    def __enter__(self) -> "WebSocketServer":
        """Enter the server context."""
        ...  # pragma: no cover

    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
        """Exit the server context."""
        ...  # pragma: no cover


@require_optional_import("websockets", "websockets")
@export_module("autogen.io")
class IOWebsockets(IOStream):
    """A websocket input/output stream."""

    def __init__(self, websocket: ServerConnection) -> None:
        """Initialize the websocket input/output stream.

        Args:
            websocket (ServerConnection): The websocket server.
        """
        self._websocket = websocket

    @staticmethod
    def _handler(websocket: ServerConnection, on_connect: Callable[["IOWebsockets"], None]) -> None:
        """The handler function for the websocket server."""
        logger.info(f" - IOWebsockets._handler(): Client connected on {websocket}")
        # create a new IOWebsockets instance using the websocket that is create when a client connects
        try:
            iowebsocket = IOWebsockets(websocket)
            with IOStream.set_default(iowebsocket):
                # call the on_connect function
                try:
                    on_connect(iowebsocket)
                except Exception as e:
                    logger.warning(f" - IOWebsockets._handler(): Error in on_connect: {e}")
        except Exception as e:
            logger.error(f" - IOWebsockets._handler(): Unexpected error in IOWebsockets: {e}")

    @staticmethod
    @contextmanager
    def run_server_in_thread(
        *,
        host: str = "127.0.0.1",
        port: int = 8765,
        on_connect: Callable[["IOWebsockets"], None],
        ssl_context: ssl.SSLContext | None = None,
        **kwargs: Any,
    ) -> Iterator[str]:
        """Factory function to create a websocket input/output stream.

        Args:
            host (str, optional): The host to bind the server to. Defaults to "127.0.0.1".
            port (int, optional): The port to bind the server to. Defaults to 8765.
            on_connect (Callable[[IOWebsockets], None]): The function to be executed on client connection. Typically creates agents and initiate chat.
            ssl_context (Optional[ssl.SSLContext], optional): The SSL context to use for secure connections. Defaults to None.
            kwargs (Any): Additional keyword arguments to pass to the websocket server.

        Yields:
            str: The URI of the websocket server.
        """
        server_dict: dict[str, WebSocketServer] = {}

        def _run_server() -> None:
            # print(f" - _run_server(): starting server on ws://{host}:{port}", flush=True)
            with ws_serve(
                handler=partial(IOWebsockets._handler, on_connect=on_connect),
                host=host,
                port=port,
                ssl_context=ssl_context,
                **kwargs,
            ) as server:
                # print(f" - _run_server(): server {server} started on ws://{host}:{port}", flush=True)

                server_dict["server"] = server

                # runs until the server is shutdown
                server.serve_forever()

                return

        # start server in a separate thread
        thread = threading.Thread(target=_run_server)
        thread.start()
        try:
            while "server" not in server_dict:
                sleep(0.1)

            yield f"ws://{host}:{port}"

        finally:
            # print(f" - run_server_in_thread(): shutting down server on ws://{host}:{port}", flush=True)
            # gracefully stop server
            if "server" in server_dict:
                # print(f" - run_server_in_thread(): shutting down server {server_dict['server']}", flush=True)
                server_dict["server"].shutdown()

            # wait for the thread to stop
            if thread:
                thread.join()

    @property
    def websocket(self) -> "ServerConnection":
        """The URI of the websocket server."""
        return self._websocket

    def print(self, *objects: Any, sep: str = " ", end: str = "\n", flush: bool = False) -> None:
        """Print data to the output stream.

        Args:
            objects (any): The data to print.
            sep (str, optional): The separator between objects. Defaults to " ".
            end (str, optional): The end of the output. Defaults to "\n".
            flush (bool, optional): Whether to flush the output. Defaults to False.
        """
        print_message = PrintEvent(*objects, sep=sep, end=end)
        self.send(print_message)

    def send(self, message: BaseEvent) -> None:
        """Send a message to the output stream.

        Args:
            message (Any): The message to send.
        """
        self._websocket.send(message.model_dump_json())

    def input(self, prompt: str = "", *, password: bool = False) -> str:
        """Read a line from the input stream.

        Args:
            prompt (str, optional): The prompt to display. Defaults to "".
            password (bool, optional): Whether to read a password. Defaults to False.

        Returns:
            str: The line read from the input stream.

        """
        if prompt != "":
            self._websocket.send(prompt)

        msg = self._websocket.recv()

        return msg.decode("utf-8") if isinstance(msg, bytes) else msg
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

import queue
from asyncio import Queue as AsyncQueue
from collections.abc import AsyncIterable, Iterable, Sequence
from typing import Any, Optional, Protocol
from uuid import UUID, uuid4

from pydantic import BaseModel, Field

from autogen.tools.tool import Tool

from ..agentchat.agent import Agent, LLMMessageType
from ..agentchat.group.context_variables import ContextVariables
from ..events.agent_events import ErrorEvent, InputRequestEvent, RunCompletionEvent
from ..events.base_event import BaseEvent
from .processors import (
    AsyncConsoleEventProcessor,
    AsyncEventProcessorProtocol,
    ConsoleEventProcessor,
    EventProcessorProtocol,
)
from .thread_io_stream import AsyncThreadIOStream, ThreadIOStream

Message = dict[str, Any]


class RunInfoProtocol(Protocol):
    @property
    def uuid(self) -> UUID: ...

    @property
    def above_run(self) -> Optional["RunResponseProtocol"]: ...


class Usage(BaseModel):
    cost: float
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class CostBreakdown(BaseModel):
    total_cost: float
    models: dict[str, Usage] = Field(default_factory=dict)

    @classmethod
    def from_raw(cls, data: dict[str, Any]) -> "CostBreakdown":
        # Extract total cost
        total_cost = data.get("total_cost", 0.0)

        # Remove total_cost key to extract models
        model_usages = {k: Usage(**v) for k, v in data.items() if k != "total_cost"}

        return cls(total_cost=total_cost, models=model_usages)


class Cost(BaseModel):
    usage_including_cached_inference: CostBreakdown
    usage_excluding_cached_inference: CostBreakdown

    @classmethod
    def from_raw(cls, data: dict[str, Any]) -> "Cost":
        return cls(
            usage_including_cached_inference=CostBreakdown.from_raw(data.get("usage_including_cached_inference", {})),
            usage_excluding_cached_inference=CostBreakdown.from_raw(data.get("usage_excluding_cached_inference", {})),
        )


class RunResponseProtocol(RunInfoProtocol, Protocol):
    @property
    def events(self) -> Iterable[BaseEvent]: ...

    @property
    def messages(self) -> Iterable[Message]: ...

    @property
    def summary(self) -> str | None: ...

    @property
    def context_variables(self) -> ContextVariables | None: ...

    @property
    def last_speaker(self) -> str | None: ...

    @property
    def cost(self) -> Cost | None: ...

    def process(self, processor: EventProcessorProtocol | None = None) -> None: ...

    def set_ui_tools(self, tools: list[Tool]) -> None: ...


class AsyncRunResponseProtocol(RunInfoProtocol, Protocol):
    @property
    def events(self) -> AsyncIterable[BaseEvent]: ...

    @property
    async def messages(self) -> Iterable[Message]: ...

    @property
    async def summary(self) -> str | None: ...

    @property
    async def context_variables(self) -> ContextVariables | None: ...

    @property
    async def last_speaker(self) -> str | None: ...

    @property
    async def cost(self) -> Cost | None: ...

    async def process(self, processor: AsyncEventProcessorProtocol | None = None) -> None: ...

    def set_ui_tools(self, tools: list[Tool]) -> None: ...


class RunResponse:
    def __init__(self, iostream: ThreadIOStream, agents: list[Agent]):
        self.iostream = iostream
        self.agents = agents
        self._summary: str | None = None
        self._messages: Sequence[LLMMessageType] = []
        self._uuid = uuid4()
        self._context_variables: ContextVariables | None = None
        self._last_speaker: str | None = None
        self._cost: Cost | None = None

    def _queue_generator(self, q: queue.Queue) -> Iterable[BaseEvent]:  # type: ignore[type-arg]
        """A generator to yield items from the queue until the termination message is found."""
        while True:
            try:
                # Get an item from the queue
                event = q.get(timeout=0.1)  # Adjust timeout as needed

                if isinstance(event, InputRequestEvent):
                    event.content.respond = lambda response: self.iostream._output_stream.put(response)  # type: ignore[attr-defined]

                yield event

                if isinstance(event, RunCompletionEvent):
                    self._messages = event.content.history  # type: ignore[attr-defined]
                    self._last_speaker = event.content.last_speaker  # type: ignore[attr-defined]
                    self._summary = event.content.summary  # type: ignore[attr-defined]
                    self._context_variables = event.content.context_variables  # type: ignore[attr-defined]
                    self.cost = event.content.cost  # type: ignore[attr-defined]
                    break

                if isinstance(event, ErrorEvent):
                    raise event.content.error  # type: ignore[attr-defined]
            except queue.Empty:
                continue  # Wait for more items in the queue

    @property
    def events(self) -> Iterable[BaseEvent]:
        return self._queue_generator(self.iostream.input_stream)

    @property
    def messages(self) -> Iterable[Message]:
        return self._messages

    @property
    def summary(self) -> str | None:
        return self._summary

    @property
    def above_run(self) -> Optional["RunResponseProtocol"]:
        return None

    @property
    def uuid(self) -> UUID:
        return self._uuid

    @property
    def context_variables(self) -> ContextVariables | None:
        return self._context_variables

    @property
    def last_speaker(self) -> str | None:
        return self._last_speaker

    @property
    def cost(self) -> Cost | None:
        return self._cost

    @cost.setter
    def cost(self, value: Cost | dict[str, Any]) -> None:
        if isinstance(value, dict):
            self._cost = Cost.from_raw(value)
        else:
            self._cost = value

    def process(self, processor: EventProcessorProtocol | None = None) -> None:
        processor = processor or ConsoleEventProcessor()
        processor.process(self)

    def set_ui_tools(self, tools: list[Tool]) -> None:
        """Set the UI tools for the agents."""
        for agent in self.agents:
            agent.set_ui_tools(tools)


class AsyncRunResponse:
    def __init__(self, iostream: AsyncThreadIOStream, agents: list[Agent]):
        self.iostream = iostream
        self.agents = agents
        self._summary: str | None = None
        self._messages: Sequence[LLMMessageType] = []
        self._uuid = uuid4()
        self._context_variables: ContextVariables | None = None
        self._last_speaker: str | None = None
        self._cost: Cost | None = None

    async def _queue_generator(self, q: AsyncQueue[Any]) -> AsyncIterable[BaseEvent]:  # type: ignore[type-arg]
        """A generator to yield items from the queue until the termination message is found."""
        while True:
            try:
                # Get an item from the queue
                event = await q.get()

                if isinstance(event, InputRequestEvent):

                    async def respond(response: str) -> None:
                        await self.iostream._output_stream.put(response)

                    event.content.respond = respond  # type: ignore[attr-defined]

                yield event

                if isinstance(event, RunCompletionEvent):
                    self._messages = event.content.history  # type: ignore[attr-defined]
                    self._last_speaker = event.content.last_speaker  # type: ignore[attr-defined]
                    self._summary = event.content.summary  # type: ignore[attr-defined]
                    self._context_variables = event.content.context_variables  # type: ignore[attr-defined]
                    self.cost = event.content.cost  # type: ignore[attr-defined]
                    break

                if isinstance(event, ErrorEvent):
                    raise event.content.error  # type: ignore[attr-defined]
            except queue.Empty:
                continue

    @property
    def events(self) -> AsyncIterable[BaseEvent]:
        return self._queue_generator(self.iostream.input_stream)

    @property
    async def messages(self) -> Iterable[Message]:
        return self._messages

    @property
    async def summary(self) -> str | None:
        return self._summary

    @property
    def above_run(self) -> Optional["RunResponseProtocol"]:
        return None

    @property
    def uuid(self) -> UUID:
        return self._uuid

    @property
    async def context_variables(self) -> ContextVariables | None:
        return self._context_variables

    @property
    async def last_speaker(self) -> str | None:
        return self._last_speaker

    @property
    async def cost(self) -> Cost | None:
        return self._cost

    @cost.setter
    def cost(self, value: Cost | dict[str, Any]) -> None:
        if isinstance(value, dict):
            self._cost = Cost.from_raw(value)
        else:
            self._cost = value

    async def process(self, processor: AsyncEventProcessorProtocol | None = None) -> None:
        processor = processor or AsyncConsoleEventProcessor()
        await processor.process(self)

    def set_ui_tools(self, tools: list[Tool]) -> None:
        """Set the UI tools for the agents."""
        for agent in self.agents:
            agent.set_ui_tools(tools)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from .base import IOStream, InputStream, OutputStream
from .console import IOConsole
from .websockets import IOWebsockets

# Set the default input/output stream to the console
IOStream.set_global_default(IOConsole())
IOStream.set_default(IOConsole())

__all__ = ("IOConsole", "IOStream", "IOWebsockets", "InputStream", "OutputStream")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import asyncio
import getpass
from typing import TYPE_CHECKING

from ...doc_utils import export_module
from ...events.agent_events import InputRequestEvent
from ...events.base_event import BaseEvent

if TYPE_CHECKING:
    from ..run_response import AsyncRunResponseProtocol, RunResponseProtocol
    from .base import AsyncEventProcessorProtocol, EventProcessorProtocol


@export_module("autogen.io")
class ConsoleEventProcessor:
    def process(self, response: "RunResponseProtocol") -> None:
        for event in response.events:
            self.process_event(event)

    def process_event(self, event: BaseEvent) -> None:
        if isinstance(event, InputRequestEvent):
            prompt = event.content.prompt  # type: ignore[attr-defined]
            if event.content.password:  # type: ignore[attr-defined]
                result = getpass.getpass(prompt if prompt != "" else "Password: ")
            else:
                result = input(prompt)
            event.content.respond(result)  # type: ignore[attr-defined]
        else:
            event.print()


@export_module("autogen.io")
class AsyncConsoleEventProcessor:
    async def process(self, response: "AsyncRunResponseProtocol") -> None:
        async for event in response.events:
            await self.process_event(event)

    async def process_event(self, event: BaseEvent) -> None:
        if isinstance(event, InputRequestEvent):
            prompt = event.content.prompt  # type: ignore[attr-defined]

            if event.content.password:  # type: ignore[attr-defined]
                result = await asyncio.to_thread(getpass.getpass, prompt if prompt != "" else "Password: ")
            else:
                result = await asyncio.to_thread(input, prompt)

            await event.content.respond(result)  # type: ignore[attr-defined]
        else:
            event.print()


if TYPE_CHECKING:

    def check_type_1(x: ConsoleEventProcessor) -> EventProcessorProtocol:
        return x

    def check_type_2(x: AsyncConsoleEventProcessor) -> AsyncEventProcessorProtocol:
        return x
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from .base import AsyncEventProcessorProtocol, EventProcessorProtocol
from .console_event_processor import AsyncConsoleEventProcessor, ConsoleEventProcessor

__all__ = [
    "AsyncConsoleEventProcessor",
    "AsyncEventProcessorProtocol",
    "ConsoleEventProcessor",
    "EventProcessorProtocol",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from typing import TYPE_CHECKING, Protocol

from ...doc_utils import export_module

if TYPE_CHECKING:
    from ..run_response import AsyncRunResponseProtocol, RunResponseProtocol

__all__ = ["AsyncEventProcessorProtocol", "EventProcessorProtocol"]


@export_module("autogen.io")
class EventProcessorProtocol(Protocol):
    def process(self, response: "RunResponseProtocol") -> None: ...


@export_module("autogen.io")
class AsyncEventProcessorProtocol(Protocol):
    async def process(self, response: "AsyncRunResponseProtocol") -> None: ...
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import queue
from asyncio import Queue as AsyncQueue
from typing import TYPE_CHECKING, Any

from autogen.io.base import AsyncIOStreamProtocol, IOStreamProtocol

from ..events.agent_events import InputRequestEvent
from ..events.print_event import PrintEvent


class ThreadIOStream:
    def __init__(self) -> None:
        self._input_stream: queue.Queue = queue.Queue()  # type: ignore[type-arg]
        self._output_stream: queue.Queue = queue.Queue()  # type: ignore[type-arg]

    def input(self, prompt: str = "", *, password: bool = False) -> str:
        self.send(InputRequestEvent(prompt=prompt, password=password))  # type: ignore[call-arg]
        return self._output_stream.get()  # type: ignore[no-any-return]

    def print(self, *objects: Any, sep: str = " ", end: str = "\n", flush: bool = False) -> None:
        print_message = PrintEvent(*objects, sep=sep, end=end)
        self.send(print_message)

    def send(self, message: Any) -> None:
        self._input_stream.put(message)

    @property
    def input_stream(self) -> queue.Queue:  # type: ignore[type-arg]
        return self._input_stream


class AsyncThreadIOStream:
    def __init__(self) -> None:
        self._input_stream: AsyncQueue = AsyncQueue()  # type: ignore[type-arg]
        self._output_stream: AsyncQueue = AsyncQueue()  # type: ignore[type-arg]

    async def input(self, prompt: str = "", *, password: bool = False) -> str:
        self.send(InputRequestEvent(prompt=prompt, password=password))  # type: ignore[call-arg]
        return await self._output_stream.get()  # type: ignore[no-any-return]

    def print(self, *objects: Any, sep: str = " ", end: str = "\n", flush: bool = False) -> None:
        print_message = PrintEvent(*objects, sep=sep, end=end)
        self.send(print_message)

    def send(self, message: Any) -> None:
        self._input_stream.put_nowait(message)

    @property
    def input_stream(self) -> AsyncQueue[Any]:
        return self._input_stream


if TYPE_CHECKING:

    def check_type_1(x: ThreadIOStream) -> IOStreamProtocol:
        return x

    def check_type_2(x: AsyncThreadIOStream) -> AsyncIOStreamProtocol:
        return x
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import logging
from collections.abc import Iterator
from contextlib import contextmanager
from contextvars import ContextVar
from typing import Any, Protocol, runtime_checkable

from ..doc_utils import export_module
from ..events.base_event import BaseEvent

__all__ = ("IOStream", "InputStream", "OutputStream")

logger = logging.getLogger(__name__)


@runtime_checkable
@export_module("autogen.io")
class OutputStream(Protocol):
    def print(self, *objects: Any, sep: str = " ", end: str = "\n", flush: bool = False) -> None:
        """Print data to the output stream.

        Args:
            objects (any): The data to print.
            sep (str, optional): The separator between objects. Defaults to " ".
            end (str, optional): The end of the output. Defaults to "\n".
            flush (bool, optional): Whether to flush the output. Defaults to False.
        """
        ...  # pragma: no cover

    def send(self, message: BaseEvent) -> None:
        """Send data to the output stream.

        Args:
            message (BaseEvent): BaseEvent from autogen.messages.base_message
        """
        ...


@runtime_checkable
@export_module("autogen.io")
class InputStream(Protocol):
    def input(self, prompt: str = "", *, password: bool = False) -> str:
        """Read a line from the input stream.

        Args:
            prompt (str, optional): The prompt to display. Defaults to "".
            password (bool, optional): Whether to read a password. Defaults to False.

        Returns:
            str: The line read from the input stream.

        """
        ...  # pragma: no cover


@runtime_checkable
@export_module("autogen.io")
class AsyncInputStream(Protocol):
    async def input(self, prompt: str = "", *, password: bool = False) -> str:
        """Read a line from the input stream.

        Args:
            prompt (str, optional): The prompt to display. Defaults to "".
            password (bool, optional): Whether to read a password. Defaults to False.

        Returns:
            str: The line read from the input stream.

        """
        ...  # pragma: no cover


@runtime_checkable
@export_module("autogen.io")
class IOStreamProtocol(InputStream, OutputStream, Protocol):
    """A protocol for input/output streams."""


@runtime_checkable
@export_module("autogen.io")
class AsyncIOStreamProtocol(AsyncInputStream, OutputStream, Protocol):
    """A protocol for input/output streams."""


iostream_union = IOStreamProtocol | AsyncIOStreamProtocol


@export_module("autogen.io")
class IOStream:
    """A protocol for input/output streams."""

    # ContextVar must be used in multithreaded or async environments
    _default_io_stream: ContextVar[iostream_union | None] = ContextVar("default_iostream", default=None)
    _default_io_stream.set(None)
    _global_default: iostream_union | None = None

    @staticmethod
    def set_global_default(stream: iostream_union) -> None:
        """Set the default input/output stream.

        Args:
            stream (IOStream): The input/output stream to set as the default.
        """
        IOStream._global_default = stream

    @staticmethod
    def get_global_default() -> iostream_union:
        """Get the default input/output stream.

        Returns:
            IOStream: The default input/output stream.
        """
        if IOStream._global_default is None:
            raise RuntimeError("No global default IOStream has been set")
        return IOStream._global_default

    @staticmethod
    def get_default() -> iostream_union:
        """Get the default input/output stream.

        Returns:
            IOStream: The default input/output stream.
        """
        iostream = IOStream._default_io_stream.get()
        if iostream is None:
            iostream = IOStream.get_global_default()
            # Set the default IOStream of the current context (thread/cooroutine)
            IOStream.set_default(iostream)
        return iostream

    @staticmethod
    @contextmanager
    def set_default(stream: iostream_union | None) -> Iterator[None]:
        """Set the default input/output stream.

        Args:
            stream (IOStream): The input/output stream to set as the default.
        """
        global _default_io_stream
        try:
            token = IOStream._default_io_stream.set(stream)
            yield
        finally:
            IOStream._default_io_stream.reset(token)

        return
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

import logging

from .agentchat import Agent
from .import_utils import optional_import_block, require_optional_import

with optional_import_block():
    import matplotlib.pyplot as plt
    import networkx as nx


def has_self_loops(allowed_speaker_transitions: dict[str, list[Agent]]) -> bool:
    """Check if there are self loops in the allowed_speaker_transitions.

    Args:
        allowed_speaker_transitions (dict[str, list[Agent]]): A dictionary of keys and list as values. The keys are the names of the agents, and the values are the names of the agents that the key agent can transition

    Returns:
        True if there are self loops in the allowed_speaker_transitions_Dict.
    """
    return any(key in value for key, value in allowed_speaker_transitions.items())


def check_graph_validity(
    allowed_speaker_transitions_dict: dict[str, list[Agent]],
    agents: list[Agent],
) -> None:
    """Check the validity of the allowed_speaker_transitions_dict.

    Args:
        allowed_speaker_transitions_dict (dict[str, list[Agent]]):
            A dictionary of keys and list as values. The keys are the names of the agents, and the values are the names of the agents that the key agent can transition to.
        agents (list[Agent]): A list of Agents

    agents: A list of Agents

    Checks for the following:
        Errors
        1. The dictionary must have a structure of keys and list as values
        2. Every key exists in agents.
        3. Every value is a list of Agents (not string).

    Warnings:
        1. Warning if there are isolated agent nodes
        2. Warning if the set of agents in allowed_speaker_transitions do not match agents
        3. Warning if there are duplicated agents in any values of `allowed_speaker_transitions_dict`
    """
    # Errors

    # Check 1. The dictionary must have a structure of keys and list as values
    if not isinstance(allowed_speaker_transitions_dict, dict):
        raise ValueError("allowed_speaker_transitions_dict must be a dictionary.")

    # All values must be lists of Agent or empty
    if not all(isinstance(value, list) for value in allowed_speaker_transitions_dict.values()):
        raise ValueError("allowed_speaker_transitions_dict must be a dictionary with lists as values.")

    # Check 2. Every key exists in agents
    if not all(key in agents for key in allowed_speaker_transitions_dict):
        raise ValueError("allowed_speaker_transitions_dict has keys not in agents.")

    # Check 3. Every value is a list of Agents or empty list (not string).
    if not all(all(isinstance(agent, Agent) for agent in value) for value in allowed_speaker_transitions_dict.values()):
        raise ValueError("allowed_speaker_transitions_dict has values that are not lists of Agents.")

    # Warnings
    # Warning 1. Warning if there are isolated agent nodes, there are not incoming nor outgoing edges
    # Concat keys if len(value) is positive
    has_outgoing_edge = []
    for key, agent_list in allowed_speaker_transitions_dict.items():
        if len(agent_list) > 0:
            has_outgoing_edge.append(key)
    no_outgoing_edges = [agent for agent in agents if agent not in has_outgoing_edge]

    # allowed_speaker_transitions_dict.values() is a list of list of Agents
    # values_all_agents is a list of all agents in allowed_speaker_transitions_dict.values()
    has_incoming_edge = []
    for agent_list in allowed_speaker_transitions_dict.values():
        if len(agent_list) > 0:
            has_incoming_edge.extend(agent_list)

    no_incoming_edges = [agent for agent in agents if agent not in has_incoming_edge]

    isolated_agents = set(no_incoming_edges).intersection(set(no_outgoing_edges))
    if len(isolated_agents) > 0:
        logging.warning(
            f"""Warning: There are isolated agent nodes, there are not incoming nor outgoing edges. Isolated agents: {[agent.name for agent in isolated_agents]}"""
        )

    # Warning 2. Warning if the set of agents in allowed_speaker_transitions do not match agents
    # Get set of agents
    agents_in_allowed_speaker_transitions = set(has_incoming_edge).union(set(has_outgoing_edge))
    full_anti_join = set(agents_in_allowed_speaker_transitions).symmetric_difference(set(agents))
    if len(full_anti_join) > 0:
        logging.warning(
            f"""Warning: The set of agents in allowed_speaker_transitions do not match agents. Offending agents: {[agent.name for agent in full_anti_join]}"""
        )

    # Warning 3. Warning if there are duplicated agents in any values of `allowed_speaker_transitions_dict`
    for key, values in allowed_speaker_transitions_dict.items():
        duplicates = [item for item in values if values.count(item) > 1]
        unique_duplicates = list(set(duplicates))
        if unique_duplicates:
            logging.warning(
                f"Agent '{key.name}' has duplicate elements: {[agent.name for agent in unique_duplicates]}. Please remove duplicates manually."
            )


def invert_disallowed_to_allowed(
    disallowed_speaker_transitions_dict: dict[str, list[Agent]], agents: list[Agent]
) -> dict[str, list[Agent]]:
    """Invert the disallowed_speaker_transitions_dict to form the allowed_speaker_transitions_dict.

    Start with a fully connected allowed_speaker_transitions_dict of all agents. Remove edges from the fully connected allowed_speaker_transitions_dict according to the disallowed_speaker_transitions_dict to form the allowed_speaker_transitions_dict.

    Args:
        disallowed_speaker_transitions_dict: A dictionary of keys and list as values. The keys are the names of the agents, and the values are the names of the agents that the key agent cannot transition to.
        agents: A list of Agents

    Returns:
        allowed_speaker_transitions_dict: A dictionary of keys and list as values. The keys are the names of the agents, and the values are the names of the agents that the key agent can transition to.
    """
    # Create a fully connected allowed_speaker_transitions_dict of all agents
    allowed_speaker_transitions_dict = {agent: list(agents) for agent in agents}

    # Remove edges from allowed_speaker_transitions_dict according to the disallowed_speaker_transitions_dict
    for key, value in disallowed_speaker_transitions_dict.items():
        allowed_speaker_transitions_dict[key] = [
            agent for agent in allowed_speaker_transitions_dict[key] if agent not in value
        ]

    return allowed_speaker_transitions_dict


@require_optional_import(["matplotlib", "networkx"], "graph")
def visualize_speaker_transitions_dict(
    speaker_transitions_dict: dict[str, list[Agent]], agents: list[Agent], export_path: str | None = None
) -> None:
    """Visualize the speaker_transitions_dict using networkx.

    Args:
        speaker_transitions_dict: A dictionary of keys and list as values. The keys are the names of the agents, and the values are the names of the agents that the key agent can transition to.
        agents: A list of Agents
        export_path: The path to export the graph. If None, the graph will be shown.

    Returns:
        None


    """
    g = nx.DiGraph()

    # Add nodes
    g.add_nodes_from([agent.name for agent in agents])

    # Add edges
    for key, value in speaker_transitions_dict.items():
        for agent in value:
            g.add_edge(key.name, agent.name)

    # Visualize
    nx.draw(g, with_labels=True, font_weight="bold")

    if export_path is not None:
        plt.savefig(export_path)
    else:
        plt.show()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import logging

from .agentchat import (
    Agent,
    AssistantAgent,
    ChatResult,
    ConversableAgent,
    GroupChat,
    GroupChatManager,
    UpdateSystemMessage,
    UserProxyAgent,
    a_initiate_swarm_chat,
    a_run_swarm,
    gather_usage_summary,
    initiate_chats,
    register_function,
    run_swarm,
)
from .agentchat.group.context_expression import ContextExpression
from .code_utils import DEFAULT_MODEL, FAST_MODEL
from .exception_utils import (
    AgentNameConflictError,
    InvalidCarryOverTypeError,
    NoEligibleSpeakerError,
    SenderRequiredError,
    UndefinedNextAgentError,
)
from .llm_config import LLMConfig, ModelClient
from .oai import (
    Cache,
    OpenAIWrapper,
    config_list_from_dotenv,
    config_list_from_json,
    config_list_from_models,
    config_list_gpt4_gpt35,
    config_list_openai_aoai,
    filter_config,
    get_config_list,
)
from .version import __version__

# Set the root logger.
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


__all__ = [
    "DEFAULT_MODEL",
    "FAST_MODEL",
    "Agent",
    "AgentNameConflictError",
    "AssistantAgent",
    "Cache",
    "ChatResult",
    "ContextExpression",
    "ConversableAgent",
    "GroupChat",
    "GroupChatManager",
    "InvalidCarryOverTypeError",
    "LLMConfig",
    "ModelClient",
    "NoEligibleSpeakerError",
    "OpenAIWrapper",
    "SenderRequiredError",
    "UndefinedNextAgentError",
    "UpdateSystemMessage",
    "UserProxyAgent",
    "__version__",
    "a_initiate_swarm_chat",
    "a_run_swarm",
    "config_list_from_dotenv",
    "config_list_from_json",
    "config_list_from_models",
    "config_list_gpt4_gpt35",
    "config_list_openai_aoai",
    "filter_config",
    "gather_usage_summary",
    "get_config_list",
    "initiate_chats",
    "register_function",
    "run_swarm",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .websurfer import WebSurferAgent

__all__ = ["WebSurferAgent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any, Literal

from .... import ConversableAgent
from ....doc_utils import export_module
from ....llm_config import LLMConfig
from ....tools import Tool
from ....tools.experimental import (
    BrowserUseTool,
    Crawl4AITool,
    DuckDuckGoSearchTool,
    FirecrawlTool,
    PerplexitySearchTool,
    SearxngSearchTool,
    TavilySearchTool,
)

__all__ = ["WebSurferAgent"]


@export_module("autogen.agents.experimental")
class WebSurferAgent(ConversableAgent):
    """An agent that uses web tools to interact with the web."""

    def __init__(
        self,
        *,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        web_tool_llm_config: LLMConfig | dict[str, Any] | None = None,
        web_tool: Literal[
            "browser_use", "crawl4ai", "duckduckgo", "firecrawl", "perplexity", "tavily", "searxng"
        ] = "browser_use",
        web_tool_kwargs: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> None:
        """Initialize the WebSurferAgent.

        Args:
            llm_config: The LLM configuration.
            web_tool_llm_config: The LLM configuration for the web tool. If not provided, the llm_config will be used.
            web_tool: The web tool to use. Defaults to "browser_use".
            web_tool_kwargs: The keyword arguments for the web tool. Defaults to None.
            **kwargs: Additional keyword arguments passed to the parent ConversableAgent class.
        """
        llm_config = LLMConfig.get_current_llm_config(llm_config)  # type: ignore[arg-type]
        web_tool_kwargs = web_tool_kwargs if web_tool_kwargs else {}
        web_tool_llm_config = web_tool_llm_config if web_tool_llm_config else llm_config
        if web_tool == "browser_use":
            self.tool: Tool = BrowserUseTool(llm_config=web_tool_llm_config, **web_tool_kwargs)  # type: ignore[arg-type]
        elif web_tool == "crawl4ai":
            self.tool = Crawl4AITool(llm_config=web_tool_llm_config, **web_tool_kwargs)
        elif web_tool == "firecrawl":
            self.tool = FirecrawlTool(llm_config=web_tool_llm_config, **web_tool_kwargs)
        elif web_tool == "perplexity":
            self.tool = PerplexitySearchTool(**web_tool_kwargs)
        elif web_tool == "tavily":
            self.tool = TavilySearchTool(llm_config=web_tool_llm_config, **web_tool_kwargs)
        elif web_tool == "duckduckgo":
            self.tool = DuckDuckGoSearchTool(**web_tool_kwargs)
        elif web_tool == "searxng":
            self.tool = SearxngSearchTool(**web_tool_kwargs)
        else:
            raise ValueError(f"Unsupported {web_tool=}.")

        super().__init__(llm_config=llm_config, **kwargs)

        self.register_for_llm()(self.tool)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import ConversableAgent
from ....doc_utils import export_module
from ....tools.experimental import DiscordRetrieveTool, DiscordSendTool

__all__ = ["DiscordAgent"]


@export_module("autogen.agents.experimental")
class DiscordAgent(ConversableAgent):
    """An agent that can send messages and retrieve messages on Discord."""

    DEFAULT_SYSTEM_MESSAGE = (
        "You are a helpful AI assistant that communicates through Discord. "
        "Remember that Discord uses Markdown for formatting and has a character limit. "
        "Keep messages clear and concise, and consider using appropriate formatting when helpful."
    )

    def __init__(
        self,
        name: str,
        system_message: str | None = None,
        *,
        bot_token: str,
        channel_name: str,
        guild_name: str,
        has_writing_instructions: bool = True,
        **kwargs: Any,
    ) -> None:
        """Initialize the DiscordAgent.

        Args:
            name: name of the agent.
            system_message: system message for the ChatCompletion inference.
            bot_token: Discord bot token
            channel_name: Channel name where messages will be sent / retrieved
            guild_name: Guild (server) name where the channel is located
            has_writing_instructions: Whether to add writing instructions to the system message. Defaults to True.
            **kwargs: Additional keyword arguments passed to the parent ConversableAgent class.
        """
        discord_system_message = system_message or self.DEFAULT_SYSTEM_MESSAGE

        self._send_tool = DiscordSendTool(bot_token=bot_token, channel_name=channel_name, guild_name=guild_name)
        self._retrieve_tool = DiscordRetrieveTool(bot_token=bot_token, channel_name=channel_name, guild_name=guild_name)

        # Add formatting instructions
        if has_writing_instructions:
            formatting_instructions = (
                "\nFormat guidelines for Discord:\n"
                "1. Max message length: 2000 characters\n"
                "2. Supports Markdown formatting\n"
                "3. Can use ** for bold, * for italic, ``` for code blocks\n"
                "4. Consider using appropriate emojis when suitable\n"
            )

            discord_system_message = discord_system_message + formatting_instructions

        super().__init__(name=name, system_message=discord_system_message, **kwargs)

        self.register_for_llm()(self._send_tool)
        self.register_for_llm()(self._retrieve_tool)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .discord import DiscordAgent

__all__ = ["DiscordAgent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .wikipedia import WikipediaAgent

__all__ = ["WikipediaAgent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import ConversableAgent
from ....doc_utils import export_module
from ....tools.experimental import WikipediaPageLoadTool, WikipediaQueryRunTool


@export_module("autogen.agents.experimental")
class WikipediaAgent(ConversableAgent):
    """An AI agent that leverages Wikipedia tools to provide accurate, concise answers
    to user queries.

    Tools:
        - WikipediaQueryRunTool: searches Wikipedia for relevant article titles.
        - WikipediaPageLoadTool: loads full Wikipedia pages (and metadata) for indepth content.

    Attributes:
        _query_run_tool (WikipediaQueryRunTool): for running title/keyword searches.
        _page_load_tool (WikipediaPageLoadTool): for fetching full page content.

    Parameters:
        system_message (Optional[Union[str, List[str]]]):
            Custom system prompt(s). If None, DEFAULT_SYSTEM_MESSAGE is used.
            Must be a str or a list of strings, otherwise raises ValueError.
        format_instructions (Optional[str]): Extra formatting instructions to append.
        language (str): ISO code for the Wikipedia language edition (default: "en").
        top_k (int): Number of top search results to return (default: 2).
        **kwargs: Passed through to the base ConversableAgent.

    Raises:
        ValueError: If `system_message` is not a str or list of str.
    """

    DEFAULT_SYSTEM_MESSAGE = (
        "You are a knowledgeable AI assistant with access to Wikipedia.\n"
        "Use your tools when necessary. Respond to user queries by providing accurate and concise information.\n"
        "If a question requires external data, utilize the appropriate tool to retrieve it."
    )

    def __init__(
        self,
        system_message: str | list[str] | None = None,
        format_instructions: str | None = None,
        language: str = "en",
        top_k: int = 2,
        **kwargs: Any,
    ) -> None:
        """Initialize the WikipediaAgent with optional custom prompts and tools.

        Args:
            system_message (Optional[Union[str, List[str]]]):
                Custom system prompt(s). If None, DEFAULT_SYSTEM_MESSAGE is used.
                Must be a str or list of strings.
            format_instructions (Optional[str]): Extra formatting instructions to append.
            language (str): Wikipedia language code (default: "en").
            top_k (int): How many top search results to fetch (default: 2).
            **kwargs: Other parameters for ConversableAgent.

        Raises:
            ValueError: If `system_message` is not a str or list of str.
        """
        # Use explicit system_message or fall back to default
        system_message = system_message or self.DEFAULT_SYSTEM_MESSAGE

        # Append formatting instructions if provided
        if format_instructions is not None:
            instructions = f"\n\nFollow this format:\n\n{format_instructions}"
            if isinstance(system_message, list):
                system_message.append(instructions)
            elif isinstance(system_message, str):
                system_message = system_message + instructions
            else:
                raise ValueError(f"system_message must be str or list[str], got {type(system_message).__name__}")

        # Initialize Wikipedia tools
        self._query_run_tool = WikipediaQueryRunTool(language=language, top_k=top_k)
        self._page_load_tool = WikipediaPageLoadTool(language=language, top_k=top_k)

        # Initialize the base ConversableAgent
        super().__init__(system_message=system_message, **kwargs)

        # Register tools for LLM recommendations
        self.register_for_llm()(self._query_run_tool)
        self.register_for_llm()(self._page_load_tool)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .deep_research import DeepResearchAgent
from .discord import DiscordAgent
from .document_agent import DocAgent, DoclingDocIngestAgent, InMemoryQueryEngine, VectorChromaQueryEngine
from .reasoning import ReasoningAgent, ThinkNode
from .slack import SlackAgent
from .telegram import TelegramAgent
from .websurfer import WebSurferAgent
from .wikipedia import WikipediaAgent

__all__ = [
    "DeepResearchAgent",
    "DiscordAgent",
    "DocAgent",
    "DoclingDocIngestAgent",
    "InMemoryQueryEngine",
    "ReasoningAgent",
    "SlackAgent",
    "TelegramAgent",
    "ThinkNode",
    "VectorChromaQueryEngine",
    "WebSurferAgent",
    "WikipediaAgent",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .reasoning_agent import ReasoningAgent, ThinkNode

__all__ = ["ReasoningAgent", "ThinkNode"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import copy
import math
import random
import re
import warnings
from typing import Any, Literal, Optional

from .... import Agent, AssistantAgent, UserProxyAgent
from ....doc_utils import export_module
from ....import_utils import optional_import_block
from ....llm_config import LLMConfig

__all__ = ["ReasoningAgent", "ThinkNode"]

EPSILON = 1e-6

REASONING_AGENT_MESSAGE = """You are a Reasoning AI Assistant. You generate high-quality responses to user questions by taking advantage of a tree-of-thought reasoning process."""

TREEOFTHOUGHT_MESSAGE = """Role: Deep Thinking AI Assistant

End Goal: Generate an efficient thinking trajectory of steps to follow in order to provide a high-quality response to the user. Think deep in complex questions and shallow in simple ones.

Current Task: Given the question and a list of previous thinking steps (the plan trajectory), you have two options:
1) Terminate: if you believe the user's question has been explored, terminate the task.
2) Continue Thinking: generate at least four innovative options for the next step in the thinking process to add in the trajectory. The user will not answer you anything.

### 1) Terminate
#### Instructions:
- Always terminate the task when you believe the user's question has been explored.
- The user wants the response the quicker possible, so when a high quality response can be crafted by the exploration performed, terminate the process.
- Don't terminate from the first step.
- Never provide additional options when terminating the task.

#### Format of Output:
REFLECTION:
*Give a few sentence reflections on the previous steps in the thinking trajectory, explaining why the task has been explored thoroughly.*

** Possible Options:**
Option 1: TERMINATE
<Short description>

### 2) Continue thinking
#### Instructions:
- Continue thinking when you believe that more exploration is needed to provide a high-quality response.
- Review the user's question and the previous steps taken. If only the question is provided and no previous steps, then make your suggestions to initiate the thinking process.
- The options you will provide must be alternatives for the next step in the thinking trajectory. Not steps that consider another option as given. So, make them focused and not too many.
- Identify any mistakes or errors in the previous thinking. If you find any mistakes, include options to correct them in your proposed options.
- If the question is a multi-choice question, you should carefully eliminate obviously wrong choices, look for contextual clues in the question, and use logical reasoning to select the most plausible answer.
- If you need to validate, simulate, or illustrate a reasoning concept (like mathematical expressions, code execution, algorithms, etc.) with Python, place the code in a fenced block like ```python ... ``` and always print the results that you want to see.

#### Options Restrictions:
- Never suggest options that access/consult/cross-check the internet, external sources, literature, datasets, books, or experts.
- Never suggest options in the physical world like conducting experiments or surveys, your approach in practical problems should still be theoretical.
- Never suggest options that require data you do not have, or suggest research to collect them.
- Never use Python when there is no need to.
- Never include a code option without the script to execute (e.g. not: Use python to make the calculations, but: Use this script to make the calculations: ```python... ```).

#### Format of Output:
REFLECTION:
*Give a few sentence reflections on the previous steps in the thinking trajectory, what is wrong and what is good.*

**Possible Options:**
Option 1: <Thinking 1>
<Short description, optional code snippet to execute>

Option 2: <Thinking 2>
<Short description, optional code snippet to execute>

Option 3: <Thinking 3>
<Short description, optional code snippet to execute>

Option 4: <Thinking 4>
<Short description, optional code snippet to execute>

...
"""

EXECUTOR_MESSAGE = "Please provide an answer for the last step in the thinking trajectory, to advance the thinking process. Keep your answers as consise as possible. Never suggest the next step."


@export_module("autogen.agents.experimental")
class ThinkNode:
    def __init__(self, content: str, parent: Optional["ThinkNode"] = None) -> None:
        """A node in a tree structure representing a step in the reasoning process.

        This class implements a tree node that stores content (text describing a reasoning step),
        maintains parent-child relationships, tracks node statistics, and provides utilities
        for traversing/visualizing the reasoning path.

        Args:
            content (str): The text content/description for this reasoning step.
            parent (Optional[ThinkNode]): The parent node in the tree, if any.

        Attributes:
            content (str): The text content/description for this reasoning step.
            value (float): A numeric score/value assigned to this node.
            parent (Optional[ThinkNode]): Reference to the parent node.
            reflection (str): A string containing reflections on the reasoning process.
            rating_details (str): A string providing details about the rating of this node.
            output (Optional[str]): The output generated at this node through the `execute_node` method.
            depth (int): The depth of this node in the tree (root = 0).
            children (list[ThinkNode]): list of child nodes.
            visits (int): Number of times this node has been visited during search.

        The node automatically maintains the tree structure by:
        - Setting its depth based on the parent's depth + 1.
        - Adding itself to the parent's children list if the parent exists.
        - Providing trajectory utilities to get the full path from root to this node.
        """
        self.content: str = content
        self.value: float = 0.0
        self.parent: ThinkNode | None = parent
        self.reflection: str = ""
        self.rating_details: str = ""
        self.output: str | None = None
        self.depth: int = parent.depth + 1 if parent is not None else 0
        self.children: list[ThinkNode] = []
        self.visits: int = 0
        if self.parent:
            self.parent.children.append(self)

    @property
    def _trajectory_arr(self) -> list[str]:
        """Gets the full path from root to this node as a list of strings.

        Returns:
            list[str]: list containing the content of each node from root to current node
        """
        step = f"Content: {self.content}"
        if self.output is not None:
            step += f"\nOutput: {self.output}"
        if self.parent:
            return self.parent._trajectory_arr + [step]
        return ["# Question:\n" + step + "\n---\n"]

    @property
    def trajectory(self) -> str:
        """Get a formatted string representation of the path from root to this node.

        Returns:
            str: A formatted string showing the question and each step in the reasoning process
        """
        traj = self._trajectory_arr
        ans = traj[0]
        ans += "# Trajectory:\n"
        for i, step in enumerate(traj[1:]):
            ans += f"\nStep {i + 1}:\n{step}"
        return ans

    def backpropagate(self, reward: float) -> None:
        """Update the score of this node and its parents using moving average.

        Args:
            reward (float): The reward to backpropagate up the tree.
        """
        node: ThinkNode | None = self
        while node is not None:
            node.visits += 1
            node.value = (node.value * (node.visits - 1) + reward) / node.visits
            node = node.parent

    def __str__(self) -> str:
        return f"{self.content} -> Depth: {self.depth} Value: {self.value} Visits: {self.visits}"

    def __repr__(self) -> str:
        return self.__str__()

    def to_dict(self) -> dict[str, Any]:
        """Convert ThinkNode to dictionary representation.

        Returns:
            dict[str, Any]: dictionary containing all node attributes and recursive children
        """
        return {
            "content": self.content,
            "value": self.value,
            "depth": self.depth,
            "reflection": self.reflection,
            "rating_details": self.rating_details,
            "output": self.output,
            "visits": self.visits,
            "children": [child.to_dict() for child in self.children],
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any], parent: Optional["ThinkNode"] = None) -> "ThinkNode":
        """Create ThinkNode from dictionary representation.

        Args:
            data (dict[str, Any]): dictionary containing node data
            parent (Optional[ThinkNode]): Parent node to attach to

        Returns:
            ThinkNode: Reconstructed node with all children
        """
        node = cls(content=data["content"], parent=parent)
        node.value = data["value"]
        node.depth = data["depth"]
        node.visits = data["visits"]
        node.reflection = data.get("reflection", "")
        node.rating_details = data.get("rating_details", "")
        node.output = data.get("output")

        # Recursively create children
        for child_data in data["children"]:
            cls.from_dict(child_data, parent=node)

        return node

    def visualize_tree(self) -> None:
        """Visualize the tree of thoughts using graphviz."""
        with optional_import_block() as result:
            from graphviz import Digraph

        if not result.is_successful:
            print("Please install graphviz: pip install graphviz")
            return

        dot = Digraph(comment="Tree of Thoughts")
        dot.attr(rankdir="TB")  # Top to Bottom direction

        def add_nodes(node: ThinkNode, node_id: str = "0") -> None:
            # Truncate long content for better visualization
            display_content = (node.content[:50] + "...") if len(node.content) > 50 else node.content

            # Add node with stats
            label = f"{display_content}\n visits: {node.visits}\n value: {node.value}"
            dot.node(node_id, label)

            # Recursively add children
            for i, child in enumerate(node.children):
                child_id = f"{node_id}_{i}"
                add_nodes(child, child_id)
                dot.edge(node_id, child_id)

        add_nodes(self)

        # Render the graph
        try:
            dot.render("tree_of_thoughts", view=False, format="png", cleanup=True)
        except Exception as e:
            print(f"Error rendering graph: {e}")
            print("Make sure graphviz is installed on your system: https://graphviz.org/download/")


def extract_sft_dataset(root: ThinkNode) -> list[dict[str, Any]]:
    """Extract the best trajectory or multiple equally good trajectories for SFT training.

    Args:
        root (ThinkNonde): The root node of the tree.

    Returns:
        list[dict]: list of best trajectories, each one is a pair of instruction and response.
    """
    instruction = root.content
    idx = len("# Question: ") + len(root.content) + 1

    def _find_leaf_nodes(node: ThinkNode) -> list[ThinkNode]:
        """Recursively find all leaf nodes."""
        if not node.children:
            return [node]
        leafs = []
        for child in node.children:
            leafs.extend(_find_leaf_nodes(child))
        return leafs

    # Step 1: Find all leaf nodes
    leaf_nodes = _find_leaf_nodes(root)

    # Step 2: Determine the highest score among leaf nodes
    max_value = max(leaf_nodes, key=lambda x: x.value).value

    # Step 3: Collect all leaf nodes with the highest score
    best_leafs = [leaf for leaf in leaf_nodes if leaf.value == max_value]

    # Step 4: Collect trajectories for all the best leaf nodes
    best_trajectories = [{"instruction": instruction, "response": leaf.trajectory[idx:]} for leaf in best_leafs]

    return best_trajectories


def extract_rlhf_preference_dataset(root: ThinkNode, contrastive_threshold: float = 0.2) -> list[dict[str, Any]]:
    """Extract and generate preference pairs for RLHF training by comparing sibling nodes.

    Args:
        root (ThinkNode): The root node of the tree.
        contrastive_threshold (float): between (0, 1), a distance measure that we are confident to call
            one is positive and another is negative.

    Returns:
        list[dict]: list of preference pairs, where each pair contains two responses and
        indicates which one is preferred.
    """
    preference_pairs = []

    assert contrastive_threshold > 0
    assert contrastive_threshold < 1

    def traverse_tree(node: ThinkNode) -> None:
        """Traverse the tree to compare sibling nodes and collect preferences."""
        if not node.children:
            return  # Leaf node, no comparisons needed

        # Step 1: Compare all sibling nodes
        for i in range(len(node.children)):
            for j in range(len(node.children)):
                if i == j:
                    continue
                child_a, child_b = node.children[i], node.children[j]

                is_a_better = False
                if child_a.visits > 0 and child_b.visits > 0:
                    # for MCTS
                    is_a_better = (
                        child_a.value / child_a.visits - child_b.value / child_b.visits > contrastive_threshold
                    )
                else:
                    # for Beam Search
                    is_a_better = child_a.value - child_b.value > contrastive_threshold
                if is_a_better:
                    preference_pairs.append({
                        "instruction": node.trajectory,
                        "reflection": node.reflection,
                        "preferred_response": f"Step {child_a.depth}: {child_a.content}",
                        "dispreferred_response": f"Step {child_b.depth}: {child_b.content}",
                    })

        # Step 2: Recurse into child nodes
        for child in node.children:
            traverse_tree(child)

    # Start traversal from the root
    traverse_tree(root)

    return preference_pairs


@export_module("autogen.agents.experimental")
class ReasoningAgent(AssistantAgent):
    def __init__(
        self,
        name: str,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        grader_llm_config: LLMConfig | dict[str, Any] | None = None,
        max_depth: int = 4,
        beam_size: int = 3,
        answer_approach: Literal["pool", "best"] = "pool",
        reason_config: dict[str, Any] | None = None,
        code_execution_config: dict[str, Any] | Literal[False] = False,
        scope: str | None = None,
        **kwargs: Any,
    ) -> None:
        """Initialize a ReasoningAgent that uses tree-of-thought reasoning.

        Args:
            name (str): Name of the agent
            llm_config (Optional[Union[LLMConfig, dict[str, Any]]]): Configuration for the language model
            grader_llm_config (Optional[Union[LLMConfig, dict[str, Any]]]): Optional separate configuration for the grader model. If not provided, uses llm_config
            max_depth (int): Maximum depth of the reasoning tree
            beam_size (int): DEPRECATED. Number of parallel reasoning paths to maintain
            answer_approach (str): DEPRECATED. Either "pool" or "best" - how to generate final answer
            reason_config (Optional[dict[str, Any]]): Configuration for the reasoning method.
                method (str): The search strategy to use. Options:
                    - "beam_search" (default): Uses beam search with parallel paths
                    - "mcts": Uses Monte Carlo Tree Search for exploration
                    - "lats": Uses Language Agent Tree Search with per-step rewards
                    - "dfs": Uses depth-first search (equivalent to beam_search with beam_size=1)

                Common parameters:
                    max_depth (int): Maximum depth of reasoning tree (default: 3)
                    forest_size (int): Number of independent trees to maintain (default: 1)
                    rating_scale (int): Scale for grading responses, e.g. 1-10 (default: 10)
                    interim_execution (bool): Whether to execute the suggested options between the steps.

                Beam Search specific:
                    beam_size (int): Number of parallel paths to maintain (default: 3)
                    answer_approach (str): How to select final answer, "pool" or "best" (default: "pool")
                    batch_grading (bool): Whether to grade all options on each beam at once or one by one (default: False).

                MCTS/LATS specific:
                    nsim (int): Number of simulations to run (default: 3)
                    exploration_constant (float): UCT exploration parameter (default: 1.41)

                Example configs:
                    `{"method": "beam_search", "beam_size": 5, "max_depth": 4}`
                    `{"method": "mcts", "nsim": 10, "exploration_constant": 2.0}`
                    `{"method": "lats", "nsim": 5, "forest_size": 3}`
            code_execution_config (dict or False): config for the code execution.
                To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:
                - work_dir (Optional, str): The working directory for the code execution.
                    If None, a default working directory will be used.
                    The default working directory is the "extensions" directory under
                    "path_to_autogen".
                - use_docker (Optional, list, str or bool): The docker image to use for code execution.
                    Default is True, which means the code will be executed in a docker container. A default list of images will be used.
                    If a list or a str of image name(s) is provided, the code will be executed in a docker container
                    with the first image successfully pulled.
                    If False, the code will be executed in the current environment.
                    We strongly recommend using docker for code execution.
                - timeout (Optional, int): The maximum execution time in seconds.
                - last_n_messages (Experimental, int or str): The number of messages to look back for code execution.
                    If set to 'auto', it will scan backwards through all messages arriving since the agent last spoke, which is typically the last time execution was attempted. (Default: auto)
            scope (Optional[str]): The scope of the agent, includes information on how the agent should operate. It is appended to all system prompts of the internal agents. If None, no scope is added to the prompts.
            **kwargs (Any): Additional keyword arguments passed to parent class
        """
        reason_config = reason_config or {}
        if "verbose" in kwargs:
            warnings.warn(
                "The parameter `verbose` in ReasoningAgent has been deprecated. "
                "Please use the `silent` parameter as other AG2 agents.",
                DeprecationWarning,
            )
            kwargs["silent"] = not kwargs.pop("verbose")

        llm_config = LLMConfig.get_current_llm_config(llm_config)  # type: ignore[arg-type]
        self._scope = scope

        system_msg = kwargs.pop("system_message", REASONING_AGENT_MESSAGE)
        system_msg = self._add_scope(system_msg)

        super().__init__(
            name=name,
            system_message=system_msg,
            llm_config=llm_config,
            code_execution_config=code_execution_config,
            **kwargs,
        )
        self._llm_config: LLMConfig | dict[str, Any] | None = llm_config
        self._grader_llm_config: LLMConfig | dict[str, Any] | None = (
            grader_llm_config if grader_llm_config else llm_config
        )

        if max_depth != 4 or beam_size != 3 or answer_approach != "pool":
            warnings.warn(
                "The parameters max_depth, beam_size, and answer_approach have been deprecated. "
                "Please use the reason_config dictionary to configure these settings instead.",
                DeprecationWarning,
            )

        self._reason_config: dict[str, Any] = reason_config or {}
        self._method: Literal["beam_search", "mcts", "lats", "dfs"] = reason_config.get("method", "beam_search")
        if self._method not in ["beam_search", "mcts", "lats", "dfs"]:
            raise ValueError(
                f"Invalid reasoning method specified: '{self._method}'. Should be one of 'beam_search', 'mcts', 'lats', or 'dfs'."
            )

        self._beam_size: int = 1
        if self._method in ["beam_search", "dfs"]:
            if self._method != "dfs":
                self._beam_size = reason_config.get("beam_size", beam_size)
            self._answer_approach: Literal["pool", "best"] = reason_config.get("answer_approach", answer_approach)
            if self._answer_approach not in ["pool", "best"]:
                raise ValueError(
                    f"Invalid answer_approach specified: '{self._answer_approach}'. Should be one of 'pool' or 'best'."
                )
            self._batch_grading: bool = reason_config.get("batch_grading", False)
        elif self._method in ["mcts", "lats"]:
            self._nsim: int = reason_config.get("nsim", 3)
            self._exploration_constant: float = reason_config.get("exploration_constant", 1.41)

        self._max_depth: int = reason_config.get("max_depth", max_depth)
        self._forest_size: int = reason_config.get("forest_size", 1)
        self._rating_scale: int = reason_config.get("rating_scale", 10)
        self._interim_execution: bool = reason_config.get("interim_execution", False)

        self._root: ThinkNode | None = None
        self._lats_context: str = ""
        self.register_reply([Agent, None], ReasoningAgent.generate_forest_response)

        # Initialize llm agent for interim step execution
        self._executor: AssistantAgent | None = None
        # Add scope if provided
        executor_msg = self._add_scope(EXECUTOR_MESSAGE)
        if self._interim_execution:
            self._executor = AssistantAgent(
                name="tot_executor", system_message=executor_msg, llm_config=self._llm_config
            )

        tot_msg = self._add_scope(TREEOFTHOUGHT_MESSAGE)

        # Initialize user proxy agent for code execution
        self._user_proxy: UserProxyAgent | None = None
        if self._code_execution_config:
            # to execute code interim_execution should be True
            if not self._interim_execution:
                raise ValueError(
                    "Code execution is enabled in the system, but interim_execution is set to False. "
                    "Please set interim_execution to True to allow code execution between reasoning steps."
                )

            self._user_proxy = UserProxyAgent(
                name="reasoner_user_proxy",
                human_input_mode="NEVER",
                code_execution_config=self._code_execution_config,
                max_consecutive_auto_reply=1,
            )

            self._code_execution_config = False  # code should only be executed by the user proxy
        else:
            # remove python instructions from the tot message
            tot_msg = "\n".join([
                line for line in tot_msg.split("\n") if not re.compile(r".*(python|```).*").search(line)
            ])

        # Initialize required agents
        self._thinker = AssistantAgent(name="tot_thinker", system_message=tot_msg, llm_config=self._llm_config)
        self._grader = AssistantAgent(name="tot_grader", llm_config=self._grader_llm_config)
        self._prompt_rewriter = AssistantAgent(name="prompt_rewriter", llm_config=self._llm_config)

    def _add_scope(self, system_prompt: str) -> str:
        """Add scope information to the system prompt.

        Args:
            system_prompt (str): The original system prompt.

        Returns:
            str: The modified system prompt with scope information.
        """
        if self._scope:
            return f"Task Scope: {self._scope}\n\n{system_prompt}"
        return system_prompt

    def generate_forest_response(
        self,
        messages: list[dict[str, Any]] | None = None,
        sender: Agent | None = None,
        config: dict[str, Any] | None = None,
    ) -> tuple[bool, str]:
        """Generate a response using tree-of-thought reasoning.

        Args:
            messages (Optional[list[dict[str, Any]]]): Input messages to respond to
            sender (Optional[Agent]): Agent sending the messages
            config (Optional[dict[str, Any]]): Optional configuration

        Returns:
            Tuple[bool, str]: Success flag and generated response
        """
        if sender == self:
            return False, ""  # Defer the LLM call to next reply functions.
        prompt, ground_truth = self._process_prompt(messages, sender)
        if not prompt:
            return True, "TERMINATE"

        forest_answers: list[str] = []
        for _ in range(self._forest_size):
            if self._method in ["beam_search", "dfs"]:
                response = self._beam_reply(prompt, ground_truth)
            elif self._method in ["mcts", "lats"]:
                response = self._mcts_reply(prompt, ground_truth)
            else:
                raise ValueError("Invalid reasoning method specified.")

            forest_answers.append(response)

        if len(forest_answers) == 1:
            return True, forest_answers[0]
        else:
            forest_answers_str = "-" + "\n-".join(forest_answers)
            self.send(
                message=f"""Given a list of different answers provide a complete response to a user's question.
Question:
{prompt}

Answers:
{forest_answers_str}

Final Answer:
""",
                recipient=self,
                request_reply=True,
                silent=self.silent,
            )
            last_msg: dict[str, Any] | None = self.last_message(self)
            if last_msg is None:
                return True, ""
            return True, last_msg["content"].strip()

    def rate_node(self, node: ThinkNode, ground_truth: str | None = None, is_outcome: bool = False) -> float:
        """Rate the quality of a reasoning path or the final answer using the grader agent.

        Args:
            node (ThinkNode): Node containing the reasoning trajectory to evaluate
            ground_truth (Optional[str]): Optional ground truth to provide to the grader
            is_outcome (bool): indicates whether the rating is for an outcome (final answer) or a process (thinking trajectory).

        Returns:
            float: Normalized score between 0 and 1 indicating trajectory quality
        """
        if node.value > 0 and node.rating_details:
            # we already calculated the rating for the node
            return node.value

        # Update Grader's system message
        if is_outcome:
            # Outcome Rating
            message = f"""Please rate the answer on a scale of 1 to {self._rating_scale}, where 1 is the worst and {self._rating_scale} is the best.

A great answer must:
- Directly address the original question
- Be factually accurate and complete
- Show clear logical reasoning

Additionally, a good answer should:
- Be concise and well-structured
- Use appropriate language and tone
- Provide relevant examples or evidence when needed
- Be free of contradictions or inconsistencies

If the answer fails to meet any of the core requirements above, it should be considered a poor response.

Also, rate poory (with 1) trajectories that:
- Require access to internet, experts opinions or external sources.
- Require research, hypotheses or data that are not provided.
- Include solutions in the physical world, like conducting experiments or surveys (code execution is fine).

Please provide your rating along with a brief explanation of your assessment.
"""
        else:
            # Process Rating
            message = f"""Please rate the thinking trajectory on a scale of 1 to {self._rating_scale}, where 1 is the worst and {self._rating_scale} is the best.

A great thinking trajectory must:
- Advance the process of solving the problem.

Additionally, a good trajectory should:
- Be appropriate in conversation.
- Contain no inaccuracies.
- Be free of any odd or irrelevant content.

If the trajectory does not meet one of the above requirements, it is considered a bad response.

Also, rate poory (with 1) trajectories that:
- Require access to internet, experts opinions or external sources.
- Require research, hypotheses or data that are not provided.
- Include solutions in the physical world, like conducting experiments or surveys (code execution is fine).

Please provide your rating along with a brief explanation of your assessment.
"""
        # Add ground truth to the message.
        if ground_truth:
            # override the system message
            message += f"--- Note that the Ground Truth is ---\n{ground_truth}\n---\n"

        # add scope if provided
        message = self._add_scope(message)

        self._grader.update_system_message(message)

        if self._method == "lats":
            prompt = self._lats_context + "\n\n---\n\n" + f"Rate:\n{node.trajectory}"
        else:
            prompt = f"Rate:\n{node.trajectory}"

        self._grader.clear_history()
        self.send(
            message=prompt,
            recipient=self._grader,
            request_reply=True,
            silent=self.silent,
        )
        rating: str = ""
        last_message: dict[str, Any] | None = self._grader.last_message()
        if last_message is not None:
            rating = last_message["content"].strip()
        node.rating_details = rating

        try:
            # Scale rating to [0, 1]
            reward = (float(re.findall(r"[\d.]+", rating)[0]) - 1.0) / (self._rating_scale - 1.0)
        except (IndexError, ValueError):
            reward = 0.0  # Default reward if parsing fails
        return reward

    def rate_batch_nodes(self, nodes: list[ThinkNode], ground_truth: str | None = None) -> list[float]:
        """Rate a batch of nodes using a single call of the grader agent. All the nodes must have the same parent.

        This method evaluates all given nodes while considering the other available options.

        Args:
            nodes (list[ThinkNode]): List of nodes to rate, all nodes must have the same parent
            ground_truth (Optional[str]): Optional ground truth to provide to the grader
        """
        # Assert that all nodes have the same parent and it is not None
        assert all(node.parent == nodes[0].parent for node in nodes), "All nodes must have the same parent."
        assert nodes[0].parent is not None, "Parent node must not be None."

        # Update Grader's system message
        message = f"""You will be provided a thinking trajectory and a list of options for the next step.
Please rate the thinking trajectory created by each option on a scale of 1 to {self._rating_scale}, where 1 is the worst and {self._rating_scale} is the best.

A great thinking trajectory must:
- Advance the process of solving the problem.

Additionally, a good trajectory should:
- Be appropriate in conversation.
- Contain no inaccuracies.
- Be free of any odd or irrelevant content.

If the trajectory does not meet one of the above requirements, it is considered a bad response.

Also, rate poorly (with 1) trajectories that:
- Require access to internet, experts opinions or external sources.
- Require research, hypotheses or data that are not provided.
- Include solutions in the physical world, like conducting experiments or surveys (code execution is fine).

Please provide your rating along with a brief explanation of your assessment.

**Output Format:**
Option 1: <your explanation here for the trajectory>
Rating: <rating>

Option 2: <your explanation here for the trajectory>
Rating: <rating>
...
"""
        # Add ground truth to the message.
        if ground_truth:
            # override the system message
            message += f"--- Note that the Ground Truth is ---\n{ground_truth}\n---\n"

        # add scope if provided
        message = self._add_scope(message)

        self._grader.update_system_message(message)

        # add lats context if necessary
        prompt = f"{self._lats_context}\n\n---\n\n" if self._method == "lats" else ""

        # add current trajectory
        prompt += f"Trajectory:\n{nodes[0].parent.trajectory}\n\n---\n\nOptions:\n"

        # add options
        for i, node in enumerate(nodes):
            prompt += f"\nOption {i + 1}:\n{node.content}"

        self._grader.clear_history()
        self.send(
            message=prompt,
            recipient=self._grader,
            request_reply=True,
            silent=self.silent,
        )
        rating: str = ""
        last_message: dict[str, Any] | None = self._grader.last_message()
        if last_message is not None:
            rating = last_message["content"].strip()

        # Extract ratings and details for each option
        options_with_ratings = re.findall(r"(Option \d+:.*?Rating:\s*[\d.]+)", rating, re.DOTALL)
        ratings = []
        for option in options_with_ratings:
            match = re.search(r"Rating:\s*([\d.]+)", option)
            if match:
                ratings.append(match.group(1))

        # if the response wasn't of the expected format, return default rewards
        if len(ratings) != len(nodes):
            return [0.0] * len(nodes)

        rewards = []
        # Get rewards and assign rating details to corresponding nodes
        for node, rating, details in zip(nodes, ratings, options_with_ratings):
            if node.value > 0 and node.rating_details:
                # we already calculated the rating for the node
                rewards.append(node.value)
                continue
            node.rating_details = details
            rewards.append((float(rating) - 1.0) / (self._rating_scale - 1.0))
        return rewards

    def execute_node(self, node: ThinkNode) -> str | None:
        """Execute the node's content to get the response.

        This method runs the node's content to get the response.
        If the content contains a Python code snippet, it sends the code to the user proxy agent for execution.
        Else, it sends the content to the LLM for generating the response.

        Args:
            node (ThinkNode): The node to run.

        Returns:
            Optional[str]: The response generated by the node, or None if the node is TERMINATE.
        """
        assert isinstance(self._executor, AssistantAgent)

        if node.output is not None:
            return node.output

        if "TERMINATE" in node.content:  # don't use _is_terminal, as the terminal node for beam search can be executed
            return None

        # check for python snippet
        if "```python" in node.content:
            # if code execution is disabled, ask to follow a different approach
            if not self._user_proxy:
                return "Python code execution is disabled. Follow a different approach."
            self._user_proxy.clear_history()
            self.send(
                message=node.content,
                recipient=self._user_proxy,
                request_reply=True,
                silent=self.silent,
            )
            user_proxy_last_msg: dict[str, Any] | None = self._user_proxy.last_message(self)
            user_proxy_last_msg_content: str = user_proxy_last_msg["content"] if user_proxy_last_msg is not None else ""
            return user_proxy_last_msg_content

        # run with the LLM
        prompt = f"{self._lats_context}\n\n---\n\n" if self._method == "lats" else ""
        prompt += f"Trajectory:\n{node.trajectory}\nOutput:"

        self._executor.clear_history()
        self.send(
            message=prompt,
            recipient=self._executor,
            request_reply=True,
            silent=self.silent,
        )

        output = ""
        last_message: dict[str, Any] | None = self._executor.last_message()

        # this agent is not supposed to write Python code, so if there is a need for that ask the thinker to do so
        if last_message is not None:
            if "```python" in last_message["content"]:
                output = (
                    "To execute Python code please provide the exact snippet in a fenced block like ```python ... ```."
                )
            else:
                output = last_message["content"].strip()

        return output

    def _process_prompt(
        self, messages: list[dict[str, Any]] | None, sender: Agent | None
    ) -> tuple[str | None, str | None]:
        """Process the incoming messages to extract the prompt and ground truth.

        This method checks if the provided messages are None and identifies the prompt.
        If there is only one message, it uses that as the prompt. Otherwise, it asks the question in the messages including also the important information from the previous messages.
        It also looks for a specific keyword "GROUND_TRUTH" in any of the messages to separate the ground truth for evaluation purposes.

        Args:
            messages (Optional[list[dict[str, Any]]]): A list of message dictionaries containing the content to process.
            sender (Optional[Agent]): The agent sending the messages.

        Returns:
            Tuple[Optional[str], Optional[str]]: A tuple containing the processed prompt and the ground truth.
            If the prompt is empty, returns (None, None).
        """
        messages = self._oai_messages[sender] if messages is None else messages
        messages_copy = copy.deepcopy(messages)

        # Extract the ground truth for more accurate evaluation.
        # TODO: in the future, allow user to pass a callable (func) to calculate reward.
        ground_truth = None
        for i, message in enumerate(messages_copy):
            if "GROUND_TRUTH" in message["content"]:
                idx = message["content"].find("GROUND_TRUTH")
                messages_copy[i]["content"], ground_truth = message["content"][:idx].rstrip(), message["content"][idx:]
                break

        if len(messages) == 1:
            # First message, no previous context
            prompt = messages_copy[0]["content"]
        else:
            rewriter_message = f"""
Task: Given a list of messages including a previous discussion, write a prompt that summarizes the discussion, including all the useful information, and asks a question.

**Messages:**
{messages_copy}

**Format of Output:**
QUESTION: *Write the initial question asked by the user here.*
SUMMARY: *summarize the existing discussions.*

ACTIVITY LOG:
- *Action 1 performed*
- *Action 2 performed*
- ...

CURRENT_QUESTION: *Write the current/last question to be addressed here. In case the task has been completed, write: "The task has now been completed, write the final response and terminate the task."*
"""
            self._prompt_rewriter.clear_history()
            self.send(
                message=rewriter_message,
                recipient=self._prompt_rewriter,
                request_reply=True,
                silent=self.silent,
            )
            last_msg: dict[str, Any] | None = self._prompt_rewriter.last_message()
            prompt = last_msg["content"].strip() if last_msg is not None else ""

        if not prompt:
            return None, None

        return prompt, ground_truth

    def _beam_reply(self, prompt: str, ground_truth: str | None = None) -> str:
        """Generate a response using tree-of-thought reasoning.

        Implements beam search through a tree of reasoning steps, using the thinker
        agent to generate possible next steps and the grader agent to evaluate paths.

        Args:
            prompt (str): The question or prompt to generate a response for.
            ground_truth (Optional[str]): The ground truth or correct answer for evaluation.

        Returns:
            str: The generated response based on the reasoning process.
        """
        root = ThinkNode(content=prompt, parent=None)
        self._root = root  # save the root node for later visualization
        prev_leafs: list[ThinkNode] = [root]
        final_answers: set[ThinkNode] = set()  # store the final answers

        while prev_leafs and len(final_answers) < self._beam_size:
            new_leafs: list[ThinkNode] = []
            new_leafs_per_beam: list[list[ThinkNode]] = []  # used for batch grading
            for node in prev_leafs:
                if self._is_terminal(node):
                    # Reached max depth; collect possible answers
                    if node.value is None:
                        node.value = self.rate_node(node, ground_truth)
                    final_answers.add(node)
                    continue

                expansion_leafs = self._expand(node)
                new_leafs += expansion_leafs
                new_leafs_per_beam.append(expansion_leafs)

            prev_leafs = new_leafs

            if len(prev_leafs) + len(final_answers) > self._beam_size:
                if len(final_answers) >= self._beam_size:
                    prev_leafs = []  # stop searching, max beam size reached
                    break

                # Rate
                if self._batch_grading:
                    for beam_nodes in new_leafs_per_beam:
                        rewards = self.rate_batch_nodes(beam_nodes, ground_truth)
                        for node, reward in zip(beam_nodes, rewards):
                            node.value = reward
                else:
                    for node in prev_leafs:
                        node.value = self.rate_node(node, ground_truth)
                # Beam search: keep top beam_size leaf nodes
                prev_leafs = sorted(prev_leafs, key=lambda x: x.value if x.value else 0, reverse=True)[
                    : self._beam_size - len(final_answers)
                ]

                # Execute
                if self._interim_execution:
                    for node in prev_leafs:
                        node.output = self.execute_node(node)

        assert final_answers, "No final answers found."
        final_answers_list = list(final_answers)

        if self._answer_approach == "best":
            # Best the final answers
            best_leaf = max(final_answers_list, key=lambda x: x.value)
            message = f"""Given a thinking process, you have to provide a complete response to a user's question.
Question:
{prompt}

Thinking process:
{best_leaf.trajectory}

Final Answer:
"""
        elif self._answer_approach == "pool":
            all_thoughts = "\n\n".join([
                f"--- Possibility {i + 1} ---\n{node.trajectory}\n" for i, node in enumerate(final_answers_list)
            ])
            message = f"""Given a list of thinking processes, you have to provide a complete response to a user's question.
Question:
{prompt}

Thinking processes:
{all_thoughts}

Final Answer:
"""
        self.send(
            message=message,
            recipient=self,
            request_reply=True,
            silent=self.silent,
        )
        last_msg: dict[str, Any] | None = self.last_message(self)
        final_answer: str = last_msg["content"].strip() if last_msg is not None else ""
        return final_answer

    def _mcts_reply(self, prompt: str, ground_truth: str | None = None) -> str:
        """Generate a response using Monte Carlo Tree Search (MCTS) reasoning.

        Args:
            prompt (str): The question or prompt to generate a response for.
            ground_truth (Optional[str]): The ground truth or correct answer for evaluation.

        Returns:
            str: The generated response based on the reasoning process.
        """
        root = ThinkNode(content=prompt, parent=None)
        self._root = root
        answer_nodes: list[ThinkNode] = []

        self._lats_context = "## Here are some previous trajectories and reflections\n\n"  # Store LATS's reflections

        # TODO: future, parallelism with Swarm agent or AsyncOpenAI client.
        for _ in range(self._nsim):
            node = root

            # Selection
            while not self._is_terminal(node) and len(node.children) > 0:
                choices_weights = [
                    (child.value / (child.visits + EPSILON))
                    + self._exploration_constant
                    * math.sqrt(2 * math.log(node.visits + EPSILON) / (child.visits + EPSILON))
                    for child in node.children
                ]
                node = node.children[choices_weights.index(max(choices_weights))]

                # Execution
                if self._interim_execution:
                    node.output = self.execute_node(node)

            # Expansion and Simulation
            while not self._is_terminal(node):
                if len(node.children) == 0:
                    self._expand(node)
                if len(node.children) == 0:
                    node.content += "\nTERMINATE"
                    break
                node = random.choice(node.children)

                # Execution
                if self._interim_execution:
                    node.output = self.execute_node(node)

            # Add answer (leaf) node and evaluate answer
            self.send(
                message=f"""Given a thinking process, you have to provide a complete response to a user's question.
Question:
{prompt}

Thinking process:
{node.trajectory}

Final Answer:
""",
                recipient=self,
                request_reply=True,
                silent=self.silent,
            )
            last_msg: dict[str, Any] | None = self.last_message(self)
            _answer: str = last_msg["content"].strip() if last_msg is not None else ""
            _ans_node = ThinkNode(content=_answer, parent=node)
            reward = self.rate_node(_ans_node, ground_truth, is_outcome=True)
            _ans_node.value = reward
            answer_nodes.append(_ans_node)
            self._lats_context += f"### Previous Tries:\n{node.trajectory}\n\nRating:{_ans_node.rating_details}\n\n"
            node.backpropagate(reward)

        best_ans_node = max(answer_nodes, key=lambda node: node.value)
        return best_ans_node.content

    def _expand(self, node: ThinkNode) -> list[ThinkNode]:
        """Expand the node by generating possible next steps based on the current trajectory.

        This method sends a message to the thinker agent, asking for possible next steps
        that can be taken from the current node's trajectory. It processes the response to
        extract the options provided by the thinker and creates new ThinkNode instances
        for each option.

        Args:
            node (ThinkNode): The node to expand, representing the current state in the reasoning process.

        Returns:
            list[ThinkNode]: A list of new ThinkNode instances created from the options provided by the thinker.
        """
        self._thinker.clear_history()

        if self._method == "lats":
            prompt = (
                self._lats_context
                + "\n\n---\n\n"
                + f"{node.trajectory}\n---\nHow should the thinking process continue?"
            )
        else:
            prompt = f"{node.trajectory}\n---\nHow should the thinking process continue?"

        self.send(
            message=prompt,
            recipient=self._thinker,
            request_reply=True,
            silent=self.silent,
        )
        last_msg: dict[str, Any] | None = self._thinker.last_message()
        reply: str = last_msg["content"].strip() if last_msg is not None else ""
        reflection = re.findall(r"REFLECTION:\s*(.+?)(?=\*\*Possible Options:\*\*|Option \d+:|$)", reply, re.DOTALL)
        if reflection:
            node.reflection += str(reflection[0].strip())
        options = re.findall(r"Option \d+:(.+?)(?=Option \d+:|$)", reply, re.DOTALL)

        option_nodes = [ThinkNode(content=option.strip().rstrip(), parent=node) for option in options]

        return option_nodes

    def _is_terminal(self, node: ThinkNode) -> bool:
        """Check if the node is a terminal state in the reasoning process.

        Args:
            node (ThinkNode): The node to check for terminal state.

        Returns:
            bool: True if the node is terminal, False otherwise.
        """
        return node.depth >= self._max_depth or "TERMINATE" in node.content

    @property
    def method(self) -> str:
        """Get the reasoning method being used.

        Returns:
            str: The name of the reasoning method
        """
        return self._method

    def visualize_tree(self) -> None:
        """Visualize the tree of thoughts using graphviz.

        Raises:
            RuntimeError: If the tree has not been generated yet.
        """
        if self._root:
            self._root.visualize_tree()
        else:
            raise RuntimeError("No tree to visualize. Run the reasoning process first.")

    def extract_sft_dataset(self) -> list[dict[str, Any]]:
        """Extract the best trajectory or multiple equally good trajectories for SFT training.

        Returns:
            list[dict]: list of best trajectories, each one is a pair of instruction and response.

        Raises:
            RuntimeError: If the tree has not been generated yet.
        """
        if self._root:
            return extract_sft_dataset(self._root)
        else:
            raise RuntimeError("No tree to extract dataset from. Run the reasoning process first.")

    def extract_rlhf_preference_dataset(self, contrastive_threshold: float = 0.2) -> list[dict[str, Any]]:
        """Extract and generate preference pairs for RLHF training by comparing sibling nodes.

        Args:
            contrastive_threshold (float): between (0, 1), a distance measure that we are confident to call
                one is positive and another is negative.

        Returns:
            list[dict]: list of preference pairs, where each pair contains two responses and
            indicates which one is preferred.

        Raises:
            RuntimeError: If the tree has not been generated yet.
        """
        if self._root:
            return extract_rlhf_preference_dataset(self._root, contrastive_threshold)
        else:
            raise RuntimeError("No tree to extract dataset from. Run the reasoning process first.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import ConversableAgent
from ....doc_utils import export_module
from ....tools.experimental import TelegramRetrieveTool, TelegramSendTool

__all__ = ["TelegramAgent"]


@export_module("autogen.agents.experimental")
class TelegramAgent(ConversableAgent):
    """An agent that can send messages and retrieve messages on Telegram."""

    DEFAULT_SYSTEM_MESSAGE = (
        "You are a helpful AI assistant that communicates through Telegram. "
        "Remember that Telegram uses Markdown-like formatting and has message length limits. "
        "Keep messages clear and concise, and consider using appropriate formatting when helpful."
    )

    def __init__(
        self,
        name: str,
        system_message: str | None = None,
        *,
        api_id: str,
        api_hash: str,
        chat_id: str,
        has_writing_instructions: bool = True,
        **kwargs: Any,
    ) -> None:
        """Initialize the TelegramAgent.

        Args:
            name: The name of the agent.
            system_message: The system message for the agent.
            api_id: Telegram API ID from https://my.telegram.org/apps.
            api_hash: Telegram API hash from https://my.telegram.org/apps.
            chat_id: The ID of the destination (Channel, Group, or User ID).
            has_writing_instructions (bool): Whether to add writing instructions to the system message. Defaults to True.
            **kwargs: Additional keyword arguments passed to the parent ConversableAgent class.
        """
        telegram_system_message = system_message or self.DEFAULT_SYSTEM_MESSAGE

        self._send_tool = TelegramSendTool(api_id=api_id, api_hash=api_hash, chat_id=chat_id)
        self._retrieve_tool = TelegramRetrieveTool(api_id=api_id, api_hash=api_hash, chat_id=chat_id)

        # Add formatting instructions
        if has_writing_instructions:
            formatting_instructions = (
                "\nFormat guidelines for Telegram:\n"
                "1. Max message length: 4096 characters\n"
                "2. HTML formatting:\n"
                "   - <b>bold</b>\n"
                "   - <i>italic</i>\n"
                "   - <code>code</code>\n"
                "   - <pre>code block</pre>\n"
                "   - <a href='URL'>link</a>\n"
                "   - <u>underline</u>\n"
                "   - <s>strikethrough</s>\n"
                "3. HTML rules:\n"
                "   - Tags must be properly closed\n"
                "   - Can't nest identical tags\n"
                "   - Links require full URLs (with http://)\n"
                "4. Supports @mentions and emoji"
            )

            telegram_system_message = telegram_system_message + formatting_instructions

        super().__init__(name=name, system_message=telegram_system_message, **kwargs)

        self.register_for_llm()(self._send_tool)
        self.register_for_llm()(self._retrieve_tool)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .telegram import TelegramAgent

__all__ = ["TelegramAgent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .slack import SlackAgent

__all__ = ["SlackAgent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import ConversableAgent
from ....doc_utils import export_module
from ....tools.experimental import SlackRetrieveRepliesTool, SlackRetrieveTool, SlackSendTool

__all__ = ["SlackAgent"]


@export_module("autogen.agents.experimental")
class SlackAgent(ConversableAgent):
    """An agent that can send messages and retrieve messages on Slack."""

    DEFAULT_SYSTEM_MESSAGE = (
        "You are a helpful AI assistant that communicates through Slack. "
        "Remember that Slack uses Markdown-like formatting and has message length limits. "
        "Keep messages clear and concise, and consider using appropriate formatting when helpful."
    )

    def __init__(
        self,
        name: str,
        system_message: str | None = None,
        *,
        bot_token: str,
        channel_id: str,
        has_writing_instructions: bool = True,
        **kwargs: Any,
    ) -> None:
        """Initialize the SlackAgent.

        Args:
            name: name of the agent.
            system_message: system message for the ChatCompletion inference.
            bot_token: Bot User OAuth Token starting with "xoxb-".
            channel_id: Channel ID where messages will be sent.
            has_writing_instructions: Whether to add writing instructions to the system message. Defaults to True.
            **kwargs: Additional keyword arguments passed to the parent ConversableAgent class.
        """
        slack_system_message = system_message or self.DEFAULT_SYSTEM_MESSAGE

        self._send_tool = SlackSendTool(bot_token=bot_token, channel_id=channel_id)
        self._retrieve_tool = SlackRetrieveTool(bot_token=bot_token, channel_id=channel_id)
        self._retrieve_replies_tool = SlackRetrieveRepliesTool(bot_token=bot_token, channel_id=channel_id)

        # Add formatting instructions
        if has_writing_instructions:
            formatting_instructions = (
                "\nFormat guidelines for Slack:\n"
                "Format guidelines for Slack:\n"
                "1. Max message length: 40,000 characters\n"
                "2. Supports Markdown-like formatting:\n"
                "   - *text* for italic\n"
                "   - **text** for bold\n"
                "   - `code` for inline code\n"
                "   - ```code block``` for multi-line code\n"
                "3. Supports message threading for organized discussions\n"
                "4. Can use :emoji_name: for emoji reactions\n"
                "5. Supports block quotes with > prefix\n"
                "6. Can use <!here> or <!channel> for notifications"
            )

            slack_system_message = slack_system_message + formatting_instructions

        super().__init__(name=name, system_message=slack_system_message, **kwargs)

        self.register_for_llm()(self._send_tool)
        self.register_for_llm()(self._retrieve_tool)
        self.register_for_llm()(self._retrieve_replies_tool)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .deep_research import DeepResearchAgent

__all__ = ["DeepResearchAgent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import ConversableAgent
from ....doc_utils import export_module
from ....llm_config import LLMConfig
from ....tools.experimental import DeepResearchTool

__all__ = ["DeepResearchAgent"]


@export_module("autogen.agents.experimental")
class DeepResearchAgent(ConversableAgent):
    """An agent that performs deep research tasks."""

    DEFAULT_PROMPT = "You are a deep research agent. You have the ability to get information from the web and perform research tasks."

    def __init__(
        self,
        name: str,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        system_message: str | list[str] | None = DEFAULT_PROMPT,
        max_web_steps: int = 30,
        **kwargs: Any,
    ) -> None:
        """Initialize the DeepResearchAgent.

        Args:
            name: The name of the agent.
            llm_config: The LLM configuration.
            system_message: The system message. Defaults to DEFAULT_PROMPT.
            max_web_steps: The maximum number of web steps. Defaults to 30.
            **kwargs: Additional keyword arguments to pass to the ConversableAgent.
        """
        llm_config = LLMConfig.get_current_llm_config(llm_config)  # type: ignore[arg-type]

        super().__init__(
            name=name,
            system_message=system_message,
            llm_config=llm_config,
            **kwargs,
        )

        self.tool = DeepResearchTool(
            llm_config=llm_config,  # type: ignore[arg-type]
            max_web_steps=max_web_steps,
        )

        self.register_for_llm()(self.tool)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
from enum import Enum
from pathlib import Path
from typing import Any
from urllib.parse import urlparse

from pydantic import BaseModel, Field

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from .url_utils import ExtensionToFormat, InputFormat, URLAnalyzer

with optional_import_block():
    import requests
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from webdriver_manager.chrome import ChromeDriverManager

__all__ = ["handle_input", "preprocess_path"]

_logger = logging.getLogger(__name__)


class QueryType(Enum):
    RAG_QUERY = "RAG_QUERY"
    # COMMON_QUESTION = "COMMON_QUESTION"


class Ingest(BaseModel):
    path_or_url: str = Field(description="The path or URL of the documents to ingest.")


class Query(BaseModel):
    query_type: QueryType = Field(description="The type of query to perform for the Document Agent.")
    query: str = Field(description="The query to perform for the Document Agent.")


def is_url(url: str) -> bool:
    """Check if the string is a valid URL.

    It checks whether the URL has a valid scheme and network location.
    """
    try:
        url = url.strip()
        result = urlparse(url)
        # urlparse will not raise an exception for invalid URLs, so we need to check the components
        return_bool = bool(result.scheme and result.netloc)
        return return_bool
    except Exception:
        return False


@require_optional_import(["selenium", "webdriver_manager", "requests"], "rag")
def _download_rendered_html(url: str) -> str:
    """Downloads a rendered HTML page of a given URL using headless ChromeDriver.

    Args:
        url (str): URL of the page to download.

    Returns:
        str: The rendered HTML content of the page.
    """
    # Set up Chrome options
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # Enable headless mode
    options.add_argument("--disable-gpu")  # Disabling GPU hardware acceleration
    options.add_argument("--no-sandbox")  # Bypass OS security model
    options.add_argument("--disable-dev-shm-usage")  # Overcome limited resource problems

    # Set the location of the ChromeDriver
    service = ChromeService(ChromeDriverManager().install())

    # Create a new instance of the Chrome driver with specified options
    driver = webdriver.Chrome(service=service, options=options)

    try:
        # Open a page
        driver.get(url)

        # Get the rendered HTML
        html_content = driver.page_source
        return str(html_content)

    finally:
        # Close the browser
        driver.quit()


@require_optional_import(["requests", "selenium", "webdriver_manager"], "rag")
def _download_binary_file(url: str, output_dir: Path) -> Path:
    """Downloads a file directly from the given URL.

    Uses appropriate mode (binary/text) based on file extension or content type.

    Args:
        url (str): URL of the file to download.
        output_dir (Path): Directory to save the file.

    Returns:
        Path: Path to the saved file.
    """
    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)

    # Use URLAnalyzer to get information about the URL
    analyzer = URLAnalyzer(url)
    analysis = analyzer.analyze(test_url=True, follow_redirects=True)

    # Get file info
    final_url = analysis.get("final_url", url)
    file_type = analysis.get("file_type")
    content_type = analysis.get("mime_type", "")

    _logger.info(f"Original URL: {url}")
    _logger.info(f"Final URL after redirects: {final_url}")
    _logger.info(f"Detected content type: {content_type}")
    _logger.info(f"Detected file type: {file_type}")

    # Check if the file type is supported
    if file_type == InputFormat.INVALID:
        raise ValueError(f"File type is not supported: {analysis}")

    # Parse URL components from the final URL
    parsed_url = urlparse(final_url)
    path = Path(parsed_url.path)

    # Extract filename and extension from URL
    filename = path.name
    suffix = path.suffix.lower()

    # For URLs without proper filename/extension, or with generic content types
    if not filename or not suffix:
        # Create a unique filename
        unique_id = abs(hash(url)) % 10000

        # Determine extension from file type
        if file_type is not None and isinstance(file_type, InputFormat):
            ext = _get_extension_from_file_type(file_type, content_type)
        else:
            ext = None

        # Create filename
        prefix = "image" if file_type == InputFormat.IMAGE else "download"
        filename = f"{prefix}_{unique_id}{ext}"

    # Ensure the filename has the correct extension
    if suffix:
        # Check if the extension is valid for the file type
        current_ext = suffix[1:] if suffix.startswith(".") else suffix
        if file_type is not None and isinstance(file_type, InputFormat):
            if not _is_valid_extension_for_file_type(current_ext, file_type):
                # If not, add the correct extension
                ext = _get_extension_from_file_type(file_type, content_type)
                filename = f"{Path(filename).stem}{ext}"
        else:
            ext = _get_extension_from_file_type(InputFormat.INVALID, content_type)
            filename = f"{Path(filename).stem}{ext}"
    else:
        # No extension, add one based on file type
        if file_type is not None and isinstance(file_type, InputFormat):
            ext = _get_extension_from_file_type(file_type, content_type)
        else:
            ext = _get_extension_from_file_type(InputFormat.INVALID, content_type)
        filename = f"{filename}{ext}"

    _logger.info(f"Using filename: {filename} for URL: {url}")

    # Create final filepath
    filepath = output_dir / filename

    # Determine if this is binary or text based on extension
    suffix = Path(filename).suffix.lower()
    text_extensions = [".md", ".txt", ".csv", ".html", ".htm", ".xml", ".json", ".adoc"]
    is_binary = suffix not in text_extensions

    # Download with appropriate mode
    try:
        if not is_binary:
            _logger.info(f"Downloading as text file: {final_url}")
            response = requests.get(final_url, timeout=30)
            response.raise_for_status()

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(response.text)
        else:
            _logger.info(f"Downloading as binary file: {final_url}")
            response = requests.get(final_url, stream=True, timeout=30)
            response.raise_for_status()

            with open(filepath, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:  # Filter out keep-alive chunks
                        f.write(chunk)
    except Exception as e:
        _logger.error(f"Download failed: {e}")
        raise

    return filepath


def _get_extension_from_file_type(file_type: InputFormat, content_type: str = "") -> str:
    """Get a file extension based on the file type and content type."""
    # Create a reverse mapping from InputFormat to a default extension
    # We choose the first extension found for each format
    format_to_extension = {}
    for ext, fmt in ExtensionToFormat.items():
        if fmt not in format_to_extension:
            format_to_extension[fmt] = ext

    # Special case for images: use content type to determine exact image format
    if file_type == InputFormat.IMAGE:
        if "jpeg" in content_type or "jpg" in content_type:
            return ".jpeg"
        elif "png" in content_type:
            return ".png"
        elif "tiff" in content_type:
            return ".tiff"
        elif "bmp" in content_type:
            return ".bmp"
        # Fallback to default image extension
        ext = format_to_extension.get(InputFormat.IMAGE, "png")
        return f".{ext}"

    # For all other formats, use the default extension
    if file_type in format_to_extension:
        return f".{format_to_extension[file_type]}"

    return ".bin"  # Default for unknown types


def _is_valid_extension_for_file_type(extension: str, file_type: InputFormat) -> bool:
    """Check if the extension is valid for the given file type."""
    # Remove leading dot if present
    if extension.startswith("."):
        extension = extension[1:]

    # Check if the extension is in URLAnalyzer.ExtensionToFormat
    # and if it maps to the given file type
    return extension in ExtensionToFormat and ExtensionToFormat[extension] == file_type


@require_optional_import(["selenium", "webdriver_manager", "requests"], "rag")
def download_url(url: Any, output_dir: str | Path | None = None) -> Path:
    """Download the content of a URL and save it as a file.

    For direct file URLs (.md, .pdf, .docx, etc.), downloads the raw file.
    For web pages without file extensions or .html/.htm extensions, uses Selenium to render the content.
    """
    url = str(url)
    output_dir = Path(output_dir) if output_dir else Path()

    # Use URLAnalyzer to determine what type of file the URL is
    analyzer = URLAnalyzer(url)
    analysis = analyzer.analyze(test_url=True, follow_redirects=True)

    # Log the analysis result
    _logger.info(f"URL analysis result: {analysis}")

    # Get the final URL after redirects
    final_url = analysis.get("final_url", url)

    # Determine the file type
    is_file = analysis.get("is_file", False)
    file_type = analysis.get("file_type")

    # If it's a direct file URL (not HTML), download it directly
    if is_file and file_type != InputFormat.HTML and file_type != InputFormat.INVALID:
        _logger.info("Detected direct file URL. Downloading...")
        return _download_binary_file(url=final_url, output_dir=output_dir)

    # If it's a web page, use Selenium to render it
    if file_type == InputFormat.HTML or not is_file:
        _logger.info("Detected web page. Rendering...")
        rendered_html = _download_rendered_html(final_url)

        # Determine filename
        parsed_url = urlparse(final_url)
        path = Path(parsed_url.path)
        filename = path.name or "downloaded_content.html"
        if not filename.endswith(".html"):
            filename += ".html"

        # Save the rendered HTML
        filepath = output_dir / filename
        with open(file=filepath, mode="w", encoding="utf-8") as f:
            f.write(rendered_html)

        return filepath

    # Otherwise, try to download as a binary file
    _logger.info("Unknown URL type. Trying to download as binary file...")
    return _download_binary_file(url=final_url, output_dir=output_dir)


def list_files(directory: Path | str) -> list[Path]:
    """Recursively list all files in a directory.

    This function will raise an exception if the directory does not exist.
    """
    path = Path(directory)

    if not path.is_dir():
        raise ValueError(f"The directory {directory} does not exist.")

    return [f for f in path.rglob("*") if f.is_file()]


@export_module("autogen.agents.experimental.document_agent")
def handle_input(input_path: Path | str, output_dir: Path | str = "./output") -> list[Path]:
    """Process the input string and return the appropriate file paths"""
    output_dir = preprocess_path(str_or_path=output_dir, is_dir=True, mk_path=True)
    if isinstance(input_path, str) and is_url(input_path):
        _logger.info("Detected URL. Downloading content...")
        try:
            return [download_url(url=input_path, output_dir=output_dir)]
        except Exception as e:
            raise e

    if isinstance(input_path, str):
        input_path = Path(input_path)
    if not input_path.exists():
        raise ValueError("The input provided does not exist.")
    elif input_path.is_dir():
        _logger.info("Detected directory. Listing files...")
        return list_files(directory=input_path)
    elif input_path.is_file():
        _logger.info("Detected file. Returning file path...")
        return [input_path]
    else:
        raise ValueError("The input provided is neither a URL, directory, nor a file path.")


@export_module("autogen.agents.experimental.document_agent")
def preprocess_path(str_or_path: Path | str, mk_path: bool = False, is_file: bool = False, is_dir: bool = True) -> Path:
    """Preprocess the path for file operations.

    Args:
        str_or_path (Union[Path, str]): The path to be processed.
        mk_path (bool, optional): Whether to create the path if it doesn't exist. Default is True.
        is_file (bool, optional): Whether the path is a file. Default is False.
        is_dir (bool, optional): Whether the path is a directory. Default is True.

    Returns:
        Path: The preprocessed path.
    """
    # Convert the input to a Path object if it's a string
    temp_path = Path(str_or_path)

    # Ensure the path is absolute
    absolute_path = temp_path.absolute()
    absolute_path = absolute_path.resolve()
    if absolute_path.exists():
        return absolute_path

    # Check if the path should be a file or directory
    if is_file and is_dir:
        raise ValueError("Path cannot be both a file and a directory.")

    # If mk_path is True, create the directory or parent directory
    if mk_path:
        if is_file and not absolute_path.parent.exists():
            absolute_path.parent.mkdir(parents=True, exist_ok=True)
        elif is_dir and not absolute_path.exists():
            absolute_path.mkdir(parents=True, exist_ok=True)

    # Perform checks based on is_file and is_dir flags
    if is_file and not absolute_path.is_file():
        raise FileNotFoundError(f"File not found: {absolute_path}")
    elif is_dir and not absolute_path.is_dir():
        raise NotADirectoryError(f"Directory not found: {absolute_path}")

    return absolute_path
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
from copy import deepcopy
from pathlib import Path
from typing import Annotated, Any, cast

from pydantic import BaseModel, Field

from .... import Agent, ConversableAgent, UpdateSystemMessage
from ....agentchat.contrib.rag.query_engine import RAGQueryEngine
from ....agentchat.group.context_condition import ExpressionContextCondition
from ....agentchat.group.context_expression import ContextExpression
from ....agentchat.group.context_variables import ContextVariables
from ....agentchat.group.llm_condition import StringLLMCondition
from ....agentchat.group.multi_agent_chat import initiate_group_chat
from ....agentchat.group.on_condition import OnCondition
from ....agentchat.group.on_context_condition import OnContextCondition
from ....agentchat.group.patterns.pattern import DefaultPattern
from ....agentchat.group.reply_result import ReplyResult
from ....agentchat.group.targets.transition_target import AgentNameTarget, AgentTarget, StayTarget, TerminateTarget
from ....doc_utils import export_module
from ....llm_config import LLMConfig
from ....oai.client import OpenAIWrapper
from .chroma_query_engine import VectorChromaQueryEngine
from .docling_doc_ingest_agent import DoclingDocIngestAgent
from .document_conditions import SummaryTaskAvailableCondition
from .document_utils import Ingest, Query

__all__ = ["DocAgent"]

logger = logging.getLogger(__name__)

DEFAULT_SYSTEM_MESSAGE = """
    You are a document agent.
    You are given a list of documents to ingest and a list of queries to perform.
    You are responsible for ingesting the documents and answering the queries.
"""
TASK_MANAGER_NAME = "TaskManagerAgent"
TASK_MANAGER_SYSTEM_MESSAGE = """
    You are a task manager agent. You have 2 priorities:
    1. You initiate the tasks which updates the context variables based on the task decisions (DocumentTask) from the DocumentTriageAgent.
    ALWAYS call initiate_tasks first when you receive a message from the DocumentTriageAgent, even if you think there are no new tasks.
    This ensures that any new ingestions or queries from the triage agent are properly recorded.
    Put all ingestion and query tasks into the one tool call.
        i.e. output
        {
            "ingestions": [
                {
                    "path_or_url": "path_or_url"
                }
            ],
            "queries": [
                {
                    "query_type": "RAG_QUERY",
                    "query": "query"
                }
            ],
            "query_results": [
                {
                    "query": "query",
                    "result": "result"
                }
            ]
        }
    2. If there are no documents to ingest and no queries to run, hand control off to the summary agent.

    Put all file paths and URLs into the ingestions. A http/https URL is also a valid path and should be ingested.

    Use the initiate_tasks tool to incorporate all ingestions and queries. Don't call it again until new ingestions or queries are raised.

    New ingestions and queries may be raised from time to time, so use the initiate_tasks again if you see new ingestions/queries.

    Transfer to the summary agent if all ingestion and query tasks are done.
    """

DEFAULT_ERROR_GROUP_CHAT_MESSAGE: str = """
Document Agent failed to perform task.
"""

ERROR_MANAGER_NAME = "ErrorManagerAgent"
ERROR_MANAGER_SYSTEM_MESSAGE = """
You communicate errors to the user. Include the original error messages in full. Use the format:
The following error(s) have occurred:
- Error 1
- Error 2
"""


class DocumentTask(BaseModel):
    """The structured output format for task decisions."""

    ingestions: list[Ingest] = Field(description="The list of documents to ingest.")
    queries: list[Query] = Field(description="The list of queries to perform.")

    def format(self) -> str:
        """Format the DocumentTask as a string for the TaskManager to work with."""
        if len(self.ingestions) == 0 and len(self.queries) == 0:
            return "There were no ingestion or query tasks detected."

        instructions = "Tasks:\n\n"
        order = 1

        if len(self.ingestions) > 0:
            instructions += "Ingestions:\n"
            for ingestion in self.ingestions:
                instructions += f"{order}: {ingestion.path_or_url}\n"
                order += 1

            instructions += "\n"

        if len(self.queries) > 0:
            instructions += "Queries:\n"
            for query in self.queries:
                instructions += f"{order}: {query.query}\n"
                order += 1

        return instructions


class DocumentTriageAgent(ConversableAgent):
    """The DocumentTriageAgent is responsible for deciding what type of task to perform from user requests."""

    def __init__(self, llm_config: LLMConfig | dict[str, Any] | None = None):
        # Add the structured message to the LLM configuration
        structured_config_list = deepcopy(llm_config)
        structured_config_list["response_format"] = DocumentTask  # type: ignore[index]

        super().__init__(
            name="DocumentTriageAgent",
            system_message=(
                "You are a document triage agent. "
                "You are responsible for deciding what type of task to perform from a user's request and populating a DocumentTask formatted response. "
                "If the user specifies files or URLs, add them as individual 'ingestions' to DocumentTask. "
                "You can access external websites if given a URL, so put them in as ingestions. "
                "Add the user's questions about the files/URLs as individual 'RAG_QUERY' queries to the 'query' list in the DocumentTask. "
                "Don't make up questions, keep it as concise and close to the user's request as possible."
            ),
            human_input_mode="NEVER",
            llm_config=structured_config_list,
        )


@export_module("autogen.agents.experimental")
class DocAgent(ConversableAgent):
    """The DocAgent is responsible for ingest and querying documents.

    Internally, it generates a group chat with a set of agents to ingest, query, and summarize.
    """

    def __init__(
        self,
        name: str | None = None,
        llm_config: LLMConfig | dict[str, Any] | None = None,
        system_message: str | None = None,
        parsed_docs_path: str | Path | None = None,
        collection_name: str | None = None,
        query_engine: RAGQueryEngine | None = None,
    ):
        """Initialize the DocAgent.

        Args:
            name (Optional[str]): The name of the DocAgent.
            llm_config (Optional[LLMConfig, dict[str, Any]]): The configuration for the LLM.
            system_message (Optional[str]): The system message for the DocAgent.
            parsed_docs_path (Union[str, Path]): The path where parsed documents will be stored.
            collection_name (Optional[str]): The unique name for the data store collection. If omitted, a random name will be used. Populate this to reuse previous ingested data.
            query_engine (Optional[RAGQueryEngine]): The query engine to use for querying documents, defaults to VectorChromaQueryEngine if none provided.
                                                     Use enable_query_citations and implement query_with_citations method to enable citation support. e.g. VectorChromaCitationQueryEngine

        The DocAgent is responsible for generating a group of agents to solve a task.

        The agents that the DocAgent generates are:
        - Triage Agent: responsible for deciding what type of task to perform from user requests.
        - Task Manager Agent: responsible for managing the tasks.
        - Parser Agent: responsible for parsing the documents.
        - Data Ingestion Agent: responsible for ingesting the documents.
        - Query Agent: responsible for answering the user's questions.
        - Error Agent: responsible for returning errors gracefully.
        - Summary Agent: responsible for generating a summary of the user's questions.
        """
        name = name or "DocAgent"
        llm_config = llm_config or LLMConfig.get_current_llm_config()
        system_message = system_message or DEFAULT_SYSTEM_MESSAGE
        parsed_docs_path = parsed_docs_path or "./parsed_docs"

        # Default Query Engine will be ChromaDB
        if query_engine is None:
            query_engine = VectorChromaQueryEngine(collection_name=collection_name)

        super().__init__(
            name=name,
            system_message=system_message,
            llm_config=llm_config,
            human_input_mode="NEVER",
        )
        self.register_reply([ConversableAgent, None], self.generate_inner_group_chat_reply, position=0)

        self._triage_agent = DocumentTriageAgent(llm_config=llm_config)

        def create_error_agent_prompt(agent: ConversableAgent, messages: list[dict[str, Any]]) -> str:
            """Create the error agent prompt, primarily used to update ingested documents for ending.

            Args:
                agent: The conversable agent requesting the prompt
                messages: List of conversation messages

            Returns:
                str: The error manager system message
            """
            update_ingested_documents()

            return ERROR_MANAGER_SYSTEM_MESSAGE

        self._error_agent = ConversableAgent(
            name=ERROR_MANAGER_NAME,
            system_message=ERROR_MANAGER_SYSTEM_MESSAGE,
            llm_config=llm_config,
            update_agent_state_before_reply=[UpdateSystemMessage(create_error_agent_prompt)],
        )

        def update_ingested_documents() -> None:
            """Updates the list of ingested documents, persisted so we can keep a list over multiple replies.

            This function updates self.documents_ingested with any new documents that have been ingested
            by the triage agent, ensuring persistence across multiple DocAgent interactions.
            """
            agent_documents_ingested = self._triage_agent.context_variables.get("DocumentsIngested", [])
            # Update self.documents_ingested with any new documents ingested
            for doc in agent_documents_ingested:  # type: ignore[union-attr]
                if doc not in self.documents_ingested:
                    self.documents_ingested.append(doc)

        class TaskInitInfo(BaseModel):
            ingestions: Annotated[list[Ingest], Field(description="List of documents, files, and URLs to ingest")]
            queries: Annotated[list[Query], Field(description="List of queries to run")]

        def _deduplicate_ingestions(
            new_ingestions: list[Ingest], existing_ingestions: list[Ingest], documents_ingested: list[str]
        ) -> tuple[list[Ingest], list[str]]:
            """Deduplicate ingestions against existing pending and already ingested documents.

            Args:
                new_ingestions: List of new ingestion requests to process
                existing_ingestions: List of ingestions already pending
                documents_ingested: List of document paths already ingested

            Returns:
                tuple: (new_unique_ingestions, ignored_duplicate_paths)
            """
            unique_ingestions = []
            ignored_paths = []

            for ingestion in new_ingestions:
                ingestion_path = ingestion.path_or_url
                # Check if already in pending ingestions
                already_pending = any(existing.path_or_url == ingestion_path for existing in existing_ingestions)
                # Check if already ingested
                already_ingested = ingestion_path in documents_ingested

                if already_pending or already_ingested:
                    ignored_paths.append(ingestion_path)
                else:
                    unique_ingestions.append(ingestion)

            return unique_ingestions, ignored_paths

        def _deduplicate_queries(
            new_queries: list[Query], existing_queries: list[Query]
        ) -> tuple[list[Query], list[str]]:
            """Deduplicate queries against existing pending queries.

            Args:
                new_queries: List of new query requests to process
                existing_queries: List of queries already pending

            Returns:
                tuple: (new_unique_queries, ignored_duplicate_query_texts)
            """
            unique_queries = []
            ignored_query_texts = []

            for query in new_queries:
                query_text = query.query
                # Check if query already exists in pending queries
                already_pending = any(existing.query == query_text for existing in existing_queries)

                if already_pending:
                    ignored_query_texts.append(query_text)
                else:
                    unique_queries.append(query)

            return unique_queries, ignored_query_texts

        def _build_response_message(
            added_ingestions: int, ignored_ingestions: list[str], added_queries: int, ignored_queries: list[str]
        ) -> str:
            """Build a descriptive response message about what was added/ignored.

            Args:
                added_ingestions: Number of unique ingestions added
                ignored_ingestions: List of duplicate ingestion paths ignored
                added_queries: Number of unique queries added
                ignored_queries: List of duplicate query texts ignored

            Returns:
                str: Formatted message describing the results
            """
            messages = []

            if added_ingestions > 0:
                messages.append(f"Added {added_ingestions} new document(s) for ingestion")

            if ignored_ingestions:
                messages.append(
                    f"Ignored {len(ignored_ingestions)} duplicate document(s): {', '.join(ignored_ingestions)}"
                )

            if added_queries > 0:
                messages.append(f"Added {added_queries} new query/queries")

            if ignored_queries:
                messages.append(f"Ignored {len(ignored_queries)} duplicate query/queries: {', '.join(ignored_queries)}")

            if messages:
                return "; ".join(messages)
            else:
                return "All requested tasks were duplicates and ignored"

        def initiate_tasks(
            task_init_info: Annotated[TaskInitInfo, "Documents, Files, URLs to ingest and the queries to run"],
            context_variables: Annotated[ContextVariables, "Context variables"],
        ) -> ReplyResult:
            """Add documents to ingest and queries to answer when received.

            Args:
                task_init_info: Information about documents to ingest and queries to run
                context_variables: The current context variables containing task state

            Returns:
                ReplyResult: Contains response message, updated context, and target agent
            """
            ingestions = task_init_info.ingestions
            queries = task_init_info.queries

            if "TaskInitiated" in context_variables:
                # Handle follow-up tasks with deduplication
                added_ingestions_count = 0
                ignored_ingestions = []
                added_queries_count = 0
                ignored_queries = []

                if ingestions:
                    existing_ingestions: list[Ingest] = context_variables.get("DocumentsToIngest", [])  # type: ignore[assignment]
                    documents_ingested: list[str] = context_variables.get("DocumentsIngested", [])  # type: ignore[assignment]

                    unique_ingestions, ignored_ingestion_paths = _deduplicate_ingestions(
                        ingestions, existing_ingestions, documents_ingested
                    )

                    if unique_ingestions:
                        context_variables["DocumentsToIngest"] = existing_ingestions + unique_ingestions
                        added_ingestions_count = len(unique_ingestions)

                    ignored_ingestions = ignored_ingestion_paths

                if queries:
                    existing_queries: list[Query] = context_variables.get("QueriesToRun", [])  # type: ignore[assignment]

                    unique_queries, ignored_query_texts = _deduplicate_queries(queries, existing_queries)

                    if unique_queries:
                        context_variables["QueriesToRun"] = existing_queries + unique_queries
                        added_queries_count = len(unique_queries)

                    ignored_queries = ignored_query_texts

                if not ingestions and not queries:
                    return ReplyResult(message="No new tasks to initiate", context_variables=context_variables)

                response_message = _build_response_message(
                    added_ingestions_count, ignored_ingestions, added_queries_count, ignored_queries
                )

            else:
                # First time initialization - no deduplication needed
                context_variables["DocumentsToIngest"] = ingestions
                context_variables["QueriesToRun"] = list(queries)
                context_variables["TaskInitiated"] = True
                response_message = "Updated context variables with task decisions"

            return ReplyResult(
                message=response_message,
                context_variables=context_variables,
                target=AgentNameTarget(agent_name=TASK_MANAGER_NAME),
            )

        self._task_manager_agent = ConversableAgent(
            name=TASK_MANAGER_NAME,
            system_message=TASK_MANAGER_SYSTEM_MESSAGE,
            llm_config=llm_config,
            functions=[initiate_tasks],
        )

        self._triage_agent.handoffs.set_after_work(target=AgentTarget(agent=self._task_manager_agent))

        self._data_ingestion_agent = DoclingDocIngestAgent(
            llm_config=llm_config,
            query_engine=query_engine,
            parsed_docs_path=parsed_docs_path,
            return_agent_success=TASK_MANAGER_NAME,
            return_agent_error=ERROR_MANAGER_NAME,
        )

        def execute_rag_query(context_variables: ContextVariables) -> ReplyResult:  # type: ignore[type-arg]
            """Execute outstanding RAG queries, call the tool once for each outstanding query. Call this tool with no arguments.

            Args:
                context_variables: The current context variables containing queries to run

            Returns:
                ReplyResult: Contains query answer, updated context, and target agent
            """
            if len(context_variables["QueriesToRun"]) == 0:
                return ReplyResult(
                    target=AgentNameTarget(agent_name=TASK_MANAGER_NAME),
                    message="No queries to run",
                    context_variables=context_variables,
                )

            query = context_variables["QueriesToRun"][0].query
            try:
                if (
                    hasattr(query_engine, "enable_query_citations")
                    and query_engine.enable_query_citations
                    and hasattr(query_engine, "query_with_citations")
                    and callable(query_engine.query_with_citations)
                ):
                    answer_with_citations = query_engine.query_with_citations(query)  # type: ignore[union-attr]
                    answer = answer_with_citations.answer
                    txt_citations = [
                        {
                            "text_chunk": source.node.get_text(),
                            "file_path": source.metadata["file_path"],
                        }
                        for source in answer_with_citations.citations
                    ]
                    logger.info(f"Citations:\n {txt_citations}")
                else:
                    answer = query_engine.query(query)
                    txt_citations = []
                context_variables["QueriesToRun"].pop(0)
                context_variables["CompletedTaskCount"] += 1
                context_variables["QueryResults"].append({"query": query, "answer": answer, "citations": txt_citations})

                # Query completed

                return ReplyResult(message=answer, context_variables=context_variables)
            except Exception as e:
                return ReplyResult(
                    target=AgentNameTarget(agent_name=ERROR_MANAGER_NAME),
                    message=f"Query failed for '{query}': {e}",
                    context_variables=context_variables,
                )

        self._query_agent = ConversableAgent(
            name="QueryAgent",
            system_message="""
            You are a query agent.
            You answer the user's questions only using the query function provided to you.
            You can only call use the execute_rag_query tool once per turn.
            """,
            llm_config=llm_config,
            functions=[execute_rag_query],
        )

        # Summary agent prompt will include the results of the ingestions and queries
        def create_summary_agent_prompt(agent: ConversableAgent, messages: list[dict[str, Any]]) -> str:
            """Create the summary agent prompt and updates ingested documents.

            Args:
                agent: The conversable agent requesting the prompt
                messages: List of conversation messages

            Returns:
                str: The summary agent system message with context information
            """
            update_ingested_documents()

            documents_to_ingest: list[Ingest] = cast(list[Ingest], agent.context_variables.get("DocumentsToIngest", []))
            queries_to_run: list[Query] = cast(list[Query], agent.context_variables.get("QueriesToRun", []))

            system_message = (
                "You are a summary agent and you provide a summary of all completed tasks and the list of queries and their answers. "
                "Output two sections: 'Ingestions:' and 'Queries:' with the results of the tasks. Number the ingestions and queries. "
                "If there are no ingestions output 'No ingestions', if there are no queries output 'No queries' under their respective sections. "
                "Don't add markdown formatting. "
                "For each query, there is one answer and, optionally, a list of citations."
                "For each citation, it contains two fields: 'text_chunk' and 'file_path'."
                "Format the Query and Answers and Citations as 'Query:\nAnswer:\n\nCitations:'. Add a number to each query if more than one. Use the context below:\n"
                "For each query, output the full citation contents and list them one by one,"
                "format each citation as '\nSource [X] (chunk file_path here):\n\nChunk X:\n(text_chunk here)' and mark a separator between each citation using '\n#########################\n\n'."
                "If there are no citations at all, DON'T INCLUDE ANY mention of citations.\n"
                f"Documents ingested: {documents_to_ingest}\n"
                f"Documents left to ingest: {len(documents_to_ingest)}\n"
                f"Queries left to run: {len(queries_to_run)}\n"
                f"Query and Answers and Citations: {queries_to_run}\n"
            )

            return system_message

        self._summary_agent = ConversableAgent(
            name="SummaryAgent",
            llm_config=llm_config,
            update_agent_state_before_reply=[UpdateSystemMessage(create_summary_agent_prompt)],
        )

        self._task_manager_agent.register_handoffs([
            OnContextCondition(  # Go straight to data ingestion agent if we have documents to ingest
                target=AgentTarget(agent=self._data_ingestion_agent),
                condition=ExpressionContextCondition(
                    expression=ContextExpression(expression="len(${DocumentsToIngest}) > 0")
                ),
            ),
            OnContextCondition(  # Go to Query agent if we have queries to run (ingestion above run first)
                target=AgentTarget(agent=self._query_agent),
                condition=ExpressionContextCondition(
                    expression=ContextExpression(expression="len(${QueriesToRun}) > 0")
                ),
            ),
            # Removed automatic context condition - let task manager decide when to summarize
            OnCondition(
                target=AgentTarget(agent=self._summary_agent),
                condition=StringLLMCondition(
                    prompt="Call this function if all work is done and a summary will be created"
                ),
                available=SummaryTaskAvailableCondition(),  # Custom AvailableCondition class
            ),
        ])
        self._task_manager_agent.handoffs.set_after_work(target=StayTarget())

        self._data_ingestion_agent.handoffs.set_after_work(target=AgentTarget(agent=self._task_manager_agent))

        self._query_agent.handoffs.set_after_work(target=AgentTarget(agent=self._task_manager_agent))

        # Summary agent terminates the DocumentAgent
        self._summary_agent.handoffs.set_after_work(target=TerminateTarget())

        # The Error Agent always terminates the DocumentAgent
        self._error_agent.handoffs.set_after_work(target=TerminateTarget())

        self.register_reply([Agent, None], DocAgent.generate_inner_group_chat_reply)

        self.documents_ingested: list[str] = []
        self._group_chat_context_variables: ContextVariables | None = None

    def generate_inner_group_chat_reply(
        self,
        messages: list[dict[str, Any]] | str | None = None,
        sender: Agent | None = None,
        config: OpenAIWrapper | None = None,
    ) -> tuple[bool, str | dict[str, Any] | None]:
        """Reply function that generates the inner group chat reply for the DocAgent.

        Args:
            messages: Input messages to process
            sender: The agent that sent the message
            config: OpenAI wrapper configuration

        Returns:
            tuple: (should_terminate, reply_message)
        """
        # Use existing context_variables if available, otherwise create new ones
        if hasattr(self, "_group_chat_context_variables") and self._group_chat_context_variables is not None:
            context_variables = self._group_chat_context_variables
            # Reset for the new run
            context_variables["DocumentsToIngest"] = []  # type: ignore[index]
        else:
            context_variables = ContextVariables(
                data={
                    "CompletedTaskCount": 0,
                    "DocumentsToIngest": [],
                    "DocumentsIngested": self.documents_ingested,
                    "QueriesToRun": [],
                    "QueryResults": [],
                }
            )
            self._group_chat_context_variables = context_variables

        group_chat_agents = [
            self._triage_agent,
            self._task_manager_agent,
            self._data_ingestion_agent,
            self._query_agent,
            self._summary_agent,
            self._error_agent,
        ]

        agent_pattern = DefaultPattern(
            initial_agent=self._triage_agent,
            agents=group_chat_agents,
            context_variables=context_variables,
            group_after_work=TerminateTarget(),
        )

        chat_result, context_variables, last_speaker = initiate_group_chat(
            pattern=agent_pattern,
            messages=self._get_document_input_message(messages),
        )
        if last_speaker == self._error_agent:
            # If we finish with the error agent, we return their message which contains the error
            return True, chat_result.summary
        if last_speaker != self._summary_agent:
            # If the group chat finished but not with the summary agent, we assume something has gone wrong with the flow
            return True, DEFAULT_ERROR_GROUP_CHAT_MESSAGE

        return True, chat_result.summary

    def _get_document_input_message(self, messages: list[dict[str, Any]] | str | None) -> str:  # type: ignore[type-arg]
        """Gets and validates the input message(s) for the document agent.

        Args:
            messages: Input messages as string or list of message dictionaries

        Returns:
            str: The extracted message content

        Raises:
            NotImplementedError: If messages format is invalid
        """
        if isinstance(messages, str):
            return messages
        elif (
            isinstance(messages, list)
            and len(messages) > 0
            and "content" in messages[-1]
            and isinstance(messages[-1]["content"], str)
        ):
            return messages[-1]["content"]
        else:
            raise NotImplementedError("Invalid messages format. Must be a list of messages or a string.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
import os
from collections.abc import Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

from pydantic import BaseModel

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import

with optional_import_block():
    import chromadb
    from chromadb.api.models.Collection import Collection
    from chromadb.api.types import EmbeddingFunction
    from chromadb.utils.embedding_functions import DefaultEmbeddingFunction
    from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex
    from llama_index.core.llms import LLM
    from llama_index.core.query_engine import CitationQueryEngine
    from llama_index.core.schema import Document as LlamaDocument
    from llama_index.core.schema import NodeWithScore
    from llama_index.llms.openai import OpenAI
    from llama_index.vector_stores.chroma import ChromaVectorStore

__all__ = ["VectorChromaCitationQueryEngine", "VectorChromaQueryEngine"]

DEFAULT_COLLECTION_NAME = "docling-parsed-docs"
EMPTY_RESPONSE_TEXT = "Empty Response"  # Indicates that the query did not return any results
EMPTY_RESPONSE_REPLY = "Sorry, I couldn't find any information on that. If you haven't ingested any documents, please try that."  # Default response for queries without results

# Set up logging
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


@require_optional_import(["chromadb", "llama_index"], "rag")
@export_module("autogen.agents.experimental")
class VectorChromaQueryEngine:
    """This engine leverages Chromadb to persist document embeddings in a named collection
    and LlamaIndex's VectorStoreIndex to efficiently index and retrieve documents, and generate an answer in response
    to natural language queries. The Chromadb collection serves as the storage layer, while
    the collection name uniquely identifies the set of documents within the persistent database.

    This implements the autogen.agentchat.contrib.rag.RAGQueryEngine protocol.
    """

    def __init__(  # type: ignore[no-any-unimported]
        self,
        db_path: str | None = None,
        embedding_function: "EmbeddingFunction[Any] | None" = None,
        metadata: dict[str, Any] | None = None,
        llm: Optional["LLM"] = None,
        collection_name: str | None = None,
    ) -> None:
        """Initializes the VectorChromaQueryEngine with db_path, metadata, and embedding function and llm.

        Args:
            db_path: The file system path where Chromadb will store its persistent data.
                If not specified, the default directory "./chroma" is used.
            embedding_function: A callable that converts text into vector embeddings. Default embedding uses Sentence Transformers model all-MiniLM-L6-v2.
                For more embeddings that ChromaDB support, please refer to [embeddings](https://docs.trychroma.com/docs/embeddings/embedding-functions)
            metadata: A dictionary containing configuration parameters for the Chromadb collection.
                This metadata is typically used to configure the HNSW indexing algorithm.
                For more details about the default metadata, please refer to [HNSW configuration](https://cookbook.chromadb.dev/core/configuration/#hnsw-configuration)
            llm: LLM model used by LlamaIndex for query processing.
                 You can find more supported LLMs at [LLM](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/)
            collection_name (str): The unique name for the Chromadb collection. If omitted, a constant name will be used. Populate this to reuse previous ingested data.
        """
        self.llm: LLM = llm or OpenAI(model="gpt-4o", temperature=0.0)  # type: ignore[no-any-unimported]
        self.embedding_function: EmbeddingFunction[Any] = embedding_function or DefaultEmbeddingFunction()  # type: ignore[no-any-unimported,assignment]
        self.metadata: dict[str, Any] = metadata or {
            "hnsw:space": "ip",
            "hnsw:construction_ef": 30,
            "hnsw:M": 32,
        }
        self.client = chromadb.PersistentClient(path=db_path or "./chroma")
        self.collection_name: str | None = collection_name

        self.connect_db()

    def connect_db(self, *args: Any, **kwargs: Any) -> bool:
        """Establish a connection to the Chromadb database and initialize the collection."""
        self.collection_name = self.collection_name or DEFAULT_COLLECTION_NAME

        if self._collection_exists(self.collection_name):
            logger.info(f"Using existing collection {self.collection_name} from the database.")
        else:
            logger.info(f"Creating new collection {self.collection_name} in the database.")

        self.collection = self.client.create_collection(
            name=self.collection_name,
            embedding_function=self.embedding_function,
            metadata=self.metadata,
            get_or_create=True,  # If collection already exists, get the collection
        )
        self.index = self._create_index(self.collection)

        return True

    def query(self, question: str) -> str:
        """Retrieve information from indexed documents by processing a natural language query.

        Args:
            question: A natural language query string used to search the indexed documents.

        Returns:
            A string containing the response generated by LLM.
        """
        self.validate_query_index()
        self.query_engine = self.index.as_query_engine(llm=self.llm)
        response = self.query_engine.query(question)

        if str(response) == EMPTY_RESPONSE_TEXT:
            return EMPTY_RESPONSE_REPLY

        return str(response)

    def add_docs(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
    ) -> None:
        """Add additional documents to the existing vector index.

        Loads new Docling-parsed Markdown files from a specified directory or a list of file paths
        and inserts them into the current index for future queries.

        Args:
            new_doc_dir: The directory path from which to load additional documents.
                If provided, all eligible files in this directory are loaded.
            new_doc_paths_or_urls: A list of file paths specifying additional documents to load.
                Each file should be a Docling-parsed Markdown file.
        """
        self.validate_query_index()
        new_doc_dir = new_doc_dir or ""
        new_doc_paths = new_doc_paths_or_urls or []
        new_docs = self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths)
        for doc in new_docs:
            self.index.insert(doc)

    def _load_doc(  # type: ignore[no-any-unimported]
        self, input_dir: Path | str | None, input_docs: Sequence[Path | str] | None
    ) -> list["LlamaDocument"]:
        """Load documents from a directory and/or a list of file paths.

        This helper method reads Docling-parsed Markdown files using LlamaIndex's
        SimpleDirectoryReader. It supports multiple file [formats]((https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/#supported-file-types)),
          but the intended use is for documents processed by Docling.

        Args:
            input_dir (Optional[Union[Path, str]]): The directory containing documents to be loaded.
                If provided, all files in the directory will be considered.
            input_docs (Optional[list[Union[Path, str]]]): A list of individual file paths to load.
                Each path must point to an existing file.

        Returns:
            A list of documents loaded as LlamaDocument objects.

        Raises:
            ValueError: If the specified directory does not exist.
            ValueError: If any provided file path does not exist.
            ValueError: If neither input_dir nor input_docs is provided.
        """
        loaded_documents = []
        if input_dir:
            logger.info(f"Loading docs from directory: {input_dir}")
            if not os.path.exists(input_dir):
                raise ValueError(f"Input directory not found: {input_dir}")
            loaded_documents.extend(SimpleDirectoryReader(input_dir=input_dir).load_data())

        if input_docs:
            for doc in input_docs:
                logger.info(f"Loading input doc: {doc}")
                if not os.path.exists(doc):
                    raise ValueError(f"Document file not found: {doc}")
            loaded_documents.extend(SimpleDirectoryReader(input_files=list(input_docs)).load_data())

        if not input_dir and not input_docs:
            raise ValueError("No input directory or docs provided!")

        return loaded_documents

    def _create_index(  # type: ignore[no-any-unimported]
        self, collection: "Collection"
    ) -> "VectorStoreIndex":
        """Build a vector index for document retrieval using a Chromadb collection.

        Wraps the provided Chromadb collection into a vector store and uses LlamaIndex's
        StorageContext to create a VectorStoreIndex from the collection.

        Args:
            collection (Collection): A Chromadb Collection object that stores the embeddings of the documents.

        Returns:
            A VectorStoreIndex object built from the provided collection.
        """
        self.vector_store = ChromaVectorStore(chroma_collection=collection)
        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)

        index = VectorStoreIndex.from_vector_store(vector_store=self.vector_store, storage_context=self.storage_context)

        return index

    def _collection_exists(self, collection_name: str) -> bool:
        """Check if a collection with the given name exists in the database.

        Args:
            collection_name (str): The name of the collection to check.

        Returns:
            True if a collection with the given name exists in the database, False otherwise.
        """
        existing_collections = self.client.list_collections()
        return any(col == collection_name for col in existing_collections)

    def get_collection_name(self) -> str:
        """Get the name of the collection used by the query engine.

        Returns:
            The name of the collection.
        """
        if self.collection_name:
            return self.collection_name
        else:
            raise ValueError("Collection name not set.")

    def validate_query_index(self) -> None:
        """Ensures an index exists"""
        if not hasattr(self, "index"):
            raise Exception("Query index is not initialized. Please ingest some documents before querying.")

    def init_db(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> bool:
        """Not required nor implemented for VectorChromaQueryEngine"""
        raise NotImplementedError("Method, init_db, not required nor implemented for VectorChromaQueryEngine")


class AnswerWithCitations(BaseModel):  # type: ignore[no-any-unimported]
    answer: str
    citations: list["NodeWithScore"]  # type: ignore[no-any-unimported]


@require_optional_import(["chromadb", "llama_index"], "rag")
@export_module("autogen.agents.experimental")
class VectorChromaCitationQueryEngine(VectorChromaQueryEngine):
    """This engine leverages VectorChromaQueryEngine and CitationQueryEngine to answer queries with citations."""

    def __init__(  # type: ignore[no-any-unimported]
        self,
        db_path: str | None = None,
        embedding_function: "EmbeddingFunction[Any] | None" = None,
        metadata: dict[str, Any] | None = None,
        llm: Optional["LLM"] = None,
        collection_name: str | None = None,
        enable_query_citations: bool = False,
        citation_chunk_size: int = 512,
    ) -> None:
        """See parent class VectorChromaQueryEngine."""
        super().__init__(db_path, embedding_function, metadata, llm, collection_name)
        self.enable_query_citations = enable_query_citations
        self.citation_chunk_size = citation_chunk_size

    def query_with_citations(self, query: str) -> AnswerWithCitations:
        """Query the index with the given query and return the answer along with citations.

        Args:
            query (str): The query to be answered.
            citation_chunk_size (int): The size of chunks to use for each citation. Default is 512.

        Returns:
            AnswerWithCitations: An object containing the answer and citations.
        """
        query_engine = CitationQueryEngine.from_args(
            index=self.index,
            citation_chunk_size=self.citation_chunk_size,
        )

        response = query_engine.query(query)

        if hasattr(response, "response"):
            return AnswerWithCitations(answer=response.response, citations=response.source_nodes)
        else:
            raise ValueError(f"Query response of type '{type(response)}' does not contain a response attribute.")


# mypy will fail if ChromaDBQueryEngine does not implement RAGQueryEngine protocol
if TYPE_CHECKING:
    from ....agentchat.contrib.rag.query_engine import RAGQueryEngine

    def _check_implement_protocol(o: VectorChromaQueryEngine) -> RAGQueryEngine:
        return o
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Any, cast

from ....agentchat.group.available_condition import AvailableCondition
from .document_utils import Ingest, Query

if TYPE_CHECKING:
    # Avoid circular import
    from ....agentchat.conversable_agent import ConversableAgent

__all__ = ["SummaryTaskAvailableCondition"]


class SummaryTaskAvailableCondition(AvailableCondition):
    """Available condition for determining if a summary task should be performed.

    This condition checks if:
    1. There are no documents left to ingest
    2. There are no queries left to run
    3. The completed task count is truthy

    If all conditions are met, the agent is ready for a summary task.
    """

    documents_var: str = "DocumentsToIngest"  # Context variable name for documents to ingest list
    queries_var: str = "QueriesToRun"  # Context variable name for queries to run list
    completed_var: str = "CompletedTaskCount"  # Context variable name for completed task count

    def is_available(self, agent: "ConversableAgent", messages: list[dict[str, Any]]) -> bool:
        """Check if all task conditions are met.

        Args:
            agent: The agent with context variables
            messages: The conversation history (not used)

        Returns:
            True if all conditions are met (ready for summary), False otherwise
        """
        # Get variables from context with appropriate casting
        documents_to_ingest: list[Ingest] = cast(list[Ingest], agent.context_variables.get(self.documents_var, []))

        queries_to_run: list[Query] = cast(list[Query], agent.context_variables.get(self.queries_var, []))

        completed_task_count = bool(agent.context_variables.get(self.completed_var, 0))

        # All conditions must be true for the function to return True
        return len(documents_to_ingest) == 0 and len(queries_to_run) == 0 and completed_task_count
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import json
import logging
import os
import time
from pathlib import Path
from typing import Annotated

from ....doc_utils import export_module
from ....import_utils import optional_import_block, require_optional_import
from .document_utils import handle_input

with optional_import_block():
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import (  # type: ignore[attr-defined]
        AcceleratorDevice,
        AcceleratorOptions,
        PdfPipelineOptions,
    )
    from docling.document_converter import DocumentConverter, PdfFormatOption

__all__ = ["docling_parse_docs"]

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


@require_optional_import(["docling"], "rag")
@export_module("autogen.agents.experimental.document_agent")
def docling_parse_docs(  # type: ignore[no-any-unimported]
    input_file_path: Annotated[Path | str, "Path to the input file or directory"],
    output_dir_path: Annotated[Path | str | None, "Path to the output directory"] = None,
    output_formats: Annotated[list[str] | None, "List of output formats (markdown, json)"] = None,
    table_output_format: str = "html",
) -> list[Path]:
    """Convert documents into a Deep Search document format using EasyOCR
    with CPU only, and export the document and its tables to the specified
    output directory.

    Supported formats:
        PDF,
        IMAGE,
        DOCX,
        HTML,
        PPTX,
        ASCIIDOC,
        MD,

    Args:
        input_file_path (Union[Path, str]): The path to the input file.
        output_dir_path (Union[Path, str]): The path to the output directory.
        output_formats (list[str], optional): The output formats. Defaults to ["markdown"].
        table_output_format (str, optional): The output format for tables. Defaults to "html".

    Returns:
        list[ConversionResult]: The result of the conversion.
    """
    output_dir_path = output_dir_path or (Path.cwd() / "output")
    output_dir_path = Path(output_dir_path).resolve()
    output_dir_path.mkdir(parents=True, exist_ok=True)
    # ToDo: For some reason, output_dir_path.mkdir is not creating the directory.
    # This is a workaround to create the directory if it does not exist.
    # Following test is failing without this workaround:
    # test/agents/experimental/document_agent/test_parser_utils.py::TestDoclingParseDocs::test_default_output_dir_path
    if not os.path.exists(output_dir_path):
        os.makedirs(output_dir_path)
    output_formats = output_formats or ["markdown"]

    input_doc_paths: list[Path] = handle_input(input_file_path, output_dir=output_dir_path)

    if not input_doc_paths:
        raise ValueError("No documents found.")

    # Docling Parse PDF with EasyOCR (CPU only)
    # ----------------------
    pdf_pipeline_options = PdfPipelineOptions()
    pdf_pipeline_options.do_ocr = True
    if hasattr(pdf_pipeline_options.ocr_options, "use_gpu"):
        pdf_pipeline_options.ocr_options.use_gpu = False  # <-- set this.
    pdf_pipeline_options.do_table_structure = True
    pdf_pipeline_options.table_structure_options.do_cell_matching = True
    pdf_pipeline_options.ocr_options.lang = ["en"]
    pdf_pipeline_options.accelerator_options = AcceleratorOptions(num_threads=4, device=AcceleratorDevice.AUTO)

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),
        },
    )

    start_time = time.time()
    conv_results = list(doc_converter.convert_all(input_doc_paths))
    end_time = time.time() - start_time

    logger.info(f"Document converted in {end_time:.2f} seconds.")

    # Export results
    output_dir = Path(output_dir_path).resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    conv_files = []

    for res in conv_results:
        out_path = Path(output_dir_path).resolve()
        doc_filename = res.input.file.stem
        logger.debug(f"Document {res.input.file.name} converted.\nSaved markdown output to: {out_path!s}")
        logger.debug(res.document._export_to_indented_text(max_text_len=16))

        if "markdown" in output_formats:
            # Export Docling document format to markdown:
            output_file = out_path / f"{doc_filename}.md"
            with output_file.open("w") as fp:
                fp.write(res.document.export_to_markdown())
                conv_files.append(output_file)

        if "json" in output_formats:
            # Export Docling document format to json
            output_file = out_path / f"{doc_filename}.json"
            with output_file.open("w") as fp:
                fp.write(json.dumps(res.document.export_to_dict()))
                conv_files.append(output_file)

        # Export tables (used for evaluating conversion)
        if table_output_format == "html":
            for table_ix, table in enumerate(res.document.tables):
                # Save the table as html
                element_html_filename = output_dir / f"{doc_filename}-table-{table_ix + 1}.html"
                logger.debug(f"Saving HTML table to {element_html_filename}")
                with element_html_filename.open("w") as fp:
                    fp.write(table.export_to_html())
    return conv_files
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .chroma_query_engine import VectorChromaQueryEngine
from .docling_doc_ingest_agent import DoclingDocIngestAgent
from .document_agent import DocAgent
from .document_utils import handle_input
from .inmemory_query_engine import InMemoryQueryEngine
from .parser_utils import docling_parse_docs

__all__ = [
    "DocAgent",
    "DoclingDocIngestAgent",
    "InMemoryQueryEngine",
    "VectorChromaQueryEngine",
    "docling_parse_docs",
    "handle_input",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import copy
import json
import os
from collections.abc import Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Any

from pydantic import BaseModel

from .... import ConversableAgent
from ....agentchat.contrib.rag import RAGQueryEngine
from ....doc_utils import export_module
from ....llm_config import LLMConfig

__all__ = ["InMemoryQueryEngine"]

# REPLIES
QUERY_NO_INGESTIONS_REPLY = "Sorry, please ingest some documents/URLs before querying."  # Default response for queries without ingested documents
EMPTY_RESPONSE_REPLY = "Sorry, I couldn't find any information on that. If you haven't ingested any documents, please try that."  # Default response for queries without results
ERROR_RESPONSE_REPLY = "Sorry, there was an error processing your query: "  # Default response for queries with errors
COULD_NOT_ANSWER_REPLY = "Sorry, I couldn't answer that question from the ingested documents/URLs"  # Default response for queries that could not be answered


# Documents and Content structure
class DocumentStore(BaseModel):
    ingestation_name: str
    content: str


# Answer question structure
class QueryAnswer(BaseModel):
    could_answer: bool
    answer: str


@export_module("autogen.agents.experimental")
class InMemoryQueryEngine:
    """This engine stores ingested documents in memory and then injects them into an internal agent's system message for answering queries.

    This implements the autogen.agentchat.contrib.rag.RAGQueryEngine protocol.
    """

    def __init__(
        self,
        llm_config: LLMConfig | dict[str, Any],
    ) -> None:
        # Deep copy the llm config to avoid changing the original
        structured_config = copy.deepcopy(llm_config)

        # The query agent will answer with a structured output
        structured_config["response_format"] = QueryAnswer

        # Our agents for querying
        self._query_agent = ConversableAgent(
            name="inmemory_query_agent",
            llm_config=structured_config,
        )

        # In-memory storage for ingested documents
        self._ingested_documents: list[DocumentStore] = []

    def query(self, question: str, *args: Any, **kwargs: Any) -> str:
        """Run a query against the ingested documents and return the answer."""
        # If no documents have been ingested, return an empty response
        if not self._ingested_documents:
            return QUERY_NO_INGESTIONS_REPLY

        # Put the context into the system message
        context_parts = []
        for i, doc in enumerate(self._ingested_documents, 1):
            context_parts.append(f"Ingested File/URL {i} - '{doc.ingestation_name}':\n{doc.content}\n")

        context = "\n".join(context_parts)

        system_message = (
            "You are a query agent tasked with answering questions based on ingested documents.\n\n"
            "AVAILABLE DOCUMENTS:\n"
            + "\n".join([f"- {doc.ingestation_name}" for doc in self._ingested_documents])
            + "\n\n"
            "When answering questions about these documents, use ONLY the information in the following context:\n\n"
            f"{context}\n\n"
            "IMPORTANT: The user will ask about these documents by name. When they do, provide helpful, detailed answers based on the document content above."
        )

        self._query_agent.update_system_message(system_message)

        message = f"Using ONLY the document content in your system message, answer this question: {question}"

        response = self._query_agent.run(
            message=message,
            max_turns=1,
        )

        response.process()

        try:
            # Get the structured output and return the answer
            answer_object = QueryAnswer.model_validate(json.loads(response.summary))  # type: ignore[arg-type]

            if answer_object.could_answer:
                return answer_object.answer
            else:
                if answer_object.answer:
                    return COULD_NOT_ANSWER_REPLY + ": " + answer_object.answer
                else:
                    return COULD_NOT_ANSWER_REPLY

        except Exception as e:
            # Error converting the response to the structured output
            return ERROR_RESPONSE_REPLY + str(e)

    def add_docs(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
    ) -> None:
        """Add additional documents to the in-memory store

        Loads new Docling-parsed Markdown files from a specified directory or a list of file paths
        and inserts them into the in-memory store.

        Args:
            new_doc_dir: The directory path from which to load additional documents.
                If provided, all eligible files in this directory are loaded.
            new_doc_paths_or_urls: A list of file paths specifying additional documents to load.
                Each file should be a Docling-parsed Markdown file.
        """
        new_doc_dir = new_doc_dir or ""
        new_doc_paths = new_doc_paths_or_urls or []
        self._load_doc(input_dir=new_doc_dir, input_docs=new_doc_paths)

    def _load_doc(self, input_dir: Path | str | None, input_docs: Sequence[Path | str] | None) -> None:
        """Load documents from a directory and/or a list of file paths into the in-memory store.

        This helper method reads files using native Python file operations and stores them
        in the in-memory document store. It supports reading text-based files, with the primary
        intended use being for documents processed by Docling.

        Args:
            input_dir (Optional[Union[Path, str]]): The directory containing documents to be loaded.
                If provided, all files in the directory will be considered.
            input_docs (Optional[list[Union[Path, str]]]): A list of individual file paths to load.
                Each path must point to an existing file.

        Raises:
            ValueError: If the specified directory does not exist.
            ValueError: If any provided file path does not exist.
            ValueError: If neither input_dir nor input_docs is provided.
        """
        if not input_dir and not input_docs:
            raise ValueError("No input directory or docs provided!")

        # Process directory if provided
        if input_dir:
            # logger.info(f"Loading docs from directory: {input_dir}")
            if not os.path.exists(input_dir):
                raise ValueError(f"Input directory not found: {input_dir}")

            # Get all files from the directory
            dir_path = Path(input_dir)
            for file_path in dir_path.iterdir():
                if file_path.is_file():
                    self._read_and_store_file(file_path)

        # Process individual files if provided
        if input_docs:
            for doc_path in input_docs:
                # logger.info(f"Loading input doc: {doc_path}")
                if not os.path.exists(doc_path):
                    raise ValueError(f"Document file not found: {doc_path}")
                self._read_and_store_file(doc_path)

    def _read_and_store_file(self, file_path: Path | str) -> None:
        """Read a file and store its content in the in-memory document store.

        Args:
            file_path (Union[Path, str]): Path to the file to be read
        """
        file_path = Path(file_path)
        try:
            with open(file_path, encoding="utf-8") as file:
                content = file.read()

            # Store the document in the in-memory store
            document = DocumentStore(ingestation_name=file_path.name, content=content)
            self._ingested_documents.append(document)
        except Exception as e:
            raise ValueError(f"Error reading file {file_path}: {str(e)}")

    def init_db(
        self,
        new_doc_dir: Path | str | None = None,
        new_doc_paths_or_urls: Sequence[Path | str] | None = None,
        *args: Any,
        **kwargs: Any,
    ) -> bool:
        """Not required nor implemented for InMemoryQueryEngine"""
        raise NotImplementedError("Method, init_db, not required nor implemented for InMemoryQueryEngine")

    def connect_db(self, *args: Any, **kwargs: Any) -> bool:
        """Not required nor implemented for InMemoryQueryEngine"""
        raise NotImplementedError("Method, connect_db, not required nor implemented for InMemoryQueryEngine")


# mypy will fail if ChromaDBQueryEngine does not implement RAGQueryEngine protocol
if TYPE_CHECKING:
    from ....agentchat.contrib.rag.query_engine import RAGQueryEngine

    def _check_implement_protocol(o: InMemoryQueryEngine) -> RAGQueryEngine:
        return o
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
from pathlib import Path
from typing import Literal

from .... import ConversableAgent
from ....agentchat.contrib.rag.query_engine import RAGQueryEngine
from ....agentchat.group.context_variables import ContextVariables
from ....agentchat.group.reply_result import ReplyResult
from ....agentchat.group.targets.transition_target import AgentNameTarget
from ....doc_utils import export_module
from ....llm_config import LLMConfig
from ..document_agent.parser_utils import docling_parse_docs
from .chroma_query_engine import VectorChromaQueryEngine
from .document_utils import preprocess_path

__all__ = ["DoclingDocIngestAgent"]

logger = logging.getLogger(__name__)

DOCLING_PARSE_TOOL_NAME = "docling_parse_docs"

DEFAULT_DOCLING_PARSER_PROMPT = f"""
You are an expert in parsing and understanding text. You can use {DOCLING_PARSE_TOOL_NAME} tool to parse various documents and extract information from them. You can only use the tool once per turn.
"""


@export_module("autogen.agents.experimental")
class DoclingDocIngestAgent(ConversableAgent):
    """A DoclingDocIngestAgent is a swarm agent that ingests documents using the docling_parse_docs tool."""

    def __init__(
        self,
        name: str | None = None,
        llm_config: LLMConfig | dict | Literal[False] | None = None,  # type: ignore[type-arg]
        parsed_docs_path: Path | str | None = None,
        query_engine: RAGQueryEngine | None = None,
        return_agent_success: str = "TaskManagerAgent",
        return_agent_error: str = "ErrorManagerAgent",
        collection_name: str | None = None,
    ):
        """Initialize the DoclingDocIngestAgent.

        Args:
            name: The name of the DoclingDocIngestAgent.
            llm_config: The configuration for the LLM.
            parsed_docs_path: The path where parsed documents will be stored.
            query_engine: The VectorChromaQueryEngine to use for querying documents.
            return_agent_success: The agent to return on successful completion of the task.
            return_agent_error: The agent to return on error.
            collection_name: The unique name for the Chromadb collection. Set this to a value to reuse a collection. If a query_engine is provided, this will be ignored.
        """
        name = name or "DoclingDocIngestAgent"

        parsed_docs_path = parsed_docs_path or Path("./parsed_docs")
        parsed_docs_path = preprocess_path(str_or_path=parsed_docs_path, mk_path=True)

        self._query_engine = query_engine or VectorChromaQueryEngine(collection_name=collection_name)

        def data_ingest_task(context_variables: ContextVariables) -> ReplyResult:
            """A tool for Swarm agent to ingests documents using the docling_parse_docs to parse documents to markdown
            and add them to the docling_query_engine.

            Args:
            context_variables (ContextVariables): The context variables for the task.

            Returns:
            ReplyResult: The result of the task.
            """
            try:
                input_file_path = ""
                tasks = context_variables.get("DocumentsToIngest", [])
                while tasks:
                    task = tasks.pop()
                    input_file_path = task.path_or_url
                    output_files = docling_parse_docs(
                        input_file_path=input_file_path, output_dir_path=parsed_docs_path, output_formats=["markdown"]
                    )

                    # Limit to one output markdown file for now.
                    if output_files:
                        output_file = output_files[0]
                        if output_file.suffix == ".md":
                            self._query_engine.add_docs(new_doc_paths_or_urls=[output_file])

                    # Keep track of documents ingested
                    context_variables["DocumentsIngested"].append(input_file_path)

                context_variables["CompletedTaskCount"] += 1
                logger.info(f"data_ingest_task context_variables: {context_variables.to_dict()}")

            except Exception as e:
                return ReplyResult(
                    target=AgentNameTarget(agent_name=return_agent_error),
                    message=f"Data Ingestion Task Failed, Error {e}: '{input_file_path}'",
                    context_variables=context_variables,
                )

            return ReplyResult(
                target=AgentNameTarget(agent_name=return_agent_success),
                message=f"Data Ingestion Task Completed for {input_file_path}",
                context_variables=context_variables,
            )

        super().__init__(
            name=name,
            llm_config=llm_config,
            functions=[data_ingest_task],
            system_message=DEFAULT_DOCLING_PARSER_PROMPT,
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from enum import Enum
from typing import Any
from urllib.parse import urlparse

from ....import_utils import optional_import_block, require_optional_import

with optional_import_block():
    import requests


class InputFormat(Enum):
    """Enum representing supported input file formats."""

    DOCX = "docx"
    PPTX = "pptx"
    HTML = "html"
    XML = "xml"
    IMAGE = "image"
    PDF = "pdf"
    ASCIIDOC = "asciidoc"
    MD = "md"
    CSV = "csv"
    XLSX = "xlsx"
    JSON = "json"
    INVALID = "invalid"  # Server errors or not a URL


# Map common file extensions to InputFormat
# INVALID means it's not supported
# See: https://github.com/DS4SD/docling/blob/e25d557c06afd77f1bb2c1ac4d2ece4dffcd52bd/docling/datamodel/base_models.py#L56
ExtensionToFormat = {
    # DOCX formats
    "docx": InputFormat.DOCX,
    "dotx": InputFormat.DOCX,
    "docm": InputFormat.DOCX,
    "dotm": InputFormat.DOCX,
    # PPTX formats
    "pptx": InputFormat.PPTX,
    "potx": InputFormat.PPTX,
    "ppsx": InputFormat.PPTX,
    "pptm": InputFormat.PPTX,
    "potm": InputFormat.PPTX,
    "ppsm": InputFormat.PPTX,
    # Excel formats
    "xlsx": InputFormat.XLSX,
    # HTML formats
    "html": InputFormat.HTML,
    "htm": InputFormat.HTML,
    "xhtml": InputFormat.HTML,
    # XML formats
    "xml": InputFormat.XML,
    "nxml": InputFormat.XML,
    "txt": InputFormat.XML,  # Note: .txt could be many formats, XML is just one possibility
    # Image formats
    "png": InputFormat.IMAGE,
    "jpg": InputFormat.IMAGE,
    "jpeg": InputFormat.IMAGE,
    "tiff": InputFormat.IMAGE,
    "tif": InputFormat.IMAGE,
    "bmp": InputFormat.IMAGE,
    # PDF format
    "pdf": InputFormat.PDF,
    # AsciiDoc formats
    "adoc": InputFormat.ASCIIDOC,
    "asciidoc": InputFormat.ASCIIDOC,
    "asc": InputFormat.ASCIIDOC,
    # Markdown formats
    "md": InputFormat.MD,
    "markdown": InputFormat.MD,
    # CSV format
    "csv": InputFormat.CSV,
    # JSON format
    "json": InputFormat.JSON,
    # Unsupported formats
    "doc": InputFormat.INVALID,
    "ppt": InputFormat.INVALID,
    "xls": InputFormat.INVALID,
    "gif": InputFormat.INVALID,
}


class URLAnalyzer:
    """A class that analyzes URLs to determine if they point to web pages or files."""

    # Mapping of input formats to their corresponding MIME types
    FormatToMimeType: dict[InputFormat, list[str]] = {
        InputFormat.DOCX: [
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.template",
        ],
        InputFormat.PPTX: [
            "application/vnd.openxmlformats-officedocument.presentationml.template",
            "application/vnd.openxmlformats-officedocument.presentationml.slideshow",
            "application/vnd.openxmlformats-officedocument.presentationml.presentation",
        ],
        InputFormat.HTML: ["text/html", "application/xhtml+xml"],
        InputFormat.XML: ["application/xml", "text/xml", "text/plain"],
        InputFormat.IMAGE: [
            "image/png",
            "image/jpeg",
            "image/tiff",
            "image/gif",
            "image/bmp",
        ],
        InputFormat.PDF: ["application/pdf"],
        InputFormat.ASCIIDOC: ["text/asciidoc"],
        InputFormat.MD: ["text/markdown", "text/x-markdown"],
        InputFormat.CSV: ["text/csv"],
        InputFormat.XLSX: ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"],
        InputFormat.JSON: ["application/json"],
    }

    # Create a reverse mapping from MIME types to formats
    # Note: For ambiguous MIME types (like "application/xml"), we'll favor the first format found
    MimeTypeToFormat = {}
    for format_type, mime_types in FormatToMimeType.items():
        for mime_type in mime_types:
            if mime_type not in MimeTypeToFormat:
                MimeTypeToFormat[mime_type] = format_type

    def __init__(self, url: str):
        """Initialize the URLAnalyzer with a URL.

        Args:
            url (str): The URL to analyze
        """
        self.url = url
        self.analysis_result: dict[str, Any] | None = None
        self.final_url: str | None = None
        self.redirect_chain: list[str] = []

    def analyze(
        self, test_url: bool = False, follow_redirects: bool = True, prioritize_extension: bool = True
    ) -> dict[str, Any]:
        """Analyze the URL to determine if it points to a web page or a file.

        Args:
            test_url (bool): Whether to test the URL by making a request
            follow_redirects (bool): Whether to follow redirects when testing the URL
            prioritize_extension (bool): Whether to prioritize file extension over MIME type

        Returns:
            dict: A dictionary containing the analysis results
        """
        result = {
            "url": self.url,
            "is_file": False,
            "file_type": None,
            "mime_type": None,
            "method": "extension_analysis",
            "redirects": False,
            "redirect_count": 0,
            "final_url": self.url,
        }

        # First try to analyze based on the URL extension
        extension_analysis = self._analyze_by_extension(self.url)
        if extension_analysis["is_file"]:
            result.update(extension_analysis)

        # If test_url is True, make a request
        if test_url:
            request_analysis = self._analyze_by_request(follow_redirects)
            if request_analysis:
                # Update the redirect information
                if self.final_url and self.final_url != self.url:
                    result["redirects"] = True
                    result["redirect_count"] = len(self.redirect_chain)
                    result["redirect_chain"] = self.redirect_chain
                    result["final_url"] = self.final_url

                    # Re-analyze based on the final URL extension
                    if self.final_url != self.url:
                        final_extension_analysis = self._analyze_by_extension(self.final_url)
                        if final_extension_analysis["is_file"]:
                            # If prioritizing extension and we have a file extension match
                            if prioritize_extension:
                                # Keep the MIME type from the request but use file type from extension
                                mime_type = request_analysis.get("mime_type")
                                request_analysis.update(final_extension_analysis)
                                if mime_type:
                                    request_analysis["mime_type"] = mime_type
                            else:
                                # Only use extension analysis if request didn't identify a file type
                                if not request_analysis.get("file_type"):
                                    request_analysis.update(final_extension_analysis)

                # If prioritize_extension is True and we have both extension and MIME type analyses
                if (
                    prioritize_extension
                    and result.get("extension")
                    and result.get("file_type")
                    and request_analysis.get("mime_type")
                ):
                    # Keep the extension-based file type but add the MIME type from the request
                    request_analysis["file_type"] = result["file_type"]
                    request_analysis["is_file"] = True

                result.update(request_analysis)
                result["method"] = "request_analysis"

        # Store the result for later access
        self.analysis_result = result

        return result

    def _analyze_by_extension(self, url: str) -> dict[str, Any]:
        """Analyze URL based on its file extension.

        Args:
            url (str): The URL to analyze

        Returns:
            dict: Analysis results based on the file extension
        """
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()

        # Check if the URL has a file extension
        if "." in path:
            extension = path.split(".")[-1]

            # Check if it's a known file extension
            if extension in ExtensionToFormat:
                format_type = ExtensionToFormat[extension]
                return {
                    "is_file": True,
                    "file_type": format_type,
                    "extension": extension,
                }

        # If no file extension was found or it's not recognized,
        # assume it's a web page (but this could be confirmed with a request)
        return {
            "is_file": False,
            "file_type": None,
            "extension": None,
        }

    @require_optional_import(["requests"], "rag")
    def _analyze_by_request(self, follow_redirects: bool = True) -> dict[str, Any] | None:
        """Analyze URL by making a HEAD request to check Content-Type.

        Args:
            follow_redirects (bool): Whether to follow redirects

        Returns:
            Optional[dict]: Analysis results based on the HTTP response or None if the request failed
        """
        try:
            # Store redirect history
            self.redirect_chain = []

            # First try a HEAD request (faster but some servers don't handle it well)
            response = requests.head(self.url, allow_redirects=follow_redirects, timeout=5)

            # If the server returns a 405 (Method Not Allowed) for HEAD, try GET
            if response.status_code == 405:
                response = requests.get(self.url, allow_redirects=follow_redirects, timeout=5, stream=True)
                # Close the connection without downloading the content
                response.close()

            # Check for non-success status codes
            if response.status_code >= 400:
                return {
                    "is_file": False,
                    "file_type": InputFormat.INVALID,
                    "mime_type": None,
                    "error": f"HTTP error: {response.status_code}",
                    "status_code": response.status_code,
                }

            # Store information about redirects
            if hasattr(response, "history") and response.history:
                self.redirect_chain = [r.url for r in response.history]
                self.final_url = response.url
            else:
                self.final_url = self.url

            # Get the Content-Type header
            content_type = response.headers.get("Content-Type", "").split(";")[0].strip()

            # Check if it matches any of our known MIME types
            format_type = self.MimeTypeToFormat.get(content_type)

            # Handle different content types
            if format_type:
                return {
                    "is_file": True,
                    "file_type": format_type,
                    "mime_type": content_type,
                }
            elif content_type in ["text/html", "application/xhtml+xml"]:
                return {
                    "is_file": False,
                    "file_type": None,
                    "mime_type": content_type,
                }
            else:
                # Content type was found but not in our mapping
                return {
                    "is_file": True,
                    "file_type": "unknown",
                    "mime_type": content_type,
                }

        except requests.exceptions.TooManyRedirects:
            # Handle redirect loops or too many redirects
            return {
                "is_file": False,
                "file_type": InputFormat.INVALID,
                "mime_type": None,
                "error": "Too many redirects",
                "redirects": True,
            }
        except requests.exceptions.ConnectionError:
            # Handle connection errors (e.g., DNS failure, refused connection)
            return {
                "is_file": False,
                "file_type": InputFormat.INVALID,
                "mime_type": None,
                "error": "Connection error - URL may be invalid or server unavailable",
            }
        except requests.exceptions.Timeout:
            # Handle timeout
            return {"is_file": False, "file_type": InputFormat.INVALID, "mime_type": None, "error": "Request timed out"}
        except requests.exceptions.InvalidURL:
            # Handle invalid URL
            return {
                "is_file": False,
                "file_type": InputFormat.INVALID,
                "mime_type": None,
                "error": "Invalid URL format",
            }
        except Exception as e:
            # If the request fails for any other reason
            return {"is_file": False, "file_type": InputFormat.INVALID, "mime_type": None, "error": str(e)}

    def get_result(self) -> dict[str, Any] | None:
        """Get the last analysis result, or None if the URL hasn't been analyzed yet.

        Returns:
            Optional[dict]: The analysis result or None
        """
        return self.analysis_result

    def get_redirect_info(self) -> dict[str, Any]:
        """Get information about redirects that occurred during the last request.

        Returns:
            dict: Information about redirects
        """
        if not self.final_url:
            return {
                "redirects": False,
                "redirect_count": 0,
                "original_url": self.url,
                "final_url": self.url,
                "redirect_chain": [],
            }

        return {
            "redirects": self.url != self.final_url,
            "redirect_count": len(self.redirect_chain),
            "original_url": self.url,
            "final_url": self.final_url,
            "redirect_chain": self.redirect_chain,
        }

    @require_optional_import(["requests"], "rag")
    def follow_redirects(self) -> tuple[str, list[str]]:
        """Follow redirects for the URL without analyzing content types.

        Returns:
            Tuple[str, list[str]]: The final URL and the redirect chain
        """
        try:
            response = requests.head(self.url, allow_redirects=True, timeout=5)

            # If the server returns a 405 (Method Not Allowed) for HEAD, try GET
            if response.status_code == 405:
                response = requests.get(self.url, allow_redirects=True, timeout=5, stream=True)
                # Close the connection without downloading the content
                response.close()

            # Update redirect information
            if hasattr(response, "history") and response.history:
                self.redirect_chain = [r.url for r in response.history]
                self.final_url = response.url
            else:
                self.final_url = self.url
                self.redirect_chain = []

            return self.final_url, self.redirect_chain

        except Exception:
            # If the request fails, return the original URL
            return self.url, []

    @classmethod
    def get_supported_formats(cls) -> list[InputFormat]:
        """Return a list of supported file formats."""
        return list(cls.FormatToMimeType.keys())

    @classmethod
    def get_supported_mime_types(cls) -> list[str]:
        """Return a list of all supported MIME types."""
        return list(cls.MimeTypeToFormat.keys())

    @classmethod
    def get_supported_extensions(cls) -> list[str]:
        """Return a list of supported file extensions."""
        return list(ExtensionToFormat.keys())
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .time import TimeReplyAgent, TimeToolAgent

__all__ = [
    "TimeReplyAgent",
    "TimeToolAgent",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import Agent, ConversableAgent, OpenAIWrapper
from ....doc_utils import export_module

__all__ = ["TimeReplyAgent"]


@export_module("autogen.agents.contrib")  # API Reference: autogen > agents > contrib > TimeReplyAgent
class TimeReplyAgent(ConversableAgent):
    """A simple agent that returns the current time.

    Use it is as a reference for creating new agents with a reply-based approach (as opposed to tool-based).

    This agent will return the date and time whenever it needs to reply.
    """

    DEFAULT_SYSTEM_MESSAGE = "You are a calendar agent that just returns the date and time."

    def __init__(
        self,
        date_time_format: str = "%Y-%m-%d %H:%M:%S",  # This is a parameter that is unique to this agent
        output_prefix: str = "Tick, tock, the current date/time is ",
        **kwargs: Any,
    ) -> None:
        """Initialize the TimeReplyAgent.

        Args:
            date_time_format: The format in which the date and time should be returned.
            output_prefix: The prefix to add to the output message.
            **kwargs: Additional parameters to pass to the base
        """
        # Here we handle a ConversableAgent parameter through the kwargs
        # We will pass this through when we run init() the base class
        # Note: For this TimeReplyAgent, the LLM is not used so this won't affect the behavior of this agent
        system_message = kwargs.pop("system_message", self.DEFAULT_SYSTEM_MESSAGE)

        # Store the date and time format on the agent, prefixed with an underscore to indicate it's a private variable
        self._date_time_format = date_time_format

        self._output_prefix = output_prefix

        # Initialise the base class, passing through the system_message parameter
        super().__init__(system_message=system_message, **kwargs)

        # Our reply function.
        # This one is simple, but yours will be more complex and
        # may even contain another AG2 workflow inside it
        def get_date_time_reply(
            agent: ConversableAgent,
            messages: list[dict[str, Any]] | None = None,
            sender: Agent | None = None,
            config: OpenAIWrapper | None = None,
        ) -> tuple[bool, dict[str, Any]]:
            from datetime import datetime

            now = datetime.now()

            # Format the date and time as a string (e.g., "2025-02-25 14:30:00")
            current_date_time = now.strftime(self._date_time_format)

            # Final reply, with the date/time as the message
            return True, {"content": f"{self._output_prefix}{current_date_time}."}

        # Register our reply function with the agent
        self.register_reply(
            trigger=[Agent, None],
            reply_func=get_date_time_reply,  # This is the function that will be called when the agent needs to reply
            remove_other_reply_funcs=True,  # Removing all other reply functions so only this one will be used
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .... import ConversableAgent
from ....doc_utils import export_module
from ....tools.contrib import TimeTool

__all__ = ["TimeToolAgent"]


@export_module("autogen.agents.contrib")  # API Reference: autogen > agents > contrib > TimeToolAgent
class TimeToolAgent(ConversableAgent):
    """A simple agent that returns the current time using tools

    Use it is as a reference for creating new agents with a tool-based approach (as opposed to reply-based).

    This agent will call the TimeTool and return the date and time whenever it needs to reply.
    """

    DEFAULT_SYSTEM_MESSAGE = (
        "You are a calendar agent that uses tools to return the date and time. "
        "When you reply, say 'Tick, tock, the current date/time is ' followed by the date and time in the exact format the tool provided."
    )

    def __init__(
        self,
        date_time_format: str = "%Y-%m-%d %H:%M:%S",  # This is a parameter that is unique to this agent
        **kwargs: Any,
    ) -> None:
        """Initialize the TimeToolAgent.

        Args:
            date_time_format: The format in which the date and time should be returned.
            **kwargs: Additional keyword arguments passed to the parent ConversableAgent class.
        """
        # Here we handle a ConversableAgent parameter through the kwargs
        # We will pass this through when we run init() the base class
        # Use this to tailor the return message
        system_message = kwargs.pop("system_message", self.DEFAULT_SYSTEM_MESSAGE)

        # Store the date and time format on the agent, prefixed with an underscore to indicate it's a private variable
        self._date_time_format = date_time_format

        self._time_tool = TimeTool(date_time_format=self._date_time_format)

        # Initialise the base class, passing through the system_message parameter
        super().__init__(system_message=system_message, **kwargs)

        self.register_for_llm()(self._time_tool)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .time_reply_agent import TimeReplyAgent
from .time_tool_agent import TimeToolAgent

__all__ = ["TimeReplyAgent", "TimeToolAgent"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import glob
import hashlib
import logging
import os
import re
from collections.abc import Callable
from typing import Any
from urllib.parse import urlparse

import requests

from .import_utils import optional_import_block, require_optional_import
from .token_count_utils import count_token

with optional_import_block():
    import chromadb
    import markdownify
    from bs4 import BeautifulSoup

    if chromadb.__version__ < "0.4.15":
        from chromadb.api import API
    else:
        from chromadb.api import ClientAPI as API  # noqa: N814
    import chromadb.utils.embedding_functions as ef
    import pypdf
    from chromadb.api.types import QueryResult


with optional_import_block() as result:
    from unstructured.partition.auto import partition

HAS_UNSTRUCTURED = result.is_successful

logger = logging.getLogger(__name__)
TEXT_FORMATS = [
    "txt",
    "json",
    "csv",
    "tsv",
    "md",
    "html",
    "htm",
    "rtf",
    "rst",
    "jsonl",
    "log",
    "xml",
    "yaml",
    "yml",
    "pdf",
    "mdx",
]
UNSTRUCTURED_FORMATS = [
    "doc",
    "docx",
    "epub",
    "msg",
    "odt",
    "org",
    "pdf",
    "ppt",
    "pptx",
    "rtf",
    "rst",
    "xlsx",
]  # These formats will be parsed by the 'unstructured' library, if installed.
if HAS_UNSTRUCTURED:
    TEXT_FORMATS += UNSTRUCTURED_FORMATS
    TEXT_FORMATS = list(set(TEXT_FORMATS))
VALID_CHUNK_MODES = frozenset({"one_line", "multi_lines"})
RAG_MINIMUM_MESSAGE_LENGTH = int(os.environ.get("RAG_MINIMUM_MESSAGE_LENGTH", 5))


def split_text_to_chunks(
    text: str,
    max_tokens: int = 4000,
    chunk_mode: str = "multi_lines",
    must_break_at_empty_line: bool = True,
    overlap: int = 0,  # number of overlapping lines
):
    """Split a long text into chunks of max_tokens."""
    if chunk_mode not in VALID_CHUNK_MODES:
        raise AssertionError
    if chunk_mode == "one_line":
        must_break_at_empty_line = False
        overlap = 0
    chunks = []
    lines = text.split("\n")
    num_lines = len(lines)
    if num_lines < 3 and must_break_at_empty_line:
        logger.warning("The input text has less than 3 lines. Set `must_break_at_empty_line` to `False`")
        must_break_at_empty_line = False
    lines_tokens = [count_token(line) for line in lines]
    sum_tokens = sum(lines_tokens)
    while sum_tokens > max_tokens:
        estimated_line_cut = 2 if chunk_mode == "one_line" else max(int(max_tokens / sum_tokens * len(lines)), 2)
        cnt = 0
        prev = ""
        for cnt in reversed(range(estimated_line_cut)):
            if must_break_at_empty_line and lines[cnt].strip() != "":
                continue
            if sum(lines_tokens[:cnt]) <= max_tokens:
                prev = "\n".join(lines[:cnt])
                break
        if cnt == 0:
            logger.warning(
                f"max_tokens is too small to fit a single line of text. Breaking this line:\n\t{lines[0][:100]} ..."
            )
            if not must_break_at_empty_line:
                split_len = max(
                    int(max_tokens / (lines_tokens[0] * 0.9 * len(lines[0]) + 0.1)), RAG_MINIMUM_MESSAGE_LENGTH
                )
                prev = lines[0][:split_len]
                lines[0] = lines[0][split_len:]
                lines_tokens[0] = count_token(lines[0])
            else:
                logger.warning("Failed to split docs with must_break_at_empty_line being True, set to False.")
                must_break_at_empty_line = False
        (
            chunks.append(prev) if len(prev) >= RAG_MINIMUM_MESSAGE_LENGTH else None
        )  # don't add chunks less than RAG_MINIMUM_MESSAGE_LENGTH characters
        lines = lines[cnt - overlap if cnt > overlap else cnt :]
        lines_tokens = lines_tokens[cnt - overlap if cnt > overlap else cnt :]
        sum_tokens = sum(lines_tokens)
    text_to_chunk = "\n".join(lines).strip()
    (
        chunks.append(text_to_chunk) if len(text_to_chunk) >= RAG_MINIMUM_MESSAGE_LENGTH else None
    )  # don't add chunks less than RAG_MINIMUM_MESSAGE_LENGTH characters
    return chunks


@require_optional_import("pypdf", "retrievechat")
def extract_text_from_pdf(file: str) -> str:
    """Extract text from PDF files"""
    text = ""
    with open(file, "rb") as f:
        reader = pypdf.PdfReader(f)
        if reader.is_encrypted:  # Check if the PDF is encrypted
            try:
                reader.decrypt("")
            except pypdf.errors.FileNotDecryptedError as e:
                logger.warning(f"Could not decrypt PDF {file}, {e}")
                return text  # Return empty text if PDF could not be decrypted

        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text += page.extract_text()

    if not text.strip():  # Debugging line to check if text is empty
        logger.warning(f"Could not decrypt PDF {file}")

    return text


def split_files_to_chunks(
    files: list[tuple[str, str] | str],
    max_tokens: int = 4000,
    chunk_mode: str = "multi_lines",
    must_break_at_empty_line: bool = True,
    custom_text_split_function: Callable[[str], list[str]] | None = None,
) -> tuple[list[str], list[dict[str, Any]]]:
    """Split a list of files into chunks of max_tokens."""
    chunks = []
    sources = []

    for file in files:
        if isinstance(file, tuple):
            url = file[1]
            file = file[0]
        else:
            url = None
        _, file_extension = os.path.splitext(file)
        file_extension = file_extension.lower()

        if HAS_UNSTRUCTURED and file_extension[1:] in UNSTRUCTURED_FORMATS:
            text = partition(file)
            text = "\n".join([t.text for t in text]) if len(text) > 0 else ""
        elif file_extension == ".pdf":
            text = extract_text_from_pdf(file)
        else:  # For non-PDF text-based files
            with open(file, encoding="utf-8", errors="ignore") as f:
                text = f.read()

        if not text.strip():  # Debugging line to check if text is empty after reading
            logger.warning(f"No text available in file: {file}")
            continue  # Skip to the next file if no text is available

        if custom_text_split_function is not None:
            tmp_chunks = custom_text_split_function(text)
        else:
            tmp_chunks = split_text_to_chunks(text, max_tokens, chunk_mode, must_break_at_empty_line)
        chunks += tmp_chunks
        sources += [{"source": url if url else file}] * len(tmp_chunks)

    return chunks, sources


def get_files_from_dir(dir_path: str | list[str], types: list[str] = TEXT_FORMATS, recursive: bool = True) -> list[Any]:
    """Return a list of all the files in a given directory, a url, a file path or a list of them."""
    if len(types) == 0:
        raise ValueError("types cannot be empty.")
    types = [t[1:].lower() if t.startswith(".") else t.lower() for t in set(types)]
    types += [t.upper() for t in types]

    files = []
    # If the path is a list of files or urls, process and return them
    if isinstance(dir_path, list):
        for item in dir_path:
            if os.path.isfile(item):
                files.append(item)
            elif is_url(item):
                filepath = get_file_from_url(item)
                if filepath:
                    files.append(filepath)
            elif os.path.exists(item):
                try:
                    files.extend(get_files_from_dir(item, types, recursive))
                except ValueError:
                    logger.warning(f"Directory {item} does not exist. Skipping.")
            else:
                logger.warning(f"File {item} does not exist. Skipping.")
        return files

    # If the path is a file, return it
    if os.path.isfile(dir_path):
        return [dir_path]

    # If the path is a url, download it and return the downloaded file
    if is_url(dir_path):
        filepath = get_file_from_url(dir_path)
        if filepath:
            return [filepath]
        else:
            return []

    if os.path.exists(dir_path):
        for type in types:
            if recursive:
                files += glob.glob(os.path.join(dir_path, f"**/*.{type}"), recursive=True)
            else:
                files += glob.glob(os.path.join(dir_path, f"*.{type}"), recursive=False)
    else:
        logger.error(f"Directory {dir_path} does not exist.")
        raise ValueError(f"Directory {dir_path} does not exist.")
    return files


@require_optional_import(["markdownify", "bs4"], "retrievechat")
def parse_html_to_markdown(html: str, url: str = None) -> str:
    """Parse HTML to markdown."""
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.string
    # Remove javascript and style blocks
    for script in soup(["script", "style"]):
        script.extract()

    # Convert to markdown -- Wikipedia gets special attention to get a clean version of the page
    if isinstance(url, str) and url.startswith("https://en.wikipedia.org/"):
        body_elm = soup.find("div", {"id": "mw-content-text"})
        title_elm = soup.find("span", {"class": "mw-page-title-main"})

        if body_elm:
            # What's the title
            main_title = soup.title.string
            if title_elm and len(title_elm) > 0:
                main_title = title_elm.string
            webpage_text = "# " + main_title + "\n\n" + markdownify.MarkdownConverter().convert_soup(body_elm)
        else:
            webpage_text = markdownify.MarkdownConverter().convert_soup(soup)
    else:
        webpage_text = markdownify.MarkdownConverter().convert_soup(soup)

    # Convert newlines
    webpage_text = re.sub(r"\r\n", "\n", webpage_text)
    webpage_text = re.sub(r"\n{2,}", "\n\n", webpage_text).strip()
    webpage_text = "# " + title + "\n\n" + webpage_text
    return webpage_text


def _generate_file_name_from_url(url: str, max_length=255) -> str:
    url_bytes = url.encode("utf-8")
    hash = hashlib.blake2b(url_bytes).hexdigest()
    parsed_url = urlparse(url)
    file_name = os.path.basename(url)
    file_name = (
        f"{parsed_url.netloc}_{file_name}_{hash[: min(8, max_length - len(parsed_url.netloc) - len(file_name) - 1)]}"
    )
    return file_name


def get_file_from_url(url: str, save_path: str = None) -> tuple[str, str]:
    """Download a file from a URL."""
    if save_path is None:
        save_path = "tmp/chromadb"
        os.makedirs(save_path, exist_ok=True)
    if os.path.isdir(save_path):
        filename = _generate_file_name_from_url(url)
        save_path = os.path.join(save_path, filename)
    else:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

    custom_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
    }
    try:
        response = requests.get(url, stream=True, headers=custom_headers, timeout=30)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logger.warning(f"Failed to download {url}, {e}")
        return None

    content_type = response.headers.get("content-type", "")
    if "text/html" in content_type:
        # Get the content of the response
        html = ""
        for chunk in response.iter_content(chunk_size=8192, decode_unicode=True):
            html += chunk
        text = parse_html_to_markdown(html, url)
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(text)
    else:
        with open(save_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
    return save_path, url


def is_url(string: str):
    """Return True if the string is a valid URL."""
    try:
        result = urlparse(string)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False


@require_optional_import("chromadb", "retrievechat")
def create_vector_db_from_dir(
    dir_path: str | list[str],
    max_tokens: int = 4000,
    client: "API" = None,
    db_path: str = "tmp/chromadb.db",
    collection_name: str = "all-my-documents",
    get_or_create: bool = False,
    chunk_mode: str = "multi_lines",
    must_break_at_empty_line: bool = True,
    embedding_model: str = "all-MiniLM-L6-v2",
    embedding_function: Callable = None,
    custom_text_split_function: Callable = None,
    custom_text_types: list[str] = TEXT_FORMATS,
    recursive: bool = True,
    extra_docs: bool = False,
) -> "API":
    """Create a vector db from all the files in a given directory, the directory can also be a single file or a url to
        a single file. We support chromadb compatible APIs to create the vector db, this function is not required if
        you prepared your own vector db.

    Args:
        dir_path (Union[str, List[str]]): the path to the directory, file, url or a list of them.
        max_tokens (Optional, int): the maximum number of tokens per chunk. Default is 4000.
        client (Optional, API): the chromadb client. Default is None.
        db_path (Optional, str): the path to the chromadb. Default is "tmp/chromadb.db". The default was `/tmp/chromadb.db` for version `<=0.2.24`.
        collection_name (Optional, str): the name of the collection. Default is "all-my-documents".
        get_or_create (Optional, bool): Whether to get or create the collection. Default is False. If True, the collection
            will be returned if it already exists. Will raise ValueError if the collection already exists and get_or_create is False.
        chunk_mode (Optional, str): the chunk mode. Default is "multi_lines".
        must_break_at_empty_line (Optional, bool): Whether to break at empty line. Default is True.
        embedding_model (Optional, str): the embedding model to use. Default is "all-MiniLM-L6-v2". Will be ignored if
            embedding_function is not None.
        embedding_function (Optional, Callable): the embedding function to use. Default is None, SentenceTransformer with
            the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding
            functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.
        custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.
            Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.
        custom_text_types (Optional, List[str]): a list of file types to be processed. Default is TEXT_FORMATS.
        recursive (Optional, bool): whether to search documents recursively in the dir_path. Default is True.
        extra_docs (Optional, bool): whether to add more documents in the collection. Default is False

    Returns:
    The chromadb client.
    """
    if client is None:
        client = chromadb.PersistentClient(path=db_path)
    try:
        embedding_function = (
            ef.SentenceTransformerEmbeddingFunction(embedding_model)
            if embedding_function is None
            else embedding_function
        )
        collection = client.create_collection(
            collection_name,
            get_or_create=get_or_create,
            embedding_function=embedding_function,
            # https://github.com/nmslib/hnswlib#supported-distances
            # https://github.com/chroma-core/chroma/blob/566bc80f6c8ee29f7d99b6322654f32183c368c4/chromadb/segment/impl/vector/local_hnsw.py#L184
            # https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md
            metadata={"hnsw:space": "ip", "hnsw:construction_ef": 30, "hnsw:M": 32},  # ip, l2, cosine
        )

        length = 0
        if extra_docs:
            length = len(collection.get()["ids"])

        if custom_text_split_function is not None:
            chunks, sources = split_files_to_chunks(
                get_files_from_dir(dir_path, custom_text_types, recursive),
                custom_text_split_function=custom_text_split_function,
            )
        else:
            chunks, sources = split_files_to_chunks(
                get_files_from_dir(dir_path, custom_text_types, recursive),
                max_tokens,
                chunk_mode,
                must_break_at_empty_line,
            )
        logger.info(f"Found {len(chunks)} chunks.")
        # Upsert in batch of 40000 or less if the total number of chunks is less than 40000
        for i in range(0, len(chunks), min(40000, len(chunks))):
            end_idx = i + min(40000, len(chunks) - i)
            collection.upsert(
                documents=chunks[i:end_idx],
                ids=[f"doc_{j + length}" for j in range(i, end_idx)],  # unique for each doc
                metadatas=sources[i:end_idx],
            )
    except ValueError as e:
        logger.warning(f"{e}")
    return client


@require_optional_import("chromadb", "retrievechat")
def query_vector_db(
    query_texts: list[str],
    n_results: int = 10,
    client: "API" = None,
    db_path: str = "tmp/chromadb.db",
    collection_name: str = "all-my-documents",
    search_string: str = "",
    embedding_model: str = "all-MiniLM-L6-v2",
    embedding_function: Callable = None,
) -> "QueryResult":
    """Query a vector db. We support chromadb compatible APIs, it's not required if you prepared your own vector db
        and query function.

    Args:
        query_texts (List[str]): the list of strings which will be used to query the vector db.
        n_results (Optional, int): the number of results to return. Default is 10.
        client (Optional, API): the chromadb compatible client. Default is None, a chromadb client will be used.
        db_path (Optional, str): the path to the vector db. Default is "tmp/chromadb.db". The default was `/tmp/chromadb.db` for version `<=0.2.24`.
        collection_name (Optional, str): the name of the collection. Default is "all-my-documents".
        search_string (Optional, str): the search string. Only docs that contain an exact match of this string will be retrieved. Default is "".
        embedding_model (Optional, str): the embedding model to use. Default is "all-MiniLM-L6-v2". Will be ignored if
            embedding_function is not None.
        embedding_function (Optional, Callable): the embedding function to use. Default is None, SentenceTransformer with
            the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding
            functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.

    Returns:
        The query result. The format is:

    ```python
    class QueryResult(TypedDict):
        ids: List[IDs]
        embeddings: Optional[List[List[Embedding]]]
        documents: Optional[List[List[Document]]]
        metadatas: Optional[List[List[Metadata]]]
        distances: Optional[List[List[float]]]
    ```
    """
    if client is None:
        client = chromadb.PersistentClient(path=db_path)
    # the collection's embedding function is always the default one, but we want to use the one we used to create the
    # collection. So we compute the embeddings ourselves and pass it to the query function.
    collection = client.get_collection(collection_name)
    embedding_function = (
        ef.SentenceTransformerEmbeddingFunction(embedding_model) if embedding_function is None else embedding_function
    )
    query_embeddings = embedding_function(query_texts)
    # Query/search n most similar results. You can also .get by id
    results = collection.query(
        query_embeddings=query_embeddings,
        n_results=n_results,
        where_document={"$contains": search_string} if search_string else None,  # optional filter
    )
    return results
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .mcp_client import create_toolkit

__all__ = ["create_toolkit"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from collections.abc import AsyncIterator
from contextlib import AbstractAsyncContextManager, AsyncExitStack, asynccontextmanager
from datetime import datetime, timedelta
from pathlib import Path
from typing import Annotated, Any, Literal, Protocol, cast

import anyio
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream
from mcp.client.session import ClientSession
from mcp.client.sse import sse_client
from mcp.client.stdio import StdioServerParameters, stdio_client
from pydantic import AnyUrl, BaseModel, Field

from ..doc_utils import export_module
from ..import_utils import optional_import_block, require_optional_import
from ..tools import Tool, Toolkit

with optional_import_block():
    from mcp.shared.message import SessionMessage
    from mcp.types import CallToolResult, ReadResourceResult, ResourceTemplate, TextContent
    from mcp.types import Tool as MCPTool

__all__ = ["ResultSaved", "create_toolkit"]

# Type definitions
EncodingErrorHandlerType = Literal["strict", "ignore", "replace"]

# Default constants
DEFAULT_TEXT_ENCODING = "utf-8"
DEFAULT_TEXT_ENCODING_ERROR_HANDLER: EncodingErrorHandlerType = "strict"
DEFAULT_HTTP_REQUEST_TIMEOUT = 5
DEFAULT_SSE_EVENT_READ_TIMEOUT = 60 * 5
DEFAULT_STREAMABLE_HTTP_REQUEST_TIMEOUT = timedelta(seconds=30)
DEFAULT_STREAMABLE_HTTP_SSE_EVENT_READ_TIMEOUT = timedelta(seconds=60 * 5)


class SessionConfigProtocol(Protocol):
    """Protocol for session configuration classes that can create MCP sessions."""

    server_name: str

    @asynccontextmanager
    async def create_session(self, exit_stack: AsyncExitStack) -> AsyncIterator[ClientSession]:
        """Create a session using the given exit stack."""
        try:
            yield cast(ClientSession, None)  # placeholder yield to satisfy AsyncIterator type
        except Exception:
            raise NotImplementedError


class BasicSessionConfig(BaseModel):
    """Basic session configuration."""

    server_name: str = Field(..., description="Name of the server")

    async def initialize(
        self,
        client: AbstractAsyncContextManager[
            tuple[
                MemoryObjectReceiveStream[SessionMessage | Exception],
                MemoryObjectSendStream[SessionMessage],
            ]
        ],
        exit_stack: AsyncExitStack,
    ) -> ClientSession:
        """Initialize the session."""
        reader, writer = await exit_stack.enter_async_context(client)
        session = cast(
            ClientSession,
            await exit_stack.enter_async_context(ClientSession(reader, writer)),
        )
        return session


class SseConfig(BasicSessionConfig):
    """Configuration for a single SSE MCP server."""

    url: str = Field(..., description="URL of the SSE server")
    headers: dict[str, Any] | None = Field(default=None, description="HTTP headers to send to the SSE endpoint")
    timeout: float = Field(default=DEFAULT_HTTP_REQUEST_TIMEOUT, description="HTTP timeout")
    sse_read_timeout: float = Field(default=DEFAULT_SSE_EVENT_READ_TIMEOUT, description="SSE read timeout")

    @asynccontextmanager
    async def create_session(self, exit_stack: AsyncExitStack) -> AsyncIterator[ClientSession]:
        """
        Create a new session to an MCP server using SSE transport.

        Args:
            exit_stack: AsyncExitStack for managing async resources

        Yields:
            ClientSession: The MCP client session
        """
        # Create and store the connection
        client = sse_client(self.url, self.headers, self.timeout, self.sse_read_timeout)
        yield await self.initialize(client, exit_stack)


class StdioConfig(BasicSessionConfig):
    """Configuration for a single stdio MCP server."""

    command: str = Field(..., description="Command to execute")
    args: list[str] = Field(..., description="Arguments for the command")
    transport: Literal["stdio"] = Field(default="stdio", description="Transport type")
    environment: dict[str, str] | None = Field(default=None, description="Environment variables")
    working_dir: str | Path | None = Field(default=None, description="Working directory")
    encoding: str = Field(default=DEFAULT_TEXT_ENCODING, description="Character encoding")
    encoding_error_handler: EncodingErrorHandlerType = Field(
        default=DEFAULT_TEXT_ENCODING_ERROR_HANDLER, description="How to handle encoding errors"
    )
    session_options: dict[str, Any] | None = Field(default=None, description="Additional session options")

    @asynccontextmanager
    async def create_session(self, exit_stack: AsyncExitStack) -> AsyncIterator[ClientSession]:
        """
        Create a new session to an MCP server using stdio transport.

        Args:
            exit_stack: AsyncExitStack for managing async resources

        Yields:
            ClientSession: The MCP client session
        """
        client = stdio_client(
            StdioServerParameters(
                command=self.command,
                args=self.args,
                env=self.environment,
                encoding=self.encoding,
                encoding_error_handler=self.encoding_error_handler,
            )
        )
        yield await self.initialize(client, exit_stack)


class MCPConfig(BaseModel):
    """Configuration for multiple MCP sessions using stdio transport."""

    # we should use final classes to allow pydantic to validate the type
    servers: list[SseConfig | StdioConfig] = Field(..., description="List of stdio & sse server configurations")


class MCPClient:
    @staticmethod
    def _convert_call_tool_result(  # type: ignore[no-any-unimported]
        call_tool_result: "CallToolResult",  # type: ignore[no-any-unimported]
    ) -> tuple[str | list[str], Any]:
        text_contents: list[TextContent] = []  # type: ignore[no-any-unimported]
        non_text_contents = []
        for content in call_tool_result.content:
            if isinstance(content, TextContent):
                text_contents.append(content)
            else:
                non_text_contents.append(content)

        tool_content: str | list[str] = [content.text for content in text_contents]
        if len(text_contents) == 1:
            tool_content = tool_content[0]

        if call_tool_result.isError:
            raise ValueError(f"Tool call failed: {tool_content}")

        return tool_content, non_text_contents or None

    @classmethod
    @require_optional_import("mcp", "mcp")
    def convert_tool(  # type: ignore[no-any-unimported]
        cls, tool: Any, session: "ClientSession", **kwargs: Any
    ) -> Tool:
        if not isinstance(tool, MCPTool):
            raise ValueError(f"Expected an instance of `mcp.types.Tool`, got {type(tool)}")

        # needed for type checking
        mcp_tool: MCPTool = tool  # type: ignore[no-any-unimported]

        async def call_tool(  # type: ignore[no-any-unimported]
            **arguments: dict[str, Any],
        ) -> tuple[str | list[str], Any]:
            call_tool_result = await session.call_tool(tool.name, arguments)
            return MCPClient._convert_call_tool_result(call_tool_result)

        ag2_tool = Tool(
            name=mcp_tool.name,
            description=mcp_tool.description,
            func_or_tool=call_tool,
            parameters_json_schema=mcp_tool.inputSchema,
        )
        return ag2_tool

    @classmethod
    @require_optional_import("mcp", "mcp")
    def convert_resource(  # type: ignore[no-any-unimported]
        cls,
        resource_template: Any,
        session: "ClientSession",
        resource_download_folder: Path | None,
        **kwargs: Any,
    ) -> Tool:
        if not isinstance(resource_template, ResourceTemplate):
            raise ValueError(f"Expected an instance of `mcp.types.ResourceTemplate`, got {type(resource_template)}")

        # needed for type checking
        mcp_resource: ResourceTemplate = resource_template  # type: ignore[no-any-unimported]

        uri_description = f"""A URI template (according to RFC 6570) that can be used to construct resource URIs.
Here is the correct format for the URI template:
{mcp_resource.uriTemplate}
"""

        async def call_resource(uri: Annotated[str, uri_description]) -> ReadResourceResult | ResultSaved:  # type: ignore[no-any-unimported]
            result = await session.read_resource(AnyUrl(uri))

            if not resource_download_folder:
                return result

            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            filename = uri.split("://")[-1] + f"_{timestamp}"
            file_path = resource_download_folder / filename

            async with await anyio.open_file(file_path, "w") as f:
                await f.write(result.model_dump_json(indent=4))

            return ResultSaved(
                explanation=f"Request for uri {uri} was saved to {file_path}",
                file_path=file_path,
            )

        # Wrap resource as AG2 tool
        ag2_tool = Tool(
            name=mcp_resource.name,
            description=mcp_resource.description,
            func_or_tool=call_resource,
        )
        return ag2_tool

    @classmethod
    @require_optional_import("mcp", "mcp")
    async def load_mcp_toolkit(
        cls,
        session: "ClientSession",
        *,
        use_mcp_tools: bool,
        use_mcp_resources: bool,
        resource_download_folder: Path | None,
    ) -> Toolkit:  # type: ignore[no-any-unimported]
        """Load all available MCP tools and convert them to AG2 Toolkit."""
        all_ag2_tools: list[Tool] = []

        if use_mcp_tools:
            tools = await session.list_tools()
            ag2_tools: list[Tool] = [cls.convert_tool(tool=tool, session=session) for tool in tools.tools]
            all_ag2_tools.extend(ag2_tools)

        if use_mcp_resources:
            resource_templates = await session.list_resource_templates()
            ag2_resources: list[Tool] = [
                cls.convert_resource(
                    resource_template=resource_template,
                    session=session,
                    resource_download_folder=resource_download_folder,
                )
                for resource_template in resource_templates.resourceTemplates
            ]
            all_ag2_tools.extend(ag2_resources)

        return Toolkit(tools=all_ag2_tools)

    @classmethod
    def get_unsupported_reason(cls) -> str | None:
        with optional_import_block() as result:
            import mcp  # noqa: F401

        if not result.is_successful:
            return "Please install `mcp` extra to use this module:\n\n\tpip install ag2[mcp]"

        return None


class MCPClientSessionManager:
    """
    A class to manage MCP client sessions.
    """

    def __init__(self) -> None:
        """Initialize the MCP client session manager."""
        self.exit_stack = AsyncExitStack()
        self.sessions: dict[str, ClientSession] = {}

    @asynccontextmanager
    async def open_session(
        self,
        config: SessionConfigProtocol,
    ) -> AsyncIterator[ClientSession]:
        """Open a new session to an MCP server based on configuration.

        Args:
            config: SessionConfigProtocol object containing session configuration

        Yields:
            ClientSession: The MCP client session
        """
        async with config.create_session(self.exit_stack) as session:
            await session.initialize()
            self.sessions[config.server_name] = session
            yield session


@export_module("autogen.mcp")
async def create_toolkit(
    session: "ClientSession",
    *,
    use_mcp_tools: bool = True,
    use_mcp_resources: bool = True,
    resource_download_folder: Path | str | None = None,
) -> Toolkit:  # type: ignore[no-any-unimported]
    """Create a toolkit from the MCP client session.

    Args:
        session (ClientSession): The MCP client session.
        use_mcp_tools (bool): Whether to include MCP tools in the toolkit.
        use_mcp_resources (bool): Whether to include MCP resources in the toolkit.
        resource_download_folder (Optional[Union[Path, str]]): The folder to download files to.

    Returns:
        Toolkit: The toolkit containing the converted tools.
    """
    if resource_download_folder is not None:
        resource_download_folder = Path(resource_download_folder)
        await anyio.to_thread.run_sync(lambda: resource_download_folder.mkdir(parents=True, exist_ok=True))

    return await MCPClient.load_mcp_toolkit(
        session=session,
        use_mcp_tools=use_mcp_tools,
        use_mcp_resources=use_mcp_resources,
        resource_download_folder=resource_download_folder,
    )


@export_module("autogen.mcp.mcp_client")
class ResultSaved(BaseModel):
    """Result saved to a file"""

    explanation: str
    file_path: Path
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from collections.abc import Iterator
from contextlib import contextmanager
from functools import cached_property

from ...import_utils import optional_import_block

with optional_import_block() as result:
    from fastapi_code_generator.parser import (
        Argument,
        OpenAPIParser,
        ParameterObject,
        ReferenceObject,
    )

SUCCESFUL_IMPORT = result.is_successful

__all__ = ["SUCCESFUL_IMPORT", "patch_get_parameter_type"]


@contextmanager
def patch_get_parameter_type() -> Iterator[None]:
    class ArgumentWithDescription(Argument):  # type: ignore[misc]
        description: str | None = None

        @cached_property
        def argument(self) -> str:
            if self.description:
                description = self.description.replace('"""', '"""')
                type_hint = f'Annotated[{self.type_hint}, """{description}"""]'
            else:
                type_hint = self.type_hint

            if self.default is None and self.required:
                return f"{self.name}: {type_hint}"

            return f"{self.name}: {type_hint} = {self.default}"

    original_get_parameter_type = OpenAPIParser.get_parameter_type

    def get_parameter_type(
        self: OpenAPIParser,
        parameters: ReferenceObject | ParameterObject,
        snake_case: bool,
        path: list[str],
    ) -> Argument | None:
        # get the original argument
        argument = original_get_parameter_type(self, parameters, snake_case, path)

        # add description to the argument
        parameters = self.resolve_object(parameters, ParameterObject)
        argument_with_description = ArgumentWithDescription(description=parameters.description, **argument.model_dump())
        return argument_with_description

    OpenAPIParser.get_parameter_type = get_parameter_type

    try:
        yield
    finally:
        OpenAPIParser.get_parameter_type = original_get_parameter_type
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from pathlib import Path

from autogen.import_utils import optional_import_block
from autogen.mcp.mcp_proxy.security import BaseSecurity

with optional_import_block() as result:
    from fastapi_code_generator.parser import OpenAPIParser
    from fastapi_code_generator.visitor import Visitor


def custom_visitor(parser: "OpenAPIParser", model_path: Path) -> dict[str, object]:
    if "components" not in parser.raw_obj or "securitySchemes" not in parser.raw_obj["components"]:
        return {}
    security_schemes = parser.raw_obj["components"]["securitySchemes"]
    server_url = parser.raw_obj["servers"][0]["url"]

    security_classes = []
    security_parameters = {}
    for k, v in security_schemes.items():
        v["server_url"] = server_url
        security_class = BaseSecurity.get_security_class(type=v["type"], schema_parameters=v)

        security_classes.append(security_class.__name__)

        security_parameters[k] = security_class.get_security_parameters(schema_parameters=v)

    return {
        "security_schemes": security_schemes,
        "security_classes": security_classes,
        "security_parameters": security_parameters,
    }


visit: "Visitor" = custom_visitor
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import json
import logging
from pathlib import Path

from autogen.import_utils import optional_import_block

with optional_import_block() as result:
    from fastapi_code_generator.parser import OpenAPIParser, Operation
    from fastapi_code_generator.visitor import Visitor

from pydantic import BaseModel

from autogen.agentchat.conversable_agent import ConversableAgent
from autogen.llm_config import LLMConfig


class Group(BaseModel):
    name: str
    description: str


class GroupSuggestions(BaseModel):
    groups: list[Group]


class GroupNames(BaseModel):
    groups: list[str]


logger = logging.getLogger(__name__)

GROUP_DISCOVERY_MESSAGE = (
    "You are a senior Python engineer. You will be shown a batch of API functions. "
    "These functions are not guaranteed to be related  your task is to analyze them individually and find meaningful groups *within the batch*.\n\n"
    "You should propose a list of group names, and for each group, provide a short description of the kind of functions it contains.\n\n"
    "How to group:\n"
    "- Focus on what the function operates on  e.g., functions that manipulate a board go in 'board_operations', functions related to users in 'user_management', etc.\n"
    "- Do NOT assume the entire batch forms a single group.\n"
    "- Do NOT use generic categories like 'misc', 'utils', or 'general'.\n"
    "- Favor *granular but meaningful* groups. For instance, 'user_profile_handling' and 'user_authentication' could be separate if their purposes are distinct.\n\n"
    "Formatting:\n"
    "- Group names must be short, descriptive, and in snake_case.\n"
    "- Return a list of group names and a 1-2 sentence description of what each group includes.\n"
)


GROUP_ASSIGNMENT_MESSAGE = (
    "You are a senior Python engineer. You will be given a function description "
    "and a list of possible groups. Choose the most suitable groups for the function.\n"
    "- A function can belong to multiple groups.\n"
    "- The groups should be relevant to the function's purpose and functionality.\n"
    "- You MUST choose a group from the possible groups, you cannot respond with empty grouping any equivalent.\n"
)


def chunk_list(items: list, size: int) -> list[list]:
    return [items[i : i + size] for i in range(0, len(items), size)]


def discover_groups(operations: list["Operation"], chunk_size: int = 30) -> dict[str, str]:
    llm_config = LLMConfig.get_current_llm_config().copy()

    for config in llm_config.config_list:
        config.response_format = GroupSuggestions

    agent = ConversableAgent(
        name="group_discovery_agent",
        system_message=GROUP_DISCOVERY_MESSAGE,
        llm_config=llm_config,
    )
    groups = {}

    for chunk in chunk_list(operations, chunk_size):
        func_descriptions = [f"- {op.function_name}: {op.summary} (args: {op.arguments})" for op in chunk]
        message = "Here are some functions:\n" + "\n".join(func_descriptions)

        response = agent.run(message=message, max_turns=1, user_input=False)

        for event in response.events:
            if event.type == "text" and event.content.sender == "group_discovery_agent":
                # Naively parse "group_name: description" from text block
                new_groups = GroupSuggestions.model_validate_json(event.content.content).groups
                groups.update(new_groups)

    logger.warning("Discovered groups: %s", groups)

    # Remove duplicates
    agent = ConversableAgent(
        name="group_refining_agent",
        system_message=GROUP_DISCOVERY_MESSAGE,
        llm_config=llm_config,
    )

    message = (
        "You need to refine the group names and descriptions to ensure they are unique.\n"
        "Here are the groups:\n" + "\n".join([f"- {name}: {desc}" for name, desc in groups.items()])
    )
    response = agent.run(message=message, max_turns=1, user_input=False)
    for event in response.events:
        if event.type == "text" and event.content.sender == "group_refining_agent":
            # Naively parse "group_name: description" from text block
            refined_groups = json.loads(event.content.content)

    return refined_groups


def assign_operation_to_group(operation: "Operation", groups: dict[str, str]) -> str:
    llm_config = LLMConfig.get_current_llm_config().copy()

    for config in llm_config.config_list:
        config.response_format = GroupNames

    agent = ConversableAgent(
        name="group_assignment_agent",
        system_message=GROUP_ASSIGNMENT_MESSAGE,
        llm_config=llm_config,
    )

    message = (
        "Function summary:\n"
        f"{operation.summary}\n\n"
        f"Arguments: {operation.arguments}\n\n"
        f"Available groups: {json.dumps(groups)}\n\n"
        "What group should this function go in?"
    )

    response = agent.run(message=message, max_turns=1, user_input=True)

    groups = []
    for event in response.events:
        if event.type == "text" and event.content.sender == "group_assignment_agent":
            groups = GroupNames.model_validate_json(event.content.content).groups

    return groups


def refine_group_names(groups: dict[str, str]) -> dict[str, str]:
    # Optional: normalize names, merge similar ones (e.g., using embeddings or string similarity)
    # Placeholder for now:
    return groups


def custom_visitor(parser: "OpenAPIParser", model_path: Path) -> dict[str, object]:
    operations = sorted(parser.operations.values(), key=lambda op: op.path)

    # ---- PASS 1: DISCOVER GROUPS ----
    logger.warning("Discovering groups...")
    discovered_groups = discover_groups(operations)
    logger.warning("Discovered groups: %s", discovered_groups)

    # ---- PASS 2: ASSIGN OPERATIONS TO GROUPS ----
    logger.warning("Assigning operations to groups...")
    for op in operations:
        logger.warning("Assigning operation %s to groups...", op.function_name)
        groups = assign_operation_to_group(op, discovered_groups)
        op.tags = groups
        logger.warning("Assigned groups: %s", groups)

    return {"operations": operations}


visit: "Visitor" = custom_visitor
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import base64
import json
import logging
from typing import Any, ClassVar, Literal, TypeAlias

import requests
from pydantic import BaseModel, model_validator

# Get the logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

BaseSecurityType: TypeAlias = type["BaseSecurity"]


class BaseSecurity(BaseModel):
    """Base class for security classes."""

    type: ClassVar[Literal["apiKey", "http", "mutualTLS", "oauth2", "openIdConnect", "unsupported"]]
    in_value: ClassVar[Literal["header", "query", "cookie", "bearer", "basic", "tls", "unsupported"]]
    name: str

    @model_validator(mode="after")  # type: ignore[misc]
    def __post_init__(
        self,
    ) -> "BaseSecurity":  # dataclasses uses __post_init__ instead of model_validator
        """Validate the in_value based on the type."""
        valid_in_values = {
            "apiKey": ["header", "query", "cookie"],
            "http": ["bearer", "basic"],
            "oauth2": ["bearer"],
            "openIdConnect": ["bearer"],
            "mutualTLS": ["tls"],
            "unsupported": ["unsupported"],
        }
        if self.in_value not in valid_in_values[self.type]:
            raise ValueError(f"Invalid in_value '{self.in_value}' for type '{self.type}'")
        return self

    def accept(self, security_params: "BaseSecurityParameters") -> bool:
        return isinstance(self, security_params.get_security_class())

    @classmethod
    def is_supported(cls, type: str, schema_parameters: dict[str, Any]) -> bool:
        return cls.type == type and cls.in_value == schema_parameters.get("in")

    @classmethod
    def get_security_class(cls, type: str, schema_parameters: dict[str, Any]) -> BaseSecurityType:
        sub_classes = cls.__subclasses__()

        for sub_class in sub_classes:
            if sub_class.is_supported(type, schema_parameters):
                return sub_class

        logger.error(f"Unsupported type '{type}' and schema_parameters '{schema_parameters}' combination")
        return UnsuportedSecurityStub

    @classmethod
    def get_security_parameters(cls, schema_parameters: dict[str, Any]) -> str:
        return f'{cls.__name__}(name="{schema_parameters.get("name")}")'

    @classmethod
    def parse_security_parameters(cls, unparsed_params: dict[str, Any]) -> "BaseSecurityParameters":
        type = unparsed_params.pop("type")
        schema_parameters = unparsed_params.pop("schema_parameters")
        security_class = cls.get_security_class(type, schema_parameters)
        return security_class.Parameters.model_validate(unparsed_params)

    @classmethod
    def parse_security_parameters_from_env(cls, env: dict[str, str]) -> "BaseSecurityParameters":
        """Parse security parameters from environment variables."""
        security_str = env.get("SECURITY")
        if not security_str:
            logger.warning("No security parameters found in environment variables.")

        return cls.parse_security_parameters(json.loads(security_str))


class BaseSecurityParameters(BaseModel):
    """Base class for security parameters."""

    def apply(
        self,
        q_params: dict[str, Any],
        body_dict: dict[str, Any],
        security: BaseSecurity,
    ) -> None: ...

    def get_security_class(self) -> type[BaseSecurity]: ...

    def dump(self) -> dict[str, Any]:
        raise NotImplementedError("Subclasses must implement the dump method")

    def to_env(self) -> dict[str, Any]:
        """Convert the security parameters to a dictionary."""
        return {
            "SECURITY": json.dumps(self.dump()),
        }


class UnsuportedSecurityStub(BaseSecurity):
    """Unsupported security stub class."""

    type: ClassVar[Literal["unsupported"]] = "unsupported"
    in_value: ClassVar[Literal["unsupported"]] = "unsupported"

    @classmethod
    def is_supported(cls, type: str, schema_parameters: dict[str, Any]) -> bool:
        return False

    def accept(self, security_params: "BaseSecurityParameters") -> bool:
        if isinstance(self, security_params.get_security_class()):
            raise RuntimeError("Trying to set UnsuportedSecurityStub params")
        return False

    class Parameters(BaseSecurityParameters):  # BaseSecurityParameters
        """API Key Header security parameters class."""

        def apply(
            self,
            q_params: dict[str, Any],
            body_dict: dict[str, Any],
            security: BaseSecurity,
        ) -> None:
            pass

        def get_security_class(self) -> type[BaseSecurity]:
            return UnsuportedSecurityStub

        def dump(self) -> dict[str, Any]:
            return {
                "type": "unsupported",
            }


class APIKeyHeader(BaseSecurity):
    """API Key Header security class."""

    type: ClassVar[Literal["apiKey"]] = "apiKey"
    in_value: ClassVar[Literal["header"]] = "header"

    class Parameters(BaseSecurityParameters):  # BaseSecurityParameters
        """API Key Header security parameters class."""

        value: str = "API_KEY"

        def apply(
            self,
            q_params: dict[str, Any],
            body_dict: dict[str, Any],
            security: BaseSecurity,
        ) -> None:
            api_key_header: APIKeyHeader = security  # type: ignore[assignment]

            if "headers" not in body_dict:
                body_dict["headers"] = {}

            body_dict["headers"][api_key_header.name] = self.value

        def get_security_class(self) -> type[BaseSecurity]:
            return APIKeyHeader

        def dump(self) -> dict[str, Any]:
            return {
                "type": "apiKey",
                "schema_parameters": {"in": "header"},
                **self.model_dump(),
            }


class APIKeyQuery(BaseSecurity):
    """API Key Query security class."""

    type: ClassVar[Literal["apiKey"]] = "apiKey"
    in_value: ClassVar[Literal["query"]] = "query"

    @classmethod
    def is_supported(cls, type: str, schema_parameters: dict[str, Any]) -> bool:
        return super().is_supported(type, schema_parameters)

    class Parameters(BaseSecurityParameters):  # BaseSecurityParameters
        """API Key Query security parameters class."""

        value: str = "API_KEY"

        def apply(
            self,
            q_params: dict[str, Any],
            body_dict: dict[str, Any],
            security: BaseSecurity,
        ) -> None:
            api_key_query: APIKeyQuery = security  # type: ignore[assignment]

            q_params[api_key_query.name] = self.value

        def get_security_class(self) -> type[BaseSecurity]:
            return APIKeyQuery

        def dump(self) -> dict[str, Any]:
            return {
                "type": "apiKey",
                "schema_parameters": {"in": "query"},
                **self.model_dump(),
            }


class APIKeyCookie(BaseSecurity):
    """API Key Cookie security class."""

    type: ClassVar[Literal["apiKey"]] = "apiKey"
    in_value: ClassVar[Literal["cookie"]] = "cookie"

    class Parameters(BaseSecurityParameters):  # BaseSecurityParameters
        """API Key Cookie security parameters class."""

        value: str = "API_KEY"

        def apply(
            self,
            q_params: dict[str, Any],
            body_dict: dict[str, Any],
            security: BaseSecurity,
        ) -> None:
            api_key_cookie: APIKeyCookie = security  # type: ignore[assignment]

            if "cookies" not in body_dict:
                body_dict["cookies"] = {}

            body_dict["cookies"][api_key_cookie.name] = self.value

        def get_security_class(self) -> type[BaseSecurity]:
            return APIKeyCookie

        def dump(self) -> dict[str, Any]:
            return {
                "type": "apiKey",
                "schema_parameters": {"in": "cookie"},
                **self.model_dump(),
            }


class HTTPBearer(BaseSecurity):
    """HTTP Bearer security class."""

    type: ClassVar[Literal["http"]] = "http"
    in_value: ClassVar[Literal["bearer"]] = "bearer"

    @classmethod
    def is_supported(cls, type: str, schema_parameters: dict[str, Any]) -> bool:
        return cls.type == type and cls.in_value == schema_parameters.get("scheme")

    class Parameters(BaseSecurityParameters):  # BaseSecurityParameters
        """HTTP Bearer security parameters class."""

        value: str = "BEARER_TOKEN"

        def apply(
            self,
            q_params: dict[str, Any],
            body_dict: dict[str, Any],
            security: BaseSecurity,
        ) -> None:
            if "headers" not in body_dict:
                body_dict["headers"] = {}

            body_dict["headers"]["Authorization"] = f"Bearer {self.value}"

        def get_security_class(self) -> type[BaseSecurity]:
            return HTTPBearer

        def dump(self) -> dict[str, Any]:
            return {
                "type": "http",
                "schema_parameters": {"scheme": "bearer"},
                **self.model_dump(),
            }


class HTTPBasic(BaseSecurity):
    """HTTP Bearer security class."""

    type: ClassVar[Literal["http"]] = "http"
    in_value: ClassVar[Literal["basic"]] = "basic"

    @classmethod
    def is_supported(cls, type: str, schema_parameters: dict[str, Any]) -> bool:
        return cls.type == type and cls.in_value == schema_parameters.get("scheme")

    class Parameters(BaseSecurityParameters):  # BaseSecurityParameters
        """HTTP Basic security parameters class."""

        username: str = "USERNAME"
        password: str = "PASSWORD"

        def apply(
            self,
            q_params: dict[str, Any],
            body_dict: dict[str, Any],
            security: BaseSecurity,
        ) -> None:
            if "headers" not in body_dict:
                body_dict["headers"] = {}

            credentials = f"{self.username}:{self.password}"
            encoded_credentials = base64.b64encode(credentials.encode("utf-8")).decode("utf-8")

            body_dict["headers"]["Authorization"] = f"Basic {encoded_credentials}"

        def get_security_class(self) -> type[BaseSecurity]:
            return HTTPBasic

        def dump(self) -> dict[str, Any]:
            return {
                "type": "http",
                "schema_parameters": {"scheme": "basic"},
                **self.model_dump(),
            }


class OAuth2PasswordBearer(BaseSecurity):
    """OAuth2 Password Bearer security class."""

    type: ClassVar[Literal["oauth2"]] = "oauth2"
    in_value: ClassVar[Literal["bearer"]] = "bearer"
    token_url: str

    @classmethod
    def is_supported(cls, type: str, schema_parameters: dict[str, Any]) -> bool:
        return type == cls.type and "password" in schema_parameters.get("flows", {})

    @classmethod
    def get_security_parameters(cls, schema_parameters: dict[str, Any]) -> str:
        name = schema_parameters.get("name")
        token_url = f"{schema_parameters.get('server_url')}/{schema_parameters['flows']['password']['tokenUrl']}"
        return f'{cls.__name__}(name="{name}", token_url="{token_url}")'

    class Parameters(BaseSecurityParameters):  # BaseSecurityParameters
        """OAuth2 Password Bearer security class."""

        username: str = "USERNAME"
        password: str = "PASSWORD"
        bearer_token: str | None = None
        token_url: str = "TOKEN_URL"

        # @model_validator(mode="before")
        # def check_credentials(cls, values: dict[str, Any]) -> Any:  # noqa
        #     username = values.get("username")
        #     password = values.get("password")
        #     bearer_token = values.get("bearer_token")

        #     if not bearer_token and (not username or not password):
        #         # If bearer_token is not provided, both username and password must be defined
        #         raise ValueError("Both username and password are required if bearer_token is not provided.")

        #     return values

        def get_token(self, token_url: str) -> str:
            # Get the token
            request = requests.post(
                token_url,
                data={
                    "username": self.username,
                    "password": self.password,
                },
                timeout=5,
            )
            request.raise_for_status()
            return request.json()["access_token"]  # type: ignore

        def apply(
            self,
            q_params: dict[str, Any],
            body_dict: dict[str, Any],
            security: BaseSecurity,
        ) -> None:
            if not self.bearer_token:
                if security.token_url is None:  # type: ignore
                    raise ValueError("Token URL is not defined")
                self.bearer_token = self.get_token(security.token_url)  # type: ignore

            if "headers" not in body_dict:
                body_dict["headers"] = {}

            body_dict["headers"]["Authorization"] = f"Bearer {self.bearer_token}"

        def get_security_class(self) -> type[BaseSecurity]:
            return OAuth2PasswordBearer

        def dump(self) -> dict[str, Any]:
            return {
                "type": "oauth2",
                "schema_parameters": {"flows": {"password": {"tokenUrl": self.token_url or ""}}},
                "username": self.username,
                "password": self.password,
                "bearer_token": self.bearer_token,
            }
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from autogen.import_utils import optional_import_block

from .patch_fastapi_code_generator import (  # noqa: E402
    SUCCESFUL_IMPORT,
    patch_function_name_parsing,
    patch_generate_code,
)

if SUCCESFUL_IMPORT:
    patch_function_name_parsing()
    patch_generate_code()

from .mcp_proxy import MCPProxy  # noqa: E402

__all__ = ["MCPProxy", "optional_import_block"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import logging
import re
from functools import cached_property, wraps
from typing import Any

from ...import_utils import optional_import_block, require_optional_import

with optional_import_block():
    import yaml

from autogen.import_utils import optional_import_block

with optional_import_block() as result:
    from fastapi_code_generator import __main__ as fastapi_code_generator_main
    from fastapi_code_generator.parser import OpenAPIParser, Operation

SUCCESFUL_IMPORT = result.is_successful

logger = logging.getLogger(__name__)


def patch_parse_schema() -> None:
    org_parse_schema = OpenAPIParser.parse_schema

    @wraps(org_parse_schema)
    def my_parse_schema(*args: Any, **kwargs: Any) -> Any:
        data_type = org_parse_schema(*args, **kwargs)
        if data_type.reference and data_type.reference.duplicate_name:
            data_type.reference.name = data_type.reference.duplicate_name
        return data_type

    OpenAPIParser.parse_schema = my_parse_schema
    logger.info("Patched OpenAPIParser.parse_schema")


def snakecase(string: str) -> str:
    string = re.sub(r"[\-\.\s]", "_", str(string))
    if not string:
        return string
    return string[0].lower() + re.sub(r"[A-Z]", lambda matched: "_" + (matched.group(0)), string[1:]).lower()


def patch_function_name_parsing() -> None:
    def function_name(self: Operation) -> str:
        if self.operationId:
            name: str = re.sub(r"[/{=}]", "_", self.operationId)
        else:
            path = re.sub(r"[/{=]", "_", self.snake_case_path).replace("}", "")
            name = f"{self.type}{path}"

        return snakecase(name)  # type: ignore[no-any-return]

    Operation.function_name = cached_property(function_name)
    Operation.function_name.__set_name__(Operation, "function_name")

    logger.info("Patched Operation.function_name")


@require_optional_import(["yaml"], "mcp-proxy-gen")
def patch_generate_code() -> None:
    # Save reference to the original generate_code function
    org_generate_code = fastapi_code_generator_main.generate_code

    @wraps(org_generate_code)
    def patched_generate_code(*args: Any, **kwargs: Any) -> Any:
        try:
            input_text: str = kwargs["input_text"]

            json_spec = yaml.safe_load(input_text)

            schemas_with_dots = sorted(
                [name for name in json_spec.get("components", {}).get("schemas", {}) if "." in name],
                key=len,
                reverse=True,  # Sort by length in descending order
            )

            for schema_name in schemas_with_dots:
                new_schema_name = schema_name.replace(".", "_")
                input_text = input_text.replace(schema_name, new_schema_name)

            kwargs["input_text"] = input_text

        except Exception as e:
            print(
                f"Patched fastapi_code_generator.__main__.generate_code raised: {e}, passing untouched arguments to original generate_code"
            )
            logger.info(
                f"Patched fastapi_code_generator.__main__.generate_code raised: {e}, passing untouched arguments to original generate_code"
            )

        return org_generate_code(*args, **kwargs)

    fastapi_code_generator_main.generate_code = patched_generate_code

    logger.info("Patched fastapi_code_generator.__main__.generate_code")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import logging
from pathlib import Path

from autogen.import_utils import optional_import_block

with optional_import_block() as result:
    from fastapi_code_generator.parser import OpenAPIParser, Operation
    from fastapi_code_generator.visitor import Visitor

from autogen.agentchat.conversable_agent import ConversableAgent

logger = logging.getLogger(__name__)

# System prompt to guide the AI agent in naming functions
SYSTEM_MESSAGE = (
    "You are a helpful expert Python programmer. Your task is to generate a clear, concise, "
    "and descriptive name for a Python function based on a user-provided summary.\n"
    "- Only return the new function name.\n"
    "- The name must be fewer than 64 characters.\n"
    "- It should reflect the purpose of the function as described.\n"
    "- You will be provided with a list of already-taken names, which must not be reused.\n"
    "- The function name should be in snake_case.\n"
)


def validate_function_name(name: str, taken_names: list[str]) -> str:
    """Validate the generated function name against length, format, and uniqueness constraints.

    Returns:
        'exit' if the name is valid, or an error message string otherwise.
    """
    if len(name) > 64:
        return "Function name is too long. Please provide a shorter name."
    if not name.islower() or " " in name:
        return "Function name must be in snake_case."
    if name in taken_names:
        return f"Function name is already taken. Please provide a different name. Taken names: {', '.join(taken_names)}"
    return "exit"


def get_new_function_name(operation: "Operation", taken_names: list[str]) -> str:
    """Ask an AI agent to generate a new function name for a given OpenAPI operation.

    Args:
        operation: The OpenAPI operation that needs renaming.
        taken_names: A list of names already used, to avoid collisions.

    Returns:
        A new, validated function name.
    """
    agent = ConversableAgent(
        name="helpful_agent",
        system_message=SYSTEM_MESSAGE,
    )

    response = agent.run(
        message=(
            "How would you name this function? \n"
            f"Info:\n"
            f"- old function name: {operation.function_name}\n"
            f"- function summary: {operation.summary}\n"
            f"- function arguments: {operation.arguments}\n"
        ),
        user_input=True,
    )

    proposed_name = None

    for event in response.events:
        if event.type == "text" and event.content.sender == "helpful_agent":
            proposed_name = event.content.content.strip()
        elif event.type == "input_request":
            reply = (
                validate_function_name(proposed_name, taken_names)
                if proposed_name
                else "Please provide a function name."
            )
            event.content.respond(reply)

    logger.warning(f"Renamed operation '{operation.function_name}' to '{response.summary}'.")
    return response.summary


def custom_visitor(parser: "OpenAPIParser", model_path: Path) -> dict[str, object]:
    """Visits and optionally renames operations in the OpenAPI parser.

    Args:
        parser: An OpenAPIParser instance containing API operations.
        model_path: Path to the model (not used in this implementation).

    Returns:
        A dictionary containing the updated list of operations.
    """
    operations = sorted(parser.operations.values(), key=lambda op: op.path)
    taken_names = []

    for op in operations:
        if len(op.function_name) > 64:
            new_name = get_new_function_name(op, taken_names)
            op.function_name = new_name
        taken_names.append(op.function_name)

    return {"operations": operations}


visit: "Visitor" = custom_visitor
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import builtins
import importlib
import inspect
import json
import re
import sys
import tempfile
from collections.abc import Callable, Iterable, Iterator, Mapping
from contextlib import contextmanager
from functools import wraps
from logging import getLogger
from pathlib import Path
from types import ModuleType
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
)

import requests
from pydantic import PydanticInvalidForJsonSchema
from pydantic_core import PydanticUndefined

from autogen.import_utils import optional_import_block, require_optional_import

from .security import BaseSecurity, BaseSecurityParameters

with optional_import_block() as result:
    import fastapi
    import yaml
    from datamodel_code_generator import DataModelType
    from fastapi_code_generator.__main__ import generate_code
    from jinja2 import Environment, FileSystemLoader
    from mcp.server.fastmcp import FastMCP


if TYPE_CHECKING:
    from autogen.agentchat import ConversableAgent

__all__ = ["MCPProxy"]

logger = getLogger(__name__)


@contextmanager
def optional_temp_path(path: str | None = None) -> Iterator[Path]:
    if path is None:
        with tempfile.TemporaryDirectory() as temp_dir:
            yield Path(temp_dir)
    else:
        yield Path(path)


@contextmanager
def add_to_builtins(new_globals: dict[str, Any]) -> Iterator[None]:
    old_globals = {key: getattr(builtins, key, None) for key in new_globals}

    try:
        for key, value in new_globals.items():
            setattr(builtins, key, value)  # Inject new global
        yield
    finally:
        for key, value in old_globals.items():
            if value is None:
                delattr(builtins, key)  # Remove added globals
            else:
                setattr(builtins, key, value)  # Restore original value


class MCPProxy:
    def __init__(self, servers: list[dict[str, Any]], title: str | None = None, **kwargs: Any) -> None:
        """Proxy class to generate client from OpenAPI schema."""
        self._servers = servers
        self._title = title or "MCP Proxy"
        self._kwargs = kwargs
        self._registered_funcs: list[Callable[..., Any]] = []
        self._globals: dict[str, Any] = {}

        self._security: dict[str, list[BaseSecurity]] = {}
        self._security_params: dict[str | None, BaseSecurityParameters] = {}
        self._tags: set[str] = set()

        self._function_group: dict[str, list[str]] = {}

    @staticmethod
    def _convert_camel_case_within_braces_to_snake(text: str) -> str:
        # Function to convert camel case to snake case
        def camel_to_snake(match: re.Match[str]) -> str:
            return re.sub(r"(?<!^)(?=[A-Z])", "_", match.group(1)).lower()

        # Find all occurrences inside curly braces and apply camel_to_snake
        result = re.sub(r"\{([a-zA-Z0-9]+)\}", lambda m: "{" + camel_to_snake(m) + "}", text)

        return result

    @staticmethod
    def _get_params(path: str, func: Callable[..., Any]) -> tuple[set[str], set[str], str | None, bool]:
        sig = inspect.signature(func)

        params_names = set(sig.parameters.keys())

        path_params = set(re.findall(r"\{(.+?)\}", path))
        if not path_params.issubset(params_names):
            raise ValueError(f"Path params {path_params} not in {params_names}")

        body = "body" if "body" in params_names else None

        security = "security" in params_names

        q_params = set(params_names) - path_params - {body} - {"security"}

        return q_params, path_params, body, security

    def get_mcp(self, **settings: Any) -> "FastMCP":
        mcp = FastMCP(name=self._title, **settings)  # newer mcp

        for func in self._registered_funcs:
            try:
                mcp.tool()(func)
            except PydanticInvalidForJsonSchema as e:
                logger.warning("Could not register function %s: %s", func.__name__, e)

        return mcp

    def _process_params(
        self, process_path: str, func: Callable[[Any], Any], **kwargs: Any
    ) -> tuple[str, dict[str, Any], dict[str, Any]]:
        process_path = MCPProxy._convert_camel_case_within_braces_to_snake(process_path)
        q_params, path_params, body, security = MCPProxy._get_params(process_path, func)

        expanded_path = process_path.format(**{p: kwargs[p] for p in path_params})

        url = self._servers[0]["url"] + expanded_path

        body_dict = {}
        if body and body in kwargs:
            body_value = kwargs[body]
            if isinstance(body_value, dict):
                body_dict = {"json": body_value}
            elif hasattr(body_value, "model_dump"):
                body_dict = {"json": body_value.model_dump()}
            else:
                body_dict = {"json": body_value.dict()}

        body_dict["headers"] = {"Content-Type": "application/json"}
        if security:
            q_params, body_dict = kwargs["security"].add_security(q_params, body_dict)
            # body_dict["headers"][security] = kwargs["security"]

        params = {k: v for k, v in kwargs.items() if k in q_params}

        return url, params, body_dict

    def set_security_params(self, security_params: BaseSecurityParameters, name: str | None = None) -> None:
        if name is not None:
            security = self._security.get(name)
            if security is None:
                raise ValueError(f"Security is not set for '{name}'")

            for match_security in security:
                if match_security.accept(security_params):
                    break
            else:
                raise ValueError(f"Security parameters {security_params} do not match security {security}")

        self._security_params[name] = security_params

    def _get_matching_security(
        self, security: list[BaseSecurity], security_params: BaseSecurityParameters
    ) -> BaseSecurity:
        # check if security matches security parameters
        for match_security in security:
            if match_security.accept(security_params):
                return match_security
        raise ValueError(f"Security parameters {security_params} does not match any given security {security}")

    def _get_security_params(self, name: str) -> tuple[BaseSecurityParameters | None, BaseSecurity | None]:
        # check if security is set for the method
        security = self._security.get(name)
        if not security:
            return None, None

        security_params = self._security_params.get(name)
        if security_params is None:
            # check if default security parameters are set
            security_params = self._security_params.get(None)
            if security_params is None:
                raise ValueError(
                    f"Security parameters are not set for {name} and there are no default security parameters"
                )

        match_security = self._get_matching_security(security, security_params)

        return security_params, match_security

    def _request(
        self,
        method: Literal["put", "get", "post", "head", "delete", "patch"],
        path: str,
        description: str | None = None,
        security: list[BaseSecurity] | None = None,
        **kwargs: Any,
    ) -> Callable[..., dict[str, Any]]:
        def decorator(func: Callable[..., Any]) -> Callable[..., dict[str, Any]]:
            name = func.__name__

            for tag in kwargs.get("tags", []):
                if tag not in self._function_group:
                    self._function_group[tag] = []
                self._function_group[tag].append(name)

            if security is not None:
                self._security[name] = security

            @wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> dict[str, Any]:
                url, params, body_dict = self._process_params(path, func, **kwargs)

                security = self._security.get(name)
                if security is not None:
                    security_params, matched_security = self._get_security_params(name)
                    if security_params is None:
                        raise ValueError(f"Security parameters are not set for '{name}'")
                    else:
                        security_params.apply(params, body_dict, matched_security)  # type: ignore [arg-type]

                response = getattr(requests, method)(url, params=params, **body_dict)
                return response.json()  # type: ignore [no-any-return]

            wrapper._description = (  # type: ignore [attr-defined]
                description or func.__doc__.strip() if func.__doc__ is not None else None
            )

            self._registered_funcs.append(wrapper)

            return wrapper

        return decorator  # type: ignore [return-value]

    def put(self, path: str, **kwargs: Any) -> Callable[..., dict[str, Any]]:
        return self._request("put", path, **kwargs)

    def get(self, path: str, **kwargs: Any) -> Callable[..., dict[str, Any]]:
        return self._request("get", path, **kwargs)

    def post(self, path: str, **kwargs: Any) -> Callable[..., dict[str, Any]]:
        return self._request("post", path, **kwargs)

    def delete(self, path: str, **kwargs: Any) -> Callable[..., dict[str, Any]]:
        return self._request("delete", path, **kwargs)

    def head(self, path: str, **kwargs: Any) -> Callable[..., dict[str, Any]]:
        return self._request("head", path, **kwargs)

    def patch(self, path: str, **kwargs: Any) -> Callable[..., dict[str, Any]]:
        return self._request("patch", path, **kwargs)

    @classmethod
    def _get_template_dir(cls) -> Path:
        path = Path(__file__).parents[3] / "templates"
        if not path.exists():
            raise RuntimeError(f"Template directory {path.resolve()} not found.")
        return path

    @classmethod
    @require_optional_import(["datamodel_code_generator", "fastapi_code_generator"], "mcp-proxy-gen")
    def generate_code(
        cls,
        input_text: str,
        output_dir: Path,
        disable_timestamp: bool = False,
        custom_visitors: list[Path] | None = None,
    ) -> str:
        if custom_visitors is None:
            custom_visitors = []
        custom_visitors.append(Path(__file__).parent / "security_schema_visitor.py")

        # with patch_get_parameter_type():
        generate_code(
            input_name="openapi.yaml",
            input_text=input_text,
            encoding="utf-8",
            output_dir=output_dir,
            template_dir=cls._get_template_dir() / "client_template",
            disable_timestamp=disable_timestamp,
            custom_visitors=custom_visitors,
            output_model_type=DataModelType.PydanticV2BaseModel,
        )

        main_path = output_dir / "main.py"

        with main_path.open("r") as f:
            main_py_code = f.read()
        # main_py_code = main_py_code.replace("from .models import", "from models import")
        main_py_code = main_py_code.replace("from .models", "from models")
        # Removing "from __future__ import annotations" to avoid ForwardRef issues, should be fixed in fastapi_code_generator
        main_py_code = main_py_code.replace("from __future__ import annotations", "")

        with main_path.open("w") as f:
            f.write(main_py_code)

        return main_path.stem

    def set_globals(self, main: ModuleType, suffix: str) -> None:
        xs = {k: v for k, v in main.__dict__.items() if not k.startswith("__")}
        self._globals = {
            k: v for k, v in xs.items() if hasattr(v, "__module__") and v.__module__ in [f"models_{suffix}", "typing"]
        }

    @classmethod
    @require_optional_import(["yaml"], "mcp-proxy-gen")
    def create(
        cls,
        *,
        openapi_specification: str | None = None,
        openapi_url: str | None = None,
        client_source_path: str | None = None,
        servers: list[dict[str, Any]] | None = None,
        rename_functions: bool = False,
        group_functions: bool = False,
        configuration_type: Literal["json", "yaml"] = "json",
    ) -> "MCPProxy":
        if (openapi_specification is None) == (openapi_url is None):
            raise ValueError("Either openapi_specification or openapi_url should be provided")

        if openapi_specification is None and openapi_url is not None:
            with requests.get(openapi_url, timeout=10) as response:
                response.raise_for_status()
                openapi_specification = response.text

        openapi_parsed = (
            json.loads(openapi_specification) if configuration_type == "json" else yaml.safe_load(openapi_specification)
        )  # type: ignore [arg-type]

        if servers:
            openapi_parsed["servers"] = servers

        yaml_friendly = yaml.safe_dump(openapi_parsed)

        with optional_temp_path(client_source_path) as td:
            suffix = td.name  # noqa F841

            custom_visitors = []

            if rename_functions:
                custom_visitors.append(Path(__file__).parent / "operation_renaming.py")

            if group_functions:
                custom_visitors.append(Path(__file__).parent / "operation_grouping.py")

            main_name = cls.generate_code(  # noqa F841
                input_text=yaml_friendly,  # type: ignore [arg-type]
                output_dir=td,
                custom_visitors=custom_visitors,
            )
            # add td to sys.path
            try:
                sys.path.append(str(td))
                main = importlib.import_module(main_name, package=td.name)  # nosemgrep
            finally:
                sys.path.remove(str(td))

            client: MCPProxy = main.app  # type: ignore [attr-defined]
            client.set_globals(main, suffix=suffix)

            client.dump_configurations(output_dir=td)

            return client

    def _get_authentications(self) -> list[dict[str, Any]]:
        seen = set()
        authentications = []

        for security_list in self._security.values():
            for security in security_list:
                params = security.Parameters().dump()

                if params.get("type") == "unsupported":
                    continue

                dumped = json.dumps(params)  # hashable
                if dumped not in seen:
                    seen.add(dumped)
                    authentications.append(security.Parameters().dump())
        return authentications

    def dump_configurations(self, output_dir: Path) -> None:
        for tag in self._function_group:
            output_file = output_dir / f"mcp_config_{tag}.json"

            functions = [
                registered_function
                for registered_function in self._registered_funcs
                if registered_function.__name__ in self._function_group[tag]
            ]

            self.dump_configuration(output_file, functions)

        self.dump_configuration(output_dir / "mcp_config.json", self._registered_funcs)

    def dump_configuration(self, output_file: Path, functions: list[Callable[..., Any]] = None) -> None:
        # Define paths
        template_dir = MCPProxy._get_template_dir() / "config_template"
        template_file = "config.jinja2"

        # Load Jinja environment
        env = Environment(loader=FileSystemLoader(template_dir), trim_blocks=True, lstrip_blocks=True)

        # Load the template
        template = env.get_template(template_file)
        # Prepare context for rendering
        context = {
            "server_url": self._servers[0]["url"],  # single or list depending on your structure
            "authentications": self._get_authentications(),  # list of auth blocks, we will also need to check _security_params
            "operations": [
                {
                    "name": op.__name__,
                    "description": op._description.replace("\n", " ").replace("\r", "").strip()
                    if op._description is not None
                    else "",
                }
                for op in functions
            ],
        }

        # Render the template
        rendered_config = template.render(context)

        # Save the output to a file
        with open(output_file, "w") as f:
            f.write(rendered_config)

    def load_configuration(self, config_file: str) -> None:
        with Path(config_file).open("r") as f:
            config_data_str = f.read()

        self.load_configuration_from_string(config_data_str)

    def load_configuration_from_string(self, config_data_str: str) -> None:
        config_data = json.loads(config_data_str)
        # Load server URL
        self._servers = [{"url": config_data["server"]["url"]}]

        # Load authentication
        for auth in config_data.get("authentication", []):
            security = BaseSecurity.parse_security_parameters(auth)
            self.set_security_params(security)

        operation_names = [op["name"] for op in config_data.get("operations", [])]

        self._registered_funcs = [func for func in self._registered_funcs if func.__name__ in operation_names]

    def _get_functions_to_register(
        self,
        functions: Iterable[str | Mapping[str, Mapping[str, str]]] | None = None,
    ) -> dict[Callable[..., Any], dict[str, str | None]]:
        if functions is None:
            return {
                f: {
                    "name": None,
                    "description": f._description if hasattr(f, "_description") else None,
                }
                for f in self._registered_funcs
            }

        functions_with_name_desc: dict[str, dict[str, str | None]] = {}

        for f in functions:
            if isinstance(f, str):
                functions_with_name_desc[f] = {"name": None, "description": None}
            elif isinstance(f, dict):
                functions_with_name_desc.update({
                    k: {
                        "name": v.get("name", None),
                        "description": v.get("description", None),
                    }
                    for k, v in f.items()
                })
            else:
                raise ValueError(f"Invalid type {type(f)} for function {f}")

        funcs_to_register: dict[Callable[..., Any], dict[str, str | None]] = {
            f: functions_with_name_desc[f.__name__]
            for f in self._registered_funcs
            if f.__name__ in functions_with_name_desc
        }
        missing_functions = set(functions_with_name_desc.keys()) - {f.__name__ for f in funcs_to_register}
        if missing_functions:
            raise ValueError(f"Following functions {missing_functions} are not valid functions")

        return funcs_to_register

    @staticmethod
    def _remove_pydantic_undefined_from_tools(
        tools: list[dict[str, Any]],
    ) -> list[dict[str, Any]]:
        for tool in tools:
            if "function" not in tool:
                continue

            function = tool["function"]
            if "parameters" not in function or "properties" not in function["parameters"]:
                continue

            required = function["parameters"].get("required", [])
            for param_name, param_value in function["parameters"]["properties"].items():
                if "default" not in param_value:
                    continue

                default = param_value.get("default")
                if (
                    isinstance(default, (fastapi.params.Path, fastapi.params.Query))
                    and param_value["default"].default is PydanticUndefined
                ):
                    param_value.pop("default")
                    # We removed the default value, so we need to add the parameter to the required list
                    if param_name not in required:
                        required.append(param_name)

        return tools

    def _register_for_llm(
        self,
        agent: "ConversableAgent",
        functions: Iterable[str | Mapping[str, Mapping[str, str]]] | None = None,
    ) -> None:
        funcs_to_register = self._get_functions_to_register(functions)

        with add_to_builtins(
            new_globals=self._globals,
        ):
            for f, v in funcs_to_register.items():
                agent.register_for_llm(name=v["name"], description=v["description"])(f)

            agent.llm_config["tools"] = MCPProxy._remove_pydantic_undefined_from_tools(agent.llm_config["tools"])

    def _register_for_execution(
        self,
        agent: "ConversableAgent",
        functions: Iterable[str | Mapping[str, Mapping[str, str]]] | None = None,
    ) -> None:
        funcs_to_register = self._get_functions_to_register(functions)

        for f, v in funcs_to_register.items():
            agent.register_for_execution(name=v["name"])(f)

    def get_functions(self) -> list[str]:
        raise DeprecationWarning("Use function_names property instead of get_functions method")

    @property
    def function_names(self) -> list[str]:
        return [f.__name__ for f in self._registered_funcs]

    def get_function(self, name: str) -> Callable[..., dict[str, Any]]:
        for f in self._registered_funcs:
            if f.__name__ == name:
                return f
        raise ValueError(f"Function {name} not found")

    def set_function(self, name: str, func: Callable[..., dict[str, Any]]) -> None:
        for i, f in enumerate(self._registered_funcs):
            if f.__name__ == name:
                self._registered_funcs[i] = func
                return

        raise ValueError(f"Function {name} not found")

    def inject_parameters(self, name: str, **kwargs: Any) -> None:
        raise NotImplementedError("Injecting parameters is not implemented yet")
        # for f in self._registered_funcs:
        #     if f.__name__ == name:
        #         return

        # raise ValueError(f"Function {name} not found")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import asyncio
import os
import signal
from asyncio.subprocess import PIPE, Process, create_subprocess_exec
from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager


@asynccontextmanager
async def run_streamable_http_client(
    *, mcp_server_path: str, env_vars: dict[str, str] | None = None, startup_wait_secs: float = 5.0
) -> AsyncGenerator[Process, None]:
    """Async context manager to run a Python subprocess for streamable-http with custom env vars.

    Args:
        mcp_server_path: Path to the Python script to run.
        env_vars: Environment variables to export to the subprocess.
        startup_wait_secs: Time to wait for the server to start (in seconds).

    Yields:
        An asyncio.subprocess.Process object.
    """
    env = os.environ.copy()
    if env_vars:
        env.update(env_vars)

    process = await create_subprocess_exec(
        "python", mcp_server_path, "streamable-http", env=env, stdout=PIPE, stderr=PIPE
    )

    # Optional startup delay to let the server initialize
    await asyncio.sleep(startup_wait_secs)

    try:
        yield process
    finally:
        if process.returncode is None:
            process.send_signal(signal.SIGINT)
            try:
                await asyncio.wait_for(process.wait(), timeout=5.0)
            except asyncio.TimeoutError:
                process.kill()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import logging
from typing import Annotated, Literal

from .. import __version__
from ..import_utils import optional_import_block, require_optional_import
from .mcp_proxy import MCPProxy

logger = logging.getLogger(__name__)

with optional_import_block():
    import typer


@require_optional_import(["typer"], "mcp-proxy-gen")
def create_typer_app() -> "typer.Typer":
    """Create a Typer app for the mcp proxy CLI."""
    app = typer.Typer(rich_markup_mode="rich")

    def version_callback(value: bool) -> None:
        if value:
            typer.echo(f"{__version__}")
            raise typer.Exit()

    @app.callback()
    def callback(
        version: Annotated[
            bool | None,
            typer.Option("--version", help="Show the version and exit.", callback=version_callback),
        ] = None,
    ) -> None:
        """AG2 mcp proxy CLI - The [bold]mcp proxy[/bold] command line app. 

        Generate mcp proxy for your [bold]AG2[/bold] projects.

        Read more in the docs: ...
        """  # noqa: D415

    @app.command()
    def create(
        openapi_specification: Annotated[
            str | None,
            "Specification of the OpenAPI to use for the proxy generation.",
        ] = None,
        openapi_url: Annotated[
            str | None,
            "URL to the OpenAPI specification to use for the proxy generation.",
        ] = None,
        client_source_path: Annotated[
            str | None,
            "Path to the generated proxy client source code.",
        ] = None,
        server_url: Annotated[
            str | None,
            "Comma-separated list of server URLs to use for the proxy generation.",
        ] = None,
        configuration_type: Annotated[
            Literal["json", "yaml"],
            "Configuration type of the specification. Can be 'json' or 'yaml'.",
        ] = "json",
    ) -> None:
        """Generate mcp proxy for your AG2 projects."""
        MCPProxy.create(
            openapi_specification=openapi_specification,
            openapi_url=openapi_url,
            client_source_path=client_source_path,
            servers=[{"url": server_url}],
            configuration_type=configuration_type,
        )

    return app


if __name__ == "__main__":
    app = create_typer_app()
    app(prog_name="mcp_proxy")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

from typing import Any, Literal, TypedDict

MessageContentType = str | list[dict[str, Any] | str] | None


class UserMessageTextContentPart(TypedDict):
    """Represents a text content part of a user message"""

    type: Literal["text"]
    """The type of the content part. Always "text" for text content parts."""
    text: str
    """The text content of the part."""


class UserMessageImageContentPart(TypedDict):
    """Represents an image content part of a user message"""

    type: Literal["image_url"]
    """The type of the content part. Always "image_url" for image content parts."""
    # Ignoring the other "detail param for now"
    image_url: dict[Literal["url"], str]
    """The URL of the image."""
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

import json
import logging
import re
from typing import Any

import tiktoken

from .agentchat.contrib.img_utils import num_tokens_from_gpt_image
from .import_utils import optional_import_block

# if PIL is not imported, we will redefine num_tokens_from_gpt_image to return 0 tokens for images
# Otherwise, it would raise an ImportError
with optional_import_block() as result:
    import PIL  # noqa: F401

pil_imported = result.is_successful
if not pil_imported:

    def num_tokens_from_gpt_image(*args, **kwargs):
        return 0


logger = logging.getLogger(__name__)
logger.img_dependency_warned = False  # member variable to track if the warning has been logged


def get_max_token_limit(model: str = "gpt-3.5-turbo-0613") -> int:
    # Handle common azure model names/aliases
    model = re.sub(r"^gpt\-?35", "gpt-3.5", model)
    model = re.sub(r"^gpt4", "gpt-4", model)

    max_token_limit = {
        "gpt-3.5-turbo": 16385,
        "gpt-3.5-turbo-0125": 16385,
        "gpt-3.5-turbo-0301": 4096,
        "gpt-3.5-turbo-0613": 4096,
        "gpt-3.5-turbo-instruct": 4096,
        "gpt-3.5-turbo-16k": 16385,
        "gpt-3.5-turbo-16k-0613": 16385,
        "gpt-3.5-turbo-1106": 16385,
        "gpt-4": 8192,
        "gpt-4-turbo": 128000,
        "gpt-4-turbo-2024-04-09": 128000,
        "gpt-4-32k": 32768,
        "gpt-4-32k-0314": 32768,  # deprecate in Sep
        "gpt-4-0314": 8192,  # deprecate in Sep
        "gpt-4-0613": 8192,
        "gpt-4-32k-0613": 32768,
        "gpt-4-1106-preview": 128000,
        "gpt-4-0125-preview": 128000,
        "gpt-4-turbo-preview": 128000,
        "gpt-4-vision-preview": 128000,
        "gpt-4o": 128000,
        "gpt-4o-2024-05-13": 128000,
        "gpt-4o-2024-08-06": 128000,
        "gpt-4o-2024-11-20": 128000,
        "gpt-4o-mini": 128000,
        "gpt-4o-mini-2024-07-18": 128000,
        "gpt-5": 128000,
        "gpt-5-mini": 128000,
        "gpt-5-nano": 128000,
    }
    return max_token_limit[model]


def percentile_used(input, model="gpt-3.5-turbo-0613"):
    return count_token(input) / get_max_token_limit(model)


def token_left(input: str | list[str] | dict[str, Any], model="gpt-3.5-turbo-0613") -> int:
    """Count number of tokens left for an OpenAI model.

    Args:
        input: (str, list, dict): Input to the model.
        model: (str): Model name.

    Returns:
        int: Number of tokens left that the model can use for completion.
    """
    return get_max_token_limit(model) - count_token(input, model=model)


def count_token(input: str | list[str] | dict[str, Any], model: str = "gpt-3.5-turbo-0613") -> int:
    """Count number of tokens used by an OpenAI model.

    Args:
        input: (str, list, dict): Input to the model.
        model: (str): Model name.

    Returns:
        int: Number of tokens from the input.
    """
    if isinstance(input, str):
        return _num_token_from_text(input, model=model)
    elif isinstance(input, (list, dict)):
        return _num_token_from_messages(input, model=model)
    else:
        raise ValueError(f"input must be str, list or dict, but we got {type(input)}")


def _num_token_from_text(text: str, model: str = "gpt-3.5-turbo-0613"):
    """Return the number of tokens used by a string."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        logger.warning(f"Model {model} not found. Using cl100k_base encoding.")
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))


def _num_token_from_messages(messages: list[str] | dict[str, Any], model="gpt-3.5-turbo-0613"):
    """Return the number of tokens used by a list of messages.

    retrieved from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb/
    """
    if isinstance(messages, dict):
        messages = [messages]

    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        logger.warning(f"Model {model} not found. Using cl100k_base encoding.")
        encoding = tiktoken.get_encoding("cl100k_base")
    if model in {
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-16k-0613",
        "gpt-4-0314",
        "gpt-4-32k-0314",
        "gpt-4-0613",
        "gpt-4-32k-0613",
        "gpt-4-turbo-preview",
        "gpt-4-vision-preview",
        "gpt-4o",
        "gpt-4o-2024-05-13",
        "gpt-4o-2024-08-06",
        "gpt-4o-2024-11-20",
        "gpt-4o-mini",
        "gpt-4o-mini-2024-07-18",
    }:
        tokens_per_message = 3
        tokens_per_name = 1
    elif model == "gpt-3.5-turbo-0301":
        tokens_per_message = 4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
        tokens_per_name = -1  # if there's a name, the role is omitted
    elif "gpt-3.5-turbo" in model:
        logger.info("gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.")
        return _num_token_from_messages(messages, model="gpt-3.5-turbo-0613")
    elif "gpt-4" in model:
        logger.info("gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.")
        return _num_token_from_messages(messages, model="gpt-4-0613")
    elif "gemini" in model:
        logger.info("Gemini is not supported in tiktoken. Returning num tokens assuming gpt-4-0613.")
        return _num_token_from_messages(messages, model="gpt-4-0613")
    elif "claude" in model:
        logger.info("Claude is not supported in tiktoken. Returning num tokens assuming gpt-4-0613.")
        return _num_token_from_messages(messages, model="gpt-4-0613")
    elif "mistral-" in model or "mixtral-" in model:
        logger.info("Mistral.AI models are not supported in tiktoken. Returning num tokens assuming gpt-4-0613.")
        return _num_token_from_messages(messages, model="gpt-4-0613")
    elif "deepseek" in model:
        logger.info("Deepseek models are not supported in tiktoken. Returning num tokens assuming gpt-4-0613.")
        return _num_token_from_messages(messages, model="gpt-4-0613")
    else:
        raise NotImplementedError(
            f"""_num_token_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."""
        )
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            if value is None:
                continue

            # handle content if images are in GPT-4-vision
            if key == "content" and isinstance(value, list):
                for part in value:
                    if not isinstance(part, dict) or "type" not in part:
                        continue
                    if part["type"] == "text":
                        num_tokens += len(encoding.encode(part["text"]))
                    if "image_url" in part:
                        assert "url" in part["image_url"]
                        if not pil_imported and not logger.img_dependency_warned:
                            logger.warning(
                                "img_utils or PIL not imported. Skipping image token count."
                                "Please install autogen with [lmm] option.",
                            )
                            logger.img_dependency_warned = True
                        is_low_quality = "detail" in part["image_url"] and part["image_url"]["detail"] == "low"
                        try:
                            num_tokens += num_tokens_from_gpt_image(
                                image_data=part["image_url"]["url"], model=model, low_quality=is_low_quality
                            )
                        except ValueError as e:
                            logger.warning(f"Error in num_tokens_from_gpt_image: {e}")
                continue

            # function calls
            if not isinstance(value, str):
                try:
                    value = json.dumps(value)
                except TypeError:
                    logger.warning(
                        f"Value {value} is not a string and cannot be converted to json. It is a type: {type(value)} Skipping."
                    )
                    continue

            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
    return num_tokens


def num_tokens_from_functions(functions, model="gpt-3.5-turbo-0613") -> int:
    """Return the number of tokens used by a list of functions.

    Args:
        functions: (list): List of function descriptions that will be passed in model.
        model: (str): Model name.

    Returns:
        int: Number of tokens from the function descriptions.
    """
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        logger.warning(f"Model {model} not found. Using cl100k_base encoding.")
        encoding = tiktoken.get_encoding("cl100k_base")

    num_tokens = 0
    for function in functions:
        function_tokens = len(encoding.encode(function["name"]))
        function_tokens += len(encoding.encode(function["description"]))
        function_tokens -= 2
        if "parameters" in function:
            parameters = function["parameters"]
            if "properties" in parameters:
                for properties_key in parameters["properties"]:
                    function_tokens += len(encoding.encode(properties_key))
                    v = parameters["properties"][properties_key]
                    for field in v:
                        if field == "type":
                            function_tokens += 2
                            function_tokens += len(encoding.encode(v["type"]))
                        elif field == "description":
                            function_tokens += 2
                            function_tokens += len(encoding.encode(v["description"]))
                        elif field == "enum":
                            function_tokens -= 3
                            for o in v["enum"]:
                                function_tokens += 3
                                function_tokens += len(encoding.encode(o))
                        else:
                            logger.warning(f"Not supported field {field}")
                function_tokens += 11
                if len(parameters["properties"]) == 0:
                    function_tokens -= 2

        num_tokens += function_tokens

    num_tokens += 12
    return num_tokens
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

__all__: list[str] = []
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create an OpenAI-compatible client using Cerebras's API.

Example:
    ```python
    llm_config = {
        "config_list": [{"api_type": "cerebras", "model": "llama3.1-8b", "api_key": os.environ.get("CEREBRAS_API_KEY")}]
    }

    agent = autogen.AssistantAgent("my_agent", llm_config=llm_config)
    ```

Install Cerebras's python library using: pip install --upgrade cerebras_cloud_sdk

Resources:
- https://inference-docs.cerebras.ai/quickstart
"""

from __future__ import annotations

import copy
import math
import os
import time
import warnings
from typing import Any, Literal

from pydantic import Field
from typing_extensions import Unpack

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import should_hide_tools, validate_parameter
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    from cerebras.cloud.sdk import Cerebras, Stream

CEREBRAS_PRICING_1K = {
    # Convert pricing per million to per thousand tokens.
    "llama3.1-8b": (0.10 / 1000, 0.10 / 1000),
    "llama-3.3-70b": (0.85 / 1000, 1.20 / 1000),
}


class CerebrasEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["cerebras"]

    seed: int | None
    stream: bool
    hide_tools: Literal["if_all_run", "if_any_run", "never"]
    tool_choice: Literal["none", "auto", "required"] | None


class CerebrasLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["cerebras"] = "cerebras"

    temperature: float | None = Field(default=None, ge=0.0, le=1.5)

    seed: int | None = None
    stream: bool = False
    hide_tools: Literal["if_all_run", "if_any_run", "never"] = "never"
    tool_choice: Literal["none", "auto", "required"] | None = None
    reasoning_effort: str | None = None

    def create_client(self):
        raise NotImplementedError("CerebrasLLMConfigEntry.create_client is not implemented.")


class CerebrasClient:
    """Client for Cerebras's API."""

    def __init__(self, api_key=None, **kwargs: Unpack[CerebrasEntryDict]):
        """Requires api_key or environment variable to be set

        Args:
            api_key (str): The API key for using Cerebras (or environment variable CEREBRAS_API_KEY needs to be set)
            **kwargs: Additional keyword arguments to pass to the Cerebras client
        """
        # Ensure we have the api_key upon instantiation
        self.api_key = api_key or os.getenv("CEREBRAS_API_KEY")

        assert self.api_key, (
            "Please include the api_key in your config list entry for Cerebras or set the CEREBRAS_API_KEY env variable."
        )

        if "response_format" in kwargs and kwargs["response_format"] is not None:
            warnings.warn("response_format is not supported for Cerebras, it will be ignored.", UserWarning)

    def message_retrieval(self, response: ChatCompletion) -> list:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        return [choice.message for choice in response.choices]

    def cost(self, response: ChatCompletion) -> float:
        # Note: This field isn't explicitly in `ChatCompletion`, but is injected during chat creation.
        return response.cost

    @staticmethod
    def get_usage(response: ChatCompletion) -> dict:
        """Return usage summary of the response using RESPONSE_USAGE_KEYS."""
        # ...  # pragma: no cover
        return {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cost": response.cost,
            "model": response.model,
        }

    def parse_params(self, params: dict[str, Any]) -> dict[str, Any]:
        """Loads the parameters for Cerebras API from the passed in parameters and returns a validated set. Checks types, ranges, and sets defaults"""
        cerebras_params = {}

        # Check that we have what we need to use Cerebras's API
        # We won't enforce the available models as they are likely to change
        cerebras_params["model"] = params.get("model")
        assert cerebras_params["model"], (
            "Please specify the 'model' in your config list entry to nominate the Cerebras model to use."
        )

        # Validate allowed Cerebras parameters
        # https://inference-docs.cerebras.ai/api-reference/chat-completions
        cerebras_params["max_tokens"] = validate_parameter(params, "max_tokens", int, True, None, (0, None), None)
        cerebras_params["temperature"] = validate_parameter(
            params, "temperature", (int, float), True, 1.0, (0, 1.5), None
        )
        cerebras_params["top_p"] = validate_parameter(params, "top_p", (int, float), True, None, (0.0, 1.0), None)
        cerebras_params["seed"] = validate_parameter(params, "seed", int, True, None, None, None)
        cerebras_params["stream"] = validate_parameter(params, "stream", bool, True, False, None, None)
        cerebras_params["tool_choice"] = validate_parameter(
            params, "tool_choice", str, True, None, None, ["none", "auto", "required"]
        )

        return cerebras_params

    @require_optional_import("cerebras", "cerebras")
    def create(self, params: dict) -> ChatCompletion:
        messages = params.get("messages", [])

        # Convert AG2 messages to Cerebras messages
        cerebras_messages = oai_messages_to_cerebras_messages(messages)

        # Parse parameters to the Cerebras API's parameters
        cerebras_params = self.parse_params(params)

        # Add tools to the call if we have them and aren't hiding them
        if "tools" in params:
            hide_tools = validate_parameter(
                params, "hide_tools", str, False, "never", None, ["if_all_run", "if_any_run", "never"]
            )
            if not should_hide_tools(cerebras_messages, params["tools"], hide_tools):
                cerebras_params["tools"] = params["tools"]

        cerebras_params["messages"] = cerebras_messages

        # We use chat model by default, and set max_retries to 5 (in line with typical retries loop)
        client = Cerebras(api_key=self.api_key, max_retries=5)

        # Token counts will be returned
        prompt_tokens = 0
        completion_tokens = 0
        total_tokens = 0

        # Streaming tool call recommendations
        streaming_tool_calls = []

        ans = None
        response = client.chat.completions.create(**cerebras_params)

        if cerebras_params["stream"]:
            # Read in the chunks as they stream, taking in tool_calls which may be across
            # multiple chunks if more than one suggested
            ans = ""
            for chunk in response:
                # Grab first choice, which _should_ always be generated.
                ans = ans + (getattr(chunk.choices[0].delta, "content", None) or "")

                if "tool_calls" in chunk.choices[0].delta:
                    # We have a tool call recommendation
                    for tool_call in chunk.choices[0].delta["tool_calls"]:
                        streaming_tool_calls.append(
                            ChatCompletionMessageToolCall(
                                id=tool_call["id"],
                                function={
                                    "name": tool_call["function"]["name"],
                                    "arguments": tool_call["function"]["arguments"],
                                },
                                type="function",
                            )
                        )

                if chunk.choices[0].finish_reason:
                    prompt_tokens = chunk.usage.prompt_tokens
                    completion_tokens = chunk.usage.completion_tokens
                    total_tokens = chunk.usage.total_tokens
        else:
            # Non-streaming finished
            ans: str = response.choices[0].message.content

            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
            total_tokens = response.usage.total_tokens

        if response is not None:
            if isinstance(response, Stream):
                # Streaming response
                if chunk.choices[0].finish_reason == "tool_calls":
                    cerebras_finish = "tool_calls"
                    tool_calls = streaming_tool_calls
                else:
                    cerebras_finish = "stop"
                    tool_calls = None

                response_content = ans
                response_id = chunk.id
            else:
                # Non-streaming response
                # If we have tool calls as the response, populate completed tool calls for our return OAI response
                if response.choices[0].finish_reason == "tool_calls":
                    cerebras_finish = "tool_calls"
                    tool_calls = []
                    for tool_call in response.choices[0].message.tool_calls:
                        tool_calls.append(
                            ChatCompletionMessageToolCall(
                                id=tool_call.id,
                                function={"name": tool_call.function.name, "arguments": tool_call.function.arguments},
                                type="function",
                            )
                        )
                else:
                    cerebras_finish = "stop"
                    tool_calls = None

                response_content = response.choices[0].message.content
                response_id = response.id

        # 3. convert output
        message = ChatCompletionMessage(
            role="assistant",
            content=response_content,
            function_call=None,
            tool_calls=tool_calls,
        )
        choices = [Choice(finish_reason=cerebras_finish, index=0, message=message)]

        response_oai = ChatCompletion(
            id=response_id,
            model=cerebras_params["model"],
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
            # Note: This seems to be a field that isn't in the schema of `ChatCompletion`, so Pydantic
            #       just adds it dynamically.
            cost=calculate_cerebras_cost(prompt_tokens, completion_tokens, cerebras_params["model"]),
        )

        return response_oai


def oai_messages_to_cerebras_messages(messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Convert messages from OAI format to Cerebras's format.
    We correct for any specific role orders and types.
    """
    cerebras_messages = copy.deepcopy(messages)

    # Remove the name field
    for message in cerebras_messages:
        if "name" in message:
            message.pop("name", None)

    return cerebras_messages


def calculate_cerebras_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    """Calculate the cost of the completion using the Cerebras pricing."""
    total = 0.0

    if model in CEREBRAS_PRICING_1K:
        input_cost_per_k, output_cost_per_k = CEREBRAS_PRICING_1K[model]
        input_cost = math.ceil((input_tokens / 1000) * input_cost_per_k * 1e6) / 1e6
        output_cost = math.ceil((output_tokens / 1000) * output_cost_per_k * 1e6) / 1e6
        total = math.ceil((input_cost + output_cost) * 1e6) / 1e6
    else:
        warnings.warn(f"Cost calculation not available for model {model}", UserWarning)

    return total
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Utilities for client classes"""

import logging
import warnings
from typing import Any, Protocol, runtime_checkable


@runtime_checkable
class FormatterProtocol(Protocol):
    """Structured Output classes with a format method"""

    def format(self) -> str: ...


def validate_parameter(
    params: dict[str, Any],
    param_name: str,
    allowed_types: tuple[Any, ...],
    allow_None: bool,  # noqa: N803
    default_value: Any,
    numerical_bound: tuple[float | None, float | None] | None,
    allowed_values: list[Any] | None,
) -> Any:
    """Validates a given config parameter, checking its type, values, and setting defaults
    Parameters:
        params (Dict[str, Any]): Dictionary containing parameters to validate.
        param_name (str): The name of the parameter to validate.
        allowed_types (Tuple): Tuple of acceptable types for the parameter.
        allow_None (bool): Whether the parameter can be `None`.
        default_value (Any): The default value to use if the parameter is invalid or missing.
        numerical_bound (Optional[Tuple[Optional[float], Optional[float]]]):
            A tuple specifying the lower and upper bounds for numerical parameters.
            Each bound can be `None` if not applicable.
        allowed_values (Optional[List[Any]]): A list of acceptable values for the parameter.
            Can be `None` if no specific values are required.

    Returns:
        Any: The validated parameter value or the default value if validation fails.

    Raises:
        TypeError: If `allowed_values` is provided but is not a list.

    Example Usage:
    ```python
        # Validating a numerical parameter within specific bounds
        params = {"temperature": 0.5, "safety_model": "Meta-Llama/Llama-Guard-7b"}
        temperature = validate_parameter(params, "temperature", (int, float), True, 0.7, (0, 1), None)
        # Result: 0.5

        # Validating a parameter that can be one of a list of allowed values
        model = validate_parameter(
        params, "safety_model", str, True, None, None, ["Meta-Llama/Llama-Guard-7b", "Meta-Llama/Llama-Guard-13b"]
        )
        # If "safety_model" is missing or invalid in params, defaults to "default"
    ```
    """
    if allowed_values is not None and not isinstance(allowed_values, list):
        raise TypeError(f"allowed_values should be a list or None, got {type(allowed_values).__name__}")

    param_value = params.get(param_name, default_value)
    warning = ""

    if param_value is None and allow_None:
        pass
    elif param_value is None:
        if not allow_None:
            warning = "cannot be None"
    elif not isinstance(param_value, allowed_types):
        # Check types and list possible types if invalid
        if isinstance(allowed_types, tuple):
            formatted_types = "(" + ", ".join(f"{t.__name__}" for t in allowed_types) + ")"
        else:
            formatted_types = f"{allowed_types.__name__}"
        warning = f"must be of type {formatted_types}{' or None' if allow_None else ''}"
    elif numerical_bound:
        # Check the value fits in possible bounds
        lower_bound, upper_bound = numerical_bound
        if (lower_bound is not None and param_value < lower_bound) or (
            upper_bound is not None and param_value > upper_bound
        ):
            warning = "has numerical bounds"
            if lower_bound is not None:
                warning += f", >= {lower_bound!s}"
            if upper_bound is not None:
                if lower_bound is not None:
                    warning += " and"
                warning += f" <= {upper_bound!s}"
            if allow_None:
                warning += ", or can be None"

    elif allowed_values:  # noqa: SIM102
        # Check if the value matches any allowed values
        if not (allow_None and param_value is None) and param_value not in allowed_values:
            warning = f"must be one of these values [{allowed_values}]{', or can be None' if allow_None else ''}"

    # If we failed any checks, warn and set to default value
    if warning:
        warnings.warn(
            f"Config error - {param_name} {warning}, defaulting to {default_value}.",
            UserWarning,
        )
        param_value = default_value

    return param_value


def should_hide_tools(messages: list[dict[str, Any]], tools: list[dict[str, Any]], hide_tools_param: str) -> bool:
    """Determines if tools should be hidden. This function is used to hide tools when they have been run, minimising the chance of the LLM choosing them when they shouldn't.
    Parameters:
        messages (List[Dict[str, Any]]): List of messages
        tools (List[Dict[str, Any]]): List of tools
        hide_tools_param (str): "hide_tools" parameter value. Can be "if_all_run" (hide tools if all tools have been run), "if_any_run" (hide tools if any of the tools have been run), "never" (never hide tools). Default is "never".

    Returns:
        bool: Indicates whether the tools should be excluded from the response create request

    Example Usage:
    ```python
        # Validating a numerical parameter within specific bounds
        messages = params.get("messages", [])
        tools = params.get("tools", None)
        hide_tools = should_hide_tools(messages, tools, params["hide_tools"])
    """
    if hide_tools_param == "never" or tools is None or len(tools) == 0:
        return False
    elif hide_tools_param == "if_any_run":
        # Return True if any tool_call_id exists, indicating a tool call has been executed. False otherwise.
        return any("tool_call_id" in dictionary for dictionary in messages)
    elif hide_tools_param == "if_all_run":
        # Return True if all tools have been executed at least once. False otherwise.

        # Get the list of tool names
        check_tool_names = [item["function"]["name"] for item in tools]

        # Prepare a list of tool call ids and related function names
        tool_call_ids = {}

        # Loop through the messages and check if the tools have been run, removing them as we go
        for message in messages:
            if "tool_calls" in message:
                # Register the tool ids and the function names (there could be multiple tool calls)
                for tool_call in message["tool_calls"]:
                    tool_call_ids[tool_call["id"]] = tool_call["function"]["name"]
            elif "tool_call_id" in message:
                # Tool called, get the name of the function based on the id
                tool_name_called = tool_call_ids[message["tool_call_id"]]

                # If we had not yet called the tool, check and remove it to indicate we have
                if tool_name_called in check_tool_names:
                    check_tool_names.remove(tool_name_called)

        # Return True if all tools have been called at least once (accounted for)
        return len(check_tool_names) == 0
    else:
        raise TypeError(
            f"hide_tools_param is not a valid value ['if_all_run','if_any_run','never'], got '{hide_tools_param}'"
        )


# Logging format (originally from FLAML)
logging_formatter = logging.Formatter(
    "[%(name)s: %(asctime)s] {%(lineno)d} %(levelname)s - %(message)s", "%m-%d %H:%M:%S"
)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create an OpenAI-compatible client using Mistral.AI's API.

Example:
    ```python
    llm_config = {
        "config_list": [
            {"api_type": "mistral", "model": "open-mixtral-8x22b", "api_key": os.environ.get("MISTRAL_API_KEY")}
        ]
    }

    agent = autogen.AssistantAgent("my_agent", llm_config=llm_config)
    ```

Install Mistral.AI python library using: pip install --upgrade mistralai

Resources:
- https://docs.mistral.ai/getting-started/quickstart/

NOTE: Requires mistralai package version >= 1.0.1
"""

import json
import os
import time
import warnings
from typing import Any, Literal

from typing_extensions import Unpack

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import should_hide_tools, validate_parameter
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    # Mistral libraries
    # pip install mistralai
    from mistralai import (
        AssistantMessage,
        Function,
        FunctionCall,
        Mistral,
        SystemMessage,
        ToolCall,
        ToolMessage,
        UserMessage,
    )


class MistralEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["mistral"]

    safe_prompt: bool
    random_seed: int | None
    stream: bool
    hide_tools: Literal["if_all_run", "if_any_run", "never"]
    tool_choice: Literal["none", "auto", "any"] | None


class MistralLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["mistral"] = "mistral"
    safe_prompt: bool = False
    random_seed: int | None = None
    stream: bool = False
    hide_tools: Literal["if_all_run", "if_any_run", "never"] = "never"
    tool_choice: Literal["none", "auto", "any"] | None = None

    def create_client(self):
        raise NotImplementedError("MistralLLMConfigEntry.create_client is not implemented.")


@require_optional_import("mistralai", "mistral")
class MistralAIClient:
    """Client for Mistral.AI's API."""

    def __init__(self, **kwargs: Unpack[MistralEntryDict]):
        """Requires api_key or environment variable to be set

        Args:
            **kwargs: Additional keyword arguments to pass to the Mistral client.
        """
        # Ensure we have the api_key upon instantiation
        self.api_key = kwargs.get("api_key")
        if not self.api_key:
            self.api_key = os.getenv("MISTRAL_API_KEY", None)

        assert self.api_key, (
            "Please specify the 'api_key' in your config list entry for Mistral or set the MISTRAL_API_KEY env variable."
        )

        if "response_format" in kwargs and kwargs["response_format"] is not None:
            warnings.warn("response_format is not supported for Mistral.AI, it will be ignored.", UserWarning)

        self._client = Mistral(api_key=self.api_key)

    def message_retrieval(self, response: ChatCompletion) -> list[str] | list[ChatCompletionMessage]:
        """Retrieve the messages from the response."""
        return [choice.message for choice in response.choices]

    def cost(self, response) -> float:
        return response.cost

    @require_optional_import("mistralai", "mistral")
    def parse_params(self, params: dict[str, Any]) -> dict[str, Any]:
        """Loads the parameters for Mistral.AI API from the passed in parameters and returns a validated set. Checks types, ranges, and sets defaults"""
        mistral_params = {}

        # 1. Validate models
        mistral_params["model"] = params.get("model")
        assert mistral_params["model"], (
            "Please specify the 'model' in your config list entry to nominate the Mistral.ai model to use."
        )

        # 2. Validate allowed Mistral.AI parameters
        mistral_params["temperature"] = validate_parameter(params, "temperature", (int, float), True, 0.7, None, None)
        mistral_params["top_p"] = validate_parameter(params, "top_p", (int, float), True, None, None, None)
        mistral_params["max_tokens"] = validate_parameter(params, "max_tokens", int, True, None, (0, None), None)
        mistral_params["safe_prompt"] = validate_parameter(
            params, "safe_prompt", bool, False, False, None, [True, False]
        )
        mistral_params["random_seed"] = validate_parameter(params, "random_seed", int, True, None, False, None)
        mistral_params["tool_choice"] = validate_parameter(
            params, "tool_choice", str, False, None, None, ["none", "auto", "any"]
        )

        # TODO
        if params.get("stream", False):
            warnings.warn(
                "Streaming is not currently supported, streaming will be disabled.",
                UserWarning,
            )

        # 3. Convert messages to Mistral format
        mistral_messages = []
        tool_call_ids = {}  # tool call ids to function name mapping
        for message in params["messages"]:
            if message["role"] == "assistant" and "tool_calls" in message and message["tool_calls"] is not None:
                # Convert OAI ToolCall to Mistral ToolCall
                mistral_messages_tools = []
                for toolcall in message["tool_calls"]:
                    mistral_messages_tools.append(
                        ToolCall(
                            id=toolcall["id"],
                            function=FunctionCall(
                                name=toolcall["function"]["name"],
                                arguments=json.loads(toolcall["function"]["arguments"]),
                            ),
                        )
                    )

                mistral_messages.append(AssistantMessage(content="", tool_calls=mistral_messages_tools))

                # Map tool call id to the function name
                for tool_call in message["tool_calls"]:
                    tool_call_ids[tool_call["id"]] = tool_call["function"]["name"]

            elif message["role"] == "system":
                if len(mistral_messages) > 0 and mistral_messages[-1].role == "assistant":
                    # System messages can't appear after an Assistant message, so use a UserMessage
                    mistral_messages.append(UserMessage(content=message["content"]))
                else:
                    mistral_messages.append(SystemMessage(content=message["content"]))
            elif message["role"] == "assistant":
                mistral_messages.append(AssistantMessage(content=message["content"]))
            elif message["role"] == "user":
                mistral_messages.append(UserMessage(content=message["content"]))

            elif message["role"] == "tool":
                # Indicates the result of a tool call, the name is the function name called
                mistral_messages.append(
                    ToolMessage(
                        name=tool_call_ids[message["tool_call_id"]],
                        content=message["content"],
                        tool_call_id=message["tool_call_id"],
                    )
                )
            else:
                warnings.warn(f"Unknown message role {message['role']}", UserWarning)

        # 4. Last message needs to be user or tool, if not, add a "please continue" message
        if not isinstance(mistral_messages[-1], UserMessage) and not isinstance(mistral_messages[-1], ToolMessage):
            mistral_messages.append(UserMessage(content="Please continue."))

        mistral_params["messages"] = mistral_messages

        # 5. Add tools to the call if we have them and aren't hiding them
        if "tools" in params:
            hide_tools = validate_parameter(
                params, "hide_tools", str, False, "never", None, ["if_all_run", "if_any_run", "never"]
            )
            if not should_hide_tools(params["messages"], params["tools"], hide_tools):
                mistral_params["tools"] = tool_def_to_mistral(params["tools"])

        return mistral_params

    @require_optional_import("mistralai", "mistral")
    def create(self, params: dict[str, Any]) -> ChatCompletion:
        # 1. Parse parameters to Mistral.AI API's parameters
        mistral_params = self.parse_params(params)

        # 2. Call Mistral.AI API
        mistral_response = self._client.chat.complete(**mistral_params)
        # TODO: Handle streaming

        # 3. Convert Mistral response to OAI compatible format
        if mistral_response.choices[0].finish_reason == "tool_calls":
            mistral_finish = "tool_calls"
            tool_calls = []
            for tool_call in mistral_response.choices[0].message.tool_calls:
                tool_calls.append(
                    ChatCompletionMessageToolCall(
                        id=tool_call.id,
                        function={"name": tool_call.function.name, "arguments": tool_call.function.arguments},
                        type="function",
                    )
                )
        else:
            mistral_finish = "stop"
            tool_calls = None

        message = ChatCompletionMessage(
            role="assistant",
            content=mistral_response.choices[0].message.content,
            function_call=None,
            tool_calls=tool_calls,
        )
        choices = [Choice(finish_reason=mistral_finish, index=0, message=message)]

        response_oai = ChatCompletion(
            id=mistral_response.id,
            model=mistral_response.model,
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=mistral_response.usage.prompt_tokens,
                completion_tokens=mistral_response.usage.completion_tokens,
                total_tokens=mistral_response.usage.prompt_tokens + mistral_response.usage.completion_tokens,
            ),
            cost=calculate_mistral_cost(
                mistral_response.usage.prompt_tokens, mistral_response.usage.completion_tokens, mistral_response.model
            ),
        )

        return response_oai

    @staticmethod
    def get_usage(response: ChatCompletion) -> dict:
        return {
            "prompt_tokens": response.usage.prompt_tokens if response.usage is not None else 0,
            "completion_tokens": response.usage.completion_tokens if response.usage is not None else 0,
            "total_tokens": (
                response.usage.prompt_tokens + response.usage.completion_tokens if response.usage is not None else 0
            ),
            "cost": response.cost if hasattr(response, "cost") else 0,
            "model": response.model,
        }


@require_optional_import("mistralai", "mistral")
def tool_def_to_mistral(tool_definitions: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Converts AG2 tool definition to a mistral tool format"""
    mistral_tools = []

    for autogen_tool in tool_definitions:
        mistral_tool = {
            "type": "function",
            "function": Function(
                name=autogen_tool["function"]["name"],
                description=autogen_tool["function"]["description"],
                parameters=autogen_tool["function"]["parameters"],
            ),
        }

        mistral_tools.append(mistral_tool)

    return mistral_tools


def calculate_mistral_cost(input_tokens: int, output_tokens: int, model_name: str) -> float:
    """Calculate the cost of the mistral response."""
    # Prices per 1 thousand tokens
    # https://mistral.ai/technology/
    model_cost_map = {
        "open-mistral-7b": {"input": 0.00025, "output": 0.00025},
        "open-mixtral-8x7b": {"input": 0.0007, "output": 0.0007},
        "open-mixtral-8x22b": {"input": 0.002, "output": 0.006},
        "mistral-small-latest": {"input": 0.001, "output": 0.003},
        "mistral-medium-latest": {"input": 0.00275, "output": 0.0081},
        "mistral-large-latest": {"input": 0.0003, "output": 0.0003},
        "mistral-large-2407": {"input": 0.0003, "output": 0.0003},
        "open-mistral-nemo-2407": {"input": 0.0003, "output": 0.0003},
        "codestral-2405": {"input": 0.001, "output": 0.003},
    }

    # Ensure we have the model they are using and return the total cost
    if model_name in model_cost_map:
        costs = model_cost_map[model_name]

        return (input_tokens * costs["input"] / 1000) + (output_tokens * costs["output"] / 1000)
    else:
        warnings.warn(f"Cost calculation is not implemented for model {model_name}, will return $0.", UserWarning)
        return 0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import copy
import warnings
from typing import TYPE_CHECKING, Any

from pydantic import BaseModel

from autogen.code_utils import content_str
from autogen.import_utils import optional_import_block, require_optional_import

if TYPE_CHECKING:
    from autogen.oai.client import ModelClient, OpenAI, OpenAILLMConfigEntry
else:
    # Import at runtime to avoid circular import
    OpenAILLMConfigEntry = None
    ModelClient = None
    OpenAI = None

with optional_import_block() as openai_result:
    from openai.types.responses.response import Response
    from openai.types.responses.response_output_item import ImageGenerationCall

# Image Costs
# Pricing per image (in USD)
PRICING = {
    "gpt-image-1": {
        "low": {"1024x1024": 0.011, "1024x1536": 0.016, "1536x1024": 0.016},
        "medium": {"1024x1024": 0.042, "1024x1536": 0.063, "1536x1024": 0.063},
        "high": {"1024x1024": 0.167, "1024x1536": 0.25, "1536x1024": 0.25},
    },
    "dall-e-3": {
        "standard": {"1024x1024": 0.040, "1024x1792": 0.080, "1792x1024": 0.080},
        "hd": {"1024x1024": 0.080, "1024x1792": 0.120, "1792x1024": 0.120},
    },
    "dall-e-2": {"standard": {"1024x1024": 0.020, "512x512": 0.018, "256x256": 0.016}},
}

# Valid sizes for each model
VALID_SIZES = {
    "gpt-image-1": ["1024x1024", "1024x1536", "1536x1024"],
    "dall-e-3": ["1024x1024", "1024x1792", "1792x1024"],
    "dall-e-2": ["1024x1024", "512x512", "256x256"],
}


def calculate_openai_image_cost(
    model: str = "gpt-image-1", size: str = "1024x1024", quality: str = "high"
) -> tuple[float, str]:
    """Calculate the cost for a single image generation.

    Args:
        model: Model name ("gpt-image-1", "dall-e-3" or "dall-e-2")
        size: Image size (e.g., "1024x1024", "1024x1536")
        quality: Quality setting:
                - For gpt-image-1: "low", "medium", or "high"
                - For dall-e-3: "standard" or "hd"
                - For dall-e-2: "standard" only

    Returns:
        Tuple of (cost, error_message)
    """
    # Normalize inputs
    model = model.lower()
    quality = quality.lower()

    # Validate model
    if model not in PRICING:
        return 0.0, f"Invalid model: {model}. Valid models: {list(PRICING.keys())}"

    # Validate size
    if size not in VALID_SIZES[model]:
        return 0.0, f"Invalid size {size} for {model}. Valid sizes: {VALID_SIZES[model]}"

    # Get the cost based on model type
    try:
        if model == "gpt-image-1" or model == "dall-e-3":
            cost = PRICING[model][quality][size]
        elif model == "dall-e-2":
            cost = PRICING[model]["standard"][size]
        else:
            return 0.0, f"Model {model} not properly configured"

        return cost, None

    except KeyError:
        return 0.0, f"Invalid quality '{quality}' for {model}"


def _get_base_class():
    """Lazy import OpenAILLMConfigEntry to avoid circular imports."""
    from autogen.oai.client import OpenAILLMConfigEntry

    return OpenAILLMConfigEntry


# -----------------------------------------------------------------------------
# OpenAI Client that calls the /responses endpoint
# -----------------------------------------------------------------------------
@require_optional_import("openai", "openai")
class OpenAIResponsesClient:
    """Minimal implementation targeting the experimental /responses endpoint.

    We purposefully keep the surface small - *create*, *message_retrieval*,
    *cost* and *get_usage* - enough for ConversableAgent to operate.  Anything
    that the new endpoint does natively (web_search, file_search, image
    generation, function calling, etc.) is transparently passed through by the
    OpenAI SDK so we don't replicate logic here.
    """

    def __init__(
        self,
        client: "OpenAI",
        response_format: BaseModel | dict[str, Any] | None = None,
    ):
        self._oai_client = client  # plain openai.OpenAI instance
        self.response_format = response_format  # kept for parity but unused for now

        # Initialize the image generation parameters
        self.image_output_params = {
            "quality": None,  # "high" or "low"
            "background": None,  # "white" or "black" or "transparent"
            "size": None,  # "1024x1024" or "1024x1792" or "1792x1024"
            "output_format": "png",  # "png", "jpg" or "jpeg" or "webp"
            "output_compression": None,  # 0-100 if output_format is "jpg" or "jpeg" or "webp"
        }
        self.previous_response_id = None

        # Image costs are calculated manually (rather than off returned information)
        self.image_costs = 0

    # ------------------------------------------------------------------ helpers
    # responses objects embed usage similarly to chat completions
    @staticmethod
    def _usage_dict(resp) -> dict:
        usage_obj = getattr(resp, "usage", None) or {}

        # Convert pydantic/BaseModel usage objects to dict for uniform access
        if hasattr(usage_obj, "model_dump"):
            usage = usage_obj.model_dump()
        elif isinstance(usage_obj, dict):
            usage = usage_obj
        else:  # fallback - unknown structure
            usage = {}

        output_tokens_details = usage.get("output_tokens_details", {})

        return {
            "prompt_tokens": usage.get("input_tokens", 0),
            "completion_tokens": usage.get("output_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0),
            "cost": getattr(resp, "cost", 0),
            "model": getattr(resp, "model", ""),
            "reasoning_tokens": output_tokens_details.get("reasoning_tokens", 0),
        }

    def _add_image_cost(self, response: "Response") -> None:
        """Add image cost to self._image_costs when an image is generated"""
        for output in response.output:
            if (
                isinstance(output, ImageGenerationCall)
                and hasattr(response.output[0], "model_extra")
                and response.output[0].model_extra
            ):
                extra_fields = output.model_extra

                image_cost, image_error = calculate_openai_image_cost(
                    model="gpt-image-1",
                    size=extra_fields.get("size", "1024x1536"),
                    quality=extra_fields.get("quality", "high"),
                )

                if not image_error and image_cost:
                    self.image_costs += image_cost

    def _get_delta_messages(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Get the delta messages from the messages."""
        delta_messages = []
        for m in messages[::-1]:
            contents = m.get("content")
            is_last_completed_response = False
            if isinstance(contents, list):
                for c in contents:
                    if "status" in c and c.get("status") == "completed":
                        is_last_completed_response = True
                        break
            elif isinstance(contents, str):
                is_last_completed_response = "status" in m and m.get("status") == "completed"

            if is_last_completed_response:
                break
            delta_messages.append(m)
        return delta_messages[::-1]

    def _parse_params(self, params: dict[str, Any]) -> None:
        if "verbosity" in params:
            verbosity = params.pop("verbosity")
            params["text"] = {"verbosity": verbosity}
        return params

    def create(self, params: dict[str, Any]) -> "Response":
        """Invoke `client.responses.create() or .parse()`.

        If the caller provided a classic *messages* array we convert it to the
        *input* format expected by the Responses API.
        """
        params = params.copy()

        image_generation_tool_params = {"type": "image_generation"}
        web_search_tool_params = {"type": "web_search_preview"}

        if self.previous_response_id is not None and "previous_response_id" not in params:
            params["previous_response_id"] = self.previous_response_id

        # Back-compat: transform messages  input if needed ------------------
        if "messages" in params and "input" not in params:
            msgs = self._get_delta_messages(params.pop("messages"))
            input_items = []
            for m in msgs[::-1]:  # reverse the list to get the last item first
                role = m.get("role", "user")
                # First, we need to convert the content to the Responses API format
                content = m.get("content")
                blocks = []
                if role != "tool":
                    if isinstance(content, list):
                        for c in content:
                            if c.get("type") in ["input_text", "text"]:
                                blocks.append({"type": "input_text", "text": c.get("text")})
                            elif c.get("type") == "input_image":
                                blocks.append({"type": "input_image", "image_url": c.get("image_url")})
                            elif c.get("type") == "image_params":
                                for k, v in c.get("image_params", {}).items():
                                    if k in self.image_output_params:
                                        image_generation_tool_params[k] = v
                            else:
                                raise ValueError(f"Invalid content type: {c.get('type')}")
                    else:
                        blocks.append({"type": "input_text", "text": content})
                    input_items.append({"role": role, "content": blocks})

                else:
                    if input_items:
                        break
                    # tool call response is the last item in the list
                    content = content_str(m.get("content"))
                    input_items.append({
                        "type": "function_call_output",
                        "call_id": m.get("tool_call_id", None),
                        "output": content,
                    })
                    break

            # Ensure we have at least one valid input item
            if input_items:
                params["input"] = input_items[::-1]
            else:
                # If no valid input items were created, create a default one
                # This prevents the API error about missing required parameters
                params["input"] = [{"role": "user", "content": [{"type": "input_text", "text": "Hello"}]}]

        # Initialize tools list
        tools_list = []
        # Back-compat: add default tools
        built_in_tools = params.pop("built_in_tools", [])
        if built_in_tools:
            if "image_generation" in built_in_tools:
                tools_list.append(image_generation_tool_params)
            if "web_search" in built_in_tools:
                tools_list.append(web_search_tool_params)

        if "tools" in params:
            for tool in params["tools"]:
                tool_item = {"type": "function"}
                if "function" in tool:
                    tool_item |= tool["function"]
                    tools_list.append(tool_item)
        params["tools"] = tools_list
        params["tool_choice"] = "auto"

        # Ensure we don't mix legacy params that Responses doesn't accept
        if params.get("stream") and params.get("background"):
            warnings.warn(
                "Streaming a background response may introduce latency.",
                UserWarning,
            )

        # Validate that we have at least one of the required parameters
        if not any(key in params for key in ["input", "previous_response_id", "prompt"]):
            # If we still don't have any required parameters, create a minimal input
            params["input"] = [{"role": "user", "content": [{"type": "input_text", "text": "Hello"}]}]

        # ------------------------------------------------------------------
        # Structured output handling - mimic OpenAIClient behaviour
        # ------------------------------------------------------------------

        if self.response_format is not None or "response_format" in params:

            def _create_or_parse(**kwargs):
                # For structured output we must convert dict / pydantic model
                # into the JSON-schema body expected by the API.
                if "stream" in kwargs:
                    kwargs.pop("stream")  # Responses API rejects stream with RF for now

                rf = kwargs.get("response_format", self.response_format)

                if isinstance(rf, dict):
                    from autogen.oai.client import _ensure_strict_json_schema

                    kwargs["text_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "schema": _ensure_strict_json_schema(rf, path=(), root=rf),
                            "name": "response_format",
                            "strict": True,
                        },
                    }
                else:
                    # pydantic.BaseModel subclass
                    from autogen.oai.client import type_to_response_format_param

                    kwargs["text_format"] = type_to_response_format_param(rf)
                if "response_format" in kwargs:
                    kwargs["text_format"] = kwargs.pop("response_format")
                try:
                    return self._oai_client.responses.parse(**kwargs)
                except TypeError as e:
                    # Older openai-python versions may not yet expose the
                    # text_format parameter on the Responses endpoint.
                    if "text_format" in str(e) and "unexpected" in str(e):
                        warnings.warn(
                            "Installed openai-python version doesn't support "
                            "`response_format` for the Responses API. "
                            "Falling back to raw text output.",
                            UserWarning,
                        )
                        kwargs.pop("text_format", None)
                        return self._oai_client.responses.create(**kwargs)

            response = _create_or_parse(**params)
            self.previous_response_id = response.id
            return response
        # No structured output
        params = self._parse_params(params)
        response = self._oai_client.responses.create(**params)
        self.previous_response_id = response.id
        # Accumulate image costs
        self._add_image_cost(response)
        return response

    def message_retrieval(self, response) -> list[str] | list["ModelClient.ModelClientResponseProtocol.Choice.Message"]:
        output = getattr(response, "output", [])
        content = []
        tool_calls = []

        for item in output:
            # Convert pydantic objects to plain dicts for uniform handling
            if hasattr(item, "model_dump"):
                item = item.model_dump()

            item_type = item.get("type")

            # Skip reasoning items - they're not messages
            if item_type == "reasoning":
                continue

            if item_type == "message":
                new_item = copy.deepcopy(item)
                new_item["type"] = "text"
                new_item["role"] = "assistant"

                blocks = item.get("content", [])
                if len(blocks) == 1 and blocks[0].get("type") == "output_text":
                    new_item["text"] = blocks[0]["text"]
                elif len(blocks) > 0:
                    # Handle multiple content blocks
                    text_parts = []
                    for block in blocks:
                        if block.get("type") == "output_text":
                            text_parts.append(block.get("text", ""))
                    new_item["text"] = " ".join(text_parts)

                if "content" in new_item:
                    del new_item["content"]
                content.append(new_item)
                continue

            # ------------------------------------------------------------------
            # 2) Custom function calls
            # ------------------------------------------------------------------
            if item_type == "function_call":
                tool_calls.append({
                    "id": item.get("call_id", None),
                    "function": {
                        "name": item.get("name", None),
                        "arguments": item.get("arguments"),
                    },
                    "type": "function_call",
                })
                continue

            # ------------------------------------------------------------------
            # 3) Built-in tool calls
            # ------------------------------------------------------------------
            if item_type and item_type.endswith("_call"):
                tool_name = item_type.replace("_call", "")
                tool_call_args = {
                    "id": item.get("id"),
                    "role": "tool_calls",
                    "type": "tool_call",  # Responses API currently routes via function-like tools
                    "name": tool_name,
                }
                if tool_name == "image_generation":
                    for k in self.image_output_params:
                        if k in item:
                            tool_call_args[k] = item[k]
                    encoded_base64_result = item.get("result", "")
                    tool_call_args["content"] = encoded_base64_result
                    # add image_url for image input back to oai response api.
                    output_format = self.image_output_params["output_format"]
                    tool_call_args["image_url"] = f"data:image/{output_format};base64,{encoded_base64_result}"
                elif tool_name == "web_search":
                    pass
                else:
                    raise ValueError(f"Invalid tool name: {tool_name}")
                content.append(tool_call_args)
                continue

            # ------------------------------------------------------------------
            # 4) Fallback - store raw dict so information isn't lost
            # ------------------------------------------------------------------
            content.append(item)

        return [
            {
                "role": "assistant",
                "id": response.id,
                "content": content if content else None,
                "tool_calls": tool_calls,
            }
        ]

    def cost(self, response):
        return self._usage_dict(response).get("cost", 0) + self.image_costs

    @staticmethod
    def get_usage(response):
        return OpenAIResponsesClient._usage_dict(response)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# SPDX-License-Identifier: MIT
from __future__ import annotations

import inspect
import json
import logging
import re
import sys
import uuid
import warnings
from collections.abc import Callable
from functools import lru_cache
from typing import Any, Literal

from pydantic import BaseModel, Field, HttpUrl
from pydantic.type_adapter import TypeAdapter

from ..cache import Cache
from ..doc_utils import export_module
from ..events.client_events import StreamEvent, UsageSummaryEvent
from ..exception_utils import ModelToolNotSupportedError
from ..import_utils import optional_import_block, require_optional_import
from ..io.base import IOStream
from ..llm_config import ModelClient
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from ..logger.logger_utils import get_current_ts
from ..runtime_logging import log_chat_completion, log_new_client, log_new_wrapper, logging_enabled
from ..token_count_utils import count_token
from .client_utils import FormatterProtocol, logging_formatter
from .openai_utils import OAI_PRICE1K, get_key, is_valid_api_key

TOOL_ENABLED = False
with optional_import_block() as openai_result:
    import openai

if openai_result.is_successful:
    # raises exception if openai>=1 is installed and something is wrong with imports
    from openai import APIError, APITimeoutError, AzureOpenAI, OpenAI
    from openai import __version__ as openai_version
    from openai.lib._parsing._completions import type_to_response_format_param
    from openai.types.chat import ChatCompletion
    from openai.types.chat.chat_completion import ChatCompletionMessage, Choice  # type: ignore [attr-defined]
    from openai.types.chat.chat_completion_chunk import (
        ChoiceDeltaFunctionCall,
        ChoiceDeltaToolCall,
        ChoiceDeltaToolCallFunction,
    )
    from openai.types.completion import Completion
    from openai.types.completion_usage import CompletionUsage

    from autogen.oai.openai_responses import OpenAIResponsesClient

    if openai.__version__ >= "1.1.0":
        TOOL_ENABLED = True
    ERROR = None
    from openai.lib._pydantic import _ensure_strict_json_schema
else:
    ERROR: ImportError | None = ImportError("Please install openai>=1 and diskcache to use autogen.OpenAIWrapper.")

    # OpenAI = object
    # AzureOpenAI = object

with optional_import_block() as cerebras_result:
    from cerebras.cloud.sdk import (  # noqa
        AuthenticationError as cerebras_AuthenticationError,
        InternalServerError as cerebras_InternalServerError,
        RateLimitError as cerebras_RateLimitError,
    )

    from .cerebras import CerebrasClient

if cerebras_result.is_successful:
    cerebras_import_exception: ImportError | None = None
else:
    cerebras_AuthenticationError = cerebras_InternalServerError = cerebras_RateLimitError = Exception  # noqa: N816
    cerebras_import_exception = ImportError("cerebras_cloud_sdk not found")

with optional_import_block() as gemini_result:
    from google.api_core.exceptions import (  # noqa
        InternalServerError as gemini_InternalServerError,
        ResourceExhausted as gemini_ResourceExhausted,
    )

    from .gemini import GeminiClient

if gemini_result.is_successful:
    gemini_import_exception: ImportError | None = None
else:
    gemini_InternalServerError = gemini_ResourceExhausted = Exception  # noqa: N816
    gemini_import_exception = ImportError("google-genai not found")

with optional_import_block() as anthropic_result:
    from anthropic import (  # noqa
        InternalServerError as anthorpic_InternalServerError,
        RateLimitError as anthorpic_RateLimitError,
    )

    from .anthropic import AnthropicClient

if anthropic_result.is_successful:
    anthropic_import_exception: ImportError | None = None
else:
    anthorpic_InternalServerError = anthorpic_RateLimitError = Exception  # noqa: N816
    anthropic_import_exception = ImportError("anthropic not found")

with optional_import_block() as mistral_result:
    from mistralai.models import (  # noqa
        HTTPValidationError as mistral_HTTPValidationError,
        SDKError as mistral_SDKError,
    )

    from .mistral import MistralAIClient

if mistral_result.is_successful:
    mistral_import_exception: ImportError | None = None
else:
    mistral_SDKError = mistral_HTTPValidationError = Exception  # noqa: N816
    mistral_import_exception = ImportError("mistralai not found")

with optional_import_block() as together_result:
    from together.error import TogetherException as together_TogetherException

    from .together import TogetherClient

if together_result.is_successful:
    together_import_exception: ImportError | None = None
else:
    together_TogetherException = Exception  # noqa: N816
    together_import_exception = ImportError("together not found")

with optional_import_block() as groq_result:
    from groq import (  # noqa
        APIConnectionError as groq_APIConnectionError,
        InternalServerError as groq_InternalServerError,
        RateLimitError as groq_RateLimitError,
    )

    from .groq import GroqClient

if groq_result.is_successful:
    groq_import_exception: ImportError | None = None
else:
    groq_InternalServerError = groq_RateLimitError = groq_APIConnectionError = Exception  # noqa: N816
    groq_import_exception = ImportError("groq not found")

with optional_import_block() as cohere_result:
    from cohere.errors import (  # noqa
        InternalServerError as cohere_InternalServerError,
        ServiceUnavailableError as cohere_ServiceUnavailableError,
        TooManyRequestsError as cohere_TooManyRequestsError,
    )

    from .cohere import CohereClient

if cohere_result.is_successful:
    cohere_import_exception: ImportError | None = None
else:
    cohere_InternalServerError = cohere_TooManyRequestsError = cohere_ServiceUnavailableError = Exception  # noqa: N816
    cohere_import_exception = ImportError("cohere not found")

with optional_import_block() as ollama_result:
    from ollama import (  # noqa
        RequestError as ollama_RequestError,
        ResponseError as ollama_ResponseError,
    )

    from .ollama import OllamaClient

if ollama_result.is_successful:
    ollama_import_exception: ImportError | None = None
else:
    ollama_RequestError = ollama_ResponseError = Exception  # noqa: N816
    ollama_import_exception = ImportError("ollama not found")

with optional_import_block() as bedrock_result:
    from botocore.exceptions import (  # noqa
        BotoCoreError as bedrock_BotoCoreError,
        ClientError as bedrock_ClientError,
    )

    from .bedrock import BedrockClient

if bedrock_result.is_successful:
    bedrock_import_exception: ImportError | None = None
else:
    bedrock_BotoCoreError = bedrock_ClientError = Exception  # noqa: N816
    bedrock_import_exception = ImportError("botocore not found")

logger = logging.getLogger(__name__)
if not logger.handlers:
    # Add the console handler.
    _ch = logging.StreamHandler(stream=sys.stdout)
    _ch.setFormatter(logging_formatter)
    logger.addHandler(_ch)

LEGACY_DEFAULT_CACHE_SEED = 41
LEGACY_CACHE_DIR = ".cache"
OPEN_API_BASE_URL_PREFIX = "https://api.openai.com"

OPENAI_FALLBACK_KWARGS = {
    "api_key",
    "organization",
    "project",
    "base_url",
    "websocket_base_url",
    "timeout",
    "max_retries",
    "default_headers",
    "default_query",
    "http_client",
    "_strict_response_validation",
    "webhook_secret",
}

AOPENAI_FALLBACK_KWARGS = {
    "azure_endpoint",
    "azure_deployment",
    "api_version",
    "api_key",
    "azure_ad_token",
    "azure_ad_token_provider",
    "organization",
    "websocket_base_url",
    "timeout",
    "max_retries",
    "default_headers",
    "default_query",
    "http_client",
    "_strict_response_validation",
    "base_url",
    "project",
    "webhook_secret",
}


@lru_cache(maxsize=128)
def log_cache_seed_value(cache_seed_value: str | int, client: ModelClient) -> None:
    logger.debug(f"Using cache with seed value {cache_seed_value} for client {client.__class__.__name__}")


class OpenAIEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["openai"]

    price: list[float] | None
    tool_choice: Literal["none", "auto", "required"] | None
    user: str | None
    stream: bool
    verbosity: Literal["low", "medium", "high"] | None
    extra_body: dict[str, Any] | None
    reasoning_effort: Literal["low", "minimal", "medium", "high"] | None
    max_completion_tokens: int | None


class OpenAILLMConfigEntry(LLMConfigEntry):
    api_type: Literal["openai"] = "openai"

    price: list[float] | None = Field(default=None, min_length=2, max_length=2)
    tool_choice: Literal["none", "auto", "required"] | None = None
    user: str | None = None
    stream: bool = False
    verbosity: Literal["low", "medium", "high"] | None = None
    #   The extra_body parameter flows from OpenAILLMConfigEntry to the LLM request through this path:
    #   1. Config Definition: extra_body is defined in OpenAILLMConfigEntry (autogen/oai/client.py:248)
    #   2. Parameter Classification: It's classified as an OpenAI client parameter (not AG2-specific) via the openai_kwargs property (autogen/oai/client.py:752-758)
    #   3. Request Separation: In _separate_create_config() (autogen/oai/client.py:842), extra_body goes into create_config since it's not in the extra_kwargs set.
    #   4. API Call: The create_config becomes params and gets passed directly to OpenAI's create() method via **params (autogen/oai/client.py:551,658)
    extra_body: dict[str, Any] | None = (
        None  # For VLLM - See here: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#extra-parameters
    )
    # reasoning models - see: https://platform.openai.com/docs/api-reference/chat/create#chat-create-reasoning_effort
    reasoning_effort: Literal["low", "minimal", "medium", "high"] | None = None
    max_completion_tokens: int | None = None

    def create_client(self) -> ModelClient:
        raise NotImplementedError("create_client method must be implemented in the derived class.")


class AzureOpenAIEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["azure"]

    azure_ad_token_provider: str | Callable[[], str] | None
    stream: bool
    tool_choice: Literal["none", "auto", "required"] | None
    user: str | None
    reasoning_effort: Literal["low", "minimal", "medium", "high"] | None
    max_completion_tokens: int | None


class AzureOpenAILLMConfigEntry(LLMConfigEntry):
    api_type: Literal["azure"] = "azure"

    azure_ad_token_provider: str | Callable[[], str] | None = None
    stream: bool = False
    tool_choice: Literal["none", "auto", "required"] | None = None
    user: str | None = None
    # reasoning models - see:
    # - https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning
    # - https://learn.microsoft.com/en-us/azure/ai-services/openai/reference-preview
    reasoning_effort: Literal["low", "minimal", "medium", "high"] | None = None
    max_completion_tokens: int | None = None

    def create_client(self) -> ModelClient:
        raise NotImplementedError


class DeepSeekEntyDict(LLMConfigEntryDict, total=False):
    api_type: Literal["deepseek"]

    base_url: HttpUrl
    stream: bool
    tool_choice: Literal["none", "auto", "required"] | None


class DeepSeekLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["deepseek"] = "deepseek"

    temperature: float | None = Field(default=None, ge=0.0, le=1.0)
    top_p: float | None = Field(None, ge=0.0, le=1.0)
    max_tokens: int = Field(8192, ge=1, le=8192)

    base_url: HttpUrl = HttpUrl("https://api.deepseek.com/v1")
    stream: bool = False
    tool_choice: Literal["none", "auto", "required"] | None = None

    def create_client(self) -> None:  # type: ignore [override]
        raise NotImplementedError("DeepSeekLLMConfigEntry.create_client is not implemented.")


class PlaceHolderClient:
    def __init__(self, config):
        self.config = config


@require_optional_import("openai>=1.66.2", "openai")
class OpenAIClient:
    """Follows the Client protocol and wraps the OpenAI client."""

    def __init__(self, client: OpenAI | AzureOpenAI, response_format: BaseModel | dict[str, Any] | None = None):
        self._oai_client = client
        self.response_format = response_format
        if (
            not isinstance(client, openai.AzureOpenAI)
            and str(client.base_url).startswith(OPEN_API_BASE_URL_PREFIX)
            and not is_valid_api_key(self._oai_client.api_key)
        ):
            logger.warning(
                "The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model."
            )

    def message_retrieval(self, response: ChatCompletion | Completion) -> list[str] | list[ChatCompletionMessage]:
        """Retrieve the messages from the response.

        Args:
            response (ChatCompletion | Completion): The response from openai.


        Returns:
            The message from the response.
        """
        choices = response.choices
        if isinstance(response, Completion):
            return [choice.text for choice in choices]  # type: ignore [union-attr]

        def _format_content(content: str) -> str:
            return (
                self.response_format.model_validate_json(content).format()
                if isinstance(self.response_format, FormatterProtocol)
                else content
            )

        if TOOL_ENABLED:
            return [  # type: ignore [return-value]
                (
                    choice.message  # type: ignore [union-attr]
                    if choice.message.function_call is not None or choice.message.tool_calls is not None  # type: ignore [union-attr]
                    else _format_content(choice.message.content)
                )  # type: ignore [union-attr]
                for choice in choices
            ]
        else:
            return [  # type: ignore [return-value]
                choice.message if choice.message.function_call is not None else _format_content(choice.message.content)  # type: ignore [union-attr]
                for choice in choices
            ]

    @staticmethod
    def _is_agent_name_error_message(message: str) -> bool:
        pattern = re.compile(r"Invalid 'messages\[\d+\]\.name': string does not match pattern.")
        return bool(pattern.match(message))

    @staticmethod
    def _move_system_message_to_beginning(messages: list[dict[str, Any]]) -> None:
        for msg in messages:
            if msg["role"] == "system":
                messages.insert(0, messages.pop(messages.index(msg)))
                break

    @staticmethod
    def _patch_messages_for_deepseek_reasoner(**kwargs: Any) -> Any:
        if (
            "model" not in kwargs
            or kwargs["model"] != "deepseek-reasoner"
            or "messages" not in kwargs
            or len(kwargs["messages"]) == 0
        ):
            return kwargs

        # The system message of deepseek-reasoner must be put on the beginning of the message sequence.
        OpenAIClient._move_system_message_to_beginning(kwargs["messages"])

        new_messages = []
        previous_role = None
        for message in kwargs["messages"]:
            if "role" in message:
                current_role = message["role"]

                # This model requires alternating roles
                if current_role == previous_role:
                    # Swap the role
                    if current_role == "user":
                        message["role"] = "assistant"
                    elif current_role == "assistant":
                        message["role"] = "user"

                previous_role = message["role"]

            new_messages.append(message)

        # The last message of deepseek-reasoner must be a user message
        # , or an assistant message with prefix mode on (but this is supported only for beta api)
        if new_messages[-1]["role"] != "user":
            new_messages.append({"role": "user", "content": "continue"})

        kwargs["messages"] = new_messages

        return kwargs

    @staticmethod
    def _handle_openai_bad_request_error(func: Callable[..., Any]) -> Callable[..., Any]:
        def wrapper(*args: Any, **kwargs: Any):
            try:
                kwargs = OpenAIClient._patch_messages_for_deepseek_reasoner(**kwargs)
                return func(*args, **kwargs)
            except openai.BadRequestError as e:
                response_json = e.response.json()
                # Check if the error message is related to the agent name. If so, raise a ValueError with a more informative message.
                if (
                    "error" in response_json
                    and "message" in response_json["error"]
                    and OpenAIClient._is_agent_name_error_message(response_json["error"]["message"])
                ):
                    error_message = (
                        f"This error typically occurs when the agent name contains invalid characters, such as spaces or special symbols.\n"
                        "Please ensure that your agent name follows the correct format and doesn't include any unsupported characters.\n"
                        "Check the agent name and try again.\n"
                        f"Here is the full BadRequestError from openai:\n{e.message}."
                    )
                    raise ValueError(error_message)

                raise e

        return wrapper

    @staticmethod
    def _convert_system_role_to_user(messages: list[dict[str, Any]]) -> None:
        for msg in messages:
            if msg.get("role", "") == "system":
                msg["role"] = "user"

    def create(self, params: dict[str, Any]) -> ChatCompletion:
        """Create a completion for a given config using openai's client.

        Args:
            params: The params for the completion.

        Returns:
            The completion.
        """
        iostream = IOStream.get_default()

        if self.response_format is not None or "response_format" in params:

            def _create_or_parse(*args, **kwargs):
                if "stream" in kwargs:
                    kwargs.pop("stream")

                if (
                    isinstance(kwargs["response_format"], dict)
                    and kwargs["response_format"].get("type") != "json_object"
                ):
                    kwargs["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "schema": _ensure_strict_json_schema(
                                kwargs["response_format"], path=(), root=kwargs["response_format"]
                            ),
                            "name": "response_format",
                            "strict": True,
                        },
                    }
                else:
                    kwargs["response_format"] = type_to_response_format_param(
                        self.response_format or params["response_format"]
                    )

                return self._oai_client.chat.completions.create(*args, **kwargs)

            create_or_parse = _create_or_parse
        else:
            completions = self._oai_client.chat.completions if "messages" in params else self._oai_client.completions  # type: ignore [attr-defined]
            create_or_parse = completions.create
        # Wrap _create_or_parse with exception handling
        create_or_parse = OpenAIClient._handle_openai_bad_request_error(create_or_parse)

        # needs to be updated when the o3 is released to generalize
        is_o1 = "model" in params and params["model"].startswith("o1")

        is_mistral = "model" in params and "mistral" in params["model"]
        if is_mistral:
            OpenAIClient._convert_system_role_to_user(params["messages"])

        # If streaming is enabled and has messages, then iterate over the chunks of the response.
        if params.get("stream", False) and "messages" in params and not is_o1:
            response_contents = [""] * params.get("n", 1)
            finish_reasons = [""] * params.get("n", 1)
            completion_tokens = 0

            # Prepare for potential function call
            full_function_call: dict[str, Any] | None = None
            full_tool_calls: list[dict[str, Any] | None] | None = None

            # Send the chat completion request to OpenAI's API and process the response in chunks
            for chunk in create_or_parse(**params):
                if chunk.choices:
                    for choice in chunk.choices:
                        content = choice.delta.content
                        tool_calls_chunks = choice.delta.tool_calls
                        finish_reasons[choice.index] = choice.finish_reason

                        # todo: remove this after function calls are removed from the API
                        # the code should work regardless of whether function calls are removed or not, but test_chat_functions_stream should fail
                        # begin block
                        function_call_chunk = (
                            choice.delta.function_call if hasattr(choice.delta, "function_call") else None
                        )
                        # Handle function call
                        if function_call_chunk:
                            # Handle function call
                            if function_call_chunk:
                                full_function_call, completion_tokens = OpenAIWrapper._update_function_call_from_chunk(
                                    function_call_chunk, full_function_call, completion_tokens
                                )
                            if not content:
                                continue
                        # end block

                        # Handle tool calls
                        if tool_calls_chunks:
                            for tool_calls_chunk in tool_calls_chunks:
                                # the current tool call to be reconstructed
                                ix = tool_calls_chunk.index
                                if full_tool_calls is None:
                                    full_tool_calls = []
                                if ix >= len(full_tool_calls):
                                    # in case ix is not sequential
                                    full_tool_calls = full_tool_calls + [None] * (ix - len(full_tool_calls) + 1)

                                full_tool_calls[ix], completion_tokens = OpenAIWrapper._update_tool_calls_from_chunk(
                                    tool_calls_chunk, full_tool_calls[ix], completion_tokens
                                )
                                if not content:
                                    continue

                        # End handle tool calls

                        # If content is present, print it to the terminal and update response variables
                        if content is not None:
                            iostream.send(StreamEvent(content=content))
                            response_contents[choice.index] += content
                            completion_tokens += 1
                        else:
                            pass

            # Prepare the final ChatCompletion object based on the accumulated data
            model = chunk.model.replace("gpt-35", "gpt-3.5")  # hack for Azure API
            prompt_tokens = count_token(params["messages"], model)
            response = ChatCompletion(
                id=chunk.id,
                model=chunk.model,
                created=chunk.created,
                object="chat.completion",
                choices=[],
                usage=CompletionUsage(
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    total_tokens=prompt_tokens + completion_tokens,
                ),
            )
            for i in range(len(response_contents)):
                if openai_version >= "1.5":  # pragma: no cover
                    # OpenAI versions 1.5.0 and above
                    choice = Choice(
                        index=i,
                        finish_reason=finish_reasons[i],
                        message=ChatCompletionMessage(
                            role="assistant",
                            content=response_contents[i],
                            function_call=full_function_call,
                            tool_calls=full_tool_calls,
                        ),
                        logprobs=None,
                    )
                else:
                    # OpenAI versions below 1.5.0
                    choice = Choice(  # type: ignore [call-arg]
                        index=i,
                        finish_reason=finish_reasons[i],
                        message=ChatCompletionMessage(
                            role="assistant",
                            content=response_contents[i],
                            function_call=full_function_call,
                            tool_calls=full_tool_calls,
                        ),
                    )

                response.choices.append(choice)
        else:
            # If streaming is not enabled, send a regular chat completion request
            params = params.copy()
            if is_o1:
                # add a warning that model does not support stream
                if params.get("stream", False):
                    warnings.warn(
                        f"The {params.get('model')} model does not support streaming. The stream will be set to False."
                    )
                if params.get("tools", False):
                    raise ModelToolNotSupportedError(params.get("model"))
                self._process_reasoning_model_params(params)
            params["stream"] = False
            response = create_or_parse(**params)
            # remove the system_message from the response and add it in the prompt at the start.
            if is_o1:
                for msg in params["messages"]:
                    if msg["role"] == "user" and msg["content"].startswith("System message: "):
                        msg["role"] = "system"
                        msg["content"] = msg["content"][len("System message: ") :]

        return response

    def _process_reasoning_model_params(self, params: dict[str, Any]) -> None:
        """Cater for the reasoning model (o1, o3..) parameters
        please refer: https://platform.openai.com/docs/guides/reasoning#limitations
        """
        # Unsupported parameters
        unsupported_params = [
            "temperature",
            "top_p",
            "frequency_penalty",
            "presence_penalty",
            "logprobs",
            "top_logprobs",
            "logit_bias",
        ]
        model_name = params.get("model")
        for param in unsupported_params:
            if param in params:
                warnings.warn(f"`{param}` is not supported with {model_name} model and will be ignored.")
                params.pop(param)
        # Replace max_tokens with max_completion_tokens as reasoning tokens are now factored in
        # and max_tokens isn't valid
        if "max_tokens" in params:
            params["max_completion_tokens"] = params.pop("max_tokens")

        # TODO - When o1-mini and o1-preview point to newer models (e.g. 2024-12-...), remove them from this list but leave the 2024-09-12 dated versions
        system_not_allowed = model_name in ("o1-mini", "o1-preview", "o1-mini-2024-09-12", "o1-preview-2024-09-12")

        if "messages" in params and system_not_allowed:
            # o1-mini (2024-09-12) and o1-preview (2024-09-12) don't support role='system' messages, only 'user' and 'assistant'
            # replace the system messages with user messages preappended with "System message: "
            for msg in params["messages"]:
                if msg["role"] == "system":
                    msg["role"] = "user"
                    msg["content"] = f"System message: {msg['content']}"

    def cost(self, response: ChatCompletion | Completion) -> float:
        """Calculate the cost of the response."""
        model = response.model
        if model not in OAI_PRICE1K:
            # log warning that the model is not found
            logger.warning(
                f'Model {model} is not found. The cost will be 0. In your config_list, add field {{"price" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.'
            )
            return 0

        n_input_tokens = response.usage.prompt_tokens if response.usage is not None else 0  # type: ignore [union-attr]
        n_output_tokens = response.usage.completion_tokens if response.usage is not None else 0  # type: ignore [union-attr]
        if n_output_tokens is None:
            n_output_tokens = 0
        tmp_price1K = OAI_PRICE1K[model]  # noqa: N806
        # First value is input token rate, second value is output token rate
        if isinstance(tmp_price1K, tuple):
            return (tmp_price1K[0] * n_input_tokens + tmp_price1K[1] * n_output_tokens) / 1000  # type: ignore [no-any-return]
        return tmp_price1K * (n_input_tokens + n_output_tokens) / 1000  # type: ignore [operator]

    @staticmethod
    def get_usage(response: ChatCompletion | Completion) -> dict:
        return {
            "prompt_tokens": response.usage.prompt_tokens if response.usage is not None else 0,
            "completion_tokens": response.usage.completion_tokens if response.usage is not None else 0,
            "total_tokens": response.usage.total_tokens if response.usage is not None else 0,
            "cost": response.cost if hasattr(response, "cost") else 0,
            "model": response.model,
        }


@export_module("autogen")
class OpenAIWrapper:
    """A wrapper class for openai client."""

    extra_kwargs = {
        "agent",
        "cache",
        "cache_seed",
        "filter_func",
        "allow_format_str_template",
        "context",
        "api_version",
        "api_type",
        "tags",
        "price",
    }

    @property
    def openai_kwargs(self) -> set[str]:
        if openai_result.is_successful:
            return set(inspect.getfullargspec(OpenAI.__init__).kwonlyargs) | set(
                inspect.getfullargspec(AzureOpenAI.__init__).kwonlyargs
            )
        else:
            return OPENAI_FALLBACK_KWARGS | AOPENAI_FALLBACK_KWARGS

    total_usage_summary: dict[str, Any] | None = None
    actual_usage_summary: dict[str, Any] | None = None

    def __init__(
        self,
        *,
        config_list: list[dict[str, Any]] | None = None,
        **base_config: Any,
    ):
        """Initialize the OpenAIWrapper.

        Args:
            config_list: a list of config dicts to override the base_config.
                They can contain additional kwargs as allowed in the [create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create) method. E.g.,

                ```python
                    config_list = [
                        {
                            "model": "gpt-4",
                            "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),
                            "api_type": "azure",
                            "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),
                            "api_version": "2024-02-01",
                        },
                        {
                            "model": "gpt-3.5-turbo",
                            "api_key": os.environ.get("OPENAI_API_KEY"),
                            "base_url": "https://api.openai.com/v1",
                        },
                        {
                            "model": "llama-7B",
                            "base_url": "http://127.0.0.1:8080",
                        },
                    ]
                ```

            base_config: base config. It can contain both keyword arguments for openai client
                and additional kwargs.
                When using OpenAI or Azure OpenAI endpoints, please specify a non-empty 'model' either in `base_config` or in each config of `config_list`.
        """
        if logging_enabled():
            log_new_wrapper(self, locals())
        openai_config, extra_kwargs = self._separate_openai_config(base_config)
        # It's OK if "model" is not provided in base_config or config_list
        # Because one can provide "model" at `create` time.

        self._clients: list[ModelClient] = []
        self._config_list: list[dict[str, Any]] = []

        # Determine routing_method from base_config only.
        self.routing_method = base_config.get("routing_method") or "fixed_order"
        self._round_robin_index = 0

        # Remove routing_method from extra_kwargs after it has been used to set self.routing_method
        # This ensures it's not part of the individual client configurations that are based on extra_kwargs.
        extra_kwargs.pop("routing_method", None)

        if config_list:
            config_list = [config.copy() for config in config_list]  # make a copy before modifying
            for config_item in config_list:
                self._register_default_client(config_item, openai_config)
                # Construct current_config_extra_kwargs using the cleaned extra_kwargs
                # (which doesn't have routing_method from base_config)
                # and specific non-openai kwargs from config_item.
                config_item_specific_extras = {k: v for k, v in config_item.items() if k not in self.openai_kwargs}
                self._config_list.append({**extra_kwargs, **config_item_specific_extras})
        else:
            # For a single config passed via base_config (already in extra_kwargs)
            self._register_default_client(extra_kwargs, openai_config)
            # extra_kwargs has already had routing_method popped.
            self._config_list = [extra_kwargs]

        self.wrapper_id = id(self)

    def _separate_openai_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        """Separate the config into openai_config and extra_kwargs."""
        openai_config = {k: v for k, v in config.items() if k in self.openai_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k not in self.openai_kwargs}
        return openai_config, extra_kwargs

    def _separate_create_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        """Separate the config into create_config and extra_kwargs."""
        create_config = {k: v for k, v in config.items() if k not in self.extra_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k in self.extra_kwargs}
        return create_config, extra_kwargs

    def _configure_azure_openai(self, config: dict[str, Any], openai_config: dict[str, Any]) -> None:
        openai_config["azure_deployment"] = openai_config.get("azure_deployment", config.get("model"))
        openai_config["azure_endpoint"] = openai_config.get("azure_endpoint", openai_config.pop("base_url", None))

        # Create a default Azure token provider if requested
        if openai_config.get("azure_ad_token_provider") == "DEFAULT":
            import azure.identity

            openai_config["azure_ad_token_provider"] = azure.identity.get_bearer_token_provider(
                azure.identity.DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
            )

    def _configure_openai_config_for_bedrock(self, config: dict[str, Any], openai_config: dict[str, Any]) -> None:
        """Update openai_config with AWS credentials from config."""
        required_keys = ["aws_access_key", "aws_secret_key", "aws_region"]
        optional_keys = ["aws_session_token", "aws_profile_name"]
        for key in required_keys:
            if key in config:
                openai_config[key] = config[key]
        for key in optional_keys:
            if key in config:
                openai_config[key] = config[key]

    def _configure_openai_config_for_vertextai(self, config: dict[str, Any], openai_config: dict[str, Any]) -> None:
        """Update openai_config with Google credentials from config."""
        required_keys = ["gcp_project_id", "gcp_region", "gcp_auth_token"]
        for key in required_keys:
            if key in config:
                openai_config[key] = config[key]

    def _configure_openai_config_for_gemini(self, config: dict[str, Any], openai_config: dict[str, Any]) -> None:
        """Update openai_config with additional gemini genai configs."""
        optional_keys = ["proxy"]
        for key in optional_keys:
            if key in config:
                openai_config[key] = config[key]

    def _register_default_client(self, config: dict[str, Any], openai_config: dict[str, Any]) -> None:
        """Create a client with the given config to override openai_config,
        after removing extra kwargs.

        For Azure models/deployment names there's a convenience modification of model removing dots in
        the it's value (Azure deployment names can't have dots). I.e. if you have Azure deployment name
        "gpt-35-turbo" and define model "gpt-3.5-turbo" in the config the function will remove the dot
        from the name and create a client that connects to "gpt-35-turbo" Azure deployment.
        """
        openai_config = {**openai_config, **{k: v for k, v in config.items() if k in self.openai_kwargs}}
        api_type = config.get("api_type")
        model_client_cls_name = config.get("model_client_cls")
        response_format = config.get("response_format")
        if model_client_cls_name is not None:
            # a config for a custom client is set
            # adding placeholder until the register_model_client is called with the appropriate class
            self._clients.append(PlaceHolderClient(config))
            # codeql[py/clear-text-logging-sensitive-data]
            logger.info(
                f"Detected custom model client in config: {model_client_cls_name}, model client can not be used until register_model_client is called."
            )
            # TODO: logging for custom client
        else:
            if api_type is not None and api_type.startswith("azure"):

                @require_optional_import("openai>=1.66.2", "openai")
                def create_azure_openai_client() -> AzureOpenAI:
                    self._configure_azure_openai(config, openai_config)
                    client = AzureOpenAI(**openai_config)
                    self._clients.append(OpenAIClient(client, response_format=response_format))
                    return client

                client = create_azure_openai_client()
            elif api_type is not None and api_type.startswith("cerebras"):
                if cerebras_import_exception:
                    raise ImportError("Please install `cerebras_cloud_sdk` to use Cerebras OpenAI API.")
                client = CerebrasClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("google"):
                if gemini_import_exception:
                    raise ImportError("Please install `google-genai` and 'vertexai' to use Google's API.")
                self._configure_openai_config_for_gemini(config, openai_config)
                client = GeminiClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("anthropic"):
                if "api_key" not in config and "aws_region" in config:
                    self._configure_openai_config_for_bedrock(config, openai_config)
                elif "api_key" not in config and "gcp_region" in config:
                    self._configure_openai_config_for_vertextai(config, openai_config)
                if anthropic_import_exception:
                    raise ImportError("Please install `anthropic` to use Anthropic API.")
                client = AnthropicClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("mistral"):
                if mistral_import_exception:
                    raise ImportError("Please install `mistralai` to use the Mistral.AI API.")
                client = MistralAIClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("together"):
                if together_import_exception:
                    raise ImportError("Please install `together` to use the Together.AI API.")
                client = TogetherClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("groq"):
                if groq_import_exception:
                    raise ImportError("Please install `groq` to use the Groq API.")
                client = GroqClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("cohere"):
                if cohere_import_exception:
                    raise ImportError("Please install `cohere` to use the Cohere API.")
                client = CohereClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("ollama"):
                if ollama_import_exception:
                    raise ImportError("Please install `ollama` and `fix-busted-json` to use the Ollama API.")
                client = OllamaClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("bedrock"):
                self._configure_openai_config_for_bedrock(config, openai_config)
                if bedrock_import_exception:
                    raise ImportError("Please install `boto3` to use the Amazon Bedrock API.")
                client = BedrockClient(response_format=response_format, **openai_config)
                self._clients.append(client)
            elif api_type is not None and api_type.startswith("responses"):
                # OpenAI Responses API (stateful). Reuse the same OpenAI SDK but call the `/responses` endpoint via the new client.
                @require_optional_import("openai>=1.66.2", "openai")
                def create_responses_client() -> OpenAI:
                    client = OpenAI(**openai_config)
                    self._clients.append(OpenAIResponsesClient(client, response_format=response_format))
                    return client

                client = create_responses_client()
            else:

                @require_optional_import("openai>=1.66.2", "openai")
                def create_openai_client() -> OpenAI:
                    client = OpenAI(**openai_config)
                    self._clients.append(OpenAIClient(client, response_format))
                    return client

                client = create_openai_client()

            if logging_enabled():
                log_new_client(client, self, openai_config)

    def register_model_client(self, model_client_cls: ModelClient, **kwargs: Any):
        """Register a model client.

        Args:
            model_client_cls: A custom client class that follows the ModelClient interface
            kwargs: The kwargs for the custom client class to be initialized with
        """
        existing_client_class = False
        for i, client in enumerate(self._clients):
            if isinstance(client, PlaceHolderClient):
                placeholder_config = client.config

                if placeholder_config.get("model_client_cls") == model_client_cls.__name__:
                    self._clients[i] = model_client_cls(placeholder_config, **kwargs)
                    return
            elif isinstance(client, model_client_cls):
                existing_client_class = True

        if existing_client_class:
            logger.warning(
                f"Model client {model_client_cls.__name__} is already registered. Add more entries in the config_list to use multiple model clients."
            )
        else:
            raise ValueError(
                f'Model client "{model_client_cls.__name__}" is being registered but was not found in the config_list. '
                f'Please make sure to include an entry in the config_list with "model_client_cls": "{model_client_cls.__name__}"'
            )

    @classmethod
    def instantiate(
        cls,
        template: str | Callable[[dict[str, Any]], str] | None,
        context: dict[str, Any] | None = None,
        allow_format_str_template: bool | None = False,
    ) -> str | None:
        if not context or template is None:
            return template  # type: ignore [return-value]
        if isinstance(template, str):
            return template.format(**context) if allow_format_str_template else template
        return template(context)

    def _construct_create_params(self, create_config: dict[str, Any], extra_kwargs: dict[str, Any]) -> dict[str, Any]:
        """Prime the create_config with additional_kwargs."""
        # Validate the config
        prompt: str | None = create_config.get("prompt")
        messages: list[dict[str, Any]] | None = create_config.get("messages")
        if (prompt is None) == (messages is None):
            raise ValueError("Either prompt or messages should be in create config but not both.")
        context = extra_kwargs.get("context")
        if context is None:
            # No need to instantiate if no context is provided.
            return create_config
        # Instantiate the prompt or messages
        allow_format_str_template = extra_kwargs.get("allow_format_str_template", False)
        # Make a copy of the config
        params = create_config.copy()
        if prompt is not None:
            # Instantiate the prompt
            params["prompt"] = self.instantiate(prompt, context, allow_format_str_template)
        elif context:
            # Instantiate the messages
            params["messages"] = [
                (
                    {
                        **m,
                        "content": self.instantiate(m["content"], context, allow_format_str_template),
                    }
                    if m.get("content")
                    else m
                )
                for m in messages  # type: ignore [union-attr]
            ]
        return params

    def create(self, **config: Any) -> ModelClient.ModelClientResponseProtocol:
        """Make a completion for a given config using available clients.
        Besides the kwargs allowed in openai's [or other] client, we allow the following additional kwargs.
        The config in each client will be overridden by the config.

        Args:
            **config: The config for the completion.

        Raises:
            RuntimeError: If all declared custom model clients are not registered
            APIError: If any model client create call raises an APIError
        """
        # if ERROR:
        #     raise ERROR
        invocation_id = str(uuid.uuid4())
        last = len(self._clients) - 1
        # Check if all configs in config list are activated
        non_activated = [
            client.config["model_client_cls"] for client in self._clients if isinstance(client, PlaceHolderClient)
        ]
        if non_activated:
            raise RuntimeError(
                f"Model client(s) {non_activated} are not activated. Please register the custom model clients using `register_model_client` or filter them out form the config list."
            )

        ordered_clients_indices = list(range(len(self._clients)))
        if self.routing_method == "round_robin" and len(self._clients) > 0:
            ordered_clients_indices = (
                ordered_clients_indices[self._round_robin_index :] + ordered_clients_indices[: self._round_robin_index]
            )
            self._round_robin_index = (self._round_robin_index + 1) % len(self._clients)

        for i in ordered_clients_indices:
            client = self._clients[i]
            # merge the input config with the i-th config in the config list
            full_config = {**config, **self._config_list[i]}
            # separate the config into create_config and extra_kwargs
            create_config, extra_kwargs = self._separate_create_config(full_config)
            # construct the create params
            params = self._construct_create_params(create_config, extra_kwargs)
            # get the cache_seed, filter_func and context
            cache_seed = extra_kwargs.get("cache_seed")
            cache = extra_kwargs.get("cache")
            filter_func = extra_kwargs.get("filter_func")
            context = extra_kwargs.get("context")
            agent = extra_kwargs.get("agent")
            price = extra_kwargs.get("price", None)
            if isinstance(price, list):
                price = tuple(price)
            elif isinstance(price, (float, int)):
                logger.warning(
                    "Input price is a float/int. Using the same price for prompt and completion tokens. Use a list/tuple if prompt and completion token prices are different."
                )
                price = (price, price)

            total_usage = None
            actual_usage = None

            cache_client = None
            if cache is not None:
                # Use the cache object if provided.
                cache_client = cache
            elif cache_seed is not None:
                # Legacy cache behavior, if cache_seed is given, use DiskCache.
                cache_client = Cache.disk(cache_seed, LEGACY_CACHE_DIR)

            log_cache_seed_value(cache if cache is not None else cache_seed, client=client)

            if cache_client is not None:
                with cache_client as cache:
                    # Try to get the response from cache
                    key = get_key(
                        {
                            **params,
                            **{"response_format": json.dumps(TypeAdapter(params["response_format"]).json_schema())},
                        }
                        if "response_format" in params and not isinstance(params["response_format"], dict)
                        else params
                    )
                    request_ts = get_current_ts()

                    response: ModelClient.ModelClientResponseProtocol = cache.get(key, None)

                    if response is not None:
                        response.message_retrieval_function = client.message_retrieval
                        try:
                            response.cost  # type: ignore [attr-defined]
                        except AttributeError:
                            # update attribute if cost is not calculated
                            response.cost = client.cost(response)
                            cache.set(key, response)
                        total_usage = client.get_usage(response)

                        if logging_enabled():
                            # Log the cache hit
                            # TODO: log the config_id and pass_filter etc.
                            log_chat_completion(
                                invocation_id=invocation_id,
                                client_id=id(client),
                                wrapper_id=id(self),
                                agent=agent,
                                request=params,
                                response=response,
                                is_cached=1,
                                cost=response.cost,
                                start_time=request_ts,
                            )

                        # check the filter
                        pass_filter = filter_func is None or filter_func(context=context, response=response)
                        if pass_filter or i == last:
                            # Return the response if it passes the filter or it is the last client
                            response.config_id = i
                            response.pass_filter = pass_filter
                            self._update_usage(actual_usage=actual_usage, total_usage=total_usage)
                            return response
                        continue  # filter is not passed; try the next config
            try:
                request_ts = get_current_ts()
                response = client.create(params)
            except Exception as e:
                if openai_result.is_successful:
                    if APITimeoutError is not None and isinstance(e, APITimeoutError):
                        # logger.debug(f"config {i} timed out", exc_info=True)
                        if i == last:
                            raise TimeoutError(
                                "OpenAI API call timed out. This could be due to congestion or too small a timeout value. The timeout can be specified by setting the 'timeout' value (in seconds) in the llm_config (if you are using agents) or the OpenAIWrapper constructor (if you are using the OpenAIWrapper directly)."
                            ) from e
                    elif APIError is not None and isinstance(e, APIError):
                        error_code = getattr(e, "code", None)
                        if logging_enabled():
                            log_chat_completion(
                                invocation_id=invocation_id,
                                client_id=id(client),
                                wrapper_id=id(self),
                                agent=agent,
                                request=params,
                                response=f"error_code:{error_code}, config {i} failed",
                                is_cached=0,
                                cost=0,
                                start_time=request_ts,
                            )

                        if error_code == "content_filter":
                            # raise the error for content_filter
                            raise
                        # logger.debug(f"config {i} failed", exc_info=True)
                        if i == last:
                            raise
                    else:
                        raise
                else:
                    raise
            except (
                gemini_InternalServerError,
                gemini_ResourceExhausted,
                anthorpic_InternalServerError,
                anthorpic_RateLimitError,
                mistral_SDKError,
                mistral_HTTPValidationError,
                together_TogetherException,
                groq_InternalServerError,
                groq_RateLimitError,
                groq_APIConnectionError,
                cohere_InternalServerError,
                cohere_TooManyRequestsError,
                cohere_ServiceUnavailableError,
                ollama_RequestError,
                ollama_ResponseError,
                bedrock_BotoCoreError,
                bedrock_ClientError,
                cerebras_AuthenticationError,
                cerebras_InternalServerError,
                cerebras_RateLimitError,
            ):
                # logger.debug(f"config {i} failed", exc_info=True)
                if i == last:
                    raise
            else:
                # add cost calculation before caching no matter filter is passed or not
                if price is not None:
                    response.cost = self._cost_with_customized_price(response, price)
                else:
                    response.cost = client.cost(response)
                actual_usage = client.get_usage(response)
                total_usage = actual_usage.copy() if actual_usage is not None else total_usage
                self._update_usage(actual_usage=actual_usage, total_usage=total_usage)

                if cache_client is not None:
                    # Cache the response
                    with cache_client as cache:
                        cache.set(key, response)

                if logging_enabled():
                    # TODO: log the config_id and pass_filter etc.
                    log_chat_completion(
                        invocation_id=invocation_id,
                        client_id=id(client),
                        wrapper_id=id(self),
                        agent=agent,
                        request=params,
                        response=response,
                        is_cached=0,
                        cost=response.cost,
                        start_time=request_ts,
                    )

                response.message_retrieval_function = client.message_retrieval
                # check the filter
                pass_filter = filter_func is None or filter_func(context=context, response=response)
                if pass_filter or i == last:
                    # Return the response if it passes the filter or it is the last client
                    response.config_id = i
                    response.pass_filter = pass_filter
                    return response
                continue  # filter is not passed; try the next config
        raise RuntimeError("Should not reach here.")

    @staticmethod
    def _cost_with_customized_price(
        response: ModelClient.ModelClientResponseProtocol, price_1k: tuple[float, float]
    ) -> None:
        """If a customized cost is passed, overwrite the cost in the response."""
        n_input_tokens = response.usage.prompt_tokens if response.usage is not None else 0  # type: ignore [union-attr]
        n_output_tokens = response.usage.completion_tokens if response.usage is not None else 0  # type: ignore [union-attr]
        if n_output_tokens is None:
            n_output_tokens = 0
        return (n_input_tokens * price_1k[0] + n_output_tokens * price_1k[1]) / 1000

    @staticmethod
    def _update_dict_from_chunk(chunk: BaseModel, d: dict[str, Any], field: str) -> int:
        """Update the dict from the chunk.

        Reads `chunk.field` and if present updates `d[field]` accordingly.

        Args:
            chunk: The chunk.
            d: The dict to be updated in place.
            field: The field.

        Returns:
            The updated dict.

        """
        completion_tokens = 0
        assert isinstance(d, dict), d
        if hasattr(chunk, field) and getattr(chunk, field) is not None:
            new_value = getattr(chunk, field)
            if isinstance(new_value, (list, dict)):
                raise NotImplementedError(
                    f"Field {field} is a list or dict, which is currently not supported. "
                    "Only string and numbers are supported."
                )
            if field not in d:
                d[field] = ""
            if isinstance(new_value, str):
                d[field] += getattr(chunk, field)
            else:
                d[field] = new_value
            completion_tokens = 1

        return completion_tokens

    @staticmethod
    def _update_function_call_from_chunk(
        function_call_chunk: ChoiceDeltaToolCallFunction | ChoiceDeltaFunctionCall,
        full_function_call: dict[str, Any] | None,
        completion_tokens: int,
    ) -> tuple[dict[str, Any], int]:
        """Update the function call from the chunk.

        Args:
            function_call_chunk: The function call chunk.
            full_function_call: The full function call.
            completion_tokens: The number of completion tokens.

        Returns:
            The updated full function call and the updated number of completion tokens.

        """
        # Handle function call
        if function_call_chunk:
            if full_function_call is None:
                full_function_call = {}
            for field in ["name", "arguments"]:
                completion_tokens += OpenAIWrapper._update_dict_from_chunk(
                    function_call_chunk, full_function_call, field
                )

        if full_function_call:
            return full_function_call, completion_tokens
        else:
            raise RuntimeError("Function call is not found, this should not happen.")

    @staticmethod
    def _update_tool_calls_from_chunk(
        tool_calls_chunk: ChoiceDeltaToolCall,
        full_tool_call: dict[str, Any] | None,
        completion_tokens: int,
    ) -> tuple[dict[str, Any], int]:
        """Update the tool call from the chunk.

        Args:
            tool_calls_chunk: The tool call chunk.
            full_tool_call: The full tool call.
            completion_tokens: The number of completion tokens.

        Returns:
            The updated full tool call and the updated number of completion tokens.

        """
        # future proofing for when tool calls other than function calls are supported
        if tool_calls_chunk.type and tool_calls_chunk.type != "function":
            raise NotImplementedError(
                f"Tool call type {tool_calls_chunk.type} is currently not supported. Only function calls are supported."
            )

        # Handle tool call
        assert full_tool_call is None or isinstance(full_tool_call, dict), full_tool_call
        if tool_calls_chunk:
            if full_tool_call is None:
                full_tool_call = {}
            for field in ["index", "id", "type"]:
                completion_tokens += OpenAIWrapper._update_dict_from_chunk(tool_calls_chunk, full_tool_call, field)

            if hasattr(tool_calls_chunk, "function") and tool_calls_chunk.function:
                if "function" not in full_tool_call:
                    full_tool_call["function"] = None

                full_tool_call["function"], completion_tokens = OpenAIWrapper._update_function_call_from_chunk(
                    tool_calls_chunk.function, full_tool_call["function"], completion_tokens
                )

        if full_tool_call:
            return full_tool_call, completion_tokens
        else:
            raise RuntimeError("Tool call is not found, this should not happen.")

    def _update_usage(self, actual_usage, total_usage):
        def update_usage(usage_summary, response_usage):
            # go through RESPONSE_USAGE_KEYS and check that they are in response_usage and if not just return usage_summary
            for key in ModelClient.RESPONSE_USAGE_KEYS:
                if key not in response_usage:
                    return usage_summary

            model = response_usage["model"]
            cost = response_usage["cost"]
            prompt_tokens = response_usage["prompt_tokens"]
            completion_tokens = response_usage["completion_tokens"]
            if completion_tokens is None:
                completion_tokens = 0
            total_tokens = response_usage["total_tokens"]

            if usage_summary is None:
                usage_summary = {"total_cost": cost}
            else:
                usage_summary["total_cost"] += cost

            usage_summary[model] = {
                "cost": usage_summary.get(model, {}).get("cost", 0) + cost,
                "prompt_tokens": usage_summary.get(model, {}).get("prompt_tokens", 0) + prompt_tokens,
                "completion_tokens": usage_summary.get(model, {}).get("completion_tokens", 0) + completion_tokens,
                "total_tokens": usage_summary.get(model, {}).get("total_tokens", 0) + total_tokens,
            }
            return usage_summary

        if total_usage is not None:
            self.total_usage_summary = update_usage(self.total_usage_summary, total_usage)
        if actual_usage is not None:
            self.actual_usage_summary = update_usage(self.actual_usage_summary, actual_usage)

    def print_usage_summary(self, mode: str | list[str] = ["actual", "total"]) -> None:
        """Print the usage summary."""
        iostream = IOStream.get_default()

        if isinstance(mode, list):
            if len(mode) == 0 or len(mode) > 2:
                raise ValueError(f'Invalid mode: {mode}, choose from "actual", "total", ["actual", "total"]')
            if "actual" in mode and "total" in mode:
                mode = "both"
            elif "actual" in mode:
                mode = "actual"
            elif "total" in mode:
                mode = "total"

        iostream.send(
            UsageSummaryEvent(
                actual_usage_summary=self.actual_usage_summary, total_usage_summary=self.total_usage_summary, mode=mode
            )
        )

    def clear_usage_summary(self) -> None:
        """Clear the usage summary."""
        self.total_usage_summary = None
        self.actual_usage_summary = None

    @classmethod
    def extract_text_or_completion_object(
        cls, response: ModelClient.ModelClientResponseProtocol
    ) -> list[str] | list[ModelClient.ModelClientResponseProtocol.Choice.Message]:
        """Extract the text or ChatCompletion objects from a completion or chat response.

        Args:
            response (ChatCompletion | Completion): The response from openai.

        Returns:
            A list of text, or a list of ChatCompletion objects if function_call/tool_calls are present.
        """
        return response.message_retrieval_function(response)


# -----------------------------------------------------------------------------
# New: Responses API config entry (OpenAI-hosted preview endpoint)
# -----------------------------------------------------------------------------


class OpenAIResponsesEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["responses"]

    tool_choice: Literal["none", "auto", "required"] | None
    built_in_tools: list[str] | None


class OpenAIResponsesLLMConfigEntry(OpenAILLMConfigEntry):
    """LLMConfig entry for the OpenAI Responses API (stateful, tool-enabled).

    This reuses all the OpenAI fields but changes *api_type* so the wrapper can
    route traffic to the `client.responses` endpoint instead of
    `chat.completions`.  It inherits everything else  including reasoning
    fields  from *OpenAILLMConfigEntry* so users can simply set

    ```python
    {
        "api_type": "responses",  # <-- key differentiator
        "model": "o3",  # reasoning model
        "reasoning_effort": "medium",  # low / medium / high
        "stream": True,
    }
    ```
    """

    api_type: Literal["responses"] = "responses"
    tool_choice: Literal["none", "auto", "required"] | None = "auto"
    built_in_tools: list[str] | None = None

    def create_client(self) -> ModelClient:  # pragma: no cover
        raise NotImplementedError("Handled via OpenAIWrapper._register_default_client")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from ..cache.cache import Cache
from .anthropic import AnthropicLLMConfigEntry
from .bedrock import BedrockLLMConfigEntry
from .cerebras import CerebrasLLMConfigEntry
from .client import (
    AzureOpenAILLMConfigEntry,
    DeepSeekLLMConfigEntry,
    OpenAILLMConfigEntry,
    OpenAIResponsesLLMConfigEntry,
    OpenAIWrapper,
)
from .cohere import CohereLLMConfigEntry
from .gemini import GeminiLLMConfigEntry
from .groq import GroqLLMConfigEntry
from .mistral import MistralLLMConfigEntry
from .ollama import OllamaLLMConfigEntry
from .openai_utils import (
    config_list_from_dotenv,
    config_list_from_json,
    config_list_from_models,
    config_list_gpt4_gpt35,
    config_list_openai_aoai,
    filter_config,
    get_config_list,
    get_first_llm_config,
)
from .together import TogetherLLMConfigEntry

__all__ = [
    "AnthropicLLMConfigEntry",
    "AzureOpenAILLMConfigEntry",
    "BedrockLLMConfigEntry",
    "Cache",
    "CerebrasLLMConfigEntry",
    "CohereLLMConfigEntry",
    "DeepSeekLLMConfigEntry",
    "GeminiLLMConfigEntry",
    "GroqLLMConfigEntry",
    "MistralLLMConfigEntry",
    "OllamaLLMConfigEntry",
    "OpenAILLMConfigEntry",
    "OpenAIResponsesLLMConfigEntry",
    "OpenAIWrapper",
    "TogetherLLMConfigEntry",
    "config_list_from_dotenv",
    "config_list_from_json",
    "config_list_from_models",
    "config_list_gpt4_gpt35",
    "config_list_openai_aoai",
    "filter_config",
    "get_config_list",
    "get_first_llm_config",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create a OpenAI-compatible client for Gemini features.

Example:
    ```python
    llm_config = {
        "config_list": [
            {
                "api_type": "google",
                "model": "gemini-pro",
                "api_key": os.environ.get("GOOGLE_GEMINI_API_KEY"),
                "safety_settings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
                ],
                "top_p": 0.5,
                "max_tokens": 2048,
                "temperature": 1.0,
                "top_k": 5,
            }
        ]
    }

    agent = autogen.AssistantAgent("my_agent", llm_config=llm_config)
    ```

Resources:
- https://ai.google.dev/docs
- https://cloud.google.com/vertex-ai/generative-ai/docs/migrate/migrate-from-azure-to-gemini
- https://blog.google/technology/ai/google-gemini-pro-imagen-duet-ai-update/
- https://ai.google.dev/api/python/google/generativeai/ChatSession
"""

from __future__ import annotations

import asyncio
import base64
import copy
import json
import logging
import os
import random
import re
import time
import warnings
from io import BytesIO
from typing import Any, Literal

import requests
from pydantic import BaseModel, Field
from typing_extensions import Unpack

from ..import_utils import optional_import_block, require_optional_import
from ..json_utils import resolve_json_references
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import FormatterProtocol
from .gemini_types import ToolConfig
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    import google.genai as genai
    import vertexai
    from PIL import Image
    from google.auth.credentials import Credentials
    from google.genai import types
    from google.genai.types import (
        Content,
        FinishReason,
        FunctionCall,
        FunctionDeclaration,
        FunctionResponse,
        GenerateContentConfig,
        GenerateContentResponse,
        GoogleSearch,
        Part,
        Schema,
        Tool,
        Type,
    )
    from jsonschema import ValidationError
    from vertexai.generative_models import Content as VertexAIContent
    from vertexai.generative_models import FunctionDeclaration as vaiFunctionDeclaration
    from vertexai.generative_models import GenerationConfig, GenerativeModel
    from vertexai.generative_models import (
        GenerationResponse as VertexAIGenerationResponse,
    )
    from vertexai.generative_models import HarmBlockThreshold as VertexAIHarmBlockThreshold
    from vertexai.generative_models import HarmCategory as VertexAIHarmCategory
    from vertexai.generative_models import Part as VertexAIPart
    from vertexai.generative_models import SafetySetting as VertexAISafetySetting
    from vertexai.generative_models import (
        Tool as vaiTool,
    )

logger = logging.getLogger(__name__)


class GeminiEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["google"]

    project_id: str | None
    location: str | None
    google_application_credentials: str | None
    credentials: Any | str | None
    stream: bool
    safety_settings: list[dict[str, Any]] | dict[str, Any] | None
    price: list[float] | None
    tool_config: ToolConfig | None
    proxy: str | None


class GeminiLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["google"] = "google"
    project_id: str | None = None
    location: str | None = None
    # google_application_credentials points to the path of the JSON Keyfile
    google_application_credentials: str | None = None
    # credentials is a google.auth.credentials.Credentials object
    credentials: Any | str | None = None
    stream: bool = False
    safety_settings: list[dict[str, Any]] | dict[str, Any] | None = None
    price: list[float] | None = Field(default=None, min_length=2, max_length=2)
    tool_config: ToolConfig | None = None
    proxy: str | None = None
    """A valid HTTP(S) proxy URL"""

    def create_client(self):
        raise NotImplementedError("GeminiLLMConfigEntry.create_client() is not implemented.")


@require_optional_import(["google", "vertexai", "PIL", "jsonschema"], "gemini")
class GeminiClient:
    """Client for Google's Gemini API."""

    # Mapping, where Key is a term used by Autogen, and Value is a term used by Gemini
    PARAMS_MAPPING = {
        "max_tokens": "max_output_tokens",
        # "n": "candidate_count", # Gemini supports only `n=1`
        "seed": "seed",
        "stop_sequences": "stop_sequences",
        "temperature": "temperature",
        "top_p": "top_p",
        "top_k": "top_k",
        "max_output_tokens": "max_output_tokens",
    }

    def _initialize_vertexai(self, **params: Unpack[GeminiEntryDict]):
        if "google_application_credentials" in params:
            # Path to JSON Keyfile
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = params["google_application_credentials"]
        vertexai_init_args = {}
        if "project_id" in params:
            vertexai_init_args["project"] = params["project_id"]
        if "location" in params:
            vertexai_init_args["location"] = params["location"]
        if "credentials" in params:
            assert isinstance(params["credentials"], Credentials), (
                "Object type google.auth.credentials.Credentials is expected!"
            )
            vertexai_init_args["credentials"] = params["credentials"]
        if vertexai_init_args:
            vertexai.init(**vertexai_init_args)

    def __init__(self, **kwargs):
        """Uses either either api_key for authentication from the LLM config
        (specifying the GOOGLE_GEMINI_API_KEY environment variable also works),
        or follows the Google authentication mechanism for VertexAI in Google Cloud if no api_key is specified,
        where project_id and location can also be passed as parameters. Previously created credentials object can be provided,
        or a Service account key file can also be used. If neither a service account key file, nor the api_key are passed,
        then the default credentials will be used, which could be a personal account if the user is already authenticated in,
        like in Google Cloud Shell.

        Args:
            **kwargs: The keyword arguments to initialize the Gemini client.
        """
        self.api_key = kwargs.get("api_key")
        if not self.api_key:
            self.api_key = os.getenv("GOOGLE_GEMINI_API_KEY")
            if self.api_key is None:
                self.use_vertexai = True
                self._initialize_vertexai(**kwargs)
            else:
                self.use_vertexai = False
        else:
            self.use_vertexai = False
        if not self.use_vertexai:
            assert ("project_id" not in kwargs) and ("location" not in kwargs), (
                "Google Cloud project and compute location cannot be set when using an API Key!"
            )

        self.api_version = kwargs.get("api_version")
        self.proxy = kwargs.get("proxy")

        # Store the response format, if provided (for structured outputs)
        self._response_format: type[BaseModel] | None = None

    def message_retrieval(self, response) -> list:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        return [choice.message for choice in response.choices]

    def cost(self, response) -> float:
        return response.cost

    @staticmethod
    def get_usage(response) -> dict:
        """Return usage summary of the response using RESPONSE_USAGE_KEYS."""
        # ...  # pragma: no cover
        return {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cost": response.cost,
            "model": response.model,
        }

    def create(self, params: dict) -> ChatCompletion:
        # When running in async context via run_in_executor from ConversableAgent.a_generate_oai_reply,
        # this method runs in a new thread that doesn't have an event loop by default. The Google Genai
        # client requires an event loop even for synchronous operations, so we need to ensure one exists.
        try:
            asyncio.get_running_loop()
        except RuntimeError:
            # No event loop exists in this thread (which happens when called from an executor)
            # Create a new event loop for this thread to satisfy Genai client requirements
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        if self.use_vertexai:
            self._initialize_vertexai(**params)
        else:
            assert ("project_id" not in params) and ("location" not in params), (
                "Google Cloud project and compute location cannot be set when using an API Key!"
            )
        model_name = params.get("model", "gemini-pro")

        if model_name == "gemini-pro-vision":
            raise ValueError(
                "Gemini 1.0 Pro vision ('gemini-pro-vision') has been deprecated, please consider switching to a different model, for example 'gemini-2.5-flash'."
            )
        elif not model_name:
            raise ValueError(
                "Please provide a model name for the Gemini Client. "
                "You can configure it in the OAI Config List file. "
                "See this [LLM configuration tutorial](https://docs.ag2.ai/latest/docs/user-guide/basic-concepts/llm-configuration/) for more details."
            )

        http_options = types.HttpOptions()
        if proxy := params.get("proxy", self.proxy):
            http_options.client_args = {"proxy": proxy}
            http_options.async_client_args = {"proxy": proxy}

        if self.api_version:
            http_options.api_version = self.api_version

        messages = params.get("messages", [])
        stream = params.get("stream", False)
        n_response = params.get("n", 1)
        system_instruction = self._extract_system_instruction(messages)
        response_validation = params.get("response_validation", True)
        tools = self._tools_to_gemini_tools(params["tools"]) if "tools" in params else None
        tool_config = params.get("tool_config")

        generation_config = {
            gemini_term: params[autogen_term]
            for autogen_term, gemini_term in self.PARAMS_MAPPING.items()
            if autogen_term in params
        }
        if self.use_vertexai:
            safety_settings = GeminiClient._to_vertexai_safety_settings(params.get("safety_settings", []))
        else:
            safety_settings = params.get("safety_settings", [])

        if stream:
            warnings.warn(
                "Streaming is not supported for Gemini yet, and it will have no effect. Please set stream=False.",
                UserWarning,
            )
            stream = False

        if n_response > 1:
            warnings.warn("Gemini only supports `n=1` for now. We only generate one response.", UserWarning)

        autogen_tool_calls = []

        # Maps the function call ids to function names so we can inject it into FunctionResponse messages
        self.tool_call_function_map: dict[str, str] = {}

        # If response_format exists, we want structured outputs
        # Based on
        # https://ai.google.dev/gemini-api/docs/structured-output?lang=python#supply-schema-in-config
        if params.get("response_format"):
            self._response_format = params.get("response_format")
            generation_config["response_mime_type"] = "application/json"

            response_format_schema_raw = params.get("response_format")

            if isinstance(response_format_schema_raw, dict):
                response_schema = resolve_json_references(response_format_schema_raw)
            else:
                response_schema = resolve_json_references(params.get("response_format").model_json_schema())
            if "$defs" in response_schema:
                response_schema.pop("$defs")
            generation_config["response_schema"] = response_schema

        # A. create and call the chat model.
        gemini_messages = self._oai_messages_to_gemini_messages(messages)
        if self.use_vertexai:
            model = GenerativeModel(
                model_name,
                generation_config=GenerationConfig(**generation_config),
                safety_settings=safety_settings,
                system_instruction=system_instruction,
                tool_config=tool_config,
                tools=tools,
            )

            chat = model.start_chat(history=gemini_messages[:-1], response_validation=response_validation)
            response = chat.send_message(gemini_messages[-1].parts, stream=stream, safety_settings=safety_settings)
        else:
            client = genai.Client(api_key=self.api_key, http_options=http_options)
            generate_content_config = GenerateContentConfig(
                safety_settings=safety_settings,
                system_instruction=system_instruction,
                tools=tools,
                tool_config=tool_config,
                **generation_config,
            )
            chat = client.chats.create(model=model_name, config=generate_content_config, history=gemini_messages[:-1])
            response = chat.send_message(message=gemini_messages[-1].parts)

        # Extract text and tools from response
        ans = ""
        random_id = random.randint(0, 10000)
        prev_function_calls = []
        error_finish_reason = None

        if isinstance(response, GenerateContentResponse):
            if len(response.candidates) != 1:
                raise ValueError(
                    f"Unexpected number of candidates in the response. Expected 1, got {len(response.candidates)}"
                )

            # Look at https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.FinishReason
            if response.candidates[0].finish_reason and response.candidates[0].finish_reason == FinishReason.RECITATION:
                recitation_part = Part(text="Unsuccessful Finish Reason: RECITATION")
                parts = [recitation_part]
                error_finish_reason = "content_filter"  # As per available finish_reason in Choice
            elif not response.candidates[0].content or not response.candidates[0].content.parts:
                error_part = Part(
                    text=f"Unsuccessful Finish Reason: ({str(response.candidates[0].finish_reason)}) NO CONTENT RETURNED"
                )
                parts = [error_part]
                error_finish_reason = "content_filter"  # No other option in Choice in chat_completion.py
            else:
                parts = response.candidates[0].content.parts
        elif isinstance(response, VertexAIGenerationResponse):  # or hasattr(response, "candidates"):
            # google.generativeai also raises an error len(candidates) != 1:
            if len(response.candidates) != 1:
                raise ValueError(
                    f"Unexpected number of candidates in the response. Expected 1, got {len(response.candidates)}"
                )
            parts = response.candidates[0].content.parts
        else:
            raise ValueError(f"Unexpected response type: {type(response)}")

        for part in parts:
            # Function calls
            if fn_call := part.function_call:
                # If we have a repeated function call, ignore it
                if fn_call not in prev_function_calls:
                    autogen_tool_calls.append(
                        ChatCompletionMessageToolCall(
                            id=str(random_id),
                            function={
                                "name": fn_call.name,
                                "arguments": (
                                    json.dumps(dict(fn_call.args.items())) if fn_call.args is not None else ""
                                ),
                            },
                            type="function",
                        )
                    )

                    prev_function_calls.append(fn_call)
                    random_id += 1

            # Plain text content
            elif text := part.text:
                ans += text

        # If we have function calls, ignore the text
        # as it can be Gemini guessing the function response
        if len(autogen_tool_calls) != 0:
            ans = ""
        else:
            autogen_tool_calls = None

        if self._response_format and ans:
            try:
                parsed_response = self._convert_json_response(ans)
                ans = _format_json_response(parsed_response, ans)
            except ValueError as e:
                ans = str(e)

        # 3. convert output
        message = ChatCompletionMessage(
            role="assistant", content=ans, function_call=None, tool_calls=autogen_tool_calls
        )
        choices = [
            Choice(
                finish_reason="tool_calls"
                if autogen_tool_calls is not None
                else error_finish_reason
                if error_finish_reason
                else "stop",
                index=0,
                message=message,
            )
        ]

        prompt_tokens = response.usage_metadata.prompt_token_count
        completion_tokens = (
            response.usage_metadata.candidates_token_count if response.usage_metadata.candidates_token_count else 0
        )

        response_oai = ChatCompletion(
            id=str(random.randint(0, 1000)),
            model=model_name,
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
            ),
            cost=calculate_gemini_cost(self.use_vertexai, prompt_tokens, completion_tokens, model_name),
        )

        return response_oai

    def _extract_system_instruction(self, messages: list[dict]) -> str | None:
        """Extract system instruction if provided."""
        if messages is None or len(messages) == 0 or messages[0].get("role") != "system":
            return None

        message = messages.pop(0)
        content = message["content"]

        # Multi-model uses a list of dictionaries as content with text for the system message
        # Otherwise normal agents will have strings as content
        content = content[0].get("text", "").strip() if isinstance(content, list) else content.strip()

        content = content if len(content) > 0 else None
        return content

    def _oai_content_to_gemini_content(self, message: dict[str, Any]) -> tuple[list[Any], str]:
        """Convert AG2 content to Gemini parts, catering for text and tool calls"""
        rst = []

        if "role" in message and message["role"] == "tool":
            # Tool call recommendation

            function_name = self.tool_call_function_map[message["tool_call_id"]]

            if self.use_vertexai:
                rst.append(
                    VertexAIPart.from_function_response(
                        name=function_name, response={"result": self._to_json_or_str(message["content"])}
                    )
                )
            else:
                rst.append(
                    Part(
                        function_response=FunctionResponse(
                            name=function_name, response={"result": self._to_json_or_str(message["content"])}
                        )
                    )
                )

            return rst, "tool"
        elif "tool_calls" in message and len(message["tool_calls"]) != 0:
            for tool_call in message["tool_calls"]:
                function_id = tool_call["id"]
                function_name = tool_call["function"]["name"]
                self.tool_call_function_map[function_id] = function_name

                if self.use_vertexai:
                    rst.append(
                        VertexAIPart.from_dict({
                            "functionCall": {
                                "name": function_name,
                                "args": json.loads(tool_call["function"]["arguments"]),
                            }
                        })
                    )
                else:
                    rst.append(
                        Part(
                            function_call=FunctionCall(
                                name=function_name,
                                args=json.loads(tool_call["function"]["arguments"]),
                            )
                        )
                    )

            return rst, "tool_call"

        elif isinstance(message["content"], str):
            content = message["content"]
            if content == "":
                content = "empty"  # Empty content is not allowed.
            if self.use_vertexai:
                rst.append(VertexAIPart.from_text(content))
            else:
                rst.append(Part(text=content))

            return rst, "text"

        # For images the message contains a list of text items
        if isinstance(message["content"], list):
            has_image = False
            for msg in message["content"]:
                if isinstance(msg, dict):
                    assert "type" in msg, f"Missing 'type' field in message: {msg}"
                    if msg["type"] == "text":
                        if self.use_vertexai:
                            rst.append(VertexAIPart.from_text(text=msg["text"]))
                        else:
                            rst.append(Part(text=msg["text"]))
                    elif msg["type"] == "image_url":
                        if self.use_vertexai:
                            img_url = msg["image_url"]["url"]
                            img_part = VertexAIPart.from_uri(img_url, mime_type="image/png")
                            rst.append(img_part)
                        else:
                            b64_img = get_image_data(msg["image_url"]["url"])
                            rst.append(Part(inline_data={"mime_type": "image/png", "data": b64_img}))

                        has_image = True
                    else:
                        raise ValueError(f"Unsupported message type: {msg['type']}")
                else:
                    raise ValueError(f"Unsupported message type: {type(msg)}")
            return rst, "image" if has_image else "text"
        else:
            raise Exception("Unable to convert content to Gemini format.")

    def _concat_parts(self, parts: list[Part]) -> list:
        """Concatenate parts with the same type.
        If two adjacent parts both have the "text" attribute, then it will be joined into one part.
        """
        if not parts:
            return []

        concatenated_parts = []
        previous_part = parts[0]

        for current_part in parts[1:]:
            if previous_part.text != "":
                if self.use_vertexai:
                    previous_part = VertexAIPart.from_text(previous_part.text + current_part.text)
                else:
                    previous_part.text += current_part.text
            else:
                concatenated_parts.append(previous_part)
                previous_part = current_part

        if previous_part.text == "":
            if self.use_vertexai:
                previous_part = VertexAIPart.from_text("empty")
            else:
                previous_part.text = "empty"  # Empty content is not allowed.
        concatenated_parts.append(previous_part)

        return concatenated_parts

    def _oai_messages_to_gemini_messages(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Convert messages from OAI format to Gemini format.
        Make sure the "user" role and "model" role are interleaved.
        Also, make sure the last item is from the "user" role.
        """
        rst = []
        for message in messages:
            parts, part_type = self._oai_content_to_gemini_content(message)
            role = "user" if message["role"] in ["user", "system"] else "model"

            if part_type == "text":
                rst.append(
                    VertexAIContent(parts=parts, role=role) if self.use_vertexai else Content(parts=parts, role=role)
                )
            elif part_type == "tool_call":
                # Function calls should be from the model/assistant
                role = "model"
                rst.append(
                    VertexAIContent(parts=parts, role=role) if self.use_vertexai else Content(parts=parts, role=role)
                )
            elif part_type == "tool":
                # Function responses should be from the user
                role = "user"
                rst.append(
                    VertexAIContent(parts=parts, role=role) if self.use_vertexai else Content(parts=parts, role=role)
                )
            elif part_type == "image":
                # Image has multiple parts, some can be text and some can be image based
                text_parts = []
                image_parts = []
                for part in parts:
                    if isinstance(part, Part):
                        # Text or non-Vertex AI image part
                        text_parts.append(part)
                    elif isinstance(part, VertexAIPart):
                        # Image
                        image_parts.append(part)
                    else:
                        raise Exception("Unable to process image part")

                if len(text_parts) > 0:
                    rst.append(
                        VertexAIContent(parts=text_parts, role=role)
                        if self.use_vertexai
                        else Content(parts=text_parts, role=role)
                    )

                if len(image_parts) > 0:
                    rst.append(
                        VertexAIContent(parts=image_parts, role=role)
                        if self.use_vertexai
                        else Content(parts=image_parts, role=role)
                    )

            if len(rst) != 0 and rst[-1] is None:
                rst.pop()

        # The Gemini is restrict on order of roles, such that
        # 1. The first message must be from the user role.
        # 2. The last message must be from the user role.
        # 3. The messages should be interleaved between user and model.
        # We add a dummy message "start chat" if the first role is not the user.
        # We add a dummy message "continue" if the last role is not the user.
        if rst[0].role != "user":
            text_part, _ = self._oai_content_to_gemini_content({"content": "start chat"})
            rst.insert(
                0,
                VertexAIContent(parts=text_part, role="user")
                if self.use_vertexai
                else Content(parts=text_part, role="user"),
            )

        if rst[-1].role != "user":
            text_part, _ = self._oai_content_to_gemini_content({"content": "continue"})
            rst.append(
                VertexAIContent(parts=text_part, role="user")
                if self.use_vertexai
                else Content(parts=text_part, role="user")
            )

        return rst

    def _convert_json_response(self, response: str) -> Any:
        """Extract and validate JSON response from the output for structured outputs.

        Args:
            response (str): The response from the API.

        Returns:
            Any: The parsed JSON response.
        """
        if not self._response_format:
            return response

        try:
            # Parse JSON and validate against the Pydantic model if Pydantic model was provided
            json_data = json.loads(response)
            if isinstance(self._response_format, dict):
                return json_data
            else:
                return self._response_format.model_validate(json_data)
        except Exception as e:
            raise ValueError(f"Failed to parse response as valid JSON matching the schema for Structured Output: {e!s}")

    @staticmethod
    def _convert_type_null_to_nullable(schema: Any) -> Any:
        """Recursively converts all occurrences of {"type": "null"} to {"nullable": True} in a schema."""
        if isinstance(schema, dict):
            # If schema matches {"type": "null"}, replace it
            if schema == {"type": "null"}:
                return {"nullable": True}
            # Otherwise, recursively process dictionary
            return {key: GeminiClient._convert_type_null_to_nullable(value) for key, value in schema.items()}
        elif isinstance(schema, list):
            # Recursively process list elements
            return [GeminiClient._convert_type_null_to_nullable(item) for item in schema]
        return schema

    @staticmethod
    def _check_if_prebuilt_google_search_tool_exists(tools: list[dict[str, Any]]) -> bool:
        """Check if the Google Search tool is present in the tools list."""
        exists = False
        for tool in tools:
            if tool["function"]["name"] == "prebuilt_google_search":
                exists = True
                break

        if exists and len(tools) > 1:
            raise ValueError(
                "Google Search tool can be used only by itself. Please remove other tools from the tools list."
            )

        return exists

    @staticmethod
    def _unwrap_references(function_parameters: dict[str, Any]) -> dict[str, Any]:
        if "properties" not in function_parameters:
            return function_parameters

        function_parameters_copy = copy.deepcopy(function_parameters)

        for property_name, property_value in function_parameters["properties"].items():
            if "$defs" in property_value:
                function_parameters_copy["properties"][property_name] = resolve_json_references(property_value)
                function_parameters_copy["properties"][property_name].pop("$defs")

        return function_parameters_copy

    def _tools_to_gemini_tools(self, tools: list[dict[str, Any]]) -> list[Tool]:
        """Create Gemini tools (as typically requires Callables)"""
        if self._check_if_prebuilt_google_search_tool_exists(tools) and not self.use_vertexai:
            return [Tool(google_search=GoogleSearch())]

        functions = []
        for tool in tools:
            if self.use_vertexai:
                tool["function"]["parameters"] = GeminiClient._convert_type_null_to_nullable(
                    tool["function"]["parameters"]
                )
                function_parameters = GeminiClient._unwrap_references(tool["function"]["parameters"])
                function = vaiFunctionDeclaration(
                    name=tool["function"]["name"],
                    description=tool["function"]["description"],
                    parameters=function_parameters,
                )
            else:
                function = GeminiClient._create_gemini_function_declaration(tool)
            functions.append(function)

        if self.use_vertexai:
            return [vaiTool(function_declarations=functions)]
        else:
            return [Tool(function_declarations=functions)]

    @staticmethod
    def _create_gemini_function_declaration(tool: dict) -> FunctionDeclaration:
        function_declaration = FunctionDeclaration()
        function_declaration.name = tool["function"]["name"]
        function_declaration.description = tool["function"]["description"]
        if len(tool["function"]["parameters"]["properties"]) != 0:
            function_declaration.parameters = GeminiClient._create_gemini_function_parameters(
                copy.deepcopy(tool["function"]["parameters"])
            )

        return function_declaration

    @staticmethod
    def _create_gemini_function_declaration_schema(json_data) -> Schema:
        """Recursively creates Schema objects for FunctionDeclaration."""
        param_schema = Schema()
        param_type = json_data["type"]

        """
        TYPE_UNSPECIFIED = 0
        STRING = 1
        INTEGER = 2
        NUMBER = 3
        OBJECT = 4
        ARRAY = 5
        BOOLEAN = 6
        """

        if param_type == "integer":
            param_schema.type = Type.INTEGER
        elif param_type == "number":
            param_schema.type = Type.NUMBER
        elif param_type == "string":
            param_schema.type = Type.STRING
        elif param_type == "boolean":
            param_schema.type = Type.BOOLEAN
        elif param_type == "array":
            param_schema.type = Type.ARRAY
            if "items" in json_data:
                param_schema.items = GeminiClient._create_gemini_function_declaration_schema(json_data["items"])
            else:
                print("Warning: Array schema missing 'items' definition.")
        elif param_type == "object":
            param_schema.type = Type.OBJECT
            param_schema.properties = {}
            if "properties" in json_data:
                for prop_name, prop_data in json_data["properties"].items():
                    param_schema.properties[prop_name] = GeminiClient._create_gemini_function_declaration_schema(
                        prop_data
                    )
                else:
                    print("Warning: Object schema missing 'properties' definition.")

        elif param_type in ("null", "any"):
            param_schema.type = Type.STRING  # Treating these as strings for simplicity
        else:
            print(f"Warning: Unsupported parameter type '{param_type}'.")

        if "description" in json_data:
            param_schema.description = json_data["description"]

        return param_schema

    @staticmethod
    def _create_gemini_function_parameters(function_parameter: dict[str, any]) -> dict[str, any]:
        """Convert function parameters to Gemini format, recursive"""
        function_parameter = GeminiClient._unwrap_references(function_parameter)

        if "type" in function_parameter:
            function_parameter["type"] = function_parameter["type"].upper()
            # If the schema was created from pydantic BaseModel, it will "title" attribute which needs to be removed
            function_parameter.pop("title", None)

        # Parameter properties and items
        if "properties" in function_parameter:
            for key in function_parameter["properties"]:
                function_parameter["properties"][key] = GeminiClient._create_gemini_function_parameters(
                    function_parameter["properties"][key]
                )

        if "items" in function_parameter:
            function_parameter["items"] = GeminiClient._create_gemini_function_parameters(function_parameter["items"])

        # Remove any attributes not needed
        for attr in ["default"]:
            if attr in function_parameter:
                del function_parameter[attr]

        return function_parameter

    @staticmethod
    def _to_vertexai_safety_settings(safety_settings):
        """Convert safety settings to VertexAI format if needed,
        like when specifying them in the OAI_CONFIG_LIST
        """
        if isinstance(safety_settings, list) and all(
            isinstance(safety_setting, dict) and not isinstance(safety_setting, VertexAISafetySetting)
            for safety_setting in safety_settings
        ):
            vertexai_safety_settings = []
            for safety_setting in safety_settings:
                if safety_setting["category"] not in VertexAIHarmCategory.__members__:
                    invalid_category = safety_setting["category"]
                    logger.error(f"Safety setting category {invalid_category} is invalid")
                elif safety_setting["threshold"] not in VertexAIHarmBlockThreshold.__members__:
                    invalid_threshold = safety_setting["threshold"]
                    logger.error(f"Safety threshold {invalid_threshold} is invalid")
                else:
                    vertexai_safety_setting = VertexAISafetySetting(
                        category=safety_setting["category"],
                        threshold=safety_setting["threshold"],
                    )
                    vertexai_safety_settings.append(vertexai_safety_setting)
            return vertexai_safety_settings
        else:
            return safety_settings

    @staticmethod
    def _to_json_or_str(data: str) -> dict | str:
        try:
            json_data = json.loads(data)
            return json_data
        except (json.JSONDecodeError, ValidationError):
            return data


@require_optional_import(["PIL"], "gemini")
def get_image_data(image_file: str, use_b64=True) -> bytes:
    if image_file.startswith("http://") or image_file.startswith("https://"):
        response = requests.get(image_file)
        content = response.content
    elif re.match(r"data:image/(?:png|jpeg);base64,", image_file):
        return re.sub(r"data:image/(?:png|jpeg);base64,", "", image_file)
    else:
        image = Image.open(image_file).convert("RGB")
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        content = buffered.getvalue()

    if use_b64:
        return base64.b64encode(content).decode("utf-8")
    else:
        return content


def _format_json_response(response: Any, original_answer: str) -> str:
    """Formats the JSON response for structured outputs using the format method if it exists."""
    return response.format() if isinstance(response, FormatterProtocol) else original_answer


def calculate_gemini_cost(use_vertexai: bool, input_tokens: int, output_tokens: int, model_name: str) -> float:
    def total_cost_mil(cost_per_mil_input: float, cost_per_mil_output: float):
        # Cost per million
        return cost_per_mil_input * input_tokens / 1e6 + cost_per_mil_output * output_tokens / 1e6

    def total_cost_k(cost_per_k_input: float, cost_per_k_output: float):
        # Cost per thousand
        return cost_per_k_input * input_tokens / 1e3 + cost_per_k_output * output_tokens / 1e3

    model_name = model_name.lower()
    up_to_128k = input_tokens <= 128000
    up_to_200k = input_tokens <= 200000

    if use_vertexai:
        # Vertex AI pricing - based on Text input
        # https://cloud.google.com/vertex-ai/generative-ai/pricing#vertex-ai-pricing

        if (
            model_name == "gemini-2.5-pro"
            or "gemini-2.5-pro-preview-06-05" in model_name
            or "gemini-2.5-pro-preview-05-06" in model_name
            or "gemini-2.5-pro-preview-03-25" in model_name
        ):
            if up_to_200k:
                return total_cost_mil(1.25, 10)
            else:
                return total_cost_mil(2.5, 15)

        elif "gemini-2.5-flash" in model_name:
            return total_cost_mil(0.3, 2.5)

        elif "gemini-2.5-flash-preview-04-17" in model_name or "gemini-2.5-flash-preview-05-20" in model_name:
            return total_cost_mil(0.15, 0.6)  # NON-THINKING OUTPUT PRICE, $3 FOR THINKING!

        elif "gemini-2.5-flash-lite-preview-06-17" in model_name:
            return total_cost_mil(0.1, 0.4)

        elif "gemini-2.0-flash-lite" in model_name:
            return total_cost_mil(0.075, 0.3)

        elif "gemini-2.0-flash" in model_name:
            return total_cost_mil(0.15, 0.6)

        elif "gemini-1.5-flash" in model_name:
            if up_to_128k:
                return total_cost_k(0.00001875, 0.000075)
            else:
                return total_cost_k(0.0000375, 0.00015)

        elif "gemini-1.5-pro" in model_name:
            if up_to_128k:
                return total_cost_k(0.0003125, 0.00125)
            else:
                return total_cost_k(0.000625, 0.0025)

        elif "gemini-1.0-pro" in model_name:
            return total_cost_k(0.000125, 0.00001875)

        else:
            warnings.warn(
                f"Cost calculation is not implemented for model {model_name}. Cost will be calculated zero.",
                UserWarning,
            )
            return 0

    else:
        # Non-Vertex AI pricing

        if (
            model_name == "gemini-2.5-pro"
            or "gemini-2.5-pro-preview-06-05" in model_name
            or "gemini-2.5-pro-preview-05-06" in model_name
            or "gemini-2.5-pro-preview-03-25" in model_name
        ):
            # https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview
            if up_to_200k:
                return total_cost_mil(1.25, 10)
            else:
                return total_cost_mil(2.5, 15)

        elif "gemini-2.5-flash" in model_name:
            # https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash
            return total_cost_mil(0.3, 2.5)

        elif "gemini-2.5-flash-preview-04-17" in model_name or "gemini-2.5-flash-preview-05-20" in model_name:
            # https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash
            return total_cost_mil(0.15, 0.6)

        elif "gemini-2.5-flash-lite-preview-06-17" in model_name:
            # https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash-lite
            return total_cost_mil(0.1, 0.4)

        elif "gemini-2.0-flash-lite" in model_name:
            # https://ai.google.dev/gemini-api/docs/pricing#gemini-2.0-flash-lite
            return total_cost_mil(0.075, 0.3)

        elif "gemini-2.0-flash" in model_name:
            # https://ai.google.dev/gemini-api/docs/pricing#gemini-2.0-flash
            return total_cost_mil(0.1, 0.4)

        elif "gemini-1.5-flash-8b" in model_name:
            # https://ai.google.dev/pricing#1_5flash-8B
            if up_to_128k:
                return total_cost_mil(0.0375, 0.15)
            else:
                return total_cost_mil(0.075, 0.3)

        elif "gemini-1.5-flash" in model_name:
            # https://ai.google.dev/pricing#1_5flash
            if up_to_128k:
                return total_cost_mil(0.075, 0.3)
            else:
                return total_cost_mil(0.15, 0.6)

        elif "gemini-1.5-pro" in model_name:
            # https://ai.google.dev/pricing#1_5pro
            if up_to_128k:
                return total_cost_mil(1.25, 5.0)
            else:
                return total_cost_mil(2.50, 10.0)

        elif "gemini-1.0-pro" in model_name:
            # https://ai.google.dev/pricing#1_5pro
            return total_cost_mil(0.50, 1.5)

        else:
            warnings.warn(
                f"Cost calculation is not implemented for model {model_name}. Cost will be calculated zero.",
                UserWarning,
            )
            return 0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create an OpenAI-compatible client using Groq's API.

Example:
    ```python
    llm_config = {
        "config_list": [{"api_type": "groq", "model": "mixtral-8x7b-32768", "api_key": os.environ.get("GROQ_API_KEY")}]
    }

    agent = autogen.AssistantAgent("my_agent", llm_config=llm_config)
    ```

Install Groq's python library using: pip install --upgrade groq

Resources:
- https://console.groq.com/docs/quickstart
"""

from __future__ import annotations

import copy
import os
import time
import warnings
from typing import Any, Literal

from pydantic import Field
from typing_extensions import Unpack

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import should_hide_tools, validate_parameter
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    from groq import Groq, Stream

# Cost per thousand tokens - Input / Output (NOTE: Convert $/Million to $/K)
GROQ_PRICING_1K = {
    "llama3-70b-8192": (0.00059, 0.00079),
    "mixtral-8x7b-32768": (0.00024, 0.00024),
    "llama3-8b-8192": (0.00005, 0.00008),
    "gemma-7b-it": (0.00007, 0.00007),
}


class GroqEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["groq"]

    frequency_penalty: float
    presence_penalty: float
    seed: int
    stream: bool
    hide_tools: Literal["if_all_run", "if_any_run", "never"]
    tool_choice: Literal["none", "auto", "required"] | None


class GroqLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["groq"] = "groq"

    frequency_penalty: float = Field(default=None, ge=-2, le=2)
    presence_penalty: float = Field(default=None, ge=-2, le=2)
    seed: int = Field(default=None)
    stream: bool = Field(default=False)
    hide_tools: Literal["if_all_run", "if_any_run", "never"] = "never"
    tool_choice: Literal["none", "auto", "required"] | None = None

    def create_client(self):
        raise NotImplementedError("GroqLLMConfigEntry.create_client is not implemented.")


class GroqClient:
    """Client for Groq's API."""

    def __init__(self, **kwargs: Unpack[GroqEntryDict]):
        """Requires api_key or environment variable to be set

        Args:
            **kwargs: Additional parameters to pass to the Groq API
        """
        # Ensure we have the api_key upon instantiation
        self.api_key = kwargs.get("api_key")
        if not self.api_key:
            self.api_key = os.getenv("GROQ_API_KEY")

        assert self.api_key, (
            "Please include the api_key in your config list entry for Groq or set the GROQ_API_KEY env variable."
        )

        if "response_format" in kwargs and kwargs["response_format"] is not None:
            warnings.warn("response_format is not supported for Groq API, it will be ignored.", UserWarning)
        self.base_url = kwargs.get("base_url")

    def message_retrieval(self, response) -> list:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        return [choice.message for choice in response.choices]

    def cost(self, response) -> float:
        return response.cost

    @staticmethod
    def get_usage(response) -> dict:
        """Return usage summary of the response using RESPONSE_USAGE_KEYS."""
        # ...  # pragma: no cover
        return {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cost": response.cost,
            "model": response.model,
        }

    def parse_params(self, params: dict[str, Any]) -> dict[str, Any]:
        """Loads the parameters for Groq API from the passed in parameters and returns a validated set. Checks types, ranges, and sets defaults"""
        groq_params = {}

        # Check that we have what we need to use Groq's API
        # We won't enforce the available models as they are likely to change
        groq_params["model"] = params.get("model")
        assert groq_params["model"], (
            "Please specify the 'model' in your config list entry to nominate the Groq model to use."
        )

        # Validate allowed Groq parameters
        # https://console.groq.com/docs/api-reference#chat
        groq_params["frequency_penalty"] = validate_parameter(
            params, "frequency_penalty", (int, float), True, None, (-2, 2), None
        )

        groq_params["max_tokens"] = validate_parameter(params, "max_tokens", int, True, None, (0, None), None)
        groq_params["temperature"] = validate_parameter(params, "temperature", (int, float), True, 1, (0, 2), None)
        groq_params["top_p"] = validate_parameter(params, "top_p", (int, float), True, None, None, None)

        groq_params["presence_penalty"] = validate_parameter(
            params, "presence_penalty", (int, float), True, None, (-2, 2), None
        )
        groq_params["seed"] = validate_parameter(params, "seed", int, True, None, None, None)
        groq_params["stream"] = validate_parameter(params, "stream", bool, True, False, None, None)

        if "tool_choice" in params:
            groq_params["tool_choice"] = validate_parameter(
                params, "tool_choice", str, True, None, None, ["none", "auto", "required"]
            )

        # Groq parameters not supported by their models yet, ignoring
        # logit_bias, logprobs, top_logprobs

        # Groq parameters we are ignoring:
        # n (must be 1), response_format (to enforce JSON but needs prompting as well), user,
        # parallel_tool_calls (defaults to True), stop
        # function_call (deprecated), functions (deprecated)
        # tool_choice (none if no tools, auto if there are tools)

        return groq_params

    @require_optional_import("groq", "groq")
    def create(self, params: dict) -> ChatCompletion:
        messages = params.get("messages", [])

        # Convert AG2 messages to Groq messages
        groq_messages = oai_messages_to_groq_messages(messages)

        # Parse parameters to the Groq API's parameters
        groq_params = self.parse_params(params)

        # Add tools to the call if we have them and aren't hiding them
        if "tools" in params:
            hide_tools = validate_parameter(
                params, "hide_tools", str, False, "never", None, ["if_all_run", "if_any_run", "never"]
            )
            if not should_hide_tools(groq_messages, params["tools"], hide_tools):
                groq_params["tools"] = params["tools"]

        groq_params["messages"] = groq_messages

        # We use chat model by default, and set max_retries to 5 (in line with typical retries loop)
        client = Groq(api_key=self.api_key, max_retries=5, base_url=self.base_url)

        # Token counts will be returned
        prompt_tokens = 0
        completion_tokens = 0
        total_tokens = 0

        # Streaming tool call recommendations
        streaming_tool_calls = []

        ans = None
        response = client.chat.completions.create(**groq_params)
        if groq_params["stream"]:
            # Read in the chunks as they stream, taking in tool_calls which may be across
            # multiple chunks if more than one suggested
            ans = ""
            for chunk in response:
                ans = ans + (chunk.choices[0].delta.content or "")

                if chunk.choices[0].delta.tool_calls:
                    # We have a tool call recommendation
                    for tool_call in chunk.choices[0].delta.tool_calls:
                        streaming_tool_calls.append(
                            ChatCompletionMessageToolCall(
                                id=tool_call.id,
                                function={
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments,
                                },
                                type="function",
                            )
                        )

                if chunk.choices[0].finish_reason:
                    prompt_tokens = chunk.x_groq.usage.prompt_tokens
                    completion_tokens = chunk.x_groq.usage.completion_tokens
                    total_tokens = chunk.x_groq.usage.total_tokens
        else:
            # Non-streaming finished
            ans: str = response.choices[0].message.content
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
            total_tokens = response.usage.total_tokens

        if response is not None:
            if isinstance(response, Stream):
                # Streaming response
                if chunk.choices[0].finish_reason == "tool_calls":
                    groq_finish = "tool_calls"
                    tool_calls = streaming_tool_calls
                else:
                    groq_finish = "stop"
                    tool_calls = None

                response_content = ans
                response_id = chunk.id
            else:
                # Non-streaming response
                # If we have tool calls as the response, populate completed tool calls for our return OAI response
                if response.choices[0].finish_reason == "tool_calls":
                    groq_finish = "tool_calls"
                    tool_calls = []
                    for tool_call in response.choices[0].message.tool_calls:
                        tool_calls.append(
                            ChatCompletionMessageToolCall(
                                id=tool_call.id,
                                function={"name": tool_call.function.name, "arguments": tool_call.function.arguments},
                                type="function",
                            )
                        )
                else:
                    groq_finish = "stop"
                    tool_calls = None

                response_content = response.choices[0].message.content
                response_id = response.id
        else:
            raise RuntimeError("Failed to get response from Groq after retrying 5 times.")

        # 3. convert output
        message = ChatCompletionMessage(
            role="assistant",
            content=response_content,
            function_call=None,
            tool_calls=tool_calls,
        )
        choices = [Choice(finish_reason=groq_finish, index=0, message=message)]

        response_oai = ChatCompletion(
            id=response_id,
            model=groq_params["model"],
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
            cost=calculate_groq_cost(prompt_tokens, completion_tokens, groq_params["model"]),
        )

        return response_oai


def oai_messages_to_groq_messages(messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Convert messages from OAI format to Groq's format.
    We correct for any specific role orders and types.
    """
    groq_messages = copy.deepcopy(messages)

    # Remove the name field
    for message in groq_messages:
        if "name" in message:
            message.pop("name", None)

    return groq_messages


def calculate_groq_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    """Calculate the cost of the completion using the Groq pricing."""
    total = 0.0

    if model in GROQ_PRICING_1K:
        input_cost_per_k, output_cost_per_k = GROQ_PRICING_1K[model]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        total = input_cost + output_cost
    else:
        warnings.warn(f"Cost calculation not available for model {model}", UserWarning)

    return total
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

import importlib
import importlib.metadata
import json
import logging
import os
import re
import tempfile
import time
import warnings
from copy import deepcopy
from pathlib import Path
from typing import TYPE_CHECKING, Any, Union

from dotenv import find_dotenv, load_dotenv
from packaging.version import parse
from pydantic_core import to_jsonable_python
from typing_extensions import deprecated

from ..llm_config.utils import config_list_from_json as latest_config_list_from_json
from ..llm_config.utils import filter_config as latest_filter

if TYPE_CHECKING:
    from openai import OpenAI
    from openai.types.beta.assistant import Assistant

    from ..llm_config import LLMConfig

from ..doc_utils import export_module

NON_CACHE_KEY = [
    "api_key",
    "base_url",
    "api_type",
    "api_version",
    "azure_ad_token",
    "azure_ad_token_provider",
    "credentials",
]
DEFAULT_AZURE_API_VERSION = "2024-02-01"

# The below pricing is for 1K tokens. Whenever there is an update in the LLM's pricing,
# Please convert it to 1K tokens and update in the below dictionary in the format: (input_token_price, output_token_price).
OAI_PRICE1K = {
    # https://openai.com/api/pricing/
    # o1
    "o1-preview-2024-09-12": (0.0015, 0.0060),
    "o1-preview": (0.0015, 0.0060),
    "o1-mini-2024-09-12": (0.0003, 0.0012),
    "o1-mini": (0.0003, 0.0012),
    "o1": (0.0015, 0.0060),
    "o1-2024-12-17": (0.0015, 0.0060),
    # o1 pro
    "o1-pro": (0.15, 0.6),  # $150 / $600!
    "o1-pro-2025-03-19": (0.15, 0.6),
    # o3
    "o3": (0.0011, 0.0044),
    "o3-mini-2025-01-31": (0.0011, 0.0044),
    # gpt-5
    "gpt-5": (0.00125, 0.00125),
    "gpt-5-2025-08-07": (0.00125, 0.00125),
    # gpt-5-mini
    "gpt-5-mini": (0.00025, 0.00025),
    "gpt-5-mini-2025-08-07": (0.00025, 0.00025),
    # gpt-5-nano
    "gpt-5-nano": (0.00005, 0.00005),
    "gpt-5-nano-2025-08-07": (0.00005, 0.00005),
    # gpt-4o
    "gpt-4o": (0.005, 0.015),
    "gpt-4o-2024-05-13": (0.005, 0.015),
    "gpt-4o-2024-08-06": (0.0025, 0.01),
    "gpt-4o-2024-11-20": (0.0025, 0.01),
    # gpt-4o-mini
    "gpt-4o-mini": (0.000150, 0.000600),
    "gpt-4o-mini-2024-07-18": (0.000150, 0.000600),
    # gpt-4-turbo
    "gpt-4-turbo-2024-04-09": (0.01, 0.03),
    # gpt-4
    "gpt-4": (0.03, 0.06),
    "gpt-4-32k": (0.06, 0.12),
    # gpt-4.1
    "gpt-4.1": (0.002, 0.008),
    "gpt-4.1-2025-04-14": (0.002, 0.008),
    # gpt-4.1 mini
    "gpt-4.1-mini": (0.0004, 0.0016),
    "gpt-4.1-mini-2025-04-14": (0.0004, 0.0016),
    # gpt-4.1 nano
    "gpt-4.1-nano": (0.0001, 0.0004),
    "gpt-4.1-nano-2025-04-14": (0.0001, 0.0004),
    # gpt-3.5 turbo
    "gpt-3.5-turbo": (0.0005, 0.0015),  # default is 0125
    "gpt-3.5-turbo-0125": (0.0005, 0.0015),  # 16k
    "gpt-3.5-turbo-instruct": (0.0015, 0.002),
    # base model
    "davinci-002": 0.002,
    "babbage-002": 0.0004,
    # old model
    "gpt-4-0125-preview": (0.01, 0.03),
    "gpt-4-1106-preview": (0.01, 0.03),
    "gpt-4-1106-vision-preview": (0.01, 0.03),  # TODO: support vision pricing of images
    "gpt-3.5-turbo-1106": (0.001, 0.002),
    "gpt-3.5-turbo-0613": (0.0015, 0.002),
    # "gpt-3.5-turbo-16k": (0.003, 0.004),
    "gpt-3.5-turbo-16k-0613": (0.003, 0.004),
    "gpt-3.5-turbo-0301": (0.0015, 0.002),
    "text-ada-001": 0.0004,
    "text-babbage-001": 0.0005,
    "text-curie-001": 0.002,
    "code-cushman-001": 0.024,
    "code-davinci-002": 0.1,
    "text-davinci-002": 0.02,
    "text-davinci-003": 0.02,
    "gpt-4-0314": (0.03, 0.06),  # deprecate in Sep
    "gpt-4-32k-0314": (0.06, 0.12),  # deprecate in Sep
    "gpt-4-0613": (0.03, 0.06),
    "gpt-4-32k-0613": (0.06, 0.12),
    "gpt-4-turbo-preview": (0.01, 0.03),
    # https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/#pricing
    "gpt-35-turbo": (0.0005, 0.0015),  # what's the default? using 0125 here.
    "gpt-35-turbo-0125": (0.0005, 0.0015),
    "gpt-35-turbo-instruct": (0.0015, 0.002),
    "gpt-35-turbo-1106": (0.001, 0.002),
    "gpt-35-turbo-0613": (0.0015, 0.002),
    "gpt-35-turbo-0301": (0.0015, 0.002),
    "gpt-35-turbo-16k": (0.003, 0.004),
    "gpt-35-turbo-16k-0613": (0.003, 0.004),
    # deepseek
    "deepseek-chat": (0.00027, 0.0011),
}


def get_key(config: dict[str, Any]) -> str:
    """Get a unique identifier of a configuration.

    Args:
        config (dict or list): A configuration.

    Returns:
        tuple: A unique identifier which can be used as a key for a dict.
    """
    copied = False
    for key in NON_CACHE_KEY:
        if key in config:
            config, copied = config.copy() if not copied else config, True
            config.pop(key)
    return to_jsonable_python(config)  # type: ignore [no-any-return]


def is_valid_api_key(api_key: str) -> bool:
    """Determine if input is valid OpenAI API key. As of 2024-09-24 there's no official definition of the key structure
    so we will allow anything starting with "sk-" and having at least 48 alphanumeric (plus underscore and dash) characters.
    Keys are known to start with "sk-", "sk-proj", "sk-None", and "sk-svcaat"

    Args:
        api_key (str): An input string to be validated.

    Returns:
        bool: A boolean that indicates if input is valid OpenAI API key.
    """
    api_key_re = re.compile(r"^sk-[A-Za-z0-9_-]{48,}$")
    return bool(re.fullmatch(api_key_re, api_key))


@export_module("autogen")
def get_config_list(
    api_keys: list[str],
    base_urls: list[str] | None = None,
    api_type: str | None = None,
    api_version: str | None = None,
) -> list[dict[str, Any]]:
    """Get a list of configs for OpenAI API client.

    Args:
        api_keys (list): The api keys for openai api calls.
        base_urls (list, optional): The api bases for openai api calls. If provided, should match the length of api_keys.
        api_type (str, optional): The api type for openai api calls.
        api_version (str, optional): The api version for openai api calls.

    Returns:
        list: A list of configs for OepnAI API calls.

    Example:
    ```python
    # Define a list of API keys
    api_keys = ["key1", "key2", "key3"]

    # Optionally, define a list of base URLs corresponding to each API key
    base_urls = ["https://api.service1.com", "https://api.service2.com", "https://api.service3.com"]

    # Optionally, define the API type and version if they are common for all keys
    api_type = "azure"
    api_version = "2024-02-01"

    # Call the get_config_list function to get a list of configuration dictionaries
    config_list = get_config_list(api_keys, base_urls, api_type, api_version)
    ```

    """
    if base_urls is not None:
        assert len(api_keys) == len(base_urls), "The length of api_keys must match the length of base_urls"
    config_list = []
    for i, api_key in enumerate(api_keys):
        if not api_key.strip():
            continue
        config = {"api_key": api_key}
        if base_urls:
            config["base_url"] = base_urls[i]
        if api_type:
            config["api_type"] = api_type
        if api_version:
            config["api_version"] = api_version
        config_list.append(config)
    return config_list


@export_module("autogen")
def get_first_llm_config(
    llm_config: Union["LLMConfig", dict[str, Any]],
) -> dict[str, Any]:
    """Get the first LLM config from the given LLM config.

    Args:
        llm_config (dict): The LLM config.

    Returns:
        dict: The first LLM config.

    Raises:
        ValueError: If the LLM config is invalid.
    """
    llm_config = deepcopy(llm_config)
    if "config_list" not in llm_config:
        if "model" in llm_config:
            return llm_config  # type: ignore [return-value]
        raise ValueError("llm_config must be a valid config dictionary.")

    if len(llm_config["config_list"]) == 0:
        raise ValueError("Config list must contain at least one config.")

    to_return = llm_config["config_list"][0]
    return to_return if isinstance(to_return, dict) else to_return.model_dump()  # type: ignore [no-any-return]


@export_module("autogen")
def config_list_openai_aoai(
    key_file_path: str | None = ".",
    openai_api_key_file: str | None = "key_openai.txt",
    aoai_api_key_file: str | None = "key_aoai.txt",
    openai_api_base_file: str | None = "base_openai.txt",
    aoai_api_base_file: str | None = "base_aoai.txt",
    exclude: str | None = None,
) -> list[dict[str, Any]]:
    """Get a list of configs for OpenAI API client (including Azure or local model deployments that support OpenAI's chat completion API).

    This function constructs configurations by reading API keys and base URLs from environment variables or text files.
    It supports configurations for both OpenAI and Azure OpenAI services, allowing for the exclusion of one or the other.
    When text files are used, the environment variables will be overwritten.
    To prevent text files from being used, set the corresponding file name to None.
    Or set key_file_path to None to disallow reading from text files.

    Args:
        key_file_path (str, optional): The directory path where the API key files are located. Defaults to the current directory.
        openai_api_key_file (str, optional): The filename containing the OpenAI API key. Defaults to 'key_openai.txt'.
        aoai_api_key_file (str, optional): The filename containing the Azure OpenAI API key. Defaults to 'key_aoai.txt'.
        openai_api_base_file (str, optional): The filename containing the OpenAI API base URL. Defaults to 'base_openai.txt'.
        aoai_api_base_file (str, optional): The filename containing the Azure OpenAI API base URL. Defaults to 'base_aoai.txt'.
        exclude (str, optional): The API type to exclude from the configuration list. Can be 'openai' or 'aoai'. Defaults to None.

    Returns:
        List[Dict]: A list of configuration dictionaries. Each dictionary contains keys for 'api_key',
            and optionally 'base_url', 'api_type', and 'api_version'.

    Raises:
        FileNotFoundError: If the specified key files are not found and the corresponding API key is not set in the environment variables.

    Example:
        # To generate configurations excluding Azure OpenAI:
        configs = config_list_openai_aoai(exclude='aoai')

    File samples:
        - key_aoai.txt

        ```
        aoai-12345abcdef67890ghijklmnopqr
        aoai-09876zyxwvuts54321fedcba
        ```

        - base_aoai.txt

        ```
        https://api.azure.com/v1
        https://api.azure2.com/v1
        ```

    Notes:
        - The function checks for API keys and base URLs in the following environment variables: 'OPENAI_API_KEY', 'AZURE_OPENAI_API_KEY',
          'OPENAI_API_BASE' and 'AZURE_OPENAI_API_BASE'. If these are not found, it attempts to read from the specified files in the
          'key_file_path' directory.
        - The API version for Azure configurations is set to DEFAULT_AZURE_API_VERSION by default.
        - If 'exclude' is set to 'openai', only Azure OpenAI configurations are returned, and vice versa.
        - The function assumes that the API keys and base URLs in the environment variables are separated by new lines if there are
          multiple entries.
    """
    if exclude != "openai" and key_file_path is not None:
        # skip if key_file_path is None
        if openai_api_key_file is not None:
            # skip if openai_api_key_file is None
            try:
                with open(f"{key_file_path}/{openai_api_key_file}") as key_file:
                    os.environ["OPENAI_API_KEY"] = key_file.read().strip()
            except FileNotFoundError:
                logging.info(
                    "OPENAI_API_KEY is not found in os.environ "
                    "and key_openai.txt is not found in the specified path. You can specify the api_key in the config_list."
                )
        if openai_api_base_file is not None:
            # skip if openai_api_base_file is None
            try:
                with open(f"{key_file_path}/{openai_api_base_file}") as key_file:
                    os.environ["OPENAI_API_BASE"] = key_file.read().strip()
            except FileNotFoundError:
                logging.info(
                    "OPENAI_API_BASE is not found in os.environ "
                    "and base_openai.txt is not found in the specified path. You can specify the base_url in the config_list."
                )
    if exclude != "aoai" and key_file_path is not None:
        # skip if key_file_path is None
        if aoai_api_key_file is not None:
            try:
                with open(f"{key_file_path}/{aoai_api_key_file}") as key_file:
                    os.environ["AZURE_OPENAI_API_KEY"] = key_file.read().strip()
            except FileNotFoundError:
                logging.info(
                    "AZURE_OPENAI_API_KEY is not found in os.environ "
                    "and key_aoai.txt is not found in the specified path. You can specify the api_key in the config_list."
                )
        if aoai_api_base_file is not None:
            try:
                with open(f"{key_file_path}/{aoai_api_base_file}") as key_file:
                    os.environ["AZURE_OPENAI_API_BASE"] = key_file.read().strip()
            except FileNotFoundError:
                logging.info(
                    "AZURE_OPENAI_API_BASE is not found in os.environ "
                    "and base_aoai.txt is not found in the specified path. You can specify the base_url in the config_list."
                )
    aoai_config = (
        get_config_list(
            # Assuming Azure OpenAI api keys in os.environ["AZURE_OPENAI_API_KEY"], in separated lines
            api_keys=os.environ.get("AZURE_OPENAI_API_KEY", "").split("\n"),
            # Assuming Azure OpenAI api bases in os.environ["AZURE_OPENAI_API_BASE"], in separated lines
            base_urls=os.environ.get("AZURE_OPENAI_API_BASE", "").split("\n"),
            api_type="azure",
            api_version=DEFAULT_AZURE_API_VERSION,
        )
        if exclude != "aoai"
        else []
    )
    # process openai base urls
    base_urls_env_var = os.environ.get("OPENAI_API_BASE", None)
    base_urls = base_urls_env_var if base_urls_env_var is None else base_urls_env_var.split("\n")
    openai_config = (
        get_config_list(
            # Assuming OpenAI API_KEY in os.environ["OPENAI_API_KEY"]
            api_keys=os.environ.get("OPENAI_API_KEY", "").split("\n"),
            base_urls=base_urls,
        )
        if exclude != "openai"
        else []
    )
    config_list = openai_config + aoai_config
    return config_list


@export_module("autogen")
def config_list_from_models(
    key_file_path: str | None = ".",
    openai_api_key_file: str | None = "key_openai.txt",
    aoai_api_key_file: str | None = "key_aoai.txt",
    aoai_api_base_file: str | None = "base_aoai.txt",
    exclude: str | None = None,
    model_list: list[str] | None = None,
) -> list[dict[str, Any]]:
    """Get a list of configs for API calls with models specified in the model list.

    This function extends `config_list_openai_aoai` by allowing to clone its' out for each of the models provided.
    Each configuration will have a 'model' key with the model name as its value. This is particularly useful when
    all endpoints have same set of models.

    Args:
        key_file_path (str, optional): The path to the key files.
        openai_api_key_file (str, optional): The file name of the OpenAI API key.
        aoai_api_key_file (str, optional): The file name of the Azure OpenAI API key.
        aoai_api_base_file (str, optional): The file name of the Azure OpenAI API base.
        exclude (str, optional): The API type to exclude, "openai" or "aoai".
        model_list (list, optional): The list of model names to include in the configs.

    Returns:
        list: A list of configs for OpenAI API calls, each including model information.

    Example:
    ```python
    # Define the path where the API key files are located
    key_file_path = "/path/to/key/files"

    # Define the file names for the OpenAI and Azure OpenAI API keys and bases
    openai_api_key_file = "key_openai.txt"
    aoai_api_key_file = "key_aoai.txt"
    aoai_api_base_file = "base_aoai.txt"

    # Define the list of models for which to create configurations
    model_list = ["gpt-4", "gpt-3.5-turbo"]

    # Call the function to get a list of configuration dictionaries
    config_list = config_list_from_models(
        key_file_path=key_file_path,
        openai_api_key_file=openai_api_key_file,
        aoai_api_key_file=aoai_api_key_file,
        aoai_api_base_file=aoai_api_base_file,
        model_list=model_list,
    )

    # The `config_list` will contain configurations for the specified models, for example:
    # [
    #     {'api_key': '...', 'base_url': 'https://api.openai.com', 'model': 'gpt-4'},
    #     {'api_key': '...', 'base_url': 'https://api.openai.com', 'model': 'gpt-3.5-turbo'}
    # ]
    ```
    """
    config_list = config_list_openai_aoai(
        key_file_path=key_file_path,
        openai_api_key_file=openai_api_key_file,
        aoai_api_key_file=aoai_api_key_file,
        aoai_api_base_file=aoai_api_base_file,
        exclude=exclude,
    )
    if model_list:
        config_list = [{**config, "model": model} for model in model_list for config in config_list]
    return config_list


@export_module("autogen")
def config_list_gpt4_gpt35(
    key_file_path: str | None = ".",
    openai_api_key_file: str | None = "key_openai.txt",
    aoai_api_key_file: str | None = "key_aoai.txt",
    aoai_api_base_file: str | None = "base_aoai.txt",
    exclude: str | None = None,
) -> list[dict[str, Any]]:
    """Get a list of configs for 'gpt-4' followed by 'gpt-3.5-turbo' API calls.

    Args:
        key_file_path (str, optional): The path to the key files.
        openai_api_key_file (str, optional): The file name of the openai api key.
        aoai_api_key_file (str, optional): The file name of the azure openai api key.
        aoai_api_base_file (str, optional): The file name of the azure openai api base.
        exclude (str, optional): The api type to exclude, "openai" or "aoai".

    Returns:
        list: A list of configs for openai api calls.
    """
    return config_list_from_models(
        key_file_path,
        openai_api_key_file,
        aoai_api_key_file,
        aoai_api_base_file,
        exclude,
        model_list=["gpt-4", "gpt-3.5-turbo"],
    )


@export_module("autogen")
@deprecated(
    "`autogen.filter_config(...)` is deprecated. "
    'Please use the "autogen.LLMConfig.from_json(path="OAI_CONFIG_LIST").where(model="gpt-4o")" method instead. '
    "Scheduled for removal in 0.11.0 version."
)
def filter_config(
    config_list: list[dict[str, Any]],
    filter_dict: dict[str, list[str | None] | set[str | None]] | None,
    exclude: bool = False,
) -> list[dict[str, Any]]:
    """Filter configuration dictionaries based on specified criteria.

    This function filters a list of configuration dictionaries by applying ALL criteria specified in `filter_dict`.
    A configuration is included in the result if it satisfies every key-value constraint in the filter dictionary.
    For each filter key, the configuration's corresponding field value must match at least one of the acceptable
    values (OR logic within each criteria, AND logic between different criteria).

    Args:
        config_list (list of dict): A list of configuration dictionaries to be filtered.

        filter_dict (dict, optional): A dictionary specifying filter criteria where:
            - Keys are field names to check in each configuration dictionary
            - Values can be:
                * a single string value (e.g., {"model": "gpt-4o"})
                * a list or set of acceptable values for that field (e.g., {"model": ["gpt-4o", "gpt-4o-mini"]})
            - A configuration matches if ALL filter keys are satisfied AND for each key,
              the config's field value matches at least one acceptable value
            - If a filter value includes None, configurations missing that field will match
            - If None, no filtering is applied

        exclude (bool, optional): If False (default), return configurations that match the filter.
                                If True, return configurations that do NOT match the filter.

    Returns:
        list of dict: Filtered list of configuration dictionaries.

    Matching Logic:
        - **Between different filter keys**: AND logic (all criteria must be satisfied)
        - **Within each filter key's values**: OR logic (any acceptable value can match)
        - **For list-type config values**: Match if there's any intersection with acceptable values
        - **For scalar config values**: Match if the value is in the list of acceptable values
        - **Missing fields**: Only match if None is included in the acceptable values for that field

    Examples:
        ```python
        configs = [
            {"model": "gpt-3.5-turbo", "api_type": "openai"},
            {"model": "gpt-4", "api_type": "openai"},
            {"model": "gpt-3.5-turbo", "api_type": "azure", "api_version": "2024-02-01"},
            {"model": "gpt-4", "tags": ["premium", "latest"]},
        ]

        # Example 1: Single criterion with single string
        filter_dict = {"model": "gpt-4o"}
        result = filter_config(configs, filter_dict)
        # Returns: [{"model": "gpt-4o", "api_type": "openai"}] if present

        # Example 2: Single criterion - matches any model in the list
        filter_dict = {"model": ["gpt-4", "gpt-4o"]}
        result = filter_config(configs, filter_dict)
        # Returns: [{"model": "gpt-4", "api_type": "openai"}, {"model": "gpt-4", "tags": ["premium", "latest"]}]

        # Example 3: Multiple criteria - must satisfy ALL the conditions
        filter_dict = {"model": ["gpt-3.5-turbo"], "api_type": ["azure"]}
        result = filter_config(configs, filter_dict)
        # Returns: [{"model": "gpt-3.5-turbo", "api_type": "azure", "api_version": "2024-02-01"}]

        # Example 4: Tag filtering with list intersection
        filter_dict = {"tags": ["premium"]}
        result = filter_config(configs, filter_dict)
        # Returns: [{"model": "gpt-4", "tags": ["premium", "latest"]}]

        # Example 5: Exclude matching configurations
        filter_dict = {"api_type": ["openai"]}
        result = filter_config(configs, filter_dict, exclude=True)
        # Returns configs that do NOT have api_type="openai"
        ```
    Note:
        - If `filter_dict` is empty or None, no filtering is applied and `config_list` is returned as is.
        - If a configuration dictionary in `config_list` does not contain a key specified in `filter_dict`,
          it is considered a non-match and is excluded from the result.

    """
    warnings.warn(
        "`autogen.filter_config(...)` is deprecated. "
        'Please use the "autogen.LLMConfig.from_json(path="OAI_CONFIG_LIST").where(model="gpt-4o")" method instead. '
        "Scheduled for removal in 0.11.0 version.",
        DeprecationWarning,
    )

    return latest_filter(config_list=config_list, filter_dict=filter_dict, exclude=exclude)


@export_module("autogen")
@deprecated(
    "`autogen.config_list_from_json(...)` is deprecated. "
    'Please use the "autogen.LLMConfig.from_json(path="OAI_CONFIG_LIST")" method instead. '
    "Scheduled for removal in 0.11.0 version."
)
def config_list_from_json(
    env_or_file: str,
    file_location: str | None = "",
    filter_dict: dict[str, list[str | None] | set[str | None]] | None = None,
) -> list[dict[str, Any]]:
    """Retrieves a list of API configurations from a JSON stored in an environment variable or a file.

    This function attempts to parse JSON data from the given `env_or_file` parameter. If `env_or_file` is an
    environment variable containing JSON data, it will be used directly. Otherwise, it is assumed to be a filename,
    and the function will attempt to read the file from the specified `file_location`.

    The `filter_dict` parameter allows for filtering the configurations based on specified criteria. Each key in the
    `filter_dict` corresponds to a field in the configuration dictionaries, and the associated value is a list or set
    of acceptable values for that field. If a field is missing in a configuration and `None` is included in the list
    of acceptable values for that field, the configuration will still be considered a match.

    Args:
        env_or_file (str): The name of the environment variable, the filename, or the environment variable of the filename
            that containing the JSON data.
        file_location (str, optional): The directory path where the file is located, if `env_or_file` is a filename.
        filter_dict (dict, optional): A dictionary specifying the filtering criteria for the configurations, with
            keys representing field names and values being lists or sets of acceptable values for those fields.

    Example:
    ```python
    # Suppose we have an environment variable 'CONFIG_JSON' with the following content:
    # '[{"model": "gpt-3.5-turbo", "api_type": "azure"}, {"model": "gpt-4"}]'

    # We can retrieve a filtered list of configurations like this:
    filter_criteria = {"model": ["gpt-3.5-turbo"]}
    configs = config_list_from_json("CONFIG_JSON", filter_dict=filter_criteria)
    # The 'configs' variable will now contain only the configurations that match the filter criteria.
    ```

    Returns:
        List[Dict]: A list of configuration dictionaries that match the filtering criteria specified in `filter_dict`.

    Raises:
        FileNotFoundError: if env_or_file is neither found as an environment variable nor a file
    """
    warnings.warn(
        "`autogen.config_list_from_json(...)` is deprecated. "
        'Please use the "autogen.LLMConfig.from_json(path="OAI_CONFIG_LIST")" method instead. '
        "Scheduled for removal in 0.11.0 version.",
        DeprecationWarning,
    )

    return latest_config_list_from_json(
        env_or_file=env_or_file,
        file_location=file_location,
        filter_dict=filter_dict,
    )


def get_config(
    api_key: str | None,
    base_url: str | None = None,
    api_type: str | None = None,
    api_version: str | None = None,
) -> dict[str, Any]:
    """Constructs a configuration dictionary for a single model with the provided API configurations.

    Example:
    ```python
    config = get_config(api_key="sk-abcdef1234567890", base_url="https://api.openai.com", api_version="v1")
    # The 'config' variable will now contain:
    # {
    #     "api_key": "sk-abcdef1234567890",
    #     "base_url": "https://api.openai.com",
    #     "api_version": "v1"
    # }
    ```

    Args:
        api_key (str): The API key for authenticating API requests.
        base_url (Optional[str]): The base URL of the API. If not provided, defaults to None.
        api_type (Optional[str]): The type of API. If not provided, defaults to None.
        api_version (Optional[str]): The version of the API. If not provided, defaults to None.

    Returns:
        Dict: A dictionary containing the provided API configurations.
    """
    config = {"api_key": api_key}
    if base_url:
        config["base_url"] = os.getenv(base_url, default=base_url)
    if api_type:
        config["api_type"] = os.getenv(api_type, default=api_type)
    if api_version:
        config["api_version"] = os.getenv(api_version, default=api_version)
    return config


@export_module("autogen")
def config_list_from_dotenv(
    dotenv_file_path: str | None = None,
    model_api_key_map: dict[str, Any] | None = None,
    filter_dict: dict[str, list[str | None] | set[str | None]] | None = None,
) -> list[dict[str, str | set[str]]]:
    """Load API configurations from a specified .env file or environment variables and construct a list of configurations.

    This function will:
    - Load API keys from a provided .env file or from existing environment variables.
    - Create a configuration dictionary for each model using the API keys and additional configurations.
    - Filter and return the configurations based on provided filters.

    model_api_key_map will default to `{"gpt-4": "OPENAI_API_KEY", "gpt-3.5-turbo": "OPENAI_API_KEY"}` if none

    Args:
        dotenv_file_path (str, optional): The path to the .env file. Defaults to None.
        model_api_key_map (str/dict, optional): A dictionary mapping models to their API key configurations.
                                           If a string is provided as configuration, it is considered as an environment
                                           variable name storing the API key.
                                           If a dict is provided, it should contain at least 'api_key_env_var' key,
                                           and optionally other API configurations like 'base_url', 'api_type', and 'api_version'.
                                           Defaults to a basic map with 'gpt-4' and 'gpt-3.5-turbo' mapped to 'OPENAI_API_KEY'.
        filter_dict (dict, optional): A dictionary containing the models to be loaded.
                                      Containing a 'model' key mapped to a set of model names to be loaded.
                                      Defaults to None, which loads all found configurations.

    Returns:
        List[Dict[str, Union[str, Set[str]]]]: A list of configuration dictionaries for each model.

    Raises:
        FileNotFoundError: If the specified .env file does not exist.
        TypeError: If an unsupported type of configuration is provided in model_api_key_map.
    """
    if dotenv_file_path:
        dotenv_path = Path(dotenv_file_path)
        if dotenv_path.exists():
            load_dotenv(dotenv_path)
        else:
            logging.warning(f"The specified .env file {dotenv_path} does not exist.")
    else:
        dotenv_path_str = find_dotenv()
        if not dotenv_path_str:
            logging.warning("No .env file found. Loading configurations from environment variables.")
        dotenv_path = Path(dotenv_path_str)
        load_dotenv(dotenv_path)

    # Ensure the model_api_key_map is not None to prevent TypeErrors during key assignment.
    model_api_key_map = model_api_key_map or {}

    # Ensure default models are always considered
    default_models = ["gpt-4", "gpt-3.5-turbo"]

    for model in default_models:
        # Only assign default API key if the model is not present in the map.
        # If model is present but set to invalid/empty, do not overwrite.
        if model not in model_api_key_map:
            model_api_key_map[model] = "OPENAI_API_KEY"

    env_var = []
    # Loop over the models and create configuration dictionaries
    for model, config in model_api_key_map.items():
        if isinstance(config, str):
            api_key_env_var = config
            config_dict = get_config(api_key=os.getenv(api_key_env_var))
        elif isinstance(config, dict):
            api_key = os.getenv(config.get("api_key_env_var", "OPENAI_API_KEY"))
            config_without_key_var = {k: v for k, v in config.items() if k != "api_key_env_var"}
            config_dict = get_config(api_key=api_key, **config_without_key_var)
        else:
            logging.warning(
                "Unsupported configuration type encountered for a model. Please check your model_api_key_map."
            )

        if not config_dict["api_key"] or config_dict["api_key"].strip() == "":
            logging.warning("API key not found or empty for a model. Please ensure path to .env file is correct.")
            continue  # Skip this configuration and continue with the next

        # Add model to the configuration and append to the list
        config_dict["model"] = model
        env_var.append(config_dict)

    fd, temp_name = tempfile.mkstemp()
    try:
        with os.fdopen(fd, "w+") as temp:
            env_var_str = json.dumps(env_var)
            temp.write(env_var_str)
            temp.flush()

            # Assuming config_list_from_json is a valid function from your code
            config_list = config_list_from_json(env_or_file=temp_name, filter_dict=filter_dict)
    finally:
        # The file is deleted after using its name (to prevent windows build from breaking)
        os.remove(temp_name)

    if len(config_list) == 0:
        logging.error("No configurations loaded.")
        return []

    logging.info(f"Models available: {[config['model'] for config in config_list]}")
    return config_list


def retrieve_assistants_by_name(client: "OpenAI", name: str) -> list["Assistant"]:
    """Return the assistants with the given name from OAI assistant API"""
    assistants = client.beta.assistants.list()
    candidate_assistants = []
    for assistant in assistants.data:
        if assistant.name == name:
            candidate_assistants.append(assistant)
    return candidate_assistants


def detect_gpt_assistant_api_version() -> str:
    """Detect the openai assistant API version"""
    oai_version = importlib.metadata.version("openai")
    return "v1" if parse(oai_version) < parse("1.21") else "v2"


def create_gpt_vector_store(client: "OpenAI", name: str, fild_ids: list[str]) -> Any:
    """Create a openai vector store for gpt assistant"""
    try:
        vector_store = client.vector_stores.create(name=name)
    except Exception as e:
        raise AttributeError(f"Failed to create vector store, please install the latest OpenAI python package: {e}")

    # poll the status of the file batch for completion.
    batch = client.vector_stores.file_batches.create_and_poll(vector_store_id=vector_store.id, file_ids=fild_ids)

    if batch.status == "in_progress":
        time.sleep(1)
        logging.debug(f"file batch status: {batch.file_counts}")
        batch = client.vector_stores.file_batches.poll(vector_store_id=vector_store.id, batch_id=batch.id)

    if batch.status == "completed":
        return vector_store

    raise ValueError(f"Failed to upload files to vector store {vector_store.id}:{batch.status}")


def create_gpt_assistant(
    client: "OpenAI", name: str, instructions: str, model: str, assistant_config: dict[str, Any]
) -> "Assistant":
    """Create a openai gpt assistant"""
    assistant_create_kwargs = {}
    gpt_assistant_api_version = detect_gpt_assistant_api_version()
    tools = assistant_config.get("tools", [])

    if gpt_assistant_api_version == "v2":
        tool_resources = assistant_config.get("tool_resources", {})
        file_ids = assistant_config.get("file_ids")
        if tool_resources.get("file_search") is not None and file_ids is not None:
            raise ValueError(
                "Cannot specify both `tool_resources['file_search']` tool and `file_ids` in the assistant config."
            )

        # Designed for backwards compatibility for the V1 API
        # Instead of V1 AssistantFile, files are attached to Assistants using the tool_resources object.
        for tool in tools:
            if tool["type"] == "retrieval":
                tool["type"] = "file_search"
                if file_ids is not None:
                    # create a vector store for the file search tool
                    vs = create_gpt_vector_store(client, f"{name}-vectorestore", file_ids)
                    tool_resources["file_search"] = {
                        "vector_store_ids": [vs.id],
                    }
            elif tool["type"] == "code_interpreter" and file_ids is not None:
                tool_resources["code_interpreter"] = {
                    "file_ids": file_ids,
                }

        assistant_create_kwargs["tools"] = tools
        if len(tool_resources) > 0:
            assistant_create_kwargs["tool_resources"] = tool_resources
    else:
        # not support forwards compatibility
        if "tool_resources" in assistant_config:
            raise ValueError("`tool_resources` argument are not supported in the openai assistant V1 API.")
        if any(tool["type"] == "file_search" for tool in tools):
            raise ValueError(
                "`file_search` tool are not supported in the openai assistant V1 API, please use `retrieval`."
            )
        assistant_create_kwargs["tools"] = tools
        assistant_create_kwargs["file_ids"] = assistant_config.get("file_ids", [])

    logging.info(f"Creating assistant with config: {assistant_create_kwargs}")
    return client.beta.assistants.create(name=name, instructions=instructions, model=model, **assistant_create_kwargs)


def update_gpt_assistant(client: "OpenAI", assistant_id: str, assistant_config: dict[str, Any]) -> "Assistant":
    """Update openai gpt assistant"""
    gpt_assistant_api_version = detect_gpt_assistant_api_version()
    assistant_update_kwargs = {}

    if assistant_config.get("tools") is not None:
        assistant_update_kwargs["tools"] = assistant_config["tools"]

    if assistant_config.get("instructions") is not None:
        assistant_update_kwargs["instructions"] = assistant_config["instructions"]

    if gpt_assistant_api_version == "v2":
        if assistant_config.get("tool_resources") is not None:
            assistant_update_kwargs["tool_resources"] = assistant_config["tool_resources"]
    else:
        if assistant_config.get("file_ids") is not None:
            assistant_update_kwargs["file_ids"] = assistant_config["file_ids"]

    return client.beta.assistants.update(assistant_id=assistant_id, **assistant_update_kwargs)


def _satisfies(config_value: Any, acceptable_values: Any) -> bool:
    if isinstance(config_value, list):
        return bool(set(config_value) & set(acceptable_values))  # Non-empty intersection
    else:
        return config_value in acceptable_values
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Taken over from https://github.com/openai/openai-python/blob/16a10604fbd0d82c1382b84b417a1d6a2d33a7f1/src/openai/types/chat/chat_completion_message.py

# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.


from typing import Literal

from ._models import BaseModel
from .chat_completion_audio import ChatCompletionAudio
from .chat_completion_message_tool_call import (
    ChatCompletionMessageCustomToolCall,
    ChatCompletionMessageFunctionToolCall,
)

__all__ = ["Annotation", "AnnotationURLCitation", "ChatCompletionMessage", "FunctionCall"]


class AnnotationURLCitation(BaseModel):
    end_index: int
    """The index of the last character of the URL citation in the message."""

    start_index: int
    """The index of the first character of the URL citation in the message."""

    title: str
    """The title of the web resource."""

    url: str
    """The URL of the web resource."""


class Annotation(BaseModel):
    type: Literal["url_citation"]
    """The type of the URL citation. Always `url_citation`."""

    url_citation: AnnotationURLCitation
    """A URL citation when using web search."""


class FunctionCall(BaseModel):
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: str
    """The name of the function to call."""


class ChatCompletionMessage(BaseModel):
    content: str | None = None
    """The contents of the message."""

    refusal: str | None = None
    """The refusal message generated by the model."""

    role: Literal["assistant"]
    """The role of the author of this message."""

    annotations: list[Annotation] | None = None
    """
    Annotations for the message, when applicable, as when using the
    [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
    """

    audio: ChatCompletionAudio | None = None
    """
    If the audio output modality is requested, this object contains data about the
    audio response from the model.
    [Learn more](https://platform.openai.com/docs/guides/audio).
    """

    function_call: FunctionCall | None = None
    """Deprecated and replaced by `tool_calls`.

    The name and arguments of a function that should be called, as generated by the
    model.
    """

    tool_calls: list[ChatCompletionMessageFunctionToolCall | ChatCompletionMessageCustomToolCall] | None = None
    """The tool calls generated by the model, such as function calls."""
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Taken over from https://github.com/openai/openai-python/blob/3e69750d47df4f0759d4a28ddc68e4b38756d9ca/src/openai/types/chat/chat_completion.py

# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from collections.abc import Callable
from typing import Any, Literal

from ._models import BaseModel
from .chat_completion_message import ChatCompletionMessage
from .chat_completion_token_logprob import ChatCompletionTokenLogprob
from .completion_usage import CompletionUsage

__all__ = ["ChatCompletion", "Choice", "ChoiceLogprobs"]


class ChoiceLogprobs(BaseModel):
    content: list[ChatCompletionTokenLogprob] | None = None
    """A list of message content tokens with log probability information."""

    refusal: list[ChatCompletionTokenLogprob] | None = None
    """A list of message refusal tokens with log probability information."""


class Choice(BaseModel):
    finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, `length` if the maximum number of tokens specified in the request was
    reached, `content_filter` if content was omitted due to a flag from our content
    filters, `tool_calls` if the model called a tool, or `function_call`
    (deprecated) if the model called a function.
    """

    index: int
    """The index of the choice in the list of choices."""

    logprobs: ChoiceLogprobs | None = None
    """Log probability information for the choice."""

    message: ChatCompletionMessage
    """A chat completion message generated by the model."""


class ChatCompletion(BaseModel):
    id: str
    """A unique identifier for the chat completion."""

    choices: list[Choice]
    """A list of chat completion choices.

    Can be more than one if `n` is greater than 1.
    """

    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    """The model used for the chat completion."""

    object: Literal["chat.completion"]
    """The object type, which is always `chat.completion`."""

    service_tier: Literal["auto", "default", "flex", "scale", "priority"] | None = None
    """The service tier used for processing the request."""

    system_fingerprint: str | None = None
    """This fingerprint represents the backend configuration that the model runs with.

    Can be used in conjunction with the `seed` request parameter to understand when
    backend changes have been made that might impact determinism.
    """

    usage: CompletionUsage | None = None
    """Usage statistics for the completion request."""


class ChatCompletionExtended(ChatCompletion):
    message_retrieval_function: Callable[[Any, "ChatCompletion"], list[ChatCompletionMessage]] | None = None
    config_id: str | None = None
    pass_filter: Callable[..., bool] | None = None
    cost: float | None = None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Taken over from https://github.com/openai/openai-python/blob/3e69750d47df4f0759d4a28ddc68e4b38756d9ca/src/openai/types/chat/chat_completion_token_logprob.py

# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.


from ._models import BaseModel

__all__ = ["ChatCompletionTokenLogprob", "TopLogprob"]


class TopLogprob(BaseModel):
    token: str
    """The token."""

    bytes: list[int] | None = None
    """A list of integers representing the UTF-8 bytes representation of the token.

    Useful in instances where characters are represented by multiple tokens and
    their byte representations must be combined to generate the correct text
    representation. Can be `null` if there is no bytes representation for the token.
    """

    logprob: float
    """The log probability of this token, if it is within the top 20 most likely
    tokens.

    Otherwise, the value `-9999.0` is used to signify that the token is very
    unlikely.
    """


class ChatCompletionTokenLogprob(BaseModel):
    token: str
    """The token."""

    bytes: list[int] | None = None
    """A list of integers representing the UTF-8 bytes representation of the token.

    Useful in instances where characters are represented by multiple tokens and
    their byte representations must be combined to generate the correct text
    representation. Can be `null` if there is no bytes representation for the token.
    """

    logprob: float
    """The log probability of this token, if it is within the top 20 most likely
    tokens.

    Otherwise, the value `-9999.0` is used to signify that the token is very
    unlikely.
    """

    top_logprobs: list[TopLogprob]
    """List of the most likely tokens and their log probability, at this token
    position.

    In rare cases, there may be fewer than the number of requested `top_logprobs`
    returned.
    """
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .chat_completion import ChatCompletionExtended as ChatCompletion
from .chat_completion import Choice
from .chat_completion_message import ChatCompletionMessage
from .chat_completion_message_tool_call import (
    ChatCompletionMessageCustomToolCall,
    ChatCompletionMessageFunctionToolCall,
    ChatCompletionMessageToolCall,
)
from .completion_usage import CompletionUsage

__all__ = [
    "ChatCompletion",
    "ChatCompletionMessage",
    "ChatCompletionMessageCustomToolCall",
    "ChatCompletionMessageFunctionToolCall",
    "ChatCompletionMessageToolCall",
    "Choice",
    "CompletionUsage",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Taken over from https://github.com/openai/openai-python/blob/3e69750d47df4f0759d4a28ddc68e4b38756d9ca/src/openai/types/completion_usage.py

# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.


from ._models import BaseModel

__all__ = ["CompletionTokensDetails", "CompletionUsage", "PromptTokensDetails"]


class CompletionTokensDetails(BaseModel):
    accepted_prediction_tokens: int | None = None
    """
    When using Predicted Outputs, the number of tokens in the prediction that
    appeared in the completion.
    """

    audio_tokens: int | None = None
    """Audio input tokens generated by the model."""

    reasoning_tokens: int | None = None
    """Tokens generated by the model for reasoning."""

    rejected_prediction_tokens: int | None = None
    """
    When using Predicted Outputs, the number of tokens in the prediction that did
    not appear in the completion. However, like reasoning tokens, these tokens are
    still counted in the total completion tokens for purposes of billing, output,
    and context window limits.
    """


class PromptTokensDetails(BaseModel):
    audio_tokens: int | None = None
    """Audio input tokens present in the prompt."""

    cached_tokens: int | None = None
    """Cached tokens present in the prompt."""


class CompletionUsage(BaseModel):
    completion_tokens: int
    """Number of tokens in the generated completion."""

    prompt_tokens: int
    """Number of tokens in the prompt."""

    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""

    completion_tokens_details: CompletionTokensDetails | None = None
    """Breakdown of tokens used in a completion."""

    prompt_tokens_details: PromptTokensDetails | None = None
    """Breakdown of tokens used in the prompt."""
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Taken over from https://github.com/openai/openai-python/blob/main/src/openai/_models.py


import pydantic
import pydantic.generics
from pydantic import ConfigDict

__all__ = ["BaseModel"]


class BaseModel(pydantic.BaseModel):
    model_config = ConfigDict(extra="allow")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Taken over from https://github.com/openai/openai-python/blob/3e69750d47df4f0759d4a28ddc68e4b38756d9ca/src/openai/types/chat/chat_completion_message_tool_call.py

# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Literal

from ._models import BaseModel

__all__ = ["ChatCompletionMessageCustomToolCall", "ChatCompletionMessageFunctionToolCall", "Custom", "Function"]


class Function(BaseModel):
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: str
    """The name of the function to call."""


class Custom(BaseModel):
    input: str
    """The input to the custom tool."""

    name: str
    """The name of the custom tool."""


class ChatCompletionMessageFunctionToolCall(BaseModel):
    id: str
    """The ID of the tool call."""

    function: Function
    """The function that the model called."""

    type: Literal["function"]
    """The type of the tool. Currently, only `function` is supported."""


class ChatCompletionMessageCustomToolCall(BaseModel):
    id: str
    """The ID of the tool call."""

    custom: Custom
    """The custom tool that the model called."""

    type: Literal["custom"]
    """The type of the tool. Currently, only `custom` is supported."""


# Backward compatibility alias
ChatCompletionMessageToolCall = ChatCompletionMessageFunctionToolCall
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

# Taken over from https://github.com/openai/openai-python/blob/3e69750d47df4f0759d4a28ddc68e4b38756d9ca/src/openai/types/chat/chat_completion_audio.py

# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.


from ._models import BaseModel

__all__ = ["ChatCompletionAudio"]


class ChatCompletionAudio(BaseModel):
    id: str
    """Unique identifier for this audio response."""

    data: str
    """
    Base64 encoded audio bytes generated by the model, in the format specified in
    the request.
    """

    expires_at: int
    """
    The Unix timestamp (in seconds) for when this audio response will no longer be
    accessible on the server for use in multi-turn conversations.
    """

    transcript: str
    """Transcript of the audio generated by the model."""
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create an OpenAI-compatible client for the Anthropic API.

Example usage:
Install the `anthropic` package by running `pip install --upgrade anthropic`.
- https://docs.anthropic.com/en/docs/quickstart-guide

```python
import autogen

config_list = [
    {
        "model": "claude-3-sonnet-20240229",
        "api_key": os.getenv("ANTHROPIC_API_KEY"),
        "api_type": "anthropic",
    }
]

assistant = autogen.AssistantAgent("assistant", llm_config={"config_list": config_list})
```

Example usage for Anthropic Bedrock:

Install the `anthropic` package by running `pip install --upgrade anthropic`.
- https://docs.anthropic.com/en/docs/quickstart-guide

```python
import autogen

config_list = [
    {
        "model": "anthropic.claude-3-5-sonnet-20240620-v1:0",
        "aws_access_key":<accessKey>,
        "aws_secret_key":<secretKey>,
        "aws_session_token":<sessionTok>,
        "aws_region":"us-east-1",
        "api_type": "anthropic",
    }
]

assistant = autogen.AssistantAgent("assistant", llm_config={"config_list": config_list})
```

Example usage for Anthropic VertexAI:

Install the `anthropic` package by running `pip install anthropic[vertex]`.
- https://docs.anthropic.com/en/docs/quickstart-guide

```python

import autogen
config_list = [
    {
        "model": "claude-3-5-sonnet-20240620-v1:0",
        "gcp_project_id": "dummy_project_id",
        "gcp_region": "us-west-2",
        "gcp_auth_token": "dummy_auth_token",
        "api_type": "anthropic",
    }
]

assistant = autogen.AssistantAgent("assistant", llm_config={"config_list": config_list})
```python
"""

from __future__ import annotations

import inspect
import json
import os
import re
import time
import warnings
from typing import Any, Literal

from pydantic import BaseModel, Field
from typing_extensions import Unpack

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import FormatterProtocol, validate_parameter
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    from anthropic import Anthropic, AnthropicBedrock, AnthropicVertex
    from anthropic import __version__ as anthropic_version
    from anthropic.types import Message, TextBlock, ToolUseBlock

    TOOL_ENABLED = anthropic_version >= "0.23.1"
    if TOOL_ENABLED:
        pass


ANTHROPIC_PRICING_1k = {
    "claude-3-7-sonnet-20250219": (0.003, 0.015),
    "claude-3-5-sonnet-20241022": (0.003, 0.015),
    "claude-3-5-haiku-20241022": (0.0008, 0.004),
    "claude-3-5-sonnet-20240620": (0.003, 0.015),
    "claude-3-sonnet-20240229": (0.003, 0.015),
    "claude-3-opus-20240229": (0.015, 0.075),
    "claude-3-haiku-20240307": (0.00025, 0.00125),
    "claude-2.1": (0.008, 0.024),
    "claude-2.0": (0.008, 0.024),
    "claude-instant-1.2": (0.008, 0.024),
}


class AnthropicEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["anthropic"]
    timeout: int | None
    stop_sequences: list[str] | None
    stream: bool
    price: list[float] | None
    tool_choice: dict | None
    thinking: dict | None
    gcp_project_id: str | None
    gcp_region: str | None
    gcp_auth_token: str | None


class AnthropicLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["anthropic"] = "anthropic"

    # Basic options
    max_tokens: int = Field(default=4096, ge=1)
    temperature: float | None = Field(default=None, ge=0.0, le=1.0)
    top_p: float | None = Field(default=None, ge=0.0, le=1.0)

    # Anthropic-specific options
    timeout: int | None = Field(default=None, ge=1)
    top_k: int | None = Field(default=None, ge=1)
    stop_sequences: list[str] | None = None
    stream: bool = False
    price: list[float] | None = Field(default=None, min_length=2, max_length=2)
    tool_choice: dict | None = None
    thinking: dict | None = None

    gcp_project_id: str | None = None
    gcp_region: str | None = None
    gcp_auth_token: str | None = None

    def create_client(self):
        raise NotImplementedError("AnthropicLLMConfigEntry.create_client is not implemented.")


@require_optional_import("anthropic", "anthropic")
class AnthropicClient:
    def __init__(self, **kwargs: Unpack[AnthropicEntryDict]):
        """Initialize the Anthropic API client.

        Args:
            **kwargs: The configuration parameters for the client.
        """
        self._api_key = kwargs.get("api_key") or os.getenv("ANTHROPIC_API_KEY")
        self._aws_access_key = kwargs.get("aws_access_key") or os.getenv("AWS_ACCESS_KEY")
        self._aws_secret_key = kwargs.get("aws_secret_key") or os.getenv("AWS_SECRET_KEY")
        self._aws_session_token = kwargs.get("aws_session_token")
        self._aws_region = kwargs.get("aws_region") or os.getenv("AWS_REGION")
        self._gcp_project_id = kwargs.get("gcp_project_id")
        self._gcp_region = kwargs.get("gcp_region") or os.getenv("GCP_REGION")
        self._gcp_auth_token = kwargs.get("gcp_auth_token")
        self._base_url = kwargs.get("base_url")

        if self._api_key is None:
            if self._aws_region:
                if self._aws_access_key is None or self._aws_secret_key is None:
                    raise ValueError("API key or AWS credentials are required to use the Anthropic API.")
            elif self._gcp_region:
                if self._gcp_project_id is None or self._gcp_region is None:
                    raise ValueError("API key or GCP credentials are required to use the Anthropic API.")
            else:
                raise ValueError("API key or AWS credentials or GCP credentials are required to use the Anthropic API.")

        if self._api_key is not None:
            client_kwargs = {"api_key": self._api_key}
            if self._base_url:
                client_kwargs["base_url"] = self._base_url
            self._client = Anthropic(**client_kwargs)
        elif self._gcp_region is not None:
            kw = {}
            for p in inspect.signature(AnthropicVertex).parameters:
                if hasattr(self, f"_gcp_{p}"):
                    kw[p] = getattr(self, f"_gcp_{p}")
            if self._base_url:
                kw["base_url"] = self._base_url
            self._client = AnthropicVertex(**kw)
        else:
            client_kwargs = {
                "aws_access_key": self._aws_access_key,
                "aws_secret_key": self._aws_secret_key,
                "aws_session_token": self._aws_session_token,
                "aws_region": self._aws_region,
            }
            if self._base_url:
                client_kwargs["base_url"] = self._base_url
            self._client = AnthropicBedrock(**client_kwargs)

        self._last_tooluse_status = {}

        # Store the response format, if provided (for structured outputs)
        self._response_format: type[BaseModel] | None = None

    def load_config(self, params: dict[str, Any]):
        """Load the configuration for the Anthropic API client."""
        anthropic_params = {}

        anthropic_params["model"] = params.get("model")
        assert anthropic_params["model"], "Please provide a `model` in the config_list to use the Anthropic API."

        anthropic_params["temperature"] = validate_parameter(
            params, "temperature", (float, int), False, 1.0, (0.0, 1.0), None
        )
        anthropic_params["max_tokens"] = validate_parameter(params, "max_tokens", int, False, 4096, (1, None), None)
        anthropic_params["timeout"] = validate_parameter(params, "timeout", int, True, None, (1, None), None)
        anthropic_params["top_k"] = validate_parameter(params, "top_k", int, True, None, (1, None), None)
        anthropic_params["top_p"] = validate_parameter(params, "top_p", (float, int), True, None, (0.0, 1.0), None)
        anthropic_params["stop_sequences"] = validate_parameter(params, "stop_sequences", list, True, None, None, None)
        anthropic_params["stream"] = validate_parameter(params, "stream", bool, False, False, None, None)
        if "thinking" in params:
            anthropic_params["thinking"] = params["thinking"]

        if anthropic_params["stream"]:
            warnings.warn(
                "Streaming is not currently supported, streaming will be disabled.",
                UserWarning,
            )
            anthropic_params["stream"] = False

        # Note the Anthropic API supports "tool" for tool_choice but you must specify the tool name so we will ignore that here
        # Dictionary, see options here: https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#controlling-claudes-output
        # type = auto, any, tool, none | name = the name of the tool if type=tool
        anthropic_params["tool_choice"] = validate_parameter(params, "tool_choice", dict, True, None, None, None)

        return anthropic_params

    def cost(self, response) -> float:
        """Calculate the cost of the completion using the Anthropic pricing."""
        return response.cost

    @property
    def api_key(self):
        return self._api_key

    @property
    def aws_access_key(self):
        return self._aws_access_key

    @property
    def aws_secret_key(self):
        return self._aws_secret_key

    @property
    def aws_session_token(self):
        return self._aws_session_token

    @property
    def aws_region(self):
        return self._aws_region

    @property
    def gcp_project_id(self):
        return self._gcp_project_id

    @property
    def gcp_region(self):
        return self._gcp_region

    @property
    def gcp_auth_token(self):
        return self._gcp_auth_token

    def create(self, params: dict[str, Any]) -> ChatCompletion:
        """Creates a completion using the Anthropic API."""
        if "tools" in params:
            converted_functions = self.convert_tools_to_functions(params["tools"])
            params["functions"] = params.get("functions", []) + converted_functions

        # Convert AG2 messages to Anthropic messages
        anthropic_messages = oai_messages_to_anthropic_messages(params)
        anthropic_params = self.load_config(params)

        # If response_format exists, we want structured outputs
        # Anthropic doesn't support response_format, so using Anthropic's "JSON Mode":
        # https://github.com/anthropics/anthropic-cookbook/blob/main/misc/how_to_enable_json_mode.ipynb
        if params.get("response_format"):
            self._response_format = params["response_format"]
            self._add_response_format_to_system(params)

        # TODO: support stream
        params = params.copy()
        if "functions" in params:
            tools_configs = params.pop("functions")
            tools_configs = [self.openai_func_to_anthropic(tool) for tool in tools_configs]
            params["tools"] = tools_configs

        # Anthropic doesn't accept None values, so we need to use keyword argument unpacking instead of setting parameters.
        # Copy params we need into anthropic_params
        # Remove any that don't have values
        anthropic_params["messages"] = anthropic_messages
        if "system" in params:
            anthropic_params["system"] = params["system"]
        if "tools" in params:
            anthropic_params["tools"] = params["tools"]
        if anthropic_params["top_k"] is None:
            del anthropic_params["top_k"]
        if anthropic_params["top_p"] is None:
            del anthropic_params["top_p"]
        if anthropic_params["stop_sequences"] is None:
            del anthropic_params["stop_sequences"]
        if anthropic_params["tool_choice"] is None:
            del anthropic_params["tool_choice"]

        response = self._client.messages.create(**anthropic_params)

        tool_calls = []
        message_text = ""

        if self._response_format:
            try:
                parsed_response = self._extract_json_response(response)
                message_text = _format_json_response(parsed_response)
            except ValueError as e:
                message_text = str(e)

            anthropic_finish = "stop"
        else:
            if response is not None:
                # If we have tool use as the response, populate completed tool calls for our return OAI response
                if response.stop_reason == "tool_use":
                    anthropic_finish = "tool_calls"
                    for content in response.content:
                        if type(content) == ToolUseBlock:
                            tool_calls.append(
                                ChatCompletionMessageToolCall(
                                    id=content.id,
                                    function={"name": content.name, "arguments": json.dumps(content.input)},
                                    type="function",
                                )
                            )
                else:
                    anthropic_finish = "stop"
                    tool_calls = None

                # Retrieve any text content from the response
                for content in response.content:
                    if type(content) == TextBlock:
                        message_text = content.text
                        break

        # Calculate and save the cost onto the response
        prompt_tokens = response.usage.input_tokens
        completion_tokens = response.usage.output_tokens

        # Convert output back to AG2 response format
        message = ChatCompletionMessage(
            role="assistant",
            content=message_text,
            function_call=None,
            tool_calls=tool_calls,
        )
        choices = [Choice(finish_reason=anthropic_finish, index=0, message=message)]

        response_oai = ChatCompletion(
            id=response.id,
            model=anthropic_params["model"],
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
            ),
            cost=_calculate_cost(prompt_tokens, completion_tokens, anthropic_params["model"]),
        )

        return response_oai

    def message_retrieval(self, response) -> list:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        return [choice.message for choice in response.choices]

    @staticmethod
    def openai_func_to_anthropic(openai_func: dict) -> dict:
        res = openai_func.copy()
        res["input_schema"] = res.pop("parameters")
        return res

    @staticmethod
    def get_usage(response: ChatCompletion) -> dict:
        """Get the usage of tokens and their cost information."""
        return {
            "prompt_tokens": response.usage.prompt_tokens if response.usage is not None else 0,
            "completion_tokens": response.usage.completion_tokens if response.usage is not None else 0,
            "total_tokens": response.usage.total_tokens if response.usage is not None else 0,
            "cost": response.cost if hasattr(response, "cost") else 0.0,
            "model": response.model,
        }

    @staticmethod
    def convert_tools_to_functions(tools: list) -> list:
        """Convert tool definitions into Anthropic-compatible functions,
        updating nested $ref paths in property schemas.

        Args:
            tools (list): List of tool definitions.

        Returns:
            list: List of functions with updated $ref paths.
        """

        def update_refs(obj, defs_keys, prop_name):
            """Recursively update $ref values that start with "#/$defs/"."""
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key == "$ref" and isinstance(value, str) and value.startswith("#/$defs/"):
                        ref_key = value[len("#/$defs/") :]
                        if ref_key in defs_keys:
                            obj[key] = f"#/properties/{prop_name}/$defs/{ref_key}"
                    else:
                        update_refs(value, defs_keys, prop_name)
            elif isinstance(obj, list):
                for item in obj:
                    update_refs(item, defs_keys, prop_name)

        functions = []
        for tool in tools:
            if tool.get("type") == "function" and "function" in tool:
                function = tool["function"]
                parameters = function.get("parameters", {})
                properties = parameters.get("properties", {})
                for prop_name, prop_schema in properties.items():
                    if "$defs" in prop_schema:
                        defs_keys = set(prop_schema["$defs"].keys())
                        update_refs(prop_schema, defs_keys, prop_name)
                functions.append(function)
        return functions

    def _add_response_format_to_system(self, params: dict[str, Any]):
        """Add prompt that will generate properly formatted JSON for structured outputs to system parameter.

        Based on Anthropic's JSON Mode cookbook, we ask the LLM to put the JSON within <json_response> tags.

        Args:
            params (dict): The client parameters
        """
        if not params.get("system"):
            return

        # Get the schema of the Pydantic model
        if isinstance(self._response_format, dict):
            schema = self._response_format
        else:
            schema = self._response_format.model_json_schema()

        # Add instructions for JSON formatting
        format_content = f"""Please provide your response as a JSON object that matches the following schema:
{json.dumps(schema, indent=2)}

Format your response as valid JSON within <json_response> tags.
Do not include any text before or after the tags.
Ensure the JSON is properly formatted and matches the schema exactly."""

        # Add formatting to last user message
        params["system"] += "\n\n" + format_content

    def _extract_json_response(self, response: Message) -> Any:
        """Extract and validate JSON response from the output for structured outputs.

        Args:
            response (Message): The response from the API.

        Returns:
            Any: The parsed JSON response.
        """
        if not self._response_format:
            return response

        # Extract content from response
        content = response.content[0].text if response.content else ""

        # Try to extract JSON from tags first
        json_match = re.search(r"<json_response>(.*?)</json_response>", content, re.DOTALL)
        if json_match:
            json_str = json_match.group(1).strip()
        else:
            # Fallback to finding first JSON object
            json_start = content.find("{")
            json_end = content.rfind("}")
            if json_start == -1 or json_end == -1:
                raise ValueError("No valid JSON found in response for Structured Output.")
            json_str = content[json_start : json_end + 1]

        try:
            # Parse JSON and validate against the Pydantic model if Pydantic model was provided
            json_data = json.loads(json_str)
            if isinstance(self._response_format, dict):
                return json_str
            else:
                return self._response_format.model_validate(json_data)

        except Exception as e:
            raise ValueError(f"Failed to parse response as valid JSON matching the schema for Structured Output: {e!s}")


def _format_json_response(response: Any) -> str:
    """Formats the JSON response for structured outputs using the format method if it exists."""
    if isinstance(response, str):
        return response
    elif isinstance(response, FormatterProtocol):
        return response.format()
    else:
        return response.model_dump_json()


def process_image_content(content_item: dict[str, Any]) -> dict[str, Any]:
    """Process an OpenAI image content item into Claude format."""
    if content_item["type"] != "image_url":
        return content_item

    url = content_item["image_url"]["url"]
    try:
        # Handle data URLs
        if url.startswith("data:"):
            data_url_pattern = r"data:image/([a-zA-Z]+);base64,(.+)"
            match = re.match(data_url_pattern, url)
            if match:
                media_type, base64_data = match.groups()
                return {
                    "type": "image",
                    "source": {"type": "base64", "media_type": f"image/{media_type}", "data": base64_data},
                }

        else:
            print("Error processing image.")
            # Return original content if image processing fails
            return content_item

    except Exception as e:
        print(f"Error processing image image: {e}")
        # Return original content if image processing fails
        return content_item


def process_message_content(message: dict[str, Any]) -> str | list[dict[str, Any]]:
    """Process message content, handling both string and list formats with images."""
    content = message.get("content", "")

    # Handle empty content
    if content == "":
        return content

    # If content is already a string, return as is
    if isinstance(content, str):
        return content

    # Handle list content (mixed text and images)
    if isinstance(content, list):
        processed_content = []
        for item in content:
            if item["type"] == "text":
                processed_content.append({"type": "text", "text": item["text"]})
            elif item["type"] == "image_url":
                processed_content.append(process_image_content(item))
        return processed_content

    return content


@require_optional_import("anthropic", "anthropic")
def oai_messages_to_anthropic_messages(params: dict[str, Any]) -> list[dict[str, Any]]:
    """Convert messages from OAI format to Anthropic format.
    We correct for any specific role orders and types, etc.
    """
    # Track whether we have tools passed in. If not,  tool use / result messages should be converted to text messages.
    # Anthropic requires a tools parameter with the tools listed, if there are other messages with tool use or tool results.
    # This can occur when we don't need tool calling, such as for group chat speaker selection.
    has_tools = "tools" in params

    # Convert messages to Anthropic compliant format
    processed_messages = []

    # Used to interweave user messages to ensure user/assistant alternating
    user_continue_message = {"content": "Please continue.", "role": "user"}
    assistant_continue_message = {"content": "Please continue.", "role": "assistant"}

    tool_use_messages = 0
    tool_result_messages = 0
    last_tool_use_index = -1
    last_tool_result_index = -1
    for message in params["messages"]:
        if message["role"] == "system":
            content = process_message_content(message)
            if isinstance(content, list):
                # For system messages with images, concatenate only the text portions
                text_content = " ".join(item.get("text", "") for item in content if item.get("type") == "text")
                params["system"] = params.get("system", "") + (" " if "system" in params else "") + text_content
            else:
                params["system"] = params.get("system", "") + ("\n" if "system" in params else "") + content
        else:
            # New messages will be added here, manage role alternations
            expected_role = "user" if len(processed_messages) % 2 == 0 else "assistant"

            if "tool_calls" in message:
                # Map the tool call options to Anthropic's ToolUseBlock
                tool_uses = []
                tool_names = []
                for tool_call in message["tool_calls"]:
                    tool_uses.append(
                        ToolUseBlock(
                            type="tool_use",
                            id=tool_call["id"],
                            name=tool_call["function"]["name"],
                            input=json.loads(tool_call["function"]["arguments"]),
                        )
                    )
                    if has_tools:
                        tool_use_messages += 1
                    tool_names.append(tool_call["function"]["name"])

                if expected_role == "user":
                    # Insert an extra user message as we will append an assistant message
                    processed_messages.append(user_continue_message)

                if has_tools:
                    processed_messages.append({"role": "assistant", "content": tool_uses})
                    last_tool_use_index = len(processed_messages) - 1
                else:
                    # Not using tools, so put in a plain text message
                    processed_messages.append({
                        "role": "assistant",
                        "content": f"Some internal function(s) that could be used: [{', '.join(tool_names)}]",
                    })
            elif "tool_call_id" in message:
                if has_tools:
                    # Map the tool usage call to tool_result for Anthropic
                    tool_result = {
                        "type": "tool_result",
                        "tool_use_id": message["tool_call_id"],
                        "content": message["content"],
                    }

                    # If the previous message also had a tool_result, add it to that
                    # Otherwise append a new message
                    if last_tool_result_index == len(processed_messages) - 1:
                        processed_messages[-1]["content"].append(tool_result)
                    else:
                        if expected_role == "assistant":
                            # Insert an extra assistant message as we will append a user message
                            processed_messages.append(assistant_continue_message)

                        processed_messages.append({"role": "user", "content": [tool_result]})
                        last_tool_result_index = len(processed_messages) - 1

                    tool_result_messages += 1
                else:
                    # Not using tools, so put in a plain text message
                    processed_messages.append({
                        "role": "user",
                        "content": f"Running the function returned: {message['content']}",
                    })
            elif message["content"] == "":
                # Ignoring empty messages
                pass
            else:
                if expected_role != message["role"]:
                    # Inserting the alternating continue message
                    processed_messages.append(
                        user_continue_message if expected_role == "user" else assistant_continue_message
                    )
                # Process messages for images
                processed_content = process_message_content(message)
                processed_message = message.copy()
                processed_message["content"] = processed_content
                processed_messages.append(processed_message)

    # We'll replace the last tool_use if there's no tool_result (occurs if we finish the conversation before running the function)
    if has_tools and tool_use_messages != tool_result_messages:
        processed_messages[last_tool_use_index] = assistant_continue_message

    # name is not a valid field on messages
    for message in processed_messages:
        if "name" in message:
            message.pop("name", None)

    # Note: When using reflection_with_llm we may end up with an "assistant" message as the last message and that may cause a blank response
    # So, if the last role is not user, add a 'user' continue message at the end
    if processed_messages[-1]["role"] != "user":
        processed_messages.append(user_continue_message)

    return processed_messages


def _calculate_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    """Calculate the cost of the completion using the Anthropic pricing."""
    total = 0.0

    if model in ANTHROPIC_PRICING_1k:
        input_cost_per_1k, output_cost_per_1k = ANTHROPIC_PRICING_1k[model]
        input_cost = (input_tokens / 1000) * input_cost_per_1k
        output_cost = (output_tokens / 1000) * output_cost_per_1k
        total = input_cost + output_cost
    else:
        warnings.warn(f"Cost calculation not available for model {model}", UserWarning)

    return total
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create an OpenAI-compatible client using Together.AI's API.

Example:
    ```python
    llm_config = {
        "config_list": [
            {
                "api_type": "together",
                "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
                "api_key": os.environ.get("TOGETHER_API_KEY"),
            }
        ]
    }

    agent = autogen.AssistantAgent("my_agent", llm_config=llm_config)
    ```

Install Together.AI python library using: pip install --upgrade together

Resources:
- https://docs.together.ai/docs/inference-python
"""

from __future__ import annotations

import copy
import os
import time
import warnings
from typing import Any, Literal

from pydantic import Field
from typing_extensions import Unpack

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import should_hide_tools, validate_parameter
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    from together import Together


class TogetherEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["together"]

    stream: bool
    top_k: int | None
    repetition_penalty: float | None
    presence_penalty: float | None
    frequency_penalty: float | None
    min_p: float | None
    safety_model: str | None
    hide_tools: Literal["if_all_run", "if_any_run", "never"]
    price: list[float] | None
    tool_choice: str | dict[str, str | dict[str, str]] | None


class TogetherLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["together"] = "together"

    max_tokens: int = Field(default=512, ge=0)

    stream: bool = False
    top_k: int | None = Field(default=None)
    repetition_penalty: float | None = Field(default=None)
    presence_penalty: float | None = Field(default=None, ge=-2, le=2)
    frequency_penalty: float | None = Field(default=None, ge=-2, le=2)
    min_p: float | None = Field(default=None, ge=0, le=1)
    safety_model: str | None = None
    hide_tools: Literal["if_all_run", "if_any_run", "never"] = "never"
    price: list[float] | None = Field(default=None, min_length=2, max_length=2)
    tool_choice: str | dict[str, str | dict[str, str]] | None = (
        None  # dict is the tool to call: {"type": "function", "function": {"name": "my_function"}}
    )

    def create_client(self):
        raise NotImplementedError("TogetherLLMConfigEntry.create_client is not implemented.")


class TogetherClient:
    """Client for Together.AI's API."""

    def __init__(self, **kwargs: Unpack[TogetherEntryDict]):
        """Requires api_key or environment variable to be set

        Args:
            **kwargs: Additional keyword arguments to pass to the client.
        """
        # Ensure we have the api_key upon instantiation
        self.api_key = kwargs.get("api_key")
        if not self.api_key:
            self.api_key = os.getenv("TOGETHER_API_KEY")

        if "response_format" in kwargs and kwargs["response_format"] is not None:
            warnings.warn("response_format is not supported for Together.AI, it will be ignored.", UserWarning)

        assert self.api_key, (
            "Please include the api_key in your config list entry for Together.AI or set the TOGETHER_API_KEY env variable."
        )

    def message_retrieval(self, response) -> list:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        return [choice.message for choice in response.choices]

    def cost(self, response) -> float:
        return response.cost

    @staticmethod
    def get_usage(response) -> dict:
        """Return usage summary of the response using RESPONSE_USAGE_KEYS."""
        # ...  # pragma: no cover
        return {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cost": response.cost,
            "model": response.model,
        }

    def parse_params(self, params: dict[str, Any]) -> dict[str, Any]:
        """Loads the parameters for Together.AI API from the passed in parameters and returns a validated set. Checks types, ranges, and sets defaults"""
        together_params = {}

        # Check that we have what we need to use Together.AI's API
        together_params["model"] = params.get("model")
        assert together_params["model"], (
            "Please specify the 'model' in your config list entry to nominate the Together.AI model to use."
        )

        # Validate allowed Together.AI parameters
        # https://github.com/togethercomputer/together-python/blob/94ffb30daf0ac3e078be986af7228f85f79bde99/src/together/resources/completions.py#L44
        together_params["max_tokens"] = validate_parameter(params, "max_tokens", int, True, 512, (0, None), None)
        together_params["stream"] = validate_parameter(params, "stream", bool, False, False, None, None)
        together_params["temperature"] = validate_parameter(params, "temperature", (int, float), True, None, None, None)
        together_params["top_p"] = validate_parameter(params, "top_p", (int, float), True, None, None, None)
        together_params["top_k"] = validate_parameter(params, "top_k", int, True, None, None, None)
        together_params["repetition_penalty"] = validate_parameter(
            params, "repetition_penalty", float, True, None, None, None
        )
        together_params["presence_penalty"] = validate_parameter(
            params, "presence_penalty", (int, float), True, None, (-2, 2), None
        )
        together_params["frequency_penalty"] = validate_parameter(
            params, "frequency_penalty", (int, float), True, None, (-2, 2), None
        )
        together_params["min_p"] = validate_parameter(params, "min_p", (int, float), True, None, (0, 1), None)
        together_params["safety_model"] = validate_parameter(
            params, "safety_model", str, True, None, None, None
        )  # We won't enforce the available models as they are likely to change

        # Check if they want to stream and use tools, which isn't currently supported (TODO)
        if together_params["stream"] and "tools" in params:
            warnings.warn(
                "Streaming is not supported when using tools, streaming will be disabled.",
                UserWarning,
            )

            together_params["stream"] = False

        if "tool_choice" in params:
            together_params["tool_choice"] = params["tool_choice"]

        return together_params

    @require_optional_import("together", "together")
    def create(self, params: dict) -> ChatCompletion:
        messages = params.get("messages", [])

        # Convert AG2 messages to Together.AI messages
        together_messages = oai_messages_to_together_messages(messages)

        # Parse parameters to Together.AI API's parameters
        together_params = self.parse_params(params)

        # Add tools to the call if we have them and aren't hiding them
        if "tools" in params:
            hide_tools = validate_parameter(
                params, "hide_tools", str, False, "never", None, ["if_all_run", "if_any_run", "never"]
            )
            if not should_hide_tools(together_messages, params["tools"], hide_tools):
                together_params["tools"] = params["tools"]

        together_params["messages"] = together_messages

        # We use chat model by default
        client = Together(api_key=self.api_key)

        # Token counts will be returned
        prompt_tokens = 0
        completion_tokens = 0
        total_tokens = 0

        response = client.chat.completions.create(**together_params)
        if together_params["stream"]:
            # Read in the chunks as they stream
            ans = ""
            for chunk in response:
                ans = ans + (chunk.choices[0].delta.content or "")

            prompt_tokens = chunk.usage.prompt_tokens
            completion_tokens = chunk.usage.completion_tokens
            total_tokens = chunk.usage.total_tokens
        else:
            ans: str = response.choices[0].message.content

            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
            total_tokens = response.usage.total_tokens

        if response.choices[0].finish_reason == "tool_calls":
            together_finish = "tool_calls"
            tool_calls = []
            for tool_call in response.choices[0].message.tool_calls:
                tool_calls.append(
                    ChatCompletionMessageToolCall(
                        id=tool_call.id,
                        function={"name": tool_call.function.name, "arguments": tool_call.function.arguments},
                        type="function",
                    )
                )
        else:
            together_finish = "stop"
            tool_calls = None

        # 3. convert output
        message = ChatCompletionMessage(
            role="assistant",
            content=response.choices[0].message.content,
            function_call=None,
            tool_calls=tool_calls,
        )
        choices = [Choice(finish_reason=together_finish, index=0, message=message)]

        response_oai = ChatCompletion(
            id=response.id,
            model=together_params["model"],
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
            cost=calculate_together_cost(prompt_tokens, completion_tokens, together_params["model"]),
        )

        return response_oai


def oai_messages_to_together_messages(messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Convert messages from OAI format to Together.AI format.
    We correct for any specific role orders and types.
    """
    together_messages = copy.deepcopy(messages)

    # If we have a message with role='tool', which occurs when a function is executed, change it to 'user'
    for msg in together_messages:
        if "role" in msg and msg["role"] == "tool":
            msg["role"] = "user"

    return together_messages


# MODELS AND COSTS
chat_lang_code_model_sizes = {
    "zero-one-ai/Yi-34B-Chat": 34,
    "allenai/OLMo-7B-Instruct": 7,
    "allenai/OLMo-7B-Twin-2T": 7,
    "allenai/OLMo-7B": 7,
    "Austism/chronos-hermes-13b": 13,
    "deepseek-ai/deepseek-coder-33b-instruct": 33,
    "deepseek-ai/deepseek-llm-67b-chat": 67,
    "garage-bAInd/Platypus2-70B-instruct": 70,
    "google/gemma-2b-it": 2,
    "google/gemma-7b-it": 7,
    "Gryphe/MythoMax-L2-13b": 13,
    "lmsys/vicuna-13b-v1.5": 13,
    "lmsys/vicuna-7b-v1.5": 7,
    "codellama/CodeLlama-13b-Instruct-hf": 13,
    "codellama/CodeLlama-34b-Instruct-hf": 34,
    "codellama/CodeLlama-70b-Instruct-hf": 70,
    "codellama/CodeLlama-7b-Instruct-hf": 7,
    "meta-llama/Llama-2-70b-chat-hf": 70,
    "meta-llama/Llama-2-13b-chat-hf": 13,
    "meta-llama/Llama-2-7b-chat-hf": 7,
    "meta-llama/Llama-3-8b-chat-hf": 8,
    "meta-llama/Llama-3-70b-chat-hf": 70,
    "mistralai/Mistral-7B-Instruct-v0.1": 7,
    "mistralai/Mistral-7B-Instruct-v0.2": 7,
    "mistralai/Mistral-7B-Instruct-v0.3": 7,
    "NousResearch/Nous-Capybara-7B-V1p9": 7,
    "NousResearch/Nous-Hermes-llama-2-7b": 7,
    "NousResearch/Nous-Hermes-Llama2-13b": 13,
    "NousResearch/Nous-Hermes-2-Yi-34B": 34,
    "openchat/openchat-3.5-1210": 7,
    "Open-Orca/Mistral-7B-OpenOrca": 7,
    "Qwen/Qwen1.5-0.5B-Chat": 0.5,
    "Qwen/Qwen1.5-1.8B-Chat": 1.8,
    "Qwen/Qwen1.5-4B-Chat": 4,
    "Qwen/Qwen1.5-7B-Chat": 7,
    "Qwen/Qwen1.5-14B-Chat": 14,
    "Qwen/Qwen1.5-32B-Chat": 32,
    "Qwen/Qwen1.5-72B-Chat": 72,
    "Qwen/Qwen1.5-110B-Chat": 110,
    "Qwen/Qwen2-72B-Instruct": 72,
    "snorkelai/Snorkel-Mistral-PairRM-DPO": 7,
    "togethercomputer/alpaca-7b": 7,
    "teknium/OpenHermes-2-Mistral-7B": 7,
    "teknium/OpenHermes-2p5-Mistral-7B": 7,
    "togethercomputer/Llama-2-7B-32K-Instruct": 7,
    "togethercomputer/RedPajama-INCITE-Chat-3B-v1": 3,
    "togethercomputer/RedPajama-INCITE-7B-Chat": 7,
    "togethercomputer/StripedHyena-Nous-7B": 7,
    "Undi95/ReMM-SLERP-L2-13B": 13,
    "Undi95/Toppy-M-7B": 7,
    "WizardLM/WizardLM-13B-V1.2": 13,
    "upstage/SOLAR-10.7B-Instruct-v1.0": 11,
}

# Cost per million tokens based on up to X Billion parameters, e.g. up 4B is $0.1/million
chat_lang_code_model_costs = {4: 0.1, 8: 0.2, 21: 0.3, 41: 0.8, 80: 0.9, 110: 1.8}

mixture_model_sizes = {
    "cognitivecomputations/dolphin-2.5-mixtral-8x7b": 56,
    "databricks/dbrx-instruct": 132,
    "mistralai/Mixtral-8x7B-Instruct-v0.1": 47,
    "mistralai/Mixtral-8x22B-Instruct-v0.1": 141,
    "NousResearch/Nous-Hermes-2-Mistral-7B-DPO": 7,
    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": 47,
    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT": 47,
    "Snowflake/snowflake-arctic-instruct": 480,
}

# Cost per million tokens based on up to X Billion parameters, e.g. up 56B is $0.6/million
mixture_costs = {56: 0.6, 176: 1.2, 480: 2.4}


def calculate_together_cost(input_tokens: int, output_tokens: int, model_name: str) -> float:
    """Cost calculation for inference"""
    if model_name in chat_lang_code_model_sizes or model_name in mixture_model_sizes:
        cost_per_mil = 0

        # Chat, Language, Code models
        if model_name in chat_lang_code_model_sizes:
            size_in_b = chat_lang_code_model_sizes[model_name]

            for top_size in chat_lang_code_model_costs:
                if size_in_b <= top_size:
                    cost_per_mil = chat_lang_code_model_costs[top_size]
                    break

        else:
            # Mixture-of-experts
            size_in_b = mixture_model_sizes[model_name]

            for top_size in mixture_costs:
                if size_in_b <= top_size:
                    cost_per_mil = mixture_costs[top_size]
                    break

        if cost_per_mil == 0:
            warnings.warn("Model size doesn't align with cost structure.", UserWarning)

        return cost_per_mil * ((input_tokens + output_tokens) / 1e6)

    else:
        # Model is not in our list of models, can't determine the cost
        warnings.warn(
            "The model isn't catered for costing, to apply costs you can use the 'price' key on your config_list.",
            UserWarning,
        )

        return 0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create an OpenAI-compatible client using Cohere's API.

Example:
    ```python
    llm_config={
        "config_list": [{
            "api_type": "cohere",
            "model": "command-r-plus",
            "api_key": os.environ.get("COHERE_API_KEY")
            "client_name": "autogen-cohere", # Optional parameter
            }
    ]}

    agent = autogen.AssistantAgent("my_agent", llm_config=llm_config)
    ```

Install Cohere's python library using: pip install --upgrade cohere

Resources:
- https://docs.cohere.com/reference/chat
"""

from __future__ import annotations

import json
import logging
import os
import sys
import time
import warnings
from typing import Any, Literal

from pydantic import BaseModel, Field
from typing_extensions import Unpack

from autogen.oai.client_utils import FormatterProtocol, logging_formatter, validate_parameter

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    from cohere import ClientV2 as CohereV2
    from cohere.types import ToolResult

logger = logging.getLogger(__name__)
if not logger.handlers:
    # Add the console handler.
    _ch = logging.StreamHandler(stream=sys.stdout)
    _ch.setFormatter(logging_formatter)
    logger.addHandler(_ch)


COHERE_PRICING_1K = {
    "command-r-plus": (0.003, 0.015),
    "command-r": (0.0005, 0.0015),
    "command-nightly": (0.00025, 0.00125),
    "command": (0.015, 0.075),
    "command-light": (0.008, 0.024),
    "command-light-nightly": (0.008, 0.024),
}


class CohereEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["cohere"]

    k: int
    seed: int | None
    frequency_penalty: float
    presence_penalty: float
    client_name: str | None
    strict_tools: bool
    stream: bool
    tool_choice: Literal["NONE", "REQUIRED"] | None


class CohereLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["cohere"] = "cohere"

    k: int = Field(default=0, ge=0, le=500)
    seed: int | None = None
    frequency_penalty: float = Field(default=0, ge=0, le=1)
    presence_penalty: float = Field(default=0, ge=0, le=1)
    client_name: str | None = None
    strict_tools: bool = False
    stream: bool = False
    tool_choice: Literal["NONE", "REQUIRED"] | None = None

    def create_client(self):
        raise NotImplementedError("CohereLLMConfigEntry.create_client is not implemented.")


class CohereClient:
    """Client for Cohere's API."""

    def __init__(self, **kwargs: Unpack[CohereEntryDict]):
        """Requires api_key or environment variable to be set

        Args:
            **kwargs: The keyword arguments to pass to the Cohere API.
        """
        # Ensure we have the api_key upon instantiation
        self.api_key = kwargs.get("api_key")
        if not self.api_key:
            self.api_key = os.getenv("COHERE_API_KEY")

        assert self.api_key, (
            "Please include the api_key in your config list entry for Cohere or set the COHERE_API_KEY env variable."
        )

        # Store the response format, if provided (for structured outputs)
        self._response_format: type[BaseModel] | None = None

    def message_retrieval(self, response) -> list:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        return [choice.message for choice in response.choices]

    def cost(self, response) -> float:
        return response.cost

    @staticmethod
    def get_usage(response) -> dict:
        """Return usage summary of the response using RESPONSE_USAGE_KEYS."""
        # ...  # pragma: no cover
        return {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cost": response.cost,
            "model": response.model,
        }

    def parse_params(self, params: dict[str, Any]) -> dict[str, Any]:
        """Loads the parameters for Cohere API from the passed in parameters and returns a validated set. Checks types, ranges, and sets defaults"""
        cohere_params = {}

        # Check that we have what we need to use Cohere's API
        # We won't enforce the available models as they are likely to change
        cohere_params["model"] = params.get("model")
        assert cohere_params["model"], (
            "Please specify the 'model' in your config list entry to nominate the Cohere model to use."
        )

        # Handle structured output response format from Pydantic model
        if "response_format" in params and params["response_format"] is not None:
            self._response_format = params.get("response_format")

            response_format = params["response_format"]

            # Check if it's a Pydantic model
            if hasattr(response_format, "model_json_schema"):
                # Get the JSON schema from the Pydantic model
                schema = response_format.model_json_schema()

                def resolve_ref(ref: str, defs: dict) -> dict:
                    """Resolve a $ref to its actual schema definition"""
                    # Extract the definition name from "#/$defs/Name"
                    def_name = ref.split("/")[-1]
                    return defs[def_name]

                def ensure_type_fields(obj: dict, defs: dict) -> dict:
                    """Recursively ensure all objects in the schema have a type and properties field"""
                    if isinstance(obj, dict):
                        # If it has a $ref, replace it with the actual definition
                        if "$ref" in obj:
                            ref_def = resolve_ref(obj["$ref"], defs)
                            # Merge the reference definition with any existing fields
                            obj = {**ref_def, **obj}
                            # Remove the $ref as we've replaced it
                            del obj["$ref"]

                        # Process each value recursively
                        return {
                            k: ensure_type_fields(v, defs) if isinstance(v, (dict, list)) else v for k, v in obj.items()
                        }
                    elif isinstance(obj, list):
                        return [ensure_type_fields(item, defs) for item in obj]
                    return obj

                # Make a copy of $defs before processing
                defs = schema.get("$defs", {})

                # Process the schema
                processed_schema = ensure_type_fields(schema, defs)

                cohere_params["response_format"] = {"type": "json_object", "json_schema": processed_schema}
            else:
                raise ValueError("response_format must be a Pydantic BaseModel")

        # Handle strict tools parameter for structured outputs with tools
        if "tools" in params:
            cohere_params["strict_tools"] = validate_parameter(params, "strict_tools", bool, False, False, None, None)

        # Validate allowed Cohere parameters
        # https://docs.cohere.com/reference/chat
        if "temperature" in params:
            cohere_params["temperature"] = validate_parameter(
                params, "temperature", (int, float), False, 0.3, (0, None), None
            )

        if "max_tokens" in params:
            cohere_params["max_tokens"] = validate_parameter(params, "max_tokens", int, True, None, (0, None), None)

        if "k" in params:
            cohere_params["k"] = validate_parameter(params, "k", int, False, 0, (0, 500), None)

        if "top_p" in params:
            cohere_params["p"] = validate_parameter(params, "top_p", (int, float), False, 0.75, (0.01, 0.99), None)

        if "p" in params:
            warnings.warn(
                (
                    "parameter 'p' is deprecated, use 'top_p' instead for consistency with OpenAI API spec. "
                    "Scheduled for removal in 0.10.0 version."
                ),
                DeprecationWarning,
            )
            cohere_params["p"] = validate_parameter(params, "p", (int, float), False, 0.75, (0.01, 0.99), None)

        if "seed" in params:
            cohere_params["seed"] = validate_parameter(params, "seed", int, True, None, None, None)

        if "frequency_penalty" in params:
            cohere_params["frequency_penalty"] = validate_parameter(
                params, "frequency_penalty", (int, float), True, 0, (0, 1), None
            )

        if "presence_penalty" in params:
            cohere_params["presence_penalty"] = validate_parameter(
                params, "presence_penalty", (int, float), True, 0, (0, 1), None
            )

        if "tool_choice" in params:
            cohere_params["tool_choice"] = validate_parameter(
                params, "tool_choice", str, True, None, None, ["NONE", "REQUIRED"]
            )

        return cohere_params

    @require_optional_import("cohere", "cohere")
    def create(self, params: dict) -> ChatCompletion:
        messages = params.get("messages", [])
        client_name = params.get("client_name") or "AG2"
        cohere_tool_names = set()
        tool_calls_modified_ids = set()

        # Parse parameters to the Cohere API's parameters
        cohere_params = self.parse_params(params)

        cohere_params["messages"] = messages

        if "tools" in params:
            cohere_tool_names = {tool["function"]["name"] for tool in params["tools"]}
            cohere_params["tools"] = params["tools"]

        # Strip out name
        for message in cohere_params["messages"]:
            message_name = message.pop("name", "")
            # Extract and prepend name to content or tool_plan if available
            message["content"] = (
                f"{message_name}: {(message.get('content') or message.get('tool_plan'))}"
                if message_name
                else (message.get("content") or message.get("tool_plan"))
            )

            # Handle tool calls
            if message.get("tool_calls") is not None and len(message["tool_calls"]) > 0:
                message["tool_plan"] = message.get("tool_plan", message["content"])
                del message["content"]  # Remove content as tool_plan is prioritized

                # If tool call name is missing or not recognized, modify role and content
                for tool_call in message["tool_calls"] or []:
                    if (not tool_call.get("function", {}).get("name")) or tool_call.get("function", {}).get(
                        "name"
                    ) not in cohere_tool_names:
                        message["role"] = "assistant"
                        message["content"] = f"{message.pop('tool_plan', '')}{str(message['tool_calls'])}"
                        tool_calls_modified_ids = tool_calls_modified_ids.union({
                            tool_call.get("id") for tool_call in message["tool_calls"]
                        })
                        del message["tool_calls"]
                        break

            # Adjust role if message comes from a tool with a modified ID
            if message.get("role") == "tool":
                tool_id = message.get("tool_call_id")
                if tool_id in tool_calls_modified_ids:
                    message["role"] = "user"
                    del message["tool_call_id"]  # Remove the tool call ID

        # We use chat model by default
        client = CohereV2(api_key=self.api_key, client_name=client_name)

        # Token counts will be returned
        prompt_tokens = 0
        completion_tokens = 0
        total_tokens = 0

        # Stream if in parameters
        streaming = params.get("stream")
        cohere_finish = "stop"
        tool_calls = None
        ans = None
        if streaming:
            response = client.chat_stream(**cohere_params)
            # Streaming...
            ans = ""
            plan = ""
            prompt_tokens = 0
            completion_tokens = 0
            for chunk in response:
                if chunk.type == "content-delta":
                    ans = ans + chunk.delta.message.content.text
                elif chunk.type == "tool-plan-delta":
                    plan = plan + chunk.delta.message.tool_plan
                elif chunk.type == "tool-call-start":
                    cohere_finish = "tool_calls"

                    # Initialize a new tool call
                    tool_call = chunk.delta.message.tool_calls
                    current_tool = {
                        "id": tool_call.id,
                        "type": "function",
                        "function": {"name": tool_call.function.name, "arguments": ""},
                    }
                elif chunk.type == "tool-call-delta":
                    # Progressively build the arguments as they stream in
                    if current_tool is not None:
                        current_tool["function"]["arguments"] += chunk.delta.message.tool_calls.function.arguments
                elif chunk.type == "tool-call-end":
                    # Append the finished tool call to the list
                    if current_tool is not None:
                        if tool_calls is None:
                            tool_calls = []
                        tool_calls.append(ChatCompletionMessageToolCall(**current_tool))
                        current_tool = None
                elif chunk.type == "message-start":
                    response_id = chunk.id
                elif chunk.type == "message-end":
                    prompt_tokens = (
                        chunk.delta.usage.billed_units.input_tokens
                    )  # Note total (billed+non-billed) available with ...usage.tokens...
                    completion_tokens = chunk.delta.usage.billed_units.output_tokens

            total_tokens = prompt_tokens + completion_tokens
        else:
            response = client.chat(**cohere_params)

            if response.message.tool_calls is not None:
                ans = response.message.tool_plan
                cohere_finish = "tool_calls"
                tool_calls = []
                for tool_call in response.message.tool_calls:
                    # if parameters are null, clear them out (Cohere can return a string "null" if no parameter values)

                    tool_calls.append(
                        ChatCompletionMessageToolCall(
                            id=tool_call.id,
                            function={
                                "name": tool_call.function.name,
                                "arguments": (
                                    "" if tool_call.function.arguments is None else tool_call.function.arguments
                                ),
                            },
                            type="function",
                        )
                    )
            else:
                ans: str = response.message.content[0].text

            # Not using billed_units, but that may be better for cost purposes
            prompt_tokens = (
                response.usage.billed_units.input_tokens
            )  # Note total (billed+non-billed) available with ...usage.tokens...
            completion_tokens = response.usage.billed_units.output_tokens
            total_tokens = prompt_tokens + completion_tokens

            response_id = response.id

        # Clean up structured output if needed
        if self._response_format:
            # ans = clean_return_response_format(ans)
            try:
                parsed_response = self._convert_json_response(ans)
                ans = _format_json_response(parsed_response, ans)
            except ValueError as e:
                ans = str(e)

        # 3. convert output
        message = ChatCompletionMessage(
            role="assistant",
            content=ans,
            function_call=None,
            tool_calls=tool_calls,
        )
        choices = [Choice(finish_reason=cohere_finish, index=0, message=message)]

        response_oai = ChatCompletion(
            id=response_id,
            model=cohere_params["model"],
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
            cost=calculate_cohere_cost(prompt_tokens, completion_tokens, cohere_params["model"]),
        )

        return response_oai

    def _convert_json_response(self, response: str) -> Any:
        """Extract and validate JSON response from the output for structured outputs.

        Args:
            response (str): The response from the API.

        Returns:
            Any: The parsed JSON response.
        """
        if not self._response_format:
            return response

        try:
            # Parse JSON and validate against the Pydantic model
            json_data = json.loads(response)
            return self._response_format.model_validate(json_data)
        except Exception as e:
            raise ValueError(
                f"Failed to parse response as valid JSON matching the schema for Structured Output: {str(e)}"
            )


def _format_json_response(response: Any, original_answer: str) -> str:
    """Formats the JSON response for structured outputs using the format method if it exists."""
    return (
        response.format() if isinstance(response, FormatterProtocol) else clean_return_response_format(original_answer)
    )


def extract_to_cohere_tool_results(tool_call_id: str, content_output: str, all_tool_calls) -> list[dict[str, Any]]:
    temp_tool_results = []

    for tool_call in all_tool_calls:
        if tool_call["id"] == tool_call_id:
            call = {
                "name": tool_call["function"]["name"],
                "parameters": json.loads(
                    tool_call["function"]["arguments"] if tool_call["function"]["arguments"] != "" else "{}"
                ),
            }
            output = [{"value": content_output}]
            temp_tool_results.append(ToolResult(call=call, outputs=output))
    return temp_tool_results


def calculate_cohere_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    """Calculate the cost of the completion using the Cohere pricing."""
    total = 0.0

    if model in COHERE_PRICING_1K:
        input_cost_per_k, output_cost_per_k = COHERE_PRICING_1K[model]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        total = input_cost + output_cost
    else:
        warnings.warn(f"Cost calculation not available for {model} model", UserWarning)

    return total


def clean_return_response_format(response_str: str) -> str:
    """Clean up the response string by parsing through json library."""
    # Parse the string to a JSON object to handle escapes
    data = json.loads(response_str)

    # Convert back to JSON string with minimal formatting
    return json.dumps(data)


class CohereError(Exception):
    """Base class for other Cohere exceptions"""

    pass


class CohereRateLimitError(CohereError):
    """Raised when rate limit is exceeded"""

    pass
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create an OpenAI-compatible client using Ollama's API.

Example:
    ```python
    llm_config = {"config_list": [{"api_type": "ollama", "model": "mistral:7b-instruct-v0.3-q6_K"}]}

    agent = autogen.AssistantAgent("my_agent", llm_config=llm_config)
    ```

Install Ollama's python library using: pip install --upgrade ollama
Install fix-busted-json library: pip install --upgrade fix-busted-json

Resources:
- https://github.com/ollama/ollama-python
"""

from __future__ import annotations

import ast
import copy
import json
import random
import re
import time
import warnings
from typing import Any, Literal

from pydantic import BaseModel, Field, HttpUrl

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import FormatterProtocol, should_hide_tools, validate_parameter
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    import ollama
    from fix_busted_json import repair_json
    from ollama import Client


class OllamaEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["ollama"]
    client_host: HttpUrl | None
    stream: bool
    num_predict: int
    num_ctx: int
    repeat_penalty: float
    seed: int
    top_k: int
    hide_tools: Literal["if_all_run", "if_any_run", "never"]
    native_tool_calls: bool


class OllamaLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["ollama"] = "ollama"
    # TODO: max_tokens
    client_host: HttpUrl | None = None
    stream: bool = False
    num_predict: int = Field(
        default=-1,
        description="Maximum number of tokens to predict, note: -1 is infinite (default), -2 is fill context.",
    )
    num_ctx: int = Field(default=2048)
    repeat_penalty: float = Field(default=1.1)
    seed: int = Field(default=0)
    top_k: int = Field(default=40)
    hide_tools: Literal["if_all_run", "if_any_run", "never"] = "never"
    native_tool_calls: bool = False

    def create_client(self):
        raise NotImplementedError("OllamaLLMConfigEntry.create_client is not implemented.")


class OllamaClient:
    """Client for Ollama's API."""

    # Defaults for manual tool calling
    # Instruction is added to the first system message and provides directions to follow a two step
    # process
    # 1. (before tools have been called) Return JSON with the functions to call
    # 2. (directly after tools have been called) Return Text describing the results of the function calls in text format

    # Override using "manual_tool_call_instruction" config parameter
    TOOL_CALL_MANUAL_INSTRUCTION = (
        "You are to follow a strict two step process that will occur over "
        "a number of interactions, so pay attention to what step you are in based on the full "
        "conversation. We will be taking turns so only do one step at a time so don't perform step "
        "2 until step 1 is complete and I've told you the result. The first step is to choose one "
        "or more functions based on the request given and return only JSON with the functions and "
        "arguments to use. The second step is to analyse the given output of the function and summarise "
        "it returning only TEXT and not Python or JSON. "
        "For argument values, be sure numbers aren't strings, they should not have double quotes around them. "
        "In terms of your response format, for step 1 return only JSON and NO OTHER text, "
        "for step 2 return only text and NO JSON/Python/Markdown. "
        'The format for running a function is [{"name": "function_name1", "arguments":{"argument_name": "argument_value"}},{"name": "function_name2", "arguments":{"argument_name": "argument_value"}}] '
        'Make sure the keys "name" and "arguments" are as described. '
        "If you don't get the format correct, try again. "
        "The following functions are available to you:[FUNCTIONS_LIST]"
    )

    # Appended to the last user message if no tools have been called
    # Override using "manual_tool_call_step1" config parameter
    TOOL_CALL_MANUAL_STEP1 = " (proceed with step 1)"

    # Appended to the user message after tools have been executed. Will create a 'user' message if one doesn't exist.
    # Override using "manual_tool_call_step2" config parameter
    TOOL_CALL_MANUAL_STEP2 = " (proceed with step 2)"

    def __init__(self, response_format: BaseModel | dict[str, Any] | None = None, **kwargs):
        """Note that no api_key or environment variable is required for Ollama."""
        # Store the response format, if provided (for structured outputs)
        self._response_format: BaseModel | dict[str, Any] | None = response_format

    def message_retrieval(self, response) -> list:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        return [choice.message for choice in response.choices]

    def cost(self, response) -> float:
        return response.cost

    @staticmethod
    def get_usage(response) -> dict:
        """Return usage summary of the response using RESPONSE_USAGE_KEYS."""
        # ...  # pragma: no cover
        return {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cost": response.cost,
            "model": response.model,
        }

    def parse_params(self, params: dict[str, Any]) -> dict[str, Any]:
        """Loads the parameters for Ollama API from the passed in parameters and returns a validated set. Checks types, ranges, and sets defaults"""
        ollama_params = {}

        # Check that we have what we need to use Ollama's API
        # https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion

        # The main parameters are model, prompt, stream, and options
        # Options is a dictionary of parameters for the model
        # There are other, advanced, parameters such as format, system (to override system message), template, raw, etc. - not used

        # We won't enforce the available models
        ollama_params["model"] = params.get("model")
        assert ollama_params["model"], (
            "Please specify the 'model' in your config list entry to nominate the Ollama model to use."
        )

        ollama_params["stream"] = validate_parameter(params, "stream", bool, True, False, None, None)

        # Build up the options dictionary
        # https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values
        options_dict = {}

        if "num_predict" in params:
            # Maximum number of tokens to predict, note: -1 is infinite, -2 is fill context, 128 is default
            options_dict["num_predict"] = validate_parameter(params, "num_predict", int, False, 128, None, None)

        if "num_ctx" in params:
            # Set size of context window used to generate next token, 2048 is default
            options_dict["num_ctx"] = validate_parameter(params, "num_ctx", int, False, 2048, None, None)

        if "repeat_penalty" in params:
            options_dict["repeat_penalty"] = validate_parameter(
                params, "repeat_penalty", (int, float), False, 1.1, None, None
            )

        if "seed" in params:
            options_dict["seed"] = validate_parameter(params, "seed", int, False, 42, None, None)

        if "temperature" in params:
            options_dict["temperature"] = validate_parameter(
                params, "temperature", (int, float), False, 0.8, None, None
            )

        if "top_k" in params:
            options_dict["top_k"] = validate_parameter(params, "top_k", int, False, 40, None, None)

        if "top_p" in params:
            options_dict["top_p"] = validate_parameter(params, "top_p", (int, float), False, 0.9, None, None)

        if self._native_tool_calls and self._tools_in_conversation and not self._should_hide_tools:
            ollama_params["tools"] = params["tools"]

            # Ollama doesn't support streaming with tools natively
            if ollama_params["stream"] and self._native_tool_calls:
                warnings.warn(
                    "Streaming is not supported when using tools and 'Native' tool calling, streaming will be disabled.",
                    UserWarning,
                )

                ollama_params["stream"] = False

        if not self._native_tool_calls and self._tools_in_conversation:
            # For manual tool calling we have injected the available tools into the prompt
            # and we don't want to force JSON mode
            ollama_params["format"] = ""  # Don't force JSON for manual tool calling mode

        if len(options_dict) != 0:
            ollama_params["options"] = options_dict

        # Structured outputs (see https://ollama.com/blog/structured-outputs)
        if not self._response_format and params.get("response_format"):
            self._response_format = params["response_format"]

        if self._response_format:
            if isinstance(self._response_format, dict):
                ollama_params["format"] = self._response_format
            else:
                # Keep self._response_format as a Pydantic model for when process the response
                ollama_params["format"] = self._response_format.model_json_schema()

        return ollama_params

    @require_optional_import(["ollama", "fix_busted_json"], "ollama")
    def create(self, params: dict) -> ChatCompletion:
        messages = params.get("messages", [])

        # Are tools involved in this conversation?
        self._tools_in_conversation = "tools" in params

        # We provide second-level filtering out of tools to avoid LLMs re-calling tools continuously
        if self._tools_in_conversation:
            hide_tools = validate_parameter(
                params, "hide_tools", str, False, "never", None, ["if_all_run", "if_any_run", "never"]
            )
            self._should_hide_tools = should_hide_tools(messages, params["tools"], hide_tools)
        else:
            self._should_hide_tools = False

        # Are we using native Ollama tool calling, otherwise we're doing manual tool calling
        # We allow the user to decide if they want to use Ollama's tool calling
        # or for tool calling to be handled manually through text messages
        # Default is True = Ollama's tool calling
        self._native_tool_calls = validate_parameter(params, "native_tool_calls", bool, False, True, None, None)

        if not self._native_tool_calls:
            # Load defaults
            self._manual_tool_call_instruction = validate_parameter(
                params, "manual_tool_call_instruction", str, False, self.TOOL_CALL_MANUAL_INSTRUCTION, None, None
            )
            self._manual_tool_call_step1 = validate_parameter(
                params, "manual_tool_call_step1", str, False, self.TOOL_CALL_MANUAL_STEP1, None, None
            )
            self._manual_tool_call_step2 = validate_parameter(
                params, "manual_tool_call_step2", str, False, self.TOOL_CALL_MANUAL_STEP2, None, None
            )

        # Convert AG2 messages to Ollama messages
        ollama_messages = self.oai_messages_to_ollama_messages(
            messages,
            (
                params["tools"]
                if (not self._native_tool_calls and self._tools_in_conversation) and not self._should_hide_tools
                else None
            ),
        )

        # Parse parameters to the Ollama API's parameters
        ollama_params = self.parse_params(params)

        ollama_params["messages"] = ollama_messages

        # Token counts will be returned
        prompt_tokens = 0
        completion_tokens = 0
        total_tokens = 0

        ans = None
        if "client_host" in params:
            # Convert client_host to string from HttpUrl
            client = Client(host=str(params["client_host"]))
            response = client.chat(**ollama_params)
        else:
            response = ollama.chat(**ollama_params)

        if ollama_params["stream"]:
            # Read in the chunks as they stream, taking in tool_calls which may be across
            # multiple chunks if more than one suggested
            ans = ""
            for chunk in response:
                ans = ans + (chunk["message"]["content"] or "")

                if "done_reason" in chunk:
                    prompt_tokens = chunk.get("prompt_eval_count", 0)
                    completion_tokens = chunk.get("eval_count", 0)
                    total_tokens = prompt_tokens + completion_tokens
        else:
            # Non-streaming finished
            ans: str = response["message"]["content"]

            prompt_tokens = response.get("prompt_eval_count", 0)
            completion_tokens = response.get("eval_count", 0)
            total_tokens = prompt_tokens + completion_tokens

        if response is not None:
            # Defaults
            ollama_finish = "stop"
            tool_calls = None

            # Id and streaming text into response
            if ollama_params["stream"]:
                response_content = ans
                response_id = chunk["created_at"]
            else:
                response_content = response["message"]["content"]
                response_id = response["created_at"]

            # Process tools in the response
            if self._tools_in_conversation:
                if self._native_tool_calls:
                    if not ollama_params["stream"]:
                        response_content = response["message"]["content"]

                        # Native tool calling
                        if "tool_calls" in response["message"]:
                            ollama_finish = "tool_calls"
                            tool_calls = []
                            random_id = random.randint(0, 10000)
                            for tool_call in response["message"]["tool_calls"]:
                                tool_calls.append(
                                    ChatCompletionMessageToolCall(
                                        id=f"ollama_func_{random_id}",
                                        function={
                                            "name": tool_call["function"]["name"],
                                            "arguments": json.dumps(tool_call["function"]["arguments"]),
                                        },
                                        type="function",
                                    )
                                )

                                random_id += 1

                elif not self._native_tool_calls:
                    # Try to convert the response to a tool call object
                    response_toolcalls = response_to_tool_call(ans)

                    # If we can, then we've got tool call(s)
                    if response_toolcalls is not None:
                        ollama_finish = "tool_calls"
                        tool_calls = []
                        random_id = random.randint(0, 10000)

                        for json_function in response_toolcalls:
                            tool_calls.append(
                                ChatCompletionMessageToolCall(
                                    id=f"ollama_manual_func_{random_id}",
                                    function={
                                        "name": json_function["name"],
                                        "arguments": (
                                            json.dumps(json_function["arguments"])
                                            if "arguments" in json_function
                                            else "{}"
                                        ),
                                    },
                                    type="function",
                                )
                            )

                            random_id += 1

                        # Blank the message content
                        response_content = ""

            if ollama_finish == "stop":  # noqa: SIM102
                # Not a tool call, so let's check if we need to process structured output
                if self._response_format and response_content:
                    try:
                        parsed_response = self._convert_json_response(response_content)
                        response_content = _format_json_response(parsed_response, response_content)
                    except ValueError as e:
                        response_content = str(e)
        else:
            raise RuntimeError("Failed to get response from Ollama.")

        # Convert response to AG2 response
        message = ChatCompletionMessage(
            role="assistant",
            content=response_content,
            function_call=None,
            tool_calls=tool_calls,
        )
        choices = [Choice(finish_reason=ollama_finish, index=0, message=message)]

        response_oai = ChatCompletion(
            id=response_id,
            model=ollama_params["model"],
            created=int(time.time()),
            object="chat.completion",
            choices=choices,
            usage=CompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
            cost=0,  # Local models, FREE!
        )

        return response_oai

    def oai_messages_to_ollama_messages(self, messages: list[dict[str, Any]], tools: list) -> list[dict[str, Any]]:
        """Convert messages from OAI format to Ollama's format.
        We correct for any specific role orders and types, and convert tools to messages (as Ollama can't use tool messages)
        """
        ollama_messages = copy.deepcopy(messages)

        # Remove the name field
        for message in ollama_messages:
            if "name" in message:
                message.pop("name", None)

        # Having a 'system' message on the end does not work well with Ollama, so we change it to 'user'
        # 'system' messages on the end are typical of the summarisation message: summary_method="reflection_with_llm"
        if len(ollama_messages) > 1 and ollama_messages[-1]["role"] == "system":
            ollama_messages[-1]["role"] = "user"

        # Process messages for tool calling manually
        if tools is not None and not self._native_tool_calls:
            # 1. We need to append instructions to the starting system message on function calling
            # 2. If we have not yet called tools we append "step 1 instruction" to the latest user message
            # 3. If we have already called tools we append "step 2 instruction" to the latest user message

            have_tool_calls = False
            have_tool_results = False
            last_tool_result_index = -1

            for i, message in enumerate(ollama_messages):
                if "tool_calls" in message:
                    have_tool_calls = True
                if "tool_call_id" in message:
                    have_tool_results = True
                    last_tool_result_index = i

            tool_result_is_last_msg = have_tool_results and last_tool_result_index == len(ollama_messages) - 1

            if ollama_messages[0]["role"] == "system":
                manual_instruction = self._manual_tool_call_instruction

                # Build a string of the functions available
                functions_string = ""
                for function in tools:
                    functions_string += f"""\n{function}\n"""

                # Replace single quotes with double questions - Not sure why this helps the LLM perform
                # better, but it seems to. Monitor and remove if not necessary.
                functions_string = functions_string.replace("'", '"')

                manual_instruction = manual_instruction.replace("[FUNCTIONS_LIST]", functions_string)

                # Update the system message with the instructions and functions
                ollama_messages[0]["content"] = ollama_messages[0]["content"] + manual_instruction.rstrip()

            # If we are still in the function calling or evaluating process, append the steps instruction
            if (not have_tool_calls or tool_result_is_last_msg) and ollama_messages[0]["role"] == "system":
                # NOTE: we require a system message to exist for the manual steps texts
                # Append the manual step instructions
                content_to_append = (
                    self._manual_tool_call_step1 if not have_tool_results else self._manual_tool_call_step2
                )

                if content_to_append != "":
                    # Append the relevant tool call instruction to the latest user message
                    if ollama_messages[-1]["role"] == "user":
                        ollama_messages[-1]["content"] = ollama_messages[-1]["content"] + content_to_append
                    else:
                        ollama_messages.append({"role": "user", "content": content_to_append})

        # Convert tool call and tool result messages to normal text messages for Ollama
        for i, message in enumerate(ollama_messages):
            if "tool_calls" in message:
                # Recommended tool calls
                content = "Run the following function(s):"
                for tool_call in message["tool_calls"]:
                    content = content + "\n" + str(tool_call)
                ollama_messages[i] = {"role": "assistant", "content": content}
            if "tool_call_id" in message:
                # Executed tool results
                message["result"] = message["content"]
                del message["content"]
                del message["role"]
                content = "The following function was run: " + str(message)
                ollama_messages[i] = {"role": "user", "content": content}

        # As we are changing messages, let's merge if they have two user messages on the end and the last one is tool call step instructions
        if (
            len(ollama_messages) >= 2
            and not self._native_tool_calls
            and ollama_messages[-2]["role"] == "user"
            and ollama_messages[-1]["role"] == "user"
            and (
                ollama_messages[-1]["content"] == self._manual_tool_call_step1
                or ollama_messages[-1]["content"] == self._manual_tool_call_step2
            )
        ):
            ollama_messages[-2]["content"] = ollama_messages[-2]["content"] + ollama_messages[-1]["content"]
            del ollama_messages[-1]

        # Ensure the last message is a user / system message, if not, add a user message
        if ollama_messages[-1]["role"] != "user" and ollama_messages[-1]["role"] != "system":
            ollama_messages.append({"role": "user", "content": "Please continue."})

        return ollama_messages

    def _convert_json_response(self, response: str) -> Any:
        """Extract and validate JSON response from the output for structured outputs.

        Args:
            response (str): The response from the API.

        Returns:
            Any: The parsed JSON response.
        """
        if not self._response_format:
            return response

        try:
            # Parse JSON and validate against the Pydantic model if Pydantic model was provided
            if isinstance(self._response_format, dict):
                return response
            else:
                return self._response_format.model_validate_json(response)
        except Exception as e:
            raise ValueError(f"Failed to parse response as valid JSON matching the schema for Structured Output: {e!s}")


def _format_json_response(response: Any, original_answer: str) -> str:
    """Formats the JSON response for structured outputs using the format method if it exists."""
    return response.format() if isinstance(response, FormatterProtocol) else original_answer


@require_optional_import("fix_busted_json", "ollama")
def response_to_tool_call(response_string: str) -> Any:
    """Attempts to convert the response to an object, aimed to align with function format `[{},{}]`"""
    # We try and detect the list[dict[str, Any]] format:
    # Pattern 1 is [{},{}]
    # Pattern 2 is {} (without the [], so could be a single function call)
    patterns = [r"\[[\s\S]*?\]", r"\{[\s\S]*\}"]

    for i, pattern in enumerate(patterns):
        # Search for the pattern in the input string
        matches = re.findall(pattern, response_string.strip())

        for match in matches:
            # It has matched, extract it and load it
            json_str = match.strip()
            data_object = None

            try:
                # Attempt to convert it as is
                data_object = json.loads(json_str)
            except Exception:
                try:
                    # If that fails, attempt to repair it

                    if i == 0:
                        # Enclose to a JSON object for repairing, which is restored upon fix
                        fixed_json = repair_json("{'temp':" + json_str + "}")
                        data_object = json.loads(fixed_json)
                        data_object = data_object["temp"]
                    else:
                        fixed_json = repair_json(json_str)
                        data_object = json.loads(fixed_json)
                except json.JSONDecodeError as e:
                    if e.msg == "Invalid \\escape":
                        # Handle Mistral/Mixtral trying to escape underlines with \\
                        try:
                            json_str = json_str.replace("\\_", "_")
                            if i == 0:
                                fixed_json = repair_json("{'temp':" + json_str + "}")
                                data_object = json.loads(fixed_json)
                                data_object = data_object["temp"]
                            else:
                                fixed_json = repair_json("{'temp':" + json_str + "}")
                                data_object = json.loads(fixed_json)
                        except Exception:
                            pass
                except Exception:
                    pass

            if data_object is not None:
                data_object = _object_to_tool_call(data_object)

                if data_object is not None:
                    return data_object

    # There's no tool call in the response
    return None


def _object_to_tool_call(data_object: Any) -> list[dict[str, Any]]:
    """Attempts to convert an object to a valid tool call object List[Dict] and returns it, if it can, otherwise None"""
    # If it's a dictionary and not a list then wrap in a list
    if isinstance(data_object, dict):
        data_object = [data_object]

    # Validate that the data is a list of dictionaries
    if isinstance(data_object, list) and all(isinstance(item, dict) for item in data_object):
        # Perfect format, a list of dictionaries

        # Check that each dictionary has at least 'name', optionally 'arguments' and no other keys
        is_invalid = False
        for item in data_object:
            if not is_valid_tool_call_item(item):
                is_invalid = True
                break

        # All passed, name and (optionally) arguments exist for all entries.
        if not is_invalid:
            return data_object
    elif isinstance(data_object, list):
        # If it's a list but the items are not dictionaries, check if they are strings that can be converted to dictionaries
        data_copy = data_object.copy()
        is_invalid = False
        for i, item in enumerate(data_copy):
            try:
                new_item = ast.literal_eval(item)
                if isinstance(new_item, dict):
                    if is_valid_tool_call_item(new_item):
                        data_object[i] = new_item
                    else:
                        is_invalid = True
                        break
                else:
                    is_invalid = True
                    break
            except Exception:
                is_invalid = True
                break

        if not is_invalid:
            return data_object

    return None


def is_valid_tool_call_item(call_item: dict) -> bool:
    """Check that a dictionary item has at least 'name', optionally 'arguments' and no other keys to match a tool call JSON"""
    if "name" not in call_item or not isinstance(call_item["name"], str):
        return False

    if set(call_item.keys()) - {"name", "arguments"}:  # noqa: SIM103
        return False

    return True
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""Create a compatible client for the Amazon Bedrock Converse API.

Example usage:
Install the `boto3` package by running `pip install --upgrade boto3`.
- https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html

```python
import autogen

config_list = [
    {
        "api_type": "bedrock",
        "model": "meta.llama3-1-8b-instruct-v1:0",
        "aws_region": "us-west-2",
        "aws_access_key": "",
        "aws_secret_key": "",
        "price": [0.003, 0.015],
    }
]

assistant = autogen.AssistantAgent("assistant", llm_config={"config_list": config_list})
```
"""

from __future__ import annotations

import base64
import json
import os
import re
import time
import warnings
from typing import Any, Literal

import requests
from pydantic import Field, SecretStr, field_serializer
from typing_extensions import Required, Unpack

from ..import_utils import optional_import_block, require_optional_import
from ..llm_config.entry import LLMConfigEntry, LLMConfigEntryDict
from .client_utils import validate_parameter
from .oai_models import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageToolCall, Choice, CompletionUsage

with optional_import_block():
    import boto3
    from botocore.config import Config


class BedrockEntryDict(LLMConfigEntryDict, total=False):
    api_type: Literal["bedrock"]
    aws_region: Required[str]
    aws_access_key: SecretStr | None
    aws_secret_key: SecretStr | None
    aws_session_token: SecretStr | None
    aws_profile_name: str | None
    top_k: int | None
    k: int | None
    seed: int | None
    cache_seed: int | None
    supports_system_prompts: bool
    price: list[float] | None
    timeout: int | None


class BedrockLLMConfigEntry(LLMConfigEntry):
    api_type: Literal["bedrock"] = "bedrock"

    # Bedrock-specific options
    aws_region: str
    aws_access_key: SecretStr | None = None
    aws_secret_key: SecretStr | None = None
    aws_session_token: SecretStr | None = None
    aws_profile_name: str | None = None
    top_k: int | None = None
    k: int | None = None
    seed: int | None = None
    cache_seed: int | None = None
    supports_system_prompts: bool = True
    price: list[float] | None = Field(default=None, min_length=2, max_length=2)
    timeout: int | None = None

    @field_serializer("aws_access_key", "aws_secret_key", "aws_session_token", when_used="unless-none")
    def serialize_aws_secrets(self, v: SecretStr) -> str:
        return v.get_secret_value()

    def create_client(self):
        raise NotImplementedError("BedrockLLMConfigEntry.create_client must be implemented.")


@require_optional_import("boto3", "bedrock")
class BedrockClient:
    """Client for Amazon's Bedrock Converse API."""

    _retries = 5

    def __init__(self, **kwargs: Unpack[BedrockEntryDict]):
        """Initialises BedrockClient for Amazon's Bedrock Converse API"""
        self._aws_access_key = kwargs.get("aws_access_key") or os.getenv("AWS_ACCESS_KEY")
        self._aws_secret_key = kwargs.get("aws_secret_key") or os.getenv("AWS_SECRET_KEY")
        self._aws_session_token = kwargs.get("aws_session_token") or os.getenv("AWS_SESSION_TOKEN")
        self._aws_region = kwargs.get("aws_region") or os.getenv("AWS_REGION")
        self._aws_profile_name = kwargs.get("aws_profile_name")
        self._timeout = kwargs.get("timeout")

        if self._aws_region is None:
            raise ValueError("Region is required to use the Amazon Bedrock API.")

        if self._timeout is None:
            self._timeout = 60

        # Initialize Bedrock client, session, and runtime
        bedrock_config = Config(
            region_name=self._aws_region,
            signature_version="v4",
            retries={"max_attempts": self._retries, "mode": "standard"},
            read_timeout=self._timeout,
        )

        session = boto3.Session(
            aws_access_key_id=self._aws_access_key,
            aws_secret_access_key=self._aws_secret_key,
            aws_session_token=self._aws_session_token,
            profile_name=self._aws_profile_name,
        )

        if "response_format" in kwargs and kwargs["response_format"] is not None:
            warnings.warn("response_format is not supported for Bedrock, it will be ignored.", UserWarning)

        # if haven't got any access_key or secret_key in environment variable or via arguments then
        if (
            self._aws_access_key is None
            or self._aws_access_key == ""
            or self._aws_secret_key is None
            or self._aws_secret_key == ""
        ):
            # attempts to get client from attached role of managed service (lambda, ec2, ecs, etc.)
            self.bedrock_runtime = boto3.client(service_name="bedrock-runtime", config=bedrock_config)
        else:
            session = boto3.Session(
                aws_access_key_id=self._aws_access_key,
                aws_secret_access_key=self._aws_secret_key,
                aws_session_token=self._aws_session_token,
                profile_name=self._aws_profile_name,
            )
            self.bedrock_runtime = session.client(service_name="bedrock-runtime", config=bedrock_config)

    def message_retrieval(self, response):
        """Retrieve the messages from the response."""
        return [choice.message for choice in response.choices]

    def parse_custom_params(self, params: dict[str, Any]):
        """Parses custom parameters for logic in this client class"""
        # Should we separate system messages into its own request parameter, default is True
        # This is required because not all models support a system prompt (e.g. Mistral Instruct).
        self._supports_system_prompts = params.get("supports_system_prompts", True)

    def parse_params(self, params: BedrockEntryDict | dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        """Loads the valid parameters required to invoke Bedrock Converse
        Returns a tuple of (base_params, additional_params)
        """
        # Amazon Bedrock  base model IDs are here:
        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html
        self._model_id = params.get("model")
        assert self._model_id, "Please provide the 'model` in the config_list to use Amazon Bedrock"

        # Parameters vary based on the model used.
        # As we won't cater for all models and parameters, it's the developer's
        # responsibility to implement the parameters and they will only be
        # included if the developer has it in the config.
        #
        # Important:
        # No defaults will be used (as they can vary per model)
        # No ranges will be used (as they can vary)
        # We will cover all the main parameters but there may be others
        # that need to be added later
        #
        # Here are some pages that show the parameters available for different models
        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html
        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html
        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command-r-plus.html
        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html
        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral-chat-completion.html

        # Here are the possible "base" parameters and their suitable types
        base_params = {}

        if "temperature" in params:
            base_params["temperature"] = validate_parameter(
                params, "temperature", (float, int), False, None, None, None
            )

        if "top_p" in params:
            base_params["topP"] = validate_parameter(params, "top_p", (float, int), False, None, None, None)

        if "topP" in params:
            warnings.warn(
                ("topP is deprecated, use top_p instead. Scheduled for removal in 0.10.0 version."), DeprecationWarning
            )
            base_params["topP"] = validate_parameter(params, "topP", (float, int), False, None, None, None)

        if "max_tokens" in params:
            base_params["maxTokens"] = validate_parameter(params, "max_tokens", (int,), False, None, None, None)

        if "maxTokens" in params:
            warnings.warn(
                ("maxTokens is deprecated, use max_tokens instead. Scheduled for removal in 0.10.0 version."),
                DeprecationWarning,
            )
            base_params["maxTokens"] = validate_parameter(params, "maxTokens", (int,), False, None, None, None)

        # Here are the possible "model-specific" parameters and their suitable types, known as additional parameters
        additional_params = {}

        for param_name, suitable_types in (
            ("top_k", (int,)),
            ("k", (int,)),
            ("seed", (int,)),
        ):
            if param_name in params:
                additional_params[param_name] = validate_parameter(
                    params, param_name, suitable_types, False, None, None, None
                )

        # For this release we will not support streaming as many models do not support streaming with tool use
        if params.get("stream", False):
            warnings.warn(
                "Streaming is not currently supported, streaming will be disabled.",
                UserWarning,
            )

        return base_params, additional_params

    def create(self, params) -> ChatCompletion:
        """Run Amazon Bedrock inference and return AG2 response"""
        # Set custom client class settings
        self.parse_custom_params(params)

        # Parse the inference parameters
        base_params, additional_params = self.parse_params(params)

        has_tools = "tools" in params
        messages = oai_messages_to_bedrock_messages(params["messages"], has_tools, self._supports_system_prompts)

        if self._supports_system_prompts:
            system_messages = extract_system_messages(params["messages"])

        tool_config = format_tools(params["tools"] if has_tools else [])

        request_args = {"messages": messages, "modelId": self._model_id}

        # Base and additional args
        if len(base_params) > 0:
            request_args["inferenceConfig"] = base_params

        if len(additional_params) > 0:
            request_args["additionalModelRequestFields"] = additional_params

        if self._supports_system_prompts:
            request_args["system"] = system_messages

        if len(tool_config["tools"]) > 0:
            request_args["toolConfig"] = tool_config

        response = self.bedrock_runtime.converse(**request_args)
        if response is None:
            raise RuntimeError(f"Failed to get response from Bedrock after retrying {self._retries} times.")

        finish_reason = convert_stop_reason_to_finish_reason(response["stopReason"])
        response_message = response["output"]["message"]

        tool_calls = format_tool_calls(response_message["content"]) if finish_reason == "tool_calls" else None

        text = ""
        for content in response_message["content"]:
            if "text" in content:
                text = content["text"]
                # NOTE: other types of output may be dealt with here

        message = ChatCompletionMessage(role="assistant", content=text, tool_calls=tool_calls)

        response_usage = response["usage"]
        usage = CompletionUsage(
            prompt_tokens=response_usage["inputTokens"],
            completion_tokens=response_usage["outputTokens"],
            total_tokens=response_usage["totalTokens"],
        )

        return ChatCompletion(
            id=response["ResponseMetadata"]["RequestId"],
            choices=[Choice(finish_reason=finish_reason, index=0, message=message)],
            created=int(time.time()),
            model=self._model_id,
            object="chat.completion",
            usage=usage,
        )

    def cost(self, response: ChatCompletion) -> float:
        """Calculate the cost of the response."""
        return calculate_cost(response.usage.prompt_tokens, response.usage.completion_tokens, response.model)

    @staticmethod
    def get_usage(response) -> dict:
        """Get the usage of tokens and their cost information."""
        return {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens,
            "cost": response.cost,
            "model": response.model,
        }


def extract_system_messages(messages: list[dict[str, Any]]) -> list:
    """Extract the system messages from the list of messages.

    Args:
        messages (list[dict[str, Any]]): List of messages.

    Returns:
        List[SystemMessage]: List of System messages.
    """
    """
    system_messages = [message.get("content")[0]["text"] for message in messages if message.get("role") == "system"]
    return system_messages # ''.join(system_messages)
    """

    for message in messages:
        if message.get("role") == "system":
            if isinstance(message["content"], str):
                return [{"text": message.get("content")}]
            else:
                return [{"text": message.get("content")[0]["text"]}]
    return []


def oai_messages_to_bedrock_messages(
    messages: list[dict[str, Any]], has_tools: bool, supports_system_prompts: bool
) -> list[dict[str, Any]]:
    """Convert messages from OAI format to Bedrock format.
    We correct for any specific role orders and types, etc.
    AWS Bedrock requires messages to alternate between user and assistant roles. This function ensures that the messages
    are in the correct order and format for Bedrock by inserting "Please continue" messages as needed.
    This is the same method as the one in the Autogen Anthropic client
    """
    # Track whether we have tools passed in. If not,  tool use / result messages should be converted to text messages.
    # Bedrock requires a tools parameter with the tools listed, if there are other messages with tool use or tool results.
    # This can occur when we don't need tool calling, such as for group chat speaker selection

    # Convert messages to Bedrock compliant format

    # Take out system messages if the model supports it, otherwise leave them in.
    if supports_system_prompts:
        messages = [x for x in messages if x["role"] != "system"]
    else:
        # Replace role="system" with role="user"
        for msg in messages:
            if msg["role"] == "system":
                msg["role"] = "user"

    processed_messages = []

    # Used to interweave user messages to ensure user/assistant alternating
    user_continue_message = {"content": [{"text": "Please continue."}], "role": "user"}
    assistant_continue_message = {
        "content": [{"text": "Please continue."}],
        "role": "assistant",
    }

    tool_use_messages = 0
    tool_result_messages = 0
    last_tool_use_index = -1
    last_tool_result_index = -1
    # user_role_index = 0 if supports_system_prompts else 1 # If system prompts are supported, messages start with user, otherwise they'll be the second message
    for message in messages:
        # New messages will be added here, manage role alternations
        expected_role = "user" if len(processed_messages) % 2 == 0 else "assistant"

        if "tool_calls" in message:
            # Map the tool call options to Bedrock's format
            tool_uses = []
            tool_names = []
            for tool_call in message["tool_calls"]:
                tool_uses.append({
                    "toolUse": {
                        "toolUseId": tool_call["id"],
                        "name": tool_call["function"]["name"],
                        "input": json.loads(tool_call["function"]["arguments"]),
                    }
                })
                if has_tools:
                    tool_use_messages += 1
                tool_names.append(tool_call["function"]["name"])

            if expected_role == "user":
                # Insert an extra user message as we will append an assistant message
                processed_messages.append(user_continue_message)

            if has_tools:
                processed_messages.append({"role": "assistant", "content": tool_uses})
                last_tool_use_index = len(processed_messages) - 1
            else:
                # Not using tools, so put in a plain text message
                processed_messages.append({
                    "role": "assistant",
                    "content": [{"text": f"Some internal function(s) that could be used: [{', '.join(tool_names)}]"}],
                })
        elif "tool_call_id" in message:
            if has_tools:
                # Map the tool usage call to tool_result for Bedrock
                tool_result = {
                    "toolResult": {
                        "toolUseId": message["tool_call_id"],
                        "content": [{"text": message["content"]}],
                    }
                }

                # If the previous message also had a tool_result, add it to that
                # Otherwise append a new message
                if last_tool_result_index == len(processed_messages) - 1:
                    processed_messages[-1]["content"].append(tool_result)
                else:
                    if expected_role == "assistant":
                        # Insert an extra assistant message as we will append a user message
                        processed_messages.append(assistant_continue_message)

                    processed_messages.append({"role": "user", "content": [tool_result]})
                    last_tool_result_index = len(processed_messages) - 1

                tool_result_messages += 1
            else:
                # Not using tools, so put in a plain text message
                processed_messages.append({
                    "role": "user",
                    "content": [{"text": f"Running the function returned: {message['content']}"}],
                })
        elif message["content"] == "":
            # Ignoring empty messages
            pass
        else:
            if expected_role != message["role"] and not (len(processed_messages) == 0 and message["role"] == "system"):
                # Inserting the alternating continue message (ignore if it's the first message and a system message)
                processed_messages.append(
                    user_continue_message if expected_role == "user" else assistant_continue_message
                )

            processed_messages.append({
                "role": message["role"],
                "content": parse_content_parts(message=message),
            })

    # We'll replace the last tool_use if there's no tool_result (occurs if we finish the conversation before running the function)
    if has_tools and tool_use_messages != tool_result_messages:
        processed_messages[last_tool_use_index] = assistant_continue_message

    # name is not a valid field on messages
    for message in processed_messages:
        if "name" in message:
            message.pop("name", None)

    # Note: When using reflection_with_llm we may end up with an "assistant" message as the last message and that may cause a blank response
    # So, if the last role is not user, add a 'user' continue message at the end
    if processed_messages[-1]["role"] != "user":
        processed_messages.append(user_continue_message)

    return processed_messages


def parse_content_parts(
    message: dict[str, Any],
) -> list[dict[str, Any]]:
    content: str | list[dict[str, Any]] = message.get("content")
    if isinstance(content, str):
        return [
            {
                "text": content,
            }
        ]
    content_parts = []
    for part in content:
        # part_content: Dict = part.get("content")
        if "text" in part:  # part_content:
            content_parts.append({
                "text": part.get("text"),
            })
        elif "image_url" in part:  # part_content:
            image_data, content_type = parse_image(part.get("image_url").get("url"))
            content_parts.append({
                "image": {
                    "format": content_type[6:],  # image/
                    "source": {"bytes": image_data},
                },
            })
        else:
            # Ignore..
            continue
    return content_parts


def parse_image(image_url: str) -> tuple[bytes, str]:
    """Try to get the raw data from an image url.

    Ref: https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ImageSource.html
    returns a tuple of (Image Data, Content Type)
    """
    pattern = r"^data:(image/[a-z]*);base64,\s*"
    content_type = re.search(pattern, image_url)
    # if already base64 encoded.
    # Only supports 'image/jpeg', 'image/png', 'image/gif' or 'image/webp'
    if content_type:
        image_data = re.sub(pattern, "", image_url)
        return base64.b64decode(image_data), content_type.group(1)

    # Send a request to the image URL
    response = requests.get(image_url)
    # Check if the request was successful
    if response.status_code == 200:
        content_type = response.headers.get("Content-Type")
        if not content_type.startswith("image"):
            content_type = "image/jpeg"
        # Get the image content
        image_content = response.content
        return image_content, content_type
    else:
        raise RuntimeError("Unable to access the image url")


def format_tools(tools: list[dict[str, Any]]) -> dict[Literal["tools"], list[dict[str, Any]]]:
    converted_schema = {"tools": []}

    for tool in tools:
        if tool["type"] == "function":
            function = tool["function"]
            converted_tool = {
                "toolSpec": {
                    "name": function["name"],
                    "description": function["description"],
                    "inputSchema": {"json": {"type": "object", "properties": {}, "required": []}},
                }
            }

            for prop_name, prop_details in function["parameters"]["properties"].items():
                converted_tool["toolSpec"]["inputSchema"]["json"]["properties"][prop_name] = {
                    "type": prop_details["type"],
                    "description": prop_details.get("description", ""),
                }
                if "enum" in prop_details:
                    converted_tool["toolSpec"]["inputSchema"]["json"]["properties"][prop_name]["enum"] = prop_details[
                        "enum"
                    ]
                if "default" in prop_details:
                    converted_tool["toolSpec"]["inputSchema"]["json"]["properties"][prop_name]["default"] = (
                        prop_details["default"]
                    )

            if "required" in function["parameters"]:
                converted_tool["toolSpec"]["inputSchema"]["json"]["required"] = function["parameters"]["required"]

            converted_schema["tools"].append(converted_tool)

    return converted_schema


def format_tool_calls(content):
    """Converts Converse API response tool calls to AG2 format"""
    tool_calls = []
    for tool_request in content:
        if "toolUse" in tool_request:
            tool = tool_request["toolUse"]

            tool_calls.append(
                ChatCompletionMessageToolCall(
                    id=tool["toolUseId"],
                    function={
                        "name": tool["name"],
                        "arguments": json.dumps(tool["input"]),
                    },
                    type="function",
                )
            )
    return tool_calls


def convert_stop_reason_to_finish_reason(
    stop_reason: str,
) -> Literal["stop", "length", "tool_calls", "content_filter"]:
    """Converts Bedrock finish reasons to our finish reasons, according to OpenAI:

    - stop: if the model hit a natural stop point or a provided stop sequence,
    - length: if the maximum number of tokens specified in the request was reached,
    - content_filter: if content was omitted due to a flag from our content filters,
    - tool_calls: if the model called a tool
    """
    if stop_reason:
        finish_reason_mapping = {
            "tool_use": "tool_calls",
            "finished": "stop",
            "end_turn": "stop",
            "max_tokens": "length",
            "stop_sequence": "stop",
            "complete": "stop",
            "content_filtered": "content_filter",
        }
        return finish_reason_mapping.get(stop_reason.lower(), stop_reason.lower())

    warnings.warn(f"Unsupported stop reason: {stop_reason}", UserWarning)
    return None


# NOTE: As this will be quite dynamic, it's expected that the developer will use the "price" parameter in their config
# These may be removed.
PRICES_PER_K_TOKENS = {
    "meta.llama3-8b-instruct-v1:0": (0.0003, 0.0006),
    "meta.llama3-70b-instruct-v1:0": (0.00265, 0.0035),
    "mistral.mistral-7b-instruct-v0:2": (0.00015, 0.0002),
    "mistral.mixtral-8x7b-instruct-v0:1": (0.00045, 0.0007),
    "mistral.mistral-large-2402-v1:0": (0.004, 0.012),
    "mistral.mistral-small-2402-v1:0": (0.001, 0.003),
}


def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> float:
    """Calculate the cost of the completion using the Bedrock pricing."""
    if model_id in PRICES_PER_K_TOKENS:
        input_cost_per_k, output_cost_per_k = PRICES_PER_K_TOKENS[model_id]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        return input_cost + output_cost
    else:
        warnings.warn(
            f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{"price" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.',
            UserWarning,
        )
        return 0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import enum
import warnings
from typing import Any, Optional, TypeVar, Union, get_args, get_origin

from pydantic import BaseModel as BaseModel
from pydantic import ConfigDict, Field, alias_generators


def _remove_extra_fields(model: Any, response: dict[str, object]) -> None:
    """Removes extra fields from the response that are not in the model.

    Mutates the response in place.
    """
    key_values = list(response.items())

    for key, value in key_values:
        # Need to convert to snake case to match model fields names
        # ex: UsageMetadata
        alias_map = {field_info.alias: key for key, field_info in model.model_fields.items()}

        if key not in model.model_fields and key not in alias_map:
            response.pop(key)
            continue

        key = alias_map.get(key, key)

        annotation = model.model_fields[key].annotation

        # Get the BaseModel if Optional
        if get_origin(annotation) is Union:
            annotation = get_args(annotation)[0]

        # if dict, assume BaseModel but also check that field type is not dict
        # example: FunctionCall.args
        if isinstance(value, dict) and get_origin(annotation) is not dict:
            _remove_extra_fields(annotation, value)
        elif isinstance(value, list):
            for item in value:
                # assume a list of dict is list of BaseModel
                if isinstance(item, dict):
                    _remove_extra_fields(get_args(annotation)[0], item)


T = TypeVar("T", bound="BaseModel")


class CommonBaseModel(BaseModel):
    model_config = ConfigDict(
        alias_generator=alias_generators.to_camel,
        populate_by_name=True,
        from_attributes=True,
        protected_namespaces=(),
        extra="forbid",
        # This allows us to use arbitrary types in the model. E.g. PIL.Image.
        arbitrary_types_allowed=True,
        ser_json_bytes="base64",
        val_json_bytes="base64",
        ignored_types=(TypeVar,),
    )

    @classmethod
    def _from_response(cls: type[T], *, response: dict[str, object], kwargs: dict[str, object]) -> T:
        # To maintain forward compatibility, we need to remove extra fields from
        # the response.
        # We will provide another mechanism to allow users to access these fields.
        _remove_extra_fields(cls, response)
        validated_response = cls.model_validate(response)
        return validated_response

    def to_json_dict(self) -> dict[str, object]:
        return self.model_dump(exclude_none=True, mode="json")


class CaseInSensitiveEnum(str, enum.Enum):
    """Case insensitive enum."""

    @classmethod
    def _missing_(cls, value: Any) -> Optional["CaseInSensitiveEnum"]:
        try:
            return cls[value.upper()]  # Try to access directly with uppercase
        except KeyError:
            try:
                return cls[value.lower()]  # Try to access directly with lowercase
            except KeyError:
                warnings.warn(f"{value} is not a valid {cls.__name__}")
                try:
                    # Creating a enum instance based on the value
                    # We need to use super() to avoid infinite recursion.
                    unknown_enum_val = super().__new__(cls, value)
                    unknown_enum_val._name_ = str(value)  # pylint: disable=protected-access
                    unknown_enum_val._value_ = value  # pylint: disable=protected-access
                    return unknown_enum_val
                except:  # noqa: E722
                    return None


class FunctionCallingConfigMode(CaseInSensitiveEnum):
    """Config for the function calling config mode."""

    MODE_UNSPECIFIED = "MODE_UNSPECIFIED"
    AUTO = "AUTO"
    ANY = "ANY"
    NONE = "NONE"
    VALIDATED = "VALIDATED"


class LatLng(CommonBaseModel):
    """An object that represents a latitude/longitude pair.

    This is expressed as a pair of doubles to represent degrees latitude and
    degrees longitude. Unless specified otherwise, this object must conform to the
    <a href="https://en.wikipedia.org/wiki/World_Geodetic_System#1984_version">
    WGS84 standard</a>. Values must be within normalized ranges.
    """

    latitude: float | None = Field(
        default=None,
        description="""The latitude in degrees. It must be in the range [-90.0, +90.0].""",
    )
    longitude: float | None = Field(
        default=None,
        description="""The longitude in degrees. It must be in the range [-180.0, +180.0]""",
    )


class FunctionCallingConfig(CommonBaseModel):
    """Function calling config."""

    mode: FunctionCallingConfigMode | None = Field(default=None, description="""Optional. Function calling mode.""")
    allowed_function_names: list[str] | None = Field(
        default=None,
        description="""Optional. Function names to call. Only set when the Mode is ANY. Function names should match [FunctionDeclaration.name]. With mode set to ANY, model will predict a function call from the set of function names provided.""",
    )


class RetrievalConfig(CommonBaseModel):
    """Retrieval config."""

    lat_lng: LatLng | None = Field(default=None, description="""Optional. The location of the user.""")
    language_code: str | None = Field(default=None, description="""The language code of the user.""")


class ToolConfig(CommonBaseModel):
    """Tool config.

    This config is shared for all tools provided in the request.
    """

    function_calling_config: FunctionCallingConfig | None = Field(
        default=None, description="""Optional. Function calling config."""
    )
    retrieval_config: RetrievalConfig | None = Field(default=None, description="""Optional. Retrieval config.""")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import logging
import os
import pathlib
import re
import string
import subprocess
import sys
import time
import venv
from collections.abc import Callable
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from hashlib import md5
from types import SimpleNamespace

import docker

from .types import UserMessageImageContentPart, UserMessageTextContentPart

SENTINEL = object()
DEFAULT_MODEL = "gpt-5"
FAST_MODEL = "gpt-5-nano"
# Regular expression for finding a code block
# ```[ \t]*(\w+)?[ \t]*\r?\n(.*?)[ \t]*\r?\n``` Matches multi-line code blocks.
#   The [ \t]* matches the potential spaces before language name.
#   The (\w+)? matches the language, where the ? indicates it is optional.
#   The [ \t]* matches the potential spaces (not newlines) after language name.
#   The \r?\n makes sure there is a linebreak after ```.
#   The (.*?) matches the code itself (non-greedy).
#   The \r?\n makes sure there is a linebreak before ```.
#   The [ \t]* matches the potential spaces before closing ``` (the spec allows indentation).
CODE_BLOCK_PATTERN = r"```[ \t]*(\w+)?[ \t]*\r?\n(.*?)\r?\n[ \t]*```"
WORKING_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), "extensions")
UNKNOWN = "unknown"
TIMEOUT_MSG = "Timeout"
DEFAULT_TIMEOUT = 600
WIN32 = sys.platform == "win32"
PATH_SEPARATOR = (WIN32 and "\\") or "/"
PYTHON_VARIANTS = ["python", "Python", "py"]

logger = logging.getLogger(__name__)


def content_str(content: str | list[UserMessageTextContentPart | UserMessageImageContentPart] | None) -> str:
    """Converts the `content` field of an OpenAI message into a string format.

    This function processes content that may be a string, a list of mixed text and image URLs, or None,
    and converts it into a string. Text is directly appended to the result string, while image URLs are
    represented by a placeholder image token. If the content is None, an empty string is returned.

    Args:
        content: The content to be processed. Can be a string, a list of dictionaries representing text and image URLs, or None.

    Returns:
        str: A string representation of the input content. Image URLs are replaced with an image token.

    Note:
    - The function expects each dictionary in the list to have a "type" key that is either "text" or "image_url".
      For "text" type, the "text" key's value is appended to the result. For "image_url", an image token is appended.
    - This function is useful for handling content that may include both text and image references, especially
      in contexts where images need to be represented as placeholders.
    """
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if not isinstance(content, list):
        raise TypeError(f"content must be None, str, or list, but got {type(content)}")

    rst = []
    for item in content:
        if not isinstance(item, dict):
            raise TypeError("Wrong content format: every element should be dict if the content is a list.")
        assert "type" in item, "Wrong content format. Missing 'type' key in content's dict."
        if item["type"] in ["text", "input_text"]:
            rst.append(item["text"])
        elif item["type"] in ["image_url", "input_image"]:
            rst.append("<image>")
        elif item["type"] in ["function", "tool_call", "tool_calls"]:
            rst.append("<function>" if "name" not in item else f"<function: {item['name']}>")
        else:
            raise ValueError(f"Wrong content format: unknown type {item['type']} within the content")
    return "\n".join(rst)


def infer_lang(code: str) -> str:
    """Infer the language for the code.
    TODO: make it robust.
    """
    if code.startswith("python ") or code.startswith("pip") or code.startswith("python3 "):
        return "sh"

    # check if code is a valid python code
    try:
        compile(code, "test", "exec")
        return "python"
    except SyntaxError:
        # not a valid python code
        return UNKNOWN


# TODO: In the future move, to better support https://spec.commonmark.org/0.30/#fenced-code-blocks
#       perhaps by using a full Markdown parser.
def extract_code(
    text: str | list, pattern: str = CODE_BLOCK_PATTERN, detect_single_line_code: bool = False
) -> list[tuple[str, str]]:
    """Extract code from a text.

    Args:
        text (str or List): The content to extract code from. The content can be
            a string or a list, as returned by standard GPT or multimodal GPT.
        pattern (str, optional): The regular expression pattern for finding the
            code block. Defaults to CODE_BLOCK_PATTERN.
        detect_single_line_code (bool, optional): Enable the new feature for
            extracting single line code. Defaults to False.

    Returns:
        list: A list of tuples, each containing the language and the code.
          If there is no code block in the input text, the language would be "unknown".
          If there is code block but the language is not specified, the language would be "".
    """
    text = content_str(text)
    if not detect_single_line_code:
        match = re.findall(pattern, text, flags=re.DOTALL)
        return match if match else [(UNKNOWN, text)]

    # Extract both multi-line and single-line code block, separated by the | operator
    # `([^`]+)`: Matches inline code.
    code_pattern = re.compile(CODE_BLOCK_PATTERN + r"|`([^`]+)`")
    code_blocks = code_pattern.findall(text)

    # Extract the individual code blocks and languages from the matched groups
    extracted = []
    for lang, group1, group2 in code_blocks:
        if group1:
            extracted.append((lang.strip(), group1.strip()))
        elif group2:
            extracted.append(("", group2.strip()))

    return extracted


def timeout_handler(signum, frame):
    raise TimeoutError("Timed out!")


def get_powershell_command():
    try:
        result = subprocess.run(["powershell", "$PSVersionTable.PSVersion.Major"], capture_output=True, text=True)
        if result.returncode == 0:
            return "powershell"
    except (FileNotFoundError, NotADirectoryError):
        # This means that 'powershell' command is not found so now we try looking for 'pwsh'
        try:
            result = subprocess.run(
                ["pwsh", "-Command", "$PSVersionTable.PSVersion.Major"], capture_output=True, text=True
            )
            if result.returncode == 0:
                return "pwsh"
        except FileExistsError as e:
            raise FileNotFoundError(
                "Neither powershell.exe nor pwsh.exe is present in the system. "
                "Please install PowerShell and try again. "
            ) from e
        except NotADirectoryError as e:
            raise NotADirectoryError(
                "PowerShell is either not installed or its path is not given "
                "properly in the environment variable PATH. Please check the "
                "path and try again. "
            ) from e
    except PermissionError as e:
        raise PermissionError("No permission to run powershell.") from e


def _cmd(lang: str) -> str:
    if lang in PYTHON_VARIANTS:
        return "python"
    if lang.startswith("python") or lang in ["bash", "sh"]:
        return lang
    if lang in ["shell"]:
        return "sh"
    if lang == "javascript":
        return "node"
    if lang in ["ps1", "pwsh", "powershell"]:
        powershell_command = get_powershell_command()
        return powershell_command

    raise NotImplementedError(f"{lang} not recognized in code execution")


def is_docker_running() -> bool:
    """Check if docker is running.

    Returns:
        bool: True if docker is running; False otherwise.
    """
    try:
        client = docker.from_env()
        client.ping()
        return True
    except docker.errors.DockerException:
        return False


def in_docker_container() -> bool:
    """Check if the code is running in a docker container.

    Returns:
        bool: True if the code is running in a docker container; False otherwise.
    """
    return os.path.exists("/.dockerenv")


def decide_use_docker(use_docker: bool | None) -> bool | None:
    if use_docker is None:
        env_var_use_docker = os.environ.get("AUTOGEN_USE_DOCKER", "True")

        truthy_values = {"1", "true", "yes", "t"}
        falsy_values = {"0", "false", "no", "f"}

        # Convert the value to lowercase for case-insensitive comparison
        env_var_use_docker_lower = env_var_use_docker.lower()

        # Determine the boolean value based on the environment variable
        if env_var_use_docker_lower in truthy_values:
            use_docker = True
        elif env_var_use_docker_lower in falsy_values:
            use_docker = False
        elif env_var_use_docker_lower == "none":  # Special case for 'None' as a string
            use_docker = None
        else:
            # Raise an error for any unrecognized value
            raise ValueError(
                f'Invalid value for AUTOGEN_USE_DOCKER: {env_var_use_docker}. Please set AUTOGEN_USE_DOCKER to "1/True/yes", "0/False/no", or "None".'
            )
    return use_docker


def check_can_use_docker_or_throw(use_docker) -> None:
    if use_docker is not None:
        inside_docker = in_docker_container()
        docker_installed_and_running = is_docker_running()
        if use_docker and not inside_docker and not docker_installed_and_running:
            raise RuntimeError(
                "Code execution is set to be run in docker (default behaviour) but docker is not running.\n"
                "The options available are:\n"
                "- Make sure docker is running (advised approach for code execution)\n"
                '- Set "use_docker": False in code_execution_config\n'
                '- Set AUTOGEN_USE_DOCKER to "0/False/no" in your environment variables'
            )


def _sanitize_filename_for_docker_tag(filename: str) -> str:
    """Convert a filename to a valid docker tag.
    See https://docs.docker.com/engine/reference/commandline/tag/ for valid tag
    format.

    Args:
        filename (str): The filename to be converted.

    Returns:
        str: The sanitized Docker tag.
    """
    # Replace any character not allowed with an underscore
    allowed_chars = set(string.ascii_letters + string.digits + "_.-")
    sanitized = "".join(char if char in allowed_chars else "_" for char in filename)

    # Ensure it does not start with a period or a dash
    if sanitized.startswith(".") or sanitized.startswith("-"):
        sanitized = "_" + sanitized[1:]

    # Truncate if longer than 128 characters
    return sanitized[:128]


def execute_code(
    code: str | None = None,
    timeout: int | None = None,
    filename: str | None = None,
    work_dir: str | None = None,
    use_docker: list[str] | str | bool = SENTINEL,
    lang: str | None = "python",
) -> tuple[int, str, str | None]:
    """Execute code in a docker container.
    This function is not tested on MacOS.

    Args:
        code (Optional, str): The code to execute.
            If None, the code from the file specified by filename will be executed.
            Either code or filename must be provided.
        timeout (Optional, int): The maximum execution time in seconds.
            If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.
        filename (Optional, str): The file name to save the code or where the code is stored when `code` is None.
            If None, a file with a randomly generated name will be created.
            The randomly generated file will be deleted after execution.
            The file name must be a relative path. Relative paths are relative to the working directory.
        work_dir (Optional, str): The working directory for the code execution.
            If None, a default working directory will be used.
            The default working directory is the "extensions" directory under
            "path_to_autogen".
        use_docker (list, str or bool): The docker image to use for code execution.
            Default is True, which means the code will be executed in a docker container. A default list of images will be used.
            If a list or a str of image name(s) is provided, the code will be executed in a docker container
            with the first image successfully pulled.
            If False, the code will be executed in the current environment.
            Expected behaviour:
                - If `use_docker` is not set (i.e. left default to True) or is explicitly set to True and the docker package is available, the code will run in a Docker container.
                - If `use_docker` is not set (i.e. left default to True) or is explicitly set to True but the Docker package is missing or docker isn't running, an error will be raised.
                - If `use_docker` is explicitly set to False, the code will run natively.
            If the code is executed in the current environment,
            the code must be trusted.
        lang (Optional, str): The language of the code. Default is "python".

    Returns:
        int: 0 if the code executes successfully.
        str: The error message if the code fails to execute; the stdout otherwise.
        image: The docker image name after container run when docker is used.
    """
    if all((code is None, filename is None)):
        error_msg = f"Either {code=} or {filename=} must be provided."
        logger.error(error_msg)
        raise AssertionError(error_msg)

    running_inside_docker = in_docker_container()
    docker_running = is_docker_running()

    # SENTINEL is used to indicate that the user did not explicitly set the argument
    if use_docker is SENTINEL:
        use_docker = decide_use_docker(use_docker=None)
    check_can_use_docker_or_throw(use_docker)

    timeout = timeout or DEFAULT_TIMEOUT
    original_filename = filename
    if WIN32 and lang in ["sh", "shell"] and (not use_docker):
        lang = "ps1"
    if filename is None:
        code_hash = md5(code.encode()).hexdigest()
        # create a file with a automatically generated name
        filename = f"tmp_code_{code_hash}.{'py' if lang.startswith('python') else lang}"
    if work_dir is None:
        work_dir = WORKING_DIR

    filepath = os.path.join(work_dir, filename)
    file_dir = os.path.dirname(filepath)
    os.makedirs(file_dir, exist_ok=True)

    if code is not None:
        with open(filepath, "w", encoding="utf-8") as fout:
            fout.write(code)

    if not use_docker or running_inside_docker:
        # already running in a docker container
        cmd = [
            sys.executable if lang.startswith("python") else _cmd(lang),
            f".\\{filename}" if WIN32 else filename,
        ]
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(
                subprocess.run,
                cmd,
                cwd=work_dir,
                capture_output=True,
                text=True,
            )
            try:
                result = future.result(timeout=timeout)
            except TimeoutError:
                if original_filename is None:
                    os.remove(filepath)
                return 1, TIMEOUT_MSG, None
        if original_filename is None:
            os.remove(filepath)
        if result.returncode:
            logs = result.stderr
            if original_filename is None:
                abs_path = str(pathlib.Path(filepath).absolute())
                logs = logs.replace(str(abs_path), "").replace(filename, "")
            else:
                abs_path = str(pathlib.Path(work_dir).absolute()) + PATH_SEPARATOR
                logs = logs.replace(str(abs_path), "")
        else:
            logs = result.stdout
        return result.returncode, logs, None

    # create a docker client
    if use_docker and not docker_running:
        raise RuntimeError(
            "Docker package is missing or docker is not running. Please make sure docker is running or set use_docker=False."
        )

    client = docker.from_env()

    image_list = (
        ["python:3-slim", "python:3", "python:3-windowsservercore"]
        if use_docker is True
        else [use_docker]
        if isinstance(use_docker, str)
        else use_docker
    )
    for image in image_list:
        # check if the image exists
        try:
            client.images.get(image)
            break
        except docker.errors.ImageNotFound:
            # pull the image
            print("Pulling image", image)
            try:
                client.images.pull(image)
                break
            except docker.errors.DockerException:
                print("Failed to pull image", image)
    # get a randomized str based on current time to wrap the exit code
    exit_code_str = f"exitcode{time.time()}"
    abs_path = pathlib.Path(work_dir).absolute()
    cmd = [
        "sh",
        "-c",
        f'{_cmd(lang)} "{filename}"; exit_code=$?; echo -n {exit_code_str}; echo -n $exit_code; echo {exit_code_str}',
    ]
    # create a docker container
    container = client.containers.run(
        image,
        command=cmd,
        working_dir="/workspace",
        detach=True,
        # get absolute path to the working directory
        volumes={abs_path: {"bind": "/workspace", "mode": "rw"}},
    )
    start_time = time.time()
    while container.status != "exited" and time.time() - start_time < timeout:
        # Reload the container object
        container.reload()
    if container.status != "exited":
        container.stop()
        container.remove()
        if original_filename is None:
            os.remove(filepath)
        return 1, TIMEOUT_MSG, image
    # get the container logs
    logs = container.logs().decode("utf-8").rstrip()
    # commit the image
    tag = _sanitize_filename_for_docker_tag(filename)
    container.commit(repository="python", tag=tag)
    # remove the container
    container.remove()
    # check if the code executed successfully
    exit_code = container.attrs["State"]["ExitCode"]
    if exit_code == 0:
        # extract the exit code from the logs
        pattern = re.compile(f"{exit_code_str}(\\d+){exit_code_str}")
        match = pattern.search(logs)
        exit_code = 1 if match is None else int(match.group(1))
        # remove the exit code from the logs
        logs = logs if match is None else pattern.sub("", logs)

    if original_filename is None:
        os.remove(filepath)
    if exit_code:
        logs = logs.replace(f"/workspace/{filename if original_filename is None else ''}", "")
    # return the exit code, logs and image
    return exit_code, logs, f"python:{tag}"


_GENERATE_ASSERTIONS_CONFIG = {
    "prompt": """Given the signature and docstring, write the exactly same number of assertion(s) for the provided example(s) in the docstring, without assertion messages.

func signature:
{definition}
assertions:""",
    "model": FAST_MODEL,
    "max_tokens": 256,
    "stop": "\n\n",
}


def _remove_check(response):
    """Remove the check function from the response."""
    # find the position of the check function
    pos = response.find("def check(")
    if pos == -1:
        return response
    return response[:pos]


def eval_function_completions(
    responses: list[str],
    definition: str,
    test: str | None = None,
    entry_point: str | None = None,
    assertions: str | Callable[[str], tuple[str, float]] | None = None,
    timeout: float | None = 3,
    use_docker: bool | None = True,
) -> dict:
    """`(openai<1)` Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test.

    Args:
        responses: The list of responses.
        definition: The input definition.
        test: The test code.
        entry_point: The name of the function.
        assertions: The assertion code which serves as a filter of the responses, or an assertion generator.
            When provided, only the responses that pass the assertions will be considered for the actual test (if provided).
        timeout: The timeout for executing the code.
        use_docker: Whether to use docker for code execution.

    Returns:
        dict: The success metrics.
    """
    n = len(responses)
    if assertions is None:
        # no assertion filter
        success_list = []
        for i in range(n):
            response = _remove_check(responses[i])
            code = (
                f"{response}\n{test}\ncheck({entry_point})"
                if response.startswith("def")
                else f"{definition}{response}\n{test}\ncheck({entry_point})"
            )
            success = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            success_list.append(success)
        return {
            "expected_success": 1 - pow(1 - sum(success_list) / n, n),
            "success": any(s for s in success_list),
        }
    if callable(assertions) and n > 1:
        # assertion generator
        assertions, gen_cost = assertions(definition)
    else:
        assertions, gen_cost = None, 0
    if n > 1 or test is None:
        for i in range(n):
            response = responses[i] = _remove_check(responses[i])
            code = (
                f"{response}\n{assertions}" if response.startswith("def") else f"{definition}{response}\n{assertions}"
            )
            succeed_assertions = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            if succeed_assertions:
                break
    else:
        # just test, no need to check assertions
        succeed_assertions = False
        i, response = 0, responses[0]
    if test is None:
        # no test code
        return {
            "index_selected": i,
            "succeed_assertions": succeed_assertions,
            "gen_cost": gen_cost,
            "assertions": assertions,
        }
    code_test = (
        f"{response}\n{test}\ncheck({entry_point})"
        if response.startswith("def")
        else f"{definition}{response}\n{test}\ncheck({entry_point})"
    )
    success = execute_code(code_test, timeout=timeout, use_docker=use_docker)[0] == 0
    return {
        "index_selected": i,
        "succeed_assertions": succeed_assertions,
        "success": success,
        "gen_cost": gen_cost,
        "assertions": assertions,
    }


_FUNC_COMPLETION_PROMPT = "# Python 3{definition}"
_FUNC_COMPLETION_STOP = ["\nclass", "\ndef", "\nif", "\nprint"]
_IMPLEMENT_CONFIGS = [
    {"model": FAST_MODEL, "prompt": _FUNC_COMPLETION_PROMPT, "temperature": 0, "cache_seed": 0},
    {"model": FAST_MODEL, "prompt": _FUNC_COMPLETION_PROMPT, "stop": _FUNC_COMPLETION_STOP, "n": 7, "cache_seed": 0},
    {"model": DEFAULT_MODEL, "prompt": _FUNC_COMPLETION_PROMPT, "temperature": 0, "cache_seed": 1},
    {"model": DEFAULT_MODEL, "prompt": _FUNC_COMPLETION_PROMPT, "stop": _FUNC_COMPLETION_STOP, "n": 2, "cache_seed": 2},
    {"model": DEFAULT_MODEL, "prompt": _FUNC_COMPLETION_PROMPT, "stop": _FUNC_COMPLETION_STOP, "n": 1, "cache_seed": 2},
]


def create_virtual_env(dir_path: str, **env_args) -> SimpleNamespace:
    """Creates a python virtual environment and returns the context.

    Args:
        dir_path (str): Directory path where the env will be created.
        **env_args: Any extra args to pass to the `EnvBuilder`

    Returns:
        SimpleNamespace: the virtual env context object.
    """
    if not env_args:
        env_args = {"with_pip": True}
    env_builder = venv.EnvBuilder(**env_args)
    env_builder.create(dir_path)
    return env_builder.ensure_directories(dir_path)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

__all__ = ["export_module"]

from collections.abc import Callable
from typing import TypeVar

T = TypeVar("T")

# Global dictionary to store export module mappings
# Key: original symbol name (qualified by module)
# Value: target module where it should be documented
_PDOC_MODULE_EXPORT_MAPPINGS: dict[str, str] = {}


def export_module(module: str) -> Callable[[T], T]:
    def decorator(cls: T) -> T:
        original_module = getattr(cls, "__module__", None)
        if original_module:
            fqn = f"{original_module}.{cls.__name__}"
            _PDOC_MODULE_EXPORT_MAPPINGS[fqn] = module
        return cls

    return decorator


def get_target_module(obj: object) -> str | None:
    """Get the target module where an object should be documented."""
    if not hasattr(obj, "__module__"):
        return None

    fqn = f"{obj.__module__}.{obj.__name__}"
    return _PDOC_MODULE_EXPORT_MAPPINGS.get(fqn)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from logging import getLogger

from .tools import get_function_schema, load_basemodels_if_needed, serialize_to_str

__all__ = ["get_function_schema", "load_basemodels_if_needed", "serialize_to_str"]

logger = getLogger(__name__)

logger.info("Importing from 'autogen.function_utils' is deprecated, import from 'autogen.tools' instead.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import argparse
import concurrent.futures
import functools
import json
import os
import shutil
import signal
import subprocess
import sys
import tempfile
import threading
import time
from collections.abc import Callable
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import Any, TypeVar

from ..import_utils import optional_import_block, require_optional_import

with optional_import_block():
    import nbformat
    from nbclient.client import NotebookClient
    from nbclient.exceptions import (
        CellExecutionError,
        CellTimeoutError,
    )
    from nbformat import NotebookNode
    from termcolor import colored


# Notebook execution based on nbmake: https://github.com/treebeardtech/nbmakes
@dataclass
class NotebookError:
    error_name: str
    error_value: str | None
    traceback: str
    cell_source: str


@dataclass
class NotebookSkip:
    reason: str


NB_VERSION = 4


class Result:
    def __init__(self, returncode: int, stdout: str, stderr: str):
        self.returncode = returncode
        self.stdout = stdout
        self.stderr = stderr


def path(path_str: str) -> Path:
    """Return a Path object."""
    return Path(path_str)


@lru_cache
def check_quarto_bin(quarto_bin: str = "quarto") -> bool:
    """Check if quarto is installed."""
    try:
        version_str = subprocess.check_output([quarto_bin, "--version"], text=True).strip()
        version = tuple(map(int, version_str.split(".")))
        return version >= (1, 5, 23)

    except FileNotFoundError:
        return False


C = TypeVar("C", bound=Callable[..., Any])


def require_quarto_bin(f: C) -> C:
    """Decorator to skip a function if quarto is not installed."""
    if check_quarto_bin():
        return f
    else:

        @functools.wraps(f)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            return ImportError("Quarto is not installed")

        return wrapper  # type: ignore[return-value]


def load_metadata(notebook: Path) -> dict[str, dict[str, str | list[str] | None]]:
    content = json.load(notebook.open(encoding="utf-8"))
    metadata: dict[str, dict[str, str | list[str] | None]] = content.get("metadata", {})
    return metadata


def skip_reason_or_none_if_ok(notebook: Path) -> str | None | dict[str, Any]:
    """Return a reason to skip the notebook, or None if it should not be skipped."""
    if notebook.suffix != ".ipynb":
        return "not a notebook"

    if not notebook.exists():
        return "file does not exist"

    # Extra checks for notebooks in the notebook directory
    if "notebook" not in notebook.parts:
        return None

    with open(notebook, encoding="utf-8") as f:
        content = f.read()

    # Load the json and get the first cell
    json_content = json.loads(content)
    first_cell = json_content["cells"][0]

    # <!-- and --> must exists on lines on their own
    if first_cell["cell_type"] == "markdown" and first_cell["source"][0].strip() == "<!--":
        raise ValueError(
            f"Error in {notebook.resolve()!s} - Front matter should be defined in the notebook metadata now."
        )

    metadata = load_metadata(notebook)

    if "skip_render" in metadata:
        return metadata["skip_render"]

    if "front_matter" not in metadata:
        return "front matter missing from notebook metadata "

    front_matter = metadata["front_matter"]

    if "tags" not in front_matter:
        return "tags is not in front matter"

    if "description" not in front_matter:
        return "description is not in front matter"

    # Make sure tags is a list of strings
    if front_matter["tags"] is not None and not all(isinstance(tag, str) for tag in front_matter["tags"]):
        return "tags must be a list of strings"

    # Make sure description is a string
    if not isinstance(front_matter["description"], str):
        return "description must be a string"

    return None


def extract_title(notebook: Path) -> str | None:
    """Extract the title of the notebook."""
    with open(notebook, encoding="utf-8") as f:
        content = f.read()

    # Load the json and get the first cell
    json_content = json.loads(content)
    first_cell = json_content["cells"][0]

    # find the # title
    for line in first_cell["source"]:
        if line.startswith("# "):
            title: str = line[2:].strip()
            # Strip off the { if it exists
            if "{" in title:
                title = title[: title.find("{")].strip()
            return title

    return None


def start_thread_to_terminate_when_parent_process_dies(ppid: int) -> None:
    pid = os.getpid()

    def f() -> None:
        while True:
            try:
                os.kill(ppid, 0)
            except OSError:
                os.kill(pid, signal.SIGTERM)
            time.sleep(1)

    thread = threading.Thread(target=f, daemon=True)
    thread.start()


@require_optional_import("termcolor", "docs")
def fmt_skip(notebook: Path, reason: str) -> str:
    return f"{colored('[Skip]', 'yellow')} {colored(notebook.name, 'blue')}: {reason}"


@require_optional_import("termcolor", "docs")
def fmt_ok(notebook: Path) -> str:
    return f"{colored('[OK]', 'green')} {colored(notebook.name, 'blue')} "


@require_optional_import("termcolor", "docs")
def fmt_error(notebook: Path, error: NotebookError | str) -> str:
    if isinstance(error, str):
        return f"{colored('[Error]', 'red')} {colored(notebook.name, 'blue')}: {error}"
    elif isinstance(error, NotebookError):
        return f"{colored('[Error]', 'red')} {colored(notebook.name, 'blue')}: {error.error_name} - {error.error_value}"
    else:
        raise ValueError("error must be a string or a NotebookError")


@require_quarto_bin
@require_optional_import("nbclient", "docs")
def test_notebook(notebook_path: Path, timeout: int = 300) -> tuple[Path, NotebookError | NotebookSkip | None]:
    nb = nbformat.read(str(notebook_path), NB_VERSION)  # type: ignore[arg-type,no-untyped-call]

    if "skip_test" in nb.metadata:
        return notebook_path, NotebookSkip(reason=nb.metadata.skip_test)

    try:
        c = NotebookClient(
            nb,
            timeout=timeout,
            allow_errors=False,
            record_timing=True,
        )
        os.environ["PYDEVD_DISABLE_FILE_VALIDATION"] = "1"
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        with tempfile.TemporaryDirectory() as tempdir:
            c.execute(cwd=tempdir)
    except CellExecutionError:
        error = get_error_info(nb)
        assert error is not None
        return notebook_path, error
    except CellTimeoutError:
        error = get_timeout_info(nb)
        assert error is not None
        return notebook_path, error

    return notebook_path, None


# Find the first code cell which did not complete.
@require_optional_import("nbclient", "docs")
def get_timeout_info(
    nb: NotebookNode,
) -> NotebookError | None:
    for i, cell in enumerate(nb.cells):
        if cell.cell_type != "code":
            continue
        if "shell.execute_reply" not in cell.metadata.execution:
            return NotebookError(
                error_name="timeout",
                error_value="",
                traceback="",
                cell_source="".join(cell["source"]),
            )

    return None


@require_optional_import("nbclient", "docs")
def get_error_info(nb: NotebookNode) -> NotebookError | None:
    for cell in nb["cells"]:  # get LAST error
        if cell["cell_type"] != "code":
            continue
        errors = [output for output in cell["outputs"] if output["output_type"] == "error" or "ename" in output]

        if errors:
            traceback = "\n".join(errors[0].get("traceback", ""))
            return NotebookError(
                error_name=errors[0].get("ename", ""),
                error_value=errors[0].get("evalue", ""),
                traceback=traceback,
                cell_source="".join(cell["source"]),
            )
    return None


def collect_notebooks(notebook_directory: Path, website_build_directory: Path) -> list[Path]:
    notebooks = list(notebook_directory.glob("*.ipynb"))
    notebooks.extend(list(website_build_directory.glob("docs/**/*.ipynb")))
    return notebooks


@require_quarto_bin
@require_optional_import(["nbclient", "termcolor"], "docs")
def process_notebook(
    src_notebook: Path,
    website_build_directory: Path,
    notebook_dir: Path,
    quarto_bin: str,
    dry_run: bool,
    target_dir_func: Callable[[Path], Path],
    post_processor: Callable[[Path, Path, dict[str, Any], Path], None] | None = None,
) -> str:
    """Process a single notebook.

    Args:
        src_notebook: Source notebook path
        website_build_directory: Output directory
        notebook_dir: Base notebooks directory
        quarto_bin: Path to quarto binary
        dry_run: If True, don't actually process
        target_dir_func: Function to determine target directory for notebooks
        post_processor: Optional callback for post-processing
    """
    in_notebook_dir = "notebook" in src_notebook.parts

    metadata = load_metadata(src_notebook)

    title = extract_title(src_notebook)
    if title is None:
        return fmt_error(src_notebook, "Title not found in notebook")

    front_matter = {}
    if "front_matter" in metadata:
        front_matter = metadata["front_matter"]

    front_matter["title"] = title

    if in_notebook_dir:
        relative_notebook = src_notebook.resolve().relative_to(notebook_dir.resolve())
        dest_dir = target_dir_func(website_build_directory)
        target_file = dest_dir / relative_notebook.with_suffix(".mdx")
        intermediate_notebook = dest_dir / relative_notebook

        # If the intermediate_notebook already exists, check if it is newer than the source file
        if target_file.exists() and target_file.stat().st_mtime > src_notebook.stat().st_mtime:
            return fmt_skip(src_notebook, f"target file ({target_file.name}) is newer ")

        if dry_run:
            return colored(f"Would process {src_notebook.name}", "green")

        # Copy notebook to target dir
        # The reason we copy the notebook is that quarto does not support rendering from a different directory
        shutil.copy(src_notebook, intermediate_notebook)

        # Check if another file has to be copied too
        # Solely added for the purpose of agent_library_example.json
        if "extra_files_to_copy" in metadata:
            for file in metadata["extra_files_to_copy"]:
                shutil.copy(src_notebook.parent / file, dest_dir / file)

        # Capture output
        result = subprocess.run([quarto_bin, "render", intermediate_notebook], capture_output=True, text=True)
        if result.returncode != 0:
            return fmt_error(
                src_notebook, f"Failed to render {src_notebook}\n\nstderr:\n{result.stderr}\nstdout:\n{result.stdout}"
            )

        # Unlink intermediate files
        intermediate_notebook.unlink()
    else:
        target_file = src_notebook.with_suffix(".mdx")

        # If the intermediate_notebook already exists, check if it is newer than the source file
        if target_file.exists() and target_file.stat().st_mtime > src_notebook.stat().st_mtime:
            return fmt_skip(src_notebook, f"target file ({target_file.name}) is newer ")

        if dry_run:
            return colored(f"Would process {src_notebook.name}", "green")

        result = subprocess.run([quarto_bin, "render", src_notebook], capture_output=True, text=True)
        if result.returncode != 0:
            return fmt_error(
                src_notebook, f"Failed to render {src_notebook}\n\nstderr:\n{result.stderr}\nstdout:\n{result.stdout}"
            )

    # Use post-processor if provided
    if post_processor and not dry_run:
        post_processor(target_file, src_notebook, front_matter, website_build_directory)

    return fmt_ok(src_notebook)


def create_base_argument_parser() -> argparse.ArgumentParser:
    """Create the base argument parser with common options."""
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="subcommand")

    parser.add_argument(
        "--notebook-directory",
        type=path,
        help="Directory containing notebooks to process",
    )
    parser.add_argument("--website-build-directory", type=path, help="Root directory of website build")
    parser.add_argument("--force", help="Force re-rendering of all notebooks", action="store_true", default=False)

    render_parser = subparsers.add_parser("render")
    render_parser.add_argument("--quarto-bin", help="Path to quarto binary", default="quarto")
    render_parser.add_argument("--dry-run", help="Don't render", action="store_true")
    render_parser.add_argument("notebooks", type=path, nargs="*", default=None)

    test_parser = subparsers.add_parser("test")
    test_parser.add_argument("--timeout", help="Timeout for each notebook", type=int, default=60)
    test_parser.add_argument("--exit-on-first-fail", "-e", help="Exit after first test fail", action="store_true")
    test_parser.add_argument("notebooks", type=path, nargs="*", default=None)
    test_parser.add_argument("--workers", help="Number of workers to use", type=int, default=-1)

    return parser


def process_notebooks_core(
    args: argparse.Namespace,
    post_process_func: Callable[[Path, Path, dict[str, Any], Path], None] | None,
    target_dir_func: Callable[[Path], Path],
) -> list[Path]:
    """Core logic for processing notebooks shared across build systems.

    Args:
        args: Command line arguments
        post_process_func: Function for post-processing rendered notebooks
        target_dir_func: Function to determine target directory for notebooks
    """
    collected_notebooks = (
        args.notebooks if args.notebooks else collect_notebooks(args.notebook_directory, args.website_build_directory)
    )

    filtered_notebooks = []
    for notebook in collected_notebooks:
        reason = skip_reason_or_none_if_ok(notebook)
        if reason and isinstance(reason, str):
            print(fmt_skip(notebook, reason))
        else:
            filtered_notebooks.append(notebook)

    if args.subcommand == "test":
        if args.workers == -1:
            args.workers = None
        failure = False
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=args.workers,
            initializer=start_thread_to_terminate_when_parent_process_dies,
            initargs=(os.getpid(),),
        ) as executor:
            futures = [executor.submit(test_notebook, f, args.timeout) for f in filtered_notebooks]
            for future in concurrent.futures.as_completed(futures):
                notebook, optional_error_or_skip = future.result()
                if isinstance(optional_error_or_skip, NotebookError):
                    if optional_error_or_skip.error_name == "timeout":
                        print(fmt_error(notebook, optional_error_or_skip.error_name))
                    else:
                        print("-" * 80)
                        print(fmt_error(notebook, optional_error_or_skip))
                        print(optional_error_or_skip.traceback)
                        print("-" * 80)
                    if args.exit_on_first_fail:
                        sys.exit(1)
                    failure = True
                elif isinstance(optional_error_or_skip, NotebookSkip):
                    print(fmt_skip(notebook, optional_error_or_skip.reason))
                else:
                    print(fmt_ok(notebook))

        if failure:
            sys.exit(1)

    elif args.subcommand == "render":
        check_quarto_bin(args.quarto_bin)

        target_dir = target_dir_func(args.website_build_directory)
        if not target_dir.exists():
            target_dir.mkdir(parents=True)

        for notebook in filtered_notebooks:
            print(
                process_notebook(
                    notebook,
                    args.website_build_directory,
                    args.notebook_directory,
                    args.quarto_bin,
                    args.dry_run,
                    target_dir_func,
                    post_process_func,
                )
            )

    return filtered_notebooks
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
# !/usr/bin/env python

from __future__ import annotations

import json
import re
import shutil
import sys
from collections.abc import Sequence
from copy import deepcopy
from pathlib import Path

from ..import_utils import optional_import_block, require_optional_import
from .notebook_processor import (
    create_base_argument_parser,
    process_notebooks_core,
)
from .utils import (
    NavigationGroup,
    add_authors_and_social_preview,
    ensure_edit_url,
    get_authors_info,
    remove_marker_blocks,
    sort_files_by_date,
)

with optional_import_block():
    import yaml
    from jinja2 import Template


def notebooks_target_dir(website_build_directory: Path) -> Path:
    """Return the target directory for notebooks."""
    return website_build_directory / "docs" / "use-cases" / "notebooks" / "notebooks"


def add_front_matter_to_metadata_mdx(
    front_matter: dict[str, str | list[str] | None], website_build_directory: Path, rendered_mdx: Path
) -> None:
    source = front_matter.get("source_notebook")
    if isinstance(source, str) and source.startswith("/website/docs/"):
        return

    metadata_mdx = website_build_directory / "snippets" / "data" / "NotebooksMetadata.mdx"

    if not metadata_mdx.exists():
        with open(metadata_mdx, "w", encoding="utf-8") as f:
            f.write(
                "{/*\nAuto-generated file - DO NOT EDIT\nPlease edit the add_front_matter_to_metadata_mdx function in process_notebooks.py\n*/}\n\n"
            )
            f.write("export const notebooksMetadata = [];\n")

    metadata = []
    with open(metadata_mdx, encoding="utf-8") as f:
        content = f.read()
        if content:
            start = content.find("export const notebooksMetadata = [")
            end = content.rfind("]")
            if start != -1 and end != -1:
                metadata = json.loads(content[start + 32 : end + 1])

    # Create new entry for current notebook
    entry = {
        "title": front_matter.get("title", ""),
        "link": f"/docs/use-cases/notebooks/notebooks/{rendered_mdx.stem}",
        "description": front_matter.get("description", ""),
        "image": front_matter.get("image"),
        "tags": front_matter.get("tags", []),
        "source": source,
    }
    # Update metadata list
    existing_entry = next((item for item in metadata if item["link"] == entry["link"]), None)
    if existing_entry:
        metadata[metadata.index(existing_entry)] = entry
    else:
        metadata.append(entry)

    # Write metadata back to file
    with open(metadata_mdx, "w", encoding="utf-8") as f:
        f.write(
            "{/*\nAuto-generated file - DO NOT EDIT\nPlease edit the add_front_matter_to_metadata_mdx function in process_notebooks.py\n*/}\n\n"
        )
        f.write("export const notebooksMetadata = ")
        f.write(json.dumps(metadata, indent=4))
        f.write(";\n")


def convert_callout_blocks(content: str) -> str:
    """Converts callout blocks in the following formats:
    1) Plain callout blocks using ::: syntax.
    2) Blocks using 3-4 backticks + (mdx-code-block or {=mdx}) + ::: syntax.
    Transforms them into custom HTML/component syntax.
    """
    callout_types = {
        "tip": "Tip",
        "note": "Note",
        "warning": "Warning",
        "info": "Info",
        "info Requirements": "Info",
        "check": "Check",
        "danger": "Warning",
        "tabs": "Tabs",
    }

    # Regex explanation (using alternation):
    #
    # -- Alternative #1: Backticks + mdx-code-block/{=mdx} --
    #
    #   ^(?P<backticks>`{3,4})(?:mdx-code-block|\{=mdx\})[ \t]*\n
    #     - Matches opening backticks and optional mdx markers.
    #   :::(?P<callout_type_backtick>...)
    #     - Captures the callout type.
    #   (.*?)
    #     - Captures the content inside the callout.
    #   ^:::[ \t]*\n
    #     - Matches the closing ::: line.
    #   (?P=backticks)
    #     - Ensures the same number of backticks close the block.
    #
    # -- Alternative #2: Plain ::: callout --
    #
    #   ^:::(?P<callout_type_no_backtick>...)
    #     - Captures the callout type after :::.
    #   (.*?)
    #     - Captures the content inside the callout.
    #   ^:::
    #     - Matches the closing ::: line.
    #
    #  (?s)(?m): DOTALL + MULTILINE flags.
    #    - DOTALL (`.` matches everything, including newlines).
    #    - MULTILINE (`^` and `$` work at the start/end of each line).

    pattern = re.compile(
        r"(?s)(?m)"
        r"(?:"
        # Alternative #1: Backticks + mdx-code-block/{=mdx}
        r"^(?P<backticks>`{3,4})(?:mdx-code-block|\{=mdx\})[ \t]*\n"
        r":::(?P<callout_type_backtick>\w+(?:\s+\w+)?)[ \t]*\n"
        r"(?P<inner_backtick>.*?)"
        r"^:::[ \t]*\n"
        r"(?P=backticks)"  # Closing backticks must match the opening count.
        r")"
        r"|"
        # Alternative #2: Plain ::: callout
        r"(?:"
        r"^:::(?P<callout_type_no_backtick>\w+(?:\s+\w+)?)[ \t]*\n"
        r"(?P<inner_no_backtick>.*?)"
        r"^:::[ \t]*(?:\n|$)"
        r")"
    )

    def replace_callout(m: re.Match[str]) -> str:
        # Determine the matched alternative and extract the corresponding groups.
        ctype = m.group("callout_type_backtick") or m.group("callout_type_no_backtick")
        inner = m.group("inner_backtick") or m.group("inner_no_backtick") or ""

        # Map the callout type to its standard representation or fallback to the original type.
        mapped_type = callout_types.get(ctype, ctype)

        # Return the formatted HTML block.
        return f"""
<div class="{ctype}">
<{mapped_type}>
{inner.strip()}
</{mapped_type}>
</div>
"""

    # Apply the regex pattern and replace matched callouts with the custom HTML structure.
    return pattern.sub(replace_callout, content)


def convert_mdx_image_blocks(content: str, rendered_mdx: Path, website_build_directory: Path) -> str:
    """Converts MDX code block image syntax to regular markdown image syntax.

    Args:
        content (str): The markdown content containing mdx-code-block image syntax
        rendered_mdx (Path): The path to the rendered mdx file
        website_build_directory (Path): The path to the website build directory

    Returns:
        str: The converted markdown content with standard image syntax
    """

    def resolve_path(match: re.Match[str]) -> str:
        img_pattern = r"!\[(.*?)\]\((.*?)\)"
        img_match = re.search(img_pattern, match.group(1))
        if not img_match:
            return match.group(0)

        alt, rel_path = img_match.groups()
        abs_path = (rendered_mdx.parent / Path(rel_path)).resolve().relative_to(website_build_directory)
        return f"![{alt}](/{abs_path})"

    pattern = r"````mdx-code-block\n(!\[.*?\]\(.*?\))\n````"
    return re.sub(pattern, resolve_path, content)


def extract_img_tag_from_figure_tag(content: str, img_rel_path: Path) -> str:
    """Extracts the img tag from the figure tag and modifies local image path.

    Quarto converts the markdown images syntax to <figure> tag while rendering and converting the notebook to mdx file.
    Mintlify could not able to process the <figure> tags properly and does not shows these images in the documentation.
    As a fix, the <img> tag present inside the <figure> is extracted and saved to the mdx file.

    Args:
        content (str): Content of the file
        img_rel_path (Path): Relative path to the image directory

    Returns:
        str: Content of the file with <img> tag extracted from <figure> tag
    """

    def replace_local_path(match: re.Match[str]) -> str:
        img_tag = match.group(1)
        # Find src attribute value
        src_match = re.search(r'src="([^"]+)"', img_tag)
        if src_match:
            src = src_match.group(1)
            # If src doesn't start with http/https, it's a local image
            if not src.startswith(("http://", "https://")):
                # Replace old src with new prefixed path
                img_tag = img_tag.replace(f'src="{src}"', f'src="/{str(img_rel_path.as_posix())}/{src}"')
        return img_tag

    pattern = r"<figure>\s*(<img\s+.*?/>)\s*<figcaption[^>]*>.*?</figcaption>\s*</figure>"
    content = re.sub(pattern, replace_local_path, content, flags=re.DOTALL)
    return content


# rendered_notebook is the final mdx file
@require_optional_import("yaml", "docs")
def post_process_mdx(
    rendered_mdx: Path,
    source_notebooks: Path,
    front_matter: dict[str, str | list[str] | None],
    website_build_directory: Path,
) -> None:
    with open(rendered_mdx, encoding="utf-8") as f:
        content = f.read()

    # If there is front matter in the mdx file, we need to remove it
    if content.startswith("---"):
        front_matter_end = content.find("---", 3)
        mdx_front_matter = yaml.safe_load(content[4:front_matter_end])
        # Merge while preserving original values
        front_matter = {**front_matter, **mdx_front_matter}
        content = content[front_matter_end + 3 :]

    # Clean heading IDs using regex - matches from # to the end of ID block
    content = re.sub(r"(#{1,6}[^{]+){#[^}]+}", r"\1", content)

    # Each intermediate path needs to be resolved for this to work reliably
    repo_root = Path(__file__).resolve().parents[2]
    repo_relative_notebook = source_notebooks.resolve().relative_to(repo_root)
    front_matter["source_notebook"] = f"/{repo_relative_notebook}"
    front_matter["custom_edit_url"] = f"https://github.com/ag2ai/ag2/edit/main/{repo_relative_notebook}"

    # Is there a title on the content? Only search up until the first code cell
    # first_code_cell = content.find("```")
    # if first_code_cell != -1:
    #     title_search_content = content[:first_code_cell]
    # else:
    #     title_search_content = content

    # title_exists = title_search_content.find("\n# ") != -1
    # if not title_exists:
    #     content = f"# {front_matter['title']}\n{content}"
    # inject in content directly after the markdown title the word done
    # Find the end of the line with the title
    # title_end = content.find("\n", content.find("#"))

    # Extract page title
    # title = content[content.find("#") + 1 : content.find("\n", content.find("#"))].strip()
    # If there is a { in the title we trim off the { and everything after it
    # if "{" in title:
    #     title = title[: title.find("{")].strip()

    github_link = f"https://github.com/ag2ai/ag2/blob/main/{repo_relative_notebook}"
    content = (
        f'\n<a href="{github_link}" class="github-badge" target="_blank">'
        + """<img noZoom src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github" alt="Open on GitHub" />"""
        + "</a>"
        + content
    )

    # If no colab link is present, insert one
    if "colab-badge.svg" not in content:
        colab_link = f"https://colab.research.google.com/github/ag2ai/ag2/blob/main/{repo_relative_notebook}"
        content = (
            f'\n<a href="{colab_link}" class="colab-badge" target="_blank">'
            + """<img noZoom src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />"""
            + "</a>"
            + content
        )

    # Create the front matter metadata js file for examples by notebook section
    add_front_matter_to_metadata_mdx(front_matter, website_build_directory, rendered_mdx)

    # Dump front_matter to yaml
    front_matter_str = yaml.dump(front_matter, default_flow_style=False)

    # Convert callout blocks
    content = convert_callout_blocks(content)

    # Convert mdx image syntax to mintlify image syntax
    content = convert_mdx_image_blocks(content, rendered_mdx, website_build_directory)

    # ensure editUrl is present
    content = ensure_edit_url(content, repo_relative_notebook)

    # convert figure tag to img tag
    img_rel_path = rendered_mdx.parent.relative_to(website_build_directory)
    content = extract_img_tag_from_figure_tag(content, img_rel_path)

    # Rewrite the content as
    # ---
    # front_matter_str
    # ---
    # content
    new_content = f"---\n{front_matter_str}---\n{content}"
    with open(rendered_mdx, "w", encoding="utf-8") as f:
        f.write(new_content)


def get_sorted_files(input_dir: Path, prefix: str) -> list[str]:
    """Get sorted list of files with prefix prepended."""
    if not input_dir.exists():
        raise FileNotFoundError(f"Directory not found: {input_dir}")

    files = sorted(input_dir.glob("**/index.mdx"), key=sort_files_by_date)
    reversed_files = files[::-1]

    return [f"{prefix}/{f.parent.relative_to(input_dir)}/index".replace("\\", "/") for f in reversed_files]


def generate_nav_group(input_dir: Path, group_header: str, prefix: str) -> dict[str, str | list[str]]:
    """Generate navigation group for a directory.

    Args:
        input_dir (Path): Directory to process
        group_header (str): Group header
        prefix (str): Prefix to prepend to file paths
    """
    sorted_dir_files = get_sorted_files(input_dir, prefix)

    return {"group": group_header, "pages": sorted_dir_files}


def extract_example_group(metadata_path: Path) -> list[str]:
    # Read NotebooksMetadata.mdx and extract metadata links
    with open(metadata_path, encoding="utf-8") as f:
        content = f.read()
        # Extract the array between the brackets
        start = content.find("export const notebooksMetadata = [")
        end = content.rfind("]")
        if start == -1 or end == -1:
            print("Could not find notebooksMetadata in the file")
            return []
        metadata_str = content[start + 32 : end + 1]
        notebooks_metadata = json.loads(metadata_str)

    # Create notebooks entry
    notebooks = ["docs/use-cases/notebooks/Notebooks"] + [
        Path(item["source"])
        .with_suffix("")
        .as_posix()
        .replace("/website/", "/")
        .replace("/notebook/", "docs/use-cases/notebooks/notebooks/")
        for item in notebooks_metadata
        if not item["source"].startswith("/website/build/docs/")
    ]

    return notebooks


def update_group_pages(
    mint_navigation: list[dict[str, str | list[str | dict[str, str | list[str]]]]],
    target_group: str,
    new_value: Sequence[str | dict[str, str | Sequence[str]]],
) -> list[dict[str, str | list[str | dict[str, str | list[str]]]]]:
    """Update mint.json navigation group with new pages."""
    nav_copy = deepcopy(mint_navigation)

    def update_recursively(
        items: list[dict[str, str | list[str | dict[str, str | list[str]]]]],
    ) -> None:
        for item in items:
            if isinstance(item, dict):
                if item.get("group") == target_group:
                    item["pages"] = new_value.copy()  # type: ignore [attr-defined]
                    return
                if isinstance(item.get("pages"), list):
                    update_recursively(item["pages"])  # type: ignore [arg-type]
            elif isinstance(item, list):
                update_recursively(item)

    update_recursively(nav_copy)
    return nav_copy


def add_notebooks_blogs_and_user_stories_to_nav(website_build_directory: Path) -> None:
    """Updates mint.json navigation to include notebooks, blogs, and user stories.

    Args:
        website_build_directory (Path): Build directory of the website
    """
    mint_json_path = (website_build_directory / "mint.json").resolve()
    metadata_path = (website_build_directory / "snippets" / "data" / "NotebooksMetadata.mdx").resolve()

    if not mint_json_path.exists():
        print(f"mint.json not found at {mint_json_path}")
        return

    if not metadata_path.exists():
        print(f"NotebooksMetadata.mdx not found at {metadata_path}")
        return

    # Read mint.json
    with open(mint_json_path, encoding="utf-8") as f:
        mint_config = json.load(f)

    # add talks to navigation
    # talks_dir = website_build_directory / "talks"
    # talks_section = generate_nav_group(talks_dir, "Talks", "talks")
    # talks_section_pages = (
    #     [talks_section["pages"]] if isinstance(talks_section["pages"], str) else talks_section["pages"]
    # )

    # Add "talks/future_talks/index" item at the beginning of the list
    # future_talks_index = talks_section_pages.pop()
    # talks_section_pages.insert(0, future_talks_index)
    # mint_config["navigation"].append(talks_section)

    # add user_stories to navigation
    user_stories_dir = website_build_directory / "docs" / "user-stories"
    user_stories_section = {
        "group": "User Stories",
        "pages": [generate_nav_group(user_stories_dir, "User Stories", "docs/user-stories")],
    }
    mint_config["navigation"].append(user_stories_section)

    # add blogs to navigation
    blogs_dir = website_build_directory / "docs" / "_blogs"
    blog_section = {"group": "Blog", "pages": [generate_nav_group(blogs_dir, "Recent posts", "docs/blog")]}
    mint_config["navigation"].append(blog_section)

    # Add examples to navigation
    notebooks_navigation = extract_example_group(metadata_path)
    updated_navigation = update_group_pages(mint_config["navigation"], "Notebooks", notebooks_navigation)
    mint_config["navigation"] = updated_navigation

    # Write back to mint.json
    with open(mint_json_path, "w", encoding="utf-8") as f:
        json.dump(mint_config, f, indent=2)
        f.write("\n")

    print(f"Updated navigation in {mint_json_path}")


def fix_internal_references(content: str, root_path: Path, current_file_path: Path) -> str:
    """Resolves internal markdown references relative to root_dir and returns fixed content.

    Args:
        content: Markdown content to fix
        root_path: Root directory for resolving paths
        current_file_path: Path of the current file being processed
    """

    def resolve_link(match: re.Match[str]) -> str:
        display_text, raw_path = match.groups()
        try:
            path_parts = raw_path.split("#")
            rel_path = path_parts[0]
            anchor = f"#{path_parts[1]}" if len(path_parts) > 1 else ""

            resolved = (current_file_path.parent / rel_path).resolve()
            final_path = (resolved.relative_to(root_path.resolve())).with_suffix("")

            return f"[{display_text}](/{final_path}{anchor})"
        except Exception:
            return match.group(0)

    pattern = r"\[([^\]]+)\]\(((?:\.\./|\./)?\w+(?:/[\w-]+)*\.md(?:#[\w-]+)?)\)"
    return re.sub(pattern, resolve_link, content)


def fix_internal_references_in_mdx_files(website_build_directory: Path) -> None:
    """Process all MDX files in directory to fix internal references."""
    for file_path in website_build_directory.glob("**/*.mdx"):
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            fixed_content = fix_internal_references(content, website_build_directory, file_path)

            if content != fixed_content:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(fixed_content)

        except Exception:
            print(f"Error: {file_path}")
            sys.exit(1)


def add_authors_and_social_img_to_blog_and_user_stories(website_build_directory: Path) -> None:
    """Add authors info to blog posts and user stories.

    Args:
        website_build_directory (Path): Build directory of the website
    """
    blog_dir = website_build_directory / "docs" / "_blogs"
    generated_blog_dir = website_build_directory / "docs" / "blog"

    authors_yml = website_build_directory / "blogs_and_user_stories_authors.yml"
    all_authors_info = get_authors_info(authors_yml)

    # Remove existing generated directory if it exists
    if generated_blog_dir.exists():
        shutil.rmtree(generated_blog_dir)

    # Copy entire blog directory structure to generated_blog
    shutil.copytree(blog_dir, generated_blog_dir)
    add_authors_and_social_preview(website_build_directory, generated_blog_dir, all_authors_info)

    user_stories_dir = website_build_directory / "docs" / "user-stories"
    add_authors_and_social_preview(website_build_directory, user_stories_dir, all_authors_info)


def ensure_mint_json_exists(website_build_directory: Path) -> None:
    mint_json_path = website_build_directory / "mint.json"
    if not mint_json_path.exists():
        print(f"mint.json not found at {mint_json_path}")
        print(
            "You can either run the 'generate_api_references.py' script before running this script or simply run the scripts/docs_build.sh script which will execute both 'generate_api_references.py' and 'process_notebooks.py' scripts in correct order."
        )
        sys.exit(1)


def cleanup_tmp_dirs(website_build_directory: Path, re_generate_notebooks: bool) -> None:
    """Remove the temporary notebooks directory if NotebooksMetadata.mdx is not found.

    This is to ensure a clean build and generate the metadata file as well as to
    update the navigation with correct entries.
    """
    delete_tmp_dir = re_generate_notebooks
    metadata_mdx = website_build_directory / "snippets" / "data" / "NotebooksMetadata.mdx"
    if not metadata_mdx.exists():
        print(f"NotebooksMetadata.mdx not found at {metadata_mdx}")
        delete_tmp_dir = True

    if delete_tmp_dir:
        notebooks_dir = notebooks_target_dir(website_build_directory)
        print(f"Removing the {notebooks_dir} and to ensure a clean build.")
        shutil.rmtree(notebooks_dir, ignore_errors=True)


def get_files_path_from_navigation(navigation: list[NavigationGroup]) -> list[Path]:
    """Extract all file paths from the navigation structure.

    Args:
        navigation: list of navigation groups containing nested pages and groups

    Returns:
        list of file paths found in the navigation structure
    """
    file_paths = []

    def extract_paths(items: Sequence[str | NavigationGroup]) -> None:
        for item in items:
            if isinstance(item, str):
                file_paths.append(Path(item))
            elif isinstance(item, dict) and "pages" in item:
                extract_paths(item["pages"])

    extract_paths(navigation)
    return file_paths


@require_optional_import("jinja2", "docs")
def add_edit_urls_and_remove_mkdocs_markers(website_build_directory: Path) -> None:
    """Add edit links to the non generated mdx files and remove mkdocs specific markers from the file.

    For the generated mdx files i.e. mdx files of _blogs and notebooks, it is added in their respective post processing functions.
    """
    mint_json_template_path = website_build_directory / "mint-json-template.json.jinja"

    mint_json_template_content = Template(mint_json_template_path.read_text(encoding="utf-8")).render()
    mint_json_data = json.loads(mint_json_template_content)

    mdx_files = get_files_path_from_navigation(mint_json_data["navigation"])
    mdx_files_with_prefix = [Path(f"{website_build_directory}/{str(file_path)}.mdx") for file_path in mdx_files]

    for mdx_file_path in mdx_files_with_prefix:
        rel_path = str(mdx_file_path.relative_to(website_build_directory.parent)).replace("build/", "website/")
        content = mdx_file_path.read_text(encoding="utf-8")
        content_with_edit_url = ensure_edit_url(content, Path(rel_path))

        # Remove mkdocs markers before building the docs
        content_without_mkdocs_marker = remove_marker_blocks(content_with_edit_url, "DELETE-ME-WHILE-BUILDING-MINTLIFY")
        mdx_file_path.write_text(content_without_mkdocs_marker, encoding="utf-8")


def copy_images_from_notebooks_dir_to_target_dir(notebook_directory: Path, target_notebooks_dir: Path) -> None:
    """Copy images from notebooks directory to the target directory."""
    # Define supported image extensions
    supported_img_extensions = {".png", ".jpg"}

    # Single loop through directory contents
    for image_path in notebook_directory.iterdir():
        if image_path.is_file() and image_path.suffix.lower() in supported_img_extensions:
            target_image = target_notebooks_dir / image_path.name
            shutil.copy(image_path, target_image)


def main() -> None:
    root_dir = Path(__file__).resolve().parents[2]
    website_dir = root_dir / "website"
    website_build_dir = website_dir / "build"
    parser = create_base_argument_parser()

    if not website_build_dir.exists():
        website_build_dir.mkdir()
        shutil.copytree(website_dir, website_build_dir, dirs_exist_ok=True)

    args = parser.parse_args()
    if args.subcommand is None:
        print("No subcommand specified")
        sys.exit(1)

    if args.website_build_directory is None:
        args.website_build_directory = website_build_dir

    if args.notebook_directory is None:
        args.notebook_directory = website_build_dir / "../../notebook"

    ensure_mint_json_exists(args.website_build_directory)
    cleanup_tmp_dirs(args.website_build_directory, args.force)

    # Process notebooks using core logic
    process_notebooks_core(args, post_process_mdx, notebooks_target_dir)

    # Post-processing steps after all notebooks are handled
    if not args.dry_run:
        target_notebooks_dir = notebooks_target_dir(args.website_build_directory)
        copy_images_from_notebooks_dir_to_target_dir(args.notebook_directory, target_notebooks_dir)
        add_notebooks_blogs_and_user_stories_to_nav(args.website_build_directory)
        fix_internal_references_in_mdx_files(args.website_build_directory)
        add_authors_and_social_img_to_blog_and_user_stories(args.website_build_directory)
        add_edit_urls_and_remove_mkdocs_markers(args.website_build_directory)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

import argparse
import importlib
import json
import os
import pkgutil
import re
import shutil
import sys
from collections.abc import Iterable, Iterator
from pathlib import Path
from types import ModuleType
from typing import Any

from ..doc_utils import get_target_module
from ..import_utils import optional_import_block, require_optional_import
from .utils import copy_only_git_tracked_and_untracked_files

with optional_import_block():
    import pdoc
    from jinja2 import Template


def import_submodules(module_name: str, *, include_root: bool = True) -> list[str]:
    """List all submodules of a given module.

    Args:
        module_name (str): The name of the module to list submodules for.
        include_root (bool, optional): Whether to include the root module in the list. Defaults to True.

    Returns:
        list: A list of submodule names.
    """
    try:
        module = importlib.import_module(module_name)  # nosemgrep
    except Exception:
        return []

    # Get the path of the module. This is necessary to find its submodules.
    module_path = module.__path__

    # Initialize an empty list to store the names of submodules
    submodules = [module_name] if include_root else []

    # Iterate over the submodules in the module's path
    for _, name, ispkg in pkgutil.iter_modules(module_path, prefix=f"{module_name}."):
        # Add the name of each submodule to the list
        submodules.append(name)

        if ispkg:
            submodules.extend(import_submodules(name, include_root=False))

    # Return the list of submodule names
    return submodules


@require_optional_import("pdoc", "docs")
def build_pdoc_dict(module: ModuleType, module_name: str) -> None:
    if not hasattr(module, "__pdoc__"):
        setattr(module, "__pdoc__", {})

    all = module.__all__ if hasattr(module, "__all__") else None

    for name, obj in module.__dict__.items():
        if all and name not in all:
            continue

        if not hasattr(obj, "__name__") or name.startswith("_"):
            continue

        target_module = get_target_module(obj)
        if target_module and target_module != module_name:
            module.__pdoc__[name] = False


@require_optional_import("pdoc", "docs")
def process_modules(submodules: list[str]) -> None:
    cached_modules: dict[str, ModuleType] = {}

    # Pass 1: Build pdoc dictionary for all submodules
    for submodule in submodules:
        module = importlib.import_module(submodule)  # nosemgrep
        cached_modules[submodule] = module
        build_pdoc_dict(module, submodule)


@require_optional_import("pdoc", "docs")
def generate_markdown(path: Path) -> None:
    modules = ["autogen"]  # Public submodules are auto-imported
    context = pdoc.Context()

    modules = [pdoc.Module(mod, context=context) for mod in modules]
    pdoc.link_inheritance(context)

    def recursive_markdown(mod: pdoc.Module) -> Iterable[tuple[str, str]]:  # type: ignore[no-any-unimported]
        # Pass our custom template here
        yield mod.name, mod.text()
        for submod in mod.submodules():
            yield from recursive_markdown(submod)

    for mod in modules:
        for module_name, text in recursive_markdown(mod):
            file_path = path / module_name.replace(".", "/") / "index.md"
            # print(f"Writing {file_path}...")
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with file_path.open("w") as f:
                f.write(text)


@require_optional_import("pdoc", "docs")
def generate(target_dir: Path, template_dir: Path) -> None:
    # Pass the custom template directory for rendering the markdown
    pdoc.tpl_lookup.directories.insert(0, str(template_dir))

    submodules = import_submodules("autogen")
    # print(f"{submodules=}")

    process_modules(submodules)

    generate_markdown(target_dir)


def fix_api_reference_links(content: str) -> str:
    """Fix the API reference links in the content."""
    # Define a pattern that matches API reference links
    pattern = r"(/docs/api-reference/[^#\)]+#)autogen\.([^\)]+)"

    # Replace with the URL part and everything after the last dot
    def replacement_func(match: re.Match[str]) -> str:
        url_part = match.group(1)
        full_name = match.group(2)

        # Get the function name (everything after the last dot if there is one, or the whole thing)
        func_name = full_name.split(".")[-1] if "." in full_name else full_name
        return f"{url_part}{func_name}"

    # Use re.sub with a replacement function
    return re.sub(pattern, replacement_func, content)


def convert_md_to_mdx(input_dir: Path) -> None:
    """Convert all .md files in directory to .mdx while preserving structure.

    Args:
        input_dir (Path): Directory containing .md files to convert
    """
    if not input_dir.exists():
        print(f"Directory not found: {input_dir}")
        sys.exit(1)

    for md_file in input_dir.rglob("*.md"):
        mdx_file = md_file.with_suffix(".mdx")

        # Read content from .md file
        content = md_file.read_text(encoding="utf-8")

        # Fix internal API references
        content = fix_api_reference_links(content)

        # Write content to .mdx file
        mdx_file.write_text(content, encoding="utf-8")

        # Remove original .md file
        md_file.unlink()
        # print(f"Converted: {md_file} -> {mdx_file}")


def get_mdx_files(directory: Path) -> list[str]:
    """Get all MDX files in directory and subdirectories."""
    return [f"{p.relative_to(directory).with_suffix('')!s}".replace("\\", "/") for p in directory.rglob("*.mdx")]


def add_prefix(path: str, parent_groups: list[str] | None = None) -> str:
    """Create full path with prefix and parent groups."""
    groups = parent_groups or []
    return f"docs/api-reference/{'/'.join(groups + [path])}"


def create_nav_structure(paths: list[str], parent_groups: list[str] | None = None) -> list[Any]:
    """Convert list of file paths into nested navigation structure."""
    groups: dict[str, list[str]] = {}
    pages = []
    parent_groups = parent_groups or []

    for path in paths:
        parts = path.split("/")
        if len(parts) == 1:
            pages.append(add_prefix(path, parent_groups))
        else:
            group = parts[0]
            subpath = "/".join(parts[1:])
            groups.setdefault(group, []).append(subpath)

    # Sort directories and create their structures
    sorted_groups = [
        {
            "group": group,
            "pages": create_nav_structure(subpaths, parent_groups + [group]),
        }
        for group, subpaths in sorted(groups.items())
    ]

    # Sort pages
    overview_page = [page for page in pages if page.endswith("overview")]
    if overview_page:
        pages.remove(overview_page[0])

    sorted_pages = sorted(pages)
    if overview_page:
        sorted_pages.insert(0, overview_page[0])

    # Return directories first, then files
    return sorted_pages + sorted_groups


def update_nav(mint_json_path: Path, new_nav_pages: list[Any]) -> None:
    """Update the 'API Reference' section in mint.json navigation with new pages.

    Args:
        mint_json_path: Path to mint.json file
        new_nav_pages: New navigation structure to replace in API Reference pages
    """
    try:
        # Read the current mint.json
        with open(mint_json_path) as f:
            mint_config = json.load(f)

        reference_section = {"group": "API Reference", "pages": new_nav_pages}
        mint_config["navigation"].append(reference_section)

        # Write back to mint.json with proper formatting
        with open(mint_json_path, "w") as f:
            json.dump(mint_config, f, indent=2)
            f.write("\n")

    except json.JSONDecodeError:
        print(f"Error: {mint_json_path} is not valid JSON")
    except Exception as e:
        print(f"Error updating mint.json: {e}")


def update_mint_json_with_api_nav(website_build_dir: Path, api_dir: Path) -> None:
    """Update mint.json with MDX files in the API directory."""
    mint_json_path = website_build_dir / "mint.json"
    if not mint_json_path.exists():
        print(f"File not found: {mint_json_path}")
        sys.exit(1)

    # Get all MDX files in the API directory
    mdx_files = get_mdx_files(api_dir)

    # Create navigation structure
    nav_structure = create_nav_structure(mdx_files)

    # Update mint.json with new navigation
    update_nav(mint_json_path, nav_structure)


@require_optional_import("jinja2", "docs")
def generate_mint_json_from_template(mint_json_template_path: Path, mint_json_path: Path) -> None:
    # if mint.json already exists, delete it
    if mint_json_path.exists():
        os.remove(mint_json_path)

    # Copy the template file to mint.json
    contents = mint_json_template_path.read_text(encoding="utf-8")
    mint_json_template_content = Template(contents).render()

    # Parse the rendered template content as JSON
    mint_json_data = json.loads(mint_json_template_content)

    # Write content to mint.json
    with open(mint_json_path, "w") as f:
        json.dump(mint_json_data, f, indent=2)


class SplitReferenceFilesBySymbols:
    def __init__(self, api_dir: Path) -> None:
        self.api_dir = api_dir
        self.tmp_dir = Path("tmp")

    def _generate_overview(self, classes: list[str], functions: list[str], output_dir: Path) -> str:
        overview = """---
sidebarTitle: Overview
title: Overview
---
"""
        if classes:
            overview += "\n\n## Classes\n"
            for symbol in sorted(classes):
                href = output_dir / symbol
                overview += f"""<p class="overview-symbol"><a href="/{str(href).replace("tmp/", "docs/api-reference/")}"><code>class {symbol}</code></a></p>"""
            overview += "\n"

        if functions:
            overview += "\n\n## Functions\n"
            for symbol in sorted(functions):
                href = output_dir / symbol
                overview += f"""<p class="overview-symbol"><a href="/{str(href).replace("tmp/", "docs/api-reference/")}"><code>{symbol}</code></a></p>"""
            overview += "\n"

        return overview

    def _extract_symbol_content(self, content: str, output_dir: Path) -> dict[str, str]:
        sections = {}
        class_symbols = []
        function_symbols = []

        for part in content.split("**** SYMBOL_START ****")[1:]:
            symbol = part.split("```python\n")[1].split("(")[0].strip()
            content = part.split("**** SYMBOL_END ****")[0].strip()
            sections[symbol] = content

            if "doc-symbol-class" in content:
                class_symbols.append(symbol)

            if "doc-symbol-function" in content:
                function_symbols.append(symbol)

        sections["overview"] = self._generate_overview(class_symbols, function_symbols, output_dir)
        return sections

    def _split_content_by_symbols(self, content: str, output_dir: Path) -> dict[str, str]:
        symbols = {}
        if "**** SYMBOL_START ****" in content:
            symbols.update(self._extract_symbol_content(content, output_dir))
        return symbols

    def _process_files(self) -> Iterator[tuple[Path, dict[str, str]]]:
        for md_file in self.api_dir.rglob("*.md"):
            output_dir = self.tmp_dir / md_file.relative_to(self.api_dir).parent
            output_dir.mkdir(parents=True, exist_ok=True)

            yield output_dir, self._split_content_by_symbols(md_file.read_text(), output_dir)

    def _clean_directory(self, directory: Path) -> None:
        for item in directory.iterdir():
            if item.is_dir():
                shutil.rmtree(item)
            else:
                item.unlink()

    def _move_generated_files_to_api_dir(self) -> None:
        self._clean_directory(self.api_dir)
        for item in self.tmp_dir.iterdir():
            dest = self.api_dir / item.relative_to(self.tmp_dir)
            dest.parent.mkdir(parents=True, exist_ok=True)
            copy_func = shutil.copytree if item.is_dir() else shutil.copy2
            print(f"Copying {'directory' if item.is_dir() else 'file'} {item} to {dest}")
            copy_func(item, dest)

    def generate(self) -> None:
        try:
            self.tmp_dir.mkdir(exist_ok=True)
            for output_dir, symbols in self._process_files():
                if symbols:
                    for name, content in symbols.items():
                        (output_dir / f"{name}.md").write_text(content, encoding="utf-8")

            self._move_generated_files_to_api_dir()
        finally:
            shutil.rmtree(self.tmp_dir)


def main() -> None:
    root_dir = Path(__file__).resolve().parents[2]
    website_dir = root_dir / "website"
    website_build_dir = website_dir / "build"

    parser = argparse.ArgumentParser(description="Process API reference documentation")
    parser.add_argument(
        "--api-dir",
        type=Path,
        help="Directory containing API documentation to process",
        default=website_build_dir / "docs" / "api-reference",
    )

    parser.add_argument("--force", action="store_true", help="Force generation")

    args = parser.parse_args()

    if args.force:
        shutil.rmtree(website_build_dir, ignore_errors=True)

    if not website_build_dir.exists():
        website_build_dir.mkdir()

    ignore_dir = "mkdocs"
    copy_only_git_tracked_and_untracked_files(website_dir, website_build_dir, ignore_dir)

    if args.api_dir.exists():
        # Force delete the directory and its contents
        shutil.rmtree(args.api_dir, ignore_errors=True)

    target_dir = args.api_dir

    template_dir = website_build_dir / "mako_templates"

    # Generate API reference documentation
    print("Generating API reference documentation...")
    generate(target_dir, template_dir)

    # Split the API reference from submodules into separate files for each symbols
    symbol_files_generator = SplitReferenceFilesBySymbols(target_dir)
    symbol_files_generator.generate()

    # Convert MD to MDX
    print("Converting MD files to MDX...")
    convert_md_to_mdx(args.api_dir)

    # Create mint.json from the template file
    mint_json_template_path = website_build_dir / "mint-json-template.json.jinja"
    mint_json_path = website_build_dir / "mint.json"

    print("Generating mint.json from template...")
    generate_mint_json_from_template(mint_json_template_path, mint_json_path)

    # Update mint.json
    update_mint_json_with_api_nav(website_build_dir, args.api_dir)

    print("API reference processing complete!")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import json
import os
import re
import shutil
from pathlib import Path

from ..import_utils import optional_import_block, require_optional_import
from .notebook_processor import (
    create_base_argument_parser,
    process_notebooks_core,
)
from .utils import (
    NavigationGroup,
    add_authors_and_social_preview,
    copy_files,
    get_authors_info,
    get_git_tracked_and_untracked_files_in_directory,
    remove_marker_blocks,
    render_gallery_html,
    separate_front_matter_and_content,
    sort_files_by_date,
)

with optional_import_block():
    import yaml
    from jinja2 import Template


root_dir = Path(__file__).resolve().parents[2]
website_dir = root_dir / "website"

mint_docs_dir = website_dir / "docs"

mkdocs_root_dir = website_dir / "mkdocs"

mkdocs_docs_dir = mkdocs_root_dir / "docs"
mkdocs_output_dir = mkdocs_root_dir / "docs" / "docs"


def filter_excluded_files(files: list[Path], exclusion_list: list[str], website_dir: Path) -> list[Path]:
    return [
        file
        for file in files
        if not any(Path(str(file.relative_to(website_dir))).as_posix().startswith(excl) for excl in exclusion_list)
    ]


def copy_file(file: Path, mkdocs_output_dir: Path) -> None:
    dest = mkdocs_output_dir / file.relative_to(file.parents[1])
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(file, dest)


def transform_tab_component(content: str) -> str:
    """Transform React-style tab components to MkDocs tab components.

    Args:
        content: String containing React-style tab components.
            Expected format is:
            <Tabs>
                <Tab title="Title 1">
                    content 1
                </Tab>
                <Tab title="Title 2">
                    content 2
                </Tab>
            </Tabs>

    Returns:
        String with MkDocs tab components:
            === "Title 1"
                content 1

            === "Title 2"
                content 2
    """
    if "<Tabs>" not in content:
        return content

    # Find and replace each Tabs section
    pattern = re.compile(r"<Tabs>(.*?)</Tabs>", re.DOTALL)

    def replace_tabs(match: re.Match[str]) -> str:
        tabs_content = match.group(1)

        # Extract all Tab elements
        tab_pattern = re.compile(r'<Tab title="([^"]+)">(.*?)</Tab>', re.DOTALL)
        tabs = tab_pattern.findall(tabs_content)

        if not tabs:
            return ""

        result = []

        for i, (title, tab_content) in enumerate(tabs):
            # Add tab header
            result.append(f'=== "{title}"')

            # Process content by maintaining indentation structure
            lines = tab_content.strip().split("\n")

            # Find minimum common indentation for non-empty lines
            non_empty_lines = [line for line in lines if line.strip()]
            min_indent = min(len(line) - len(line.lstrip()) for line in non_empty_lines) if non_empty_lines else 0

            # Remove common indentation and add 4-space indent
            processed_lines = []
            for line in lines:
                if line.strip():
                    # Remove the common indentation but preserve relative indentation
                    if len(line) >= min_indent:
                        processed_lines.append("    " + line[min_indent:])
                    else:
                        processed_lines.append("    " + line.lstrip())
                else:
                    processed_lines.append("")

            result.append("\n".join(processed_lines))

            # Add a blank line between tabs (but not after the last one)
            if i < len(tabs) - 1:
                result.append("")

        return "\n".join(result)

    # Replace each Tabs section
    result = pattern.sub(replace_tabs, content)

    return result


def transform_card_grp_component(content: str) -> str:
    # Replace CardGroup tags
    modified_content = re.sub(r"<CardGroup\s+cols=\{(\d+)\}>\s*", "", content)
    modified_content = re.sub(r"\s*</CardGroup>", "", modified_content)

    # Replace Card tags with title and href attributes
    pattern = r'<Card\s+title="([^"]*)"\s+href="([^"]*)">(.*?)</Card>'
    replacement = r'<a class="card" href="\2">\n<h2>\1</h2>\3</a>'
    modified_content = re.sub(pattern, replacement, modified_content, flags=re.DOTALL)

    # Replace simple Card tags
    modified_content = re.sub(r"<Card>", '<div class="card">', modified_content)
    modified_content = re.sub(r"</Card>", "</div>", modified_content)

    return modified_content


def fix_asset_path(content: str) -> str:
    # Replace static/img paths with /assets/img
    modified_content = re.sub(r'src="/static/img/([^"]+)"', r'src="/assets/img/\1"', content)
    modified_content = re.sub(r"!\[([^\]]*)\]\(/static/img/([^)]+)\)", r"![\1](/assets/img/\2)", modified_content)

    # Replace docs paths with /docs
    modified_content = re.sub(r'href="/docs/([^"]+)"', r'href="/docs/\1"', modified_content)

    return modified_content


def fix_internal_references(abs_file_url: str, mkdocs_docs_dir: Path = mkdocs_docs_dir) -> str:
    # Special case for the API Reference
    if abs_file_url in {"/docs/api-reference", "/docs/api-reference/autogen"}:
        return (
            f"{abs_file_url}/autogen/AfterWork"
            if abs_file_url == "/docs/api-reference"
            else f"{abs_file_url}/AfterWork"
        )

    # Handle API reference URLs with hash fragments
    if abs_file_url.startswith("/docs/api-reference/") and "#" in abs_file_url:
        base_url, fragment = abs_file_url.split("#")
        module_prefix = base_url.replace("/docs/api-reference/", "").replace("/", ".")
        return f"{base_url}#{module_prefix}.{fragment.replace('-', '_')}"

    file_path = mkdocs_docs_dir / (abs_file_url.lstrip("/") + ".md")
    if file_path.is_file():
        return abs_file_url

    full_path = mkdocs_docs_dir / abs_file_url.lstrip("/")

    if not full_path.is_dir():
        return abs_file_url

    # Find the first .md file in the directory
    md_files = sorted(full_path.glob("*.md"))
    return f"{abs_file_url}/{md_files[0].stem}"


def absolute_to_relative(source_path: str, dest_path: str) -> str:
    """Convert an absolute path to a relative path from the source directory.

    Args:
        source_path: The source file's absolute path (e.g., "/docs/home/quick-start.md")
        dest_path: The destination file's absolute path (e.g., "/docs/user-guide/basic-concepts/installing-ag2")

    Returns:
        A relative path from source to destination (e.g., "../../user-guide/basic-concepts/installing-ag2")
    """
    sep = os.sep
    try:
        # Primary approach: Use pathlib for clean path calculation
        rel_path = str(Path(dest_path).relative_to(Path(source_path).parent))
        return f".{sep}{rel_path}" if Path(source_path).stem == "index" else f"..{sep}{rel_path}"
    except ValueError:
        # Fallback approach: Use os.path.relpath when paths don't share a common parent
        rel_path = os.path.relpath(dest_path, source_path)

        # Special case for blog directories: add deeper path traversal
        ret_val = os.path.join("..", "..", "..", rel_path) if "blog" in source_path else rel_path

        # Special case for index files: strip leading "../"
        if Path(source_path).stem == "index":
            ret_val = ret_val[3:]

        return ret_val


def fix_internal_links(source_path: str, content: str) -> str:
    """Detect internal links in content that start with '/docs' and convert them to relative paths.

    Args:
        source_path: The source file's absolute path
        content: The content with potential internal links

    Returns:
        Content with internal links converted to relative paths
    """
    # Define regex patterns for HTML and Markdown links
    html_link_pattern = r'href="(/docs/[^"]*)"'
    html_img_src_pattern = r'src="(/snippets/[^"]+)"'
    html_assets_src_pattern = r'src="(/assets/[^"]+)"'

    markdown_link_pattern = r"\[([^\]]+)\]\((/docs/[^)]*)\)"
    markdown_img_pattern = r"!\[([^\]]*)\]\((/snippets/[^)]+)\)"
    markdown_assets_pattern = r"!\[([^\]]*)\]\((/assets/[^)]+)\)"

    def handle_blog_url(url: str) -> str:
        """Special handling for blog URLs, converting date format from YYYY-MM-DD to YYYY/MM/DD.

        Args:
            url: The URL to process

        Returns:
            The URL with date format converted if it matches the blog URL pattern
        """
        blog_date_pattern = r"/docs/blog/(\d{4})-(\d{2})-(\d{2})-([\w-]+)"

        if re.match(blog_date_pattern, url):
            return re.sub(blog_date_pattern, r"/docs/blog/\1/\2/\3/\4", url)

        return url

    # Convert HTML links
    def replace_html(match: re.Match[str], attr_type: str) -> str:
        # There's only one group in the pattern, which is the path
        absolute_link = match.group(1)

        absolute_link = handle_blog_url(absolute_link)
        abs_file_path = fix_internal_references(absolute_link)
        relative_link = absolute_to_relative(source_path, abs_file_path)
        return f'{attr_type}="{relative_link}"'

    # Convert Markdown links
    def replace_markdown(match: re.Match[str], is_image: bool) -> str:
        text = match.group(1)
        absolute_link = match.group(2)

        absolute_link = handle_blog_url(absolute_link)
        abs_file_path = fix_internal_references(absolute_link)
        relative_link = absolute_to_relative(source_path, abs_file_path)
        prefix = "!" if is_image else ""
        return f"{prefix}[{text}]({relative_link})"

    # Apply replacements
    content = re.sub(html_link_pattern, lambda match: replace_html(match, "href"), content)
    content = re.sub(html_img_src_pattern, lambda match: replace_html(match, "src"), content)
    content = re.sub(html_assets_src_pattern, lambda match: replace_html(match, "src"), content)

    content = re.sub(markdown_link_pattern, lambda match: replace_markdown(match, False), content)
    content = re.sub(markdown_img_pattern, lambda match: replace_markdown(match, True), content)
    content = re.sub(markdown_assets_pattern, lambda match: replace_markdown(match, True), content)

    return content


def transform_content_for_mkdocs(content: str, rel_file_path: str) -> str:
    # Transform admonitions (Tip, Warning, Note)
    tag_mappings = {
        "Tip": "tip",
        "Warning": "warning",
        "Note": "note",
        "Danger": "danger",
    }
    for html_tag, mkdocs_type in tag_mappings.items():
        pattern = f"<{html_tag}>(.*?)</{html_tag}>"

        def replacement(match: re.Match[str]) -> str:
            inner_content = match.group(1).strip()

            lines = inner_content.split("\n")

            non_empty_lines = [line for line in lines if line.strip()]
            min_indent = min(len(line) - len(line.lstrip()) for line in non_empty_lines) if non_empty_lines else 0

            # Process each line
            processed_lines = []
            for line in lines:
                if line.strip():
                    # Remove common indentation and add 4-space indent
                    if len(line) >= min_indent:
                        processed_lines.append("    " + line[min_indent:])
                    else:
                        processed_lines.append("    " + line.lstrip())
                else:
                    processed_lines.append("")

            # Format the admonition with properly indented content
            return f"!!! {mkdocs_type.lstrip()}\n" + "\n".join(processed_lines)

        content = re.sub(pattern, replacement, content, flags=re.DOTALL)

    # Clean up style tags with double curly braces
    style_pattern = r"style\s*=\s*{{\s*([^}]+)\s*}}"

    def style_replacement(match: re.Match[str]) -> str:
        style_content = match.group(1).strip()
        return f"style={{ {style_content} }}"

    content = re.sub(style_pattern, style_replacement, content)

    # Fix snippet imports
    content = fix_snippet_imports(content)

    # Transform tab components
    content = transform_tab_component(content)

    # Transform CardGroup components
    content = transform_card_grp_component(content)

    # Fix assets path
    content = fix_asset_path(content)

    # Remove the mintlify specific markers
    content = remove_marker_blocks(content, "DELETE-ME-WHILE-BUILDING-MKDOCS")

    # Fix Internal links
    content = fix_internal_links(rel_file_path, content)

    return content


def rename_user_story(p: Path) -> Path:
    name = p.parent.name.split("-")[3:]
    return p.parent / ("_".join(name).lower() + p.suffix)


def process_and_copy_files(input_dir: Path, output_dir: Path, files: list[Path]) -> None:
    sep = os.sep
    # Keep track of MD files we need to process
    md_files_to_process = []

    # Step 1: First copy mdx files to destination as md files
    for file in files:
        if file.suffix == ".mdx":
            dest = output_dir / file.relative_to(input_dir).with_suffix(".md")

            if file.name == "home.mdx":
                dest = output_dir / "home.md"

            if f"{sep}user-stories{sep}" in str(dest):
                dest = rename_user_story(dest)

            dest.parent.mkdir(parents=True, exist_ok=True)
            dest.write_text(file.read_text())
            md_files_to_process.append(dest)
        else:
            copy_files(input_dir, output_dir, [file])

    # Step 2: Process the MD files we created
    for md_file in md_files_to_process:
        content = md_file.read_text()

        rel_path = f"{sep}{md_file.relative_to(output_dir.parents[0])}"
        processed_content = transform_content_for_mkdocs(content, rel_path)

        md_file.write_text(processed_content)


def format_title(file_path_str: str, keywords: dict[str, str], mkdocs_docs_dir: Path) -> str:
    """Format a page title with proper capitalization for special keywords."""
    file_path = mkdocs_docs_dir / Path(file_path_str)

    # Default formatting function for filenames
    def format_with_keywords(text: str) -> str:
        words = text.replace("-", " ").title().split()
        return " ".join(keywords.get(word, word) for word in words)

    try:
        front_matter_string, _ = separate_front_matter_and_content(file_path)
        if front_matter_string:
            front_matter = yaml.safe_load(front_matter_string[4:-3])
            sidebar_title: str = front_matter.get("sidebarTitle")
            if sidebar_title:
                return sidebar_title
    except (FileNotFoundError, yaml.YAMLError):
        pass

    # Fall back to filename if file not found or no sidebarTitle
    return format_with_keywords(Path(file_path_str).stem)


def format_page_entry(page_loc: str, indent: str, keywords: dict[str, str], mkdocs_docs_dir: Path) -> str:
    """Format a single page entry as either a parenthesized path or a markdown link."""
    file_path_str = f"{page_loc}.md"
    title = format_title(file_path_str, keywords, mkdocs_docs_dir)
    return f"{indent}    - [{title}]({file_path_str})"


def format_navigation(
    nav: list[NavigationGroup],
    mkdocs_docs_dir: Path = mkdocs_docs_dir,
    depth: int = 0,
    keywords: dict[str, str] | None = None,
) -> str:
    """Recursively format navigation structure into markdown-style nested list.

    Args:
        nav: List of navigation items with groups and pages
        mkdocs_docs_dir: Directory where the markdown files are located
        depth: Current indentation depth
        keywords: Dictionary of special case word capitalizations

    Returns:
        Formatted navigation as a string
    """
    if keywords is None:
        keywords = {
            "Ag2": "AG2",
            "Rag": "RAG",
            "Llm": "LLM",
        }

    indent = "    " * depth
    result = []

    for item in nav:
        # Add group header
        result.append(f"{indent}- {item['group']}")

        # Process each page
        for page in item["pages"]:
            if isinstance(page, dict):
                # Handle nested navigation groups
                result.append(format_navigation([page], mkdocs_docs_dir, depth + 1, keywords))
            else:
                # Handle individual pages
                result.append(format_page_entry(page, indent, keywords, mkdocs_docs_dir))

    ret_val = "\n".join(result)

    ret_val = ret_val.replace(
        "- Quick Start\n    - [Quick Start](docs/quick-start.md)\n",
        "- [Quick Start](docs/quick-start.md)\n",
    )
    ret_val = ret_val.replace(
        "- Basic Concepts\n",
        "- [Basic Concepts](docs/user-guide/basic-concepts/overview.md)\n",
    )
    ret_val = ret_val.replace("- FAQs\n    - [Faq](docs/faq/FAQ.md)\n", "- [FAQs](docs/faq/FAQ.md)\n")
    return ret_val


def add_api_ref_to_mkdocs_template(mkdocs_nav: str, section_to_follow: str) -> str:
    """Add API Reference section to the navigation template."""
    api_reference_section = """- API References
{api}
"""
    section_to_follow_marker = f"- {section_to_follow}"

    replacement_content = f"{api_reference_section}{section_to_follow_marker}"
    ret_val = mkdocs_nav.replace(section_to_follow_marker, replacement_content)
    return ret_val


@require_optional_import("jinja2", "docs")
def generate_mkdocs_navigation(website_dir: Path, mkdocs_root_dir: Path, nav_exclusions: list[str]) -> None:
    mintlify_nav_template_path = website_dir / "mint-json-template.json.jinja"
    mkdocs_nav_path = mkdocs_root_dir / "docs" / "navigation_template.txt"
    summary_md_path = mkdocs_root_dir / "docs" / "SUMMARY.md"

    mintlify_json = json.loads(Template(mintlify_nav_template_path.read_text(encoding="utf-8")).render())
    mintlify_nav = mintlify_json["navigation"]
    filtered_nav = [item for item in mintlify_nav if item["group"] not in nav_exclusions]

    mkdocs_docs_dir = mkdocs_root_dir / "docs"
    mkdocs_nav = format_navigation(filtered_nav, mkdocs_docs_dir)
    mkdocs_nav_with_api_ref = add_api_ref_to_mkdocs_template(mkdocs_nav, "Contributor Guide")

    blog_nav = "- Blog\n    - [Blog](docs/blog/index.md)"

    mkdocs_nav_content = "---\nsearch:\n  exclude: true\n---\n" + mkdocs_nav_with_api_ref + "\n" + blog_nav + "\n"
    mkdocs_nav_path.write_text(mkdocs_nav_content)
    summary_md_path.write_text(mkdocs_nav_content)


def copy_assets(website_dir: Path) -> None:
    src_dir = website_dir / "static" / "img"
    dest_dir = website_dir / "mkdocs" / "docs" / "assets" / "img"

    git_tracked_img_files = get_git_tracked_and_untracked_files_in_directory(website_dir / "static" / "img")
    copy_files(src_dir, dest_dir, git_tracked_img_files)


def add_excerpt_marker(content: str) -> str:
    """Add <!-- more --> marker before the second heading in markdown body content.

    Args:
        content (str): Body content of the markdown file (without frontmatter)

    Returns:
        str: Modified body content with <!-- more --> added
    """
    if "<!-- more -->" in content:
        return content.replace(r"\<!-- more -->", "<!-- more -->")

    # Find all headings
    heading_pattern = re.compile(r"^(#{1,6}\s+.+?)$", re.MULTILINE)
    headings = list(heading_pattern.finditer(content))

    # If there are fewer than 2 headings, add the marker at the end
    if len(headings) < 2:
        # If there's content, add the marker at the end
        return content.rstrip() + "\n\n<!-- more -->\n"

    # Get position of the second heading
    second_heading = headings[1]
    position = second_heading.start()

    # Insert the more marker before the second heading
    return content[:position] + "\n<!-- more -->\n\n" + content[position:]


def generate_url_slug(file: Path) -> str:
    parent_dir = file.parts[-2]
    slug = "-".join(parent_dir.split("-")[3:])
    return f"\nslug: {slug}"


def process_blog_contents(contents: str, file: Path) -> str:
    # Split the content into parts
    parts = contents.split("---", 2)
    if len(parts) < 3:
        return contents

    frontmatter = parts[1]
    content = parts[2]

    # Extract tags
    tags_match = re.search(r"tags:\s*\[(.*?)\]", frontmatter)
    if not tags_match:
        return contents

    tags_str = tags_match.group(1)
    tags = [tag.strip() for tag in tags_str.split(",")]

    # Extract date from second-to-last part of file path
    date_match = re.match(r"(\d{4}-\d{2}-\d{2})", file.parts[-2])
    date = date_match.group(1) if date_match else None

    # Remove original tags
    frontmatter = re.sub(r"tags:\s*\[.*?\]", "", frontmatter).strip()

    # Format tags and categories as YAML lists
    tags_yaml = "tags:\n    - " + "\n    - ".join(tags)
    categories_yaml = "categories:\n    - " + "\n    - ".join(tags)

    # Add date to metadata
    date_yaml = f"\ndate: {date}" if date else ""

    # Add URL slug metadata
    url_slug = generate_url_slug(file)

    # add the excerpt marker in the content
    content_with_excerpt_marker = add_excerpt_marker(content)

    return f"---\n{frontmatter}\n{tags_yaml}\n{categories_yaml}{date_yaml}{url_slug}\n---{content_with_excerpt_marker}"


def fix_snippet_imports(content: str, snippets_dir: Path = mkdocs_output_dir.parent / "snippets") -> str:
    """Replace import statements for MDX files from snippets directory with the target format.

    Args:
        content (str): Content containing import statements
        snippets_dir (Path): Path to the snippets directory

    Returns:
        str: Content with import statements replaced
    """
    # Regular expression to find import statements for MDX files from /snippets/
    import_pattern = re.compile(r'import\s+(\w+)\s+from\s+"(/snippets/[^"]+\.mdx)"\s*;')

    # Process all matches
    matches = list(import_pattern.finditer(content))

    # Process matches in reverse order to avoid offset issues when replacing text
    for match in reversed(matches):
        imported_path = match.group(2)

        # Check if the path starts with /snippets/
        if not imported_path.startswith("/snippets/"):
            continue

        # Extract the relative path (without the /snippets/ prefix)
        relative_path = imported_path[len("/snippets/") :]

        # Construct the full file path
        file_path = snippets_dir / relative_path

        # Read the file content
        with open(file_path) as f:
            file_content = f.read()

        # Replace the import statement with the file content
        start, end = match.span()
        content = content[:start] + file_content + content[end:]

    return content


def process_blog_files(mkdocs_output_dir: Path, authors_yml_path: Path, snippets_src_path: Path) -> None:
    src_blog_dir = mkdocs_output_dir / "_blogs"
    target_blog_dir = mkdocs_output_dir / "blog"
    target_posts_dir = target_blog_dir / "posts"
    snippets_dir = mkdocs_output_dir.parent / "snippets"

    # Create the target posts directory
    target_posts_dir.mkdir(parents=True, exist_ok=True)

    # Create the index file in the target blog directory
    index_file = target_blog_dir / "index.md"
    index_file.write_text("# Blog\n\n")

    # Get all files to copy
    files_to_copy = list(src_blog_dir.rglob("*"))

    # process blog metadata
    for file in files_to_copy:
        if file.suffix == ".md":
            contents = file.read_text()
            processed_contents = process_blog_contents(contents, file)
            processed_contents = fix_snippet_imports(processed_contents, snippets_dir)
            file.write_text(processed_contents)

    # Copy files from source to target
    copy_files(src_blog_dir, target_posts_dir, files_to_copy)

    # Copy snippets directory
    snippets_files_to_copy = list(snippets_src_path.rglob("*"))
    copy_files(snippets_src_path, snippets_dir, snippets_files_to_copy)

    # Copy authors_yml_path to the target_blog_dir and rename it as .authors.yml
    target_authors_yml_path = target_blog_dir / ".authors.yml"
    shutil.copy2(authors_yml_path, target_authors_yml_path)


_is_first_notebook = True


def add_front_matter_to_metadata_yml(
    front_matter: dict[str, str | list[str] | None], website_build_directory: Path, rendered_mdx: Path
) -> None:
    """Add notebook metadata to a YAML file containing metadata for all notebooks."""
    global _is_first_notebook

    source = front_matter.get("source_notebook")
    if isinstance(source, str) and source.startswith("/website/docs/"):
        return

    # Get the metadata file path
    metadata_yml_path = website_build_directory / "../../data/notebooks_metadata.yml"

    # Create parent directories if they don't exist
    metadata_yml_path.parent.mkdir(parents=True, exist_ok=True)

    # If this is the first notebook, delete the existing file
    if _is_first_notebook and metadata_yml_path.exists():
        metadata_yml_path.unlink()
        _is_first_notebook = False

    # Create new entry for current notebook
    title = front_matter.get("title", "")
    link = f"/docs/use-cases/notebooks/notebooks/{rendered_mdx.stem}.md"
    rel_link = f"../notebooks/{rendered_mdx.stem}"
    description = front_matter.get("description", "")
    tags = front_matter.get("tags", []) or []

    # Escape quotes in strings
    title = str(title).replace('"', '\\"')
    description = str(description).replace('"', '\\"')
    source_str = str(source or "").replace('"', '\\"')

    # Open file in append mode
    with open(metadata_yml_path, "a", encoding="utf-8") as f:
        # Write the entry
        f.write(f'- title: "{title}"\n')
        f.write(f'  link: "{link}"\n')
        f.write(f'  rel_link: "{rel_link}"\n')
        f.write(f'  description: "{description}"\n')
        f.write('  image: ""\n')

        # Write tags
        if tags:
            f.write("  tags:\n")
            for tag in tags:
                if tag:  # Only write non-empty tags
                    tag_str = str(tag).replace('"', '\\"')
                    f.write(f'    - "{tag_str}"\n')
        else:
            f.write("  tags: []\n")

        # Write source
        f.write(f'  source: "{source_str}"\n')
        f.write("\n")


def transform_admonition_blocks(content: str) -> str:
    """Transform admonition blocks from ::: syntax to Material for MkDocs syntax.

    Converts blocks like:
    :::info Requirements
    content here
    :::

    To:
    !!! info "Requirements"
        content here

    Args:
        content: String containing ::: syntax admonition blocks

    Returns:
        String with Material for MkDocs admonition blocks
    """
    tag_mappings = {
        "Tip": "tip",
        "Warning": "warning",
        "Note": "note",
        "Danger": "danger",
    }

    # Simplified approach: first detect admonition blocks boundaries
    lines = content.split("\n")
    admonition_start = None
    admonition_type = None
    admonition_title = None
    admonition_content: list[str] = []
    result_lines = []

    i = 0
    while i < len(lines):
        line = lines[i]

        # Check for admonition start
        if line.strip().startswith(":::") and admonition_start is None:
            admonition_start = i
            # Extract admonition type and optional title
            match = re.match(r":::(\w+)(?:\s+(.+))?", line.strip())
            if match:
                admonition_type = match.group(1)
                admonition_title = match.group(2) if match.group(2) else ""
            else:
                # No match for admonition type means we couldn't parse the format
                admonition_type = None
            i += 1
            continue

        # Check for admonition end
        elif line.strip() == ":::" and admonition_start is not None:
            # If admonition_type is None, preserve the original content
            if admonition_type is None:
                # Add back the original admonition block without transformation
                original_lines = []
                original_lines.append(lines[admonition_start])  # Opening :::
                original_lines.extend(admonition_content)  # Content
                original_lines.append(line)  # Closing :::
                result_lines.extend(original_lines)
            else:
                # Process as before for valid admonition types
                # Map the admonition type
                if admonition_type in tag_mappings:
                    mapped_type = tag_mappings[admonition_type]
                else:
                    # Try case-insensitive match
                    for tag, mapped in tag_mappings.items():
                        if tag.lower() == admonition_type.lower():
                            mapped_type = mapped
                            break
                    else:
                        # Default to lowercase of original if no mapping found
                        mapped_type = admonition_type.lower()

                # Process indentation
                if admonition_content:
                    # Find minimum common indentation
                    non_empty_lines = [line for line in admonition_content if line.strip()]
                    min_indent = min((len(line) - len(line.lstrip()) for line in non_empty_lines), default=0)

                    # Remove common indentation and add 4-space indent
                    processed_content = []
                    for line in admonition_content:
                        if line.strip():
                            if len(line) >= min_indent:
                                processed_content.append("    " + line[min_indent:])
                            else:
                                processed_content.append("    " + line.lstrip())
                        else:
                            processed_content.append("")
                else:
                    processed_content = []

                # Create the MkDocs admonition
                if admonition_title:
                    mkdocs_admonition = [f'!!! {mapped_type} "{admonition_title}"'] + processed_content
                else:
                    mkdocs_admonition = [f"!!! {mapped_type}"] + processed_content

                # Add the processed admonition
                result_lines.extend(mkdocs_admonition)

            # Reset admonition tracking
            admonition_start = None
            admonition_type = None
            admonition_title = None
            admonition_content = []
            i += 1
            continue

        elif admonition_start is not None:
            admonition_content.append(line)
            i += 1
            continue

        else:
            result_lines.append(line)
            i += 1

    if admonition_start is not None:
        for j in range(admonition_start, len(lines)):
            result_lines.append(lines[j])

    return "\n".join(result_lines)


def remove_mdx_code_blocks(content: str) -> str:
    """Remove ````mdx-code-block and ```` markers from the content.

    This function removes the mdx-code-block markers while preserving the content inside.

    Args:
        content: String containing mdx-code-block markers

    Returns:
        String with mdx-code-block markers removed
    """
    # Pattern to match mdx-code-block sections
    # Captures everything between ````mdx-code-block and ````
    pattern = re.compile(r"````mdx-code-block\n(.*?)\n````", re.DOTALL)

    # Replace with just the content (group 1)
    result = pattern.sub(r"\1", content)

    return result


@require_optional_import("yaml", "docs")
def post_process_func(
    rendered_mdx: Path,
    source_notebooks: Path,
    front_matter: dict[str, str | list[str] | None],
    website_build_directory: Path,
) -> None:
    with open(rendered_mdx, encoding="utf-8") as f:
        content = f.read()

    # If there is front matter in the mdx file, we need to remove it
    if content.startswith("---"):
        front_matter_end = content.find("---", 3)
        mdx_front_matter = yaml.safe_load(content[4:front_matter_end])
        # Merge while preserving original values
        front_matter = {**front_matter, **mdx_front_matter}
        content = content[front_matter_end + 3 :]

    # Clean heading IDs using regex - matches from # to the end of ID block
    content = re.sub(r"(#{1,6}[^{]+){#[^}]+}", r"\1", content)

    # Each intermediate path needs to be resolved for this to work reliably
    repo_root = Path(__file__).resolve().parents[2]
    repo_relative_notebook = source_notebooks.resolve().relative_to(repo_root)
    front_matter["source_notebook"] = f"/{repo_relative_notebook}"
    front_matter["custom_edit_url"] = f"https://github.com/ag2ai/ag2/edit/main/{repo_relative_notebook}"

    github_link = f"https://github.com/ag2ai/ag2/blob/main/{repo_relative_notebook}"
    content = (
        f'\n<a href="{github_link}" class="github-badge" target="_blank">'
        + """<img noZoom src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github" alt="Open on GitHub" />"""
        + "</a>"
        + content
    )

    # If no colab link is present, insert one
    if "colab-badge.svg" not in content:
        colab_link = f"https://colab.research.google.com/github/ag2ai/ag2/blob/main/{repo_relative_notebook}"
        content = (
            f'\n<a href="{colab_link}" class="colab-badge" target="_blank">'
            + """<img noZoom src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />"""
            + "</a>"
            + content
        )

    # Create the front matter metadata js file for examples by notebook section
    add_front_matter_to_metadata_yml(front_matter, website_build_directory, rendered_mdx)

    # Dump front_matter to yaml
    front_matter_str = yaml.dump(front_matter, default_flow_style=False)

    # Add render_macros: false to the front matter
    front_matter_str += "render_macros: false\n"

    # transform content for mkdocs
    rel_path = f"/{rendered_mdx.relative_to(website_build_directory.parents[0])}"
    content = transform_content_for_mkdocs(content, rel_path)

    # Convert mdx image syntax to mintlify image syntax
    # content = convert_mdx_image_blocks(content, rendered_mdx, website_build_directory)

    # ensure editUrl is present
    # content = ensure_edit_url(content, repo_relative_notebook)

    # Remove admonition blocks
    content = transform_admonition_blocks(content)

    # Remove mdx-code-block markers
    content = remove_mdx_code_blocks(content)

    # Generate the page title
    page_header = front_matter.get("title")
    page_title = f"# {page_header}\n\n" if page_header else ""

    # Rewrite the content as
    # ---
    # front_matter_str
    # ---
    # content
    new_content = f"---\n{front_matter_str}---\n\n{page_title}\n{content}"

    # Change the file extension to .md
    rendered_md = rendered_mdx.with_suffix(".md")

    with open(rendered_md, "w", encoding="utf-8") as f:
        f.write(new_content)

    # Optionally, remove the original .mdx file
    rendered_mdx.unlink()


def target_dir_func(website_build_directory: Path) -> Path:
    """Return the target directory for notebooks."""
    return website_build_directory / "use-cases" / "notebooks" / "notebooks"


def inject_gallery_html(notebooks_md_path: Path, metadata_yml_path: Path) -> None:
    """Generate the index.html file for the notebooks section."""
    with open(notebooks_md_path, encoding="utf-8") as f:
        content = f.read()

    gallery_html = render_gallery_html(metadata_yml_path)

    updated_content = content.replace("{{ render_gallery(gallery_items) }}", gallery_html)
    with open(notebooks_md_path, "w", encoding="utf-8") as f:
        f.write(updated_content)


@require_optional_import("yaml", "docs")
def add_notebooks_nav(mkdocs_nav_path: Path, metadata_yml_path: Path) -> None:
    """Add notebooks navigation to the summary markdown file.

    Args:
        mkdocs_nav_path: Path to the mkdocs navigation template file
        metadata_yml_path: Path to the notebooks metadata YAML file
    """
    # Read the metadata file to get notebook items
    with open(metadata_yml_path) as file:
        items = yaml.safe_load(file)

    # Create navigation list entries for each notebook
    nav_list = []
    for item in items:
        _link = item["link"][1:] if item["link"].startswith("/") else item["link"]
        nav_list.append(f"        - [{item['title']}]({_link})\n")

    # Read the summary file
    with open(mkdocs_nav_path) as file:
        lines = file.readlines()

    # Find where to insert the notebook entries
    for i, line in enumerate(lines):
        if line.strip() == "- [All Notebooks](docs/use-cases/notebooks/Notebooks.md)":
            # Insert all notebook items after the Notebooks line
            # No need to insert extra blank lines, just the notebook entries
            for j, nav_item in enumerate(nav_list):
                lines.insert(i + 1 + j, nav_item)
            break

    # Write the updated content back to the summary file
    with open(mkdocs_nav_path, "w") as file:
        file.writelines(lines)


def _generate_navigation_entries(dir_path: Path, mkdocs_output_dir: Path) -> list[str]:
    """Generate navigation entries for user stories and community talks.

    Args:
        dir_path (Path): Path to the directory containing user stories or community talks.
        mkdocs_output_dir (Path): Path to the MkDocs output directory.

    Returns:
        str: Formatted navigation entries.
    """
    # Read all user story files and sort them by date (newest first)
    files = sorted(dir_path.glob("**/*.md"), key=sort_files_by_date, reverse=True)

    # Prepare user stories navigation entries
    entries = []
    for file in files:
        # Extract the title from the frontmatter using a simpler split approach
        content = file.read_text()

        # Split content at the "---" markers
        parts = content.split("---", 2)
        if len(parts) < 3:
            # No valid frontmatter found, use directory name as title
            title = file.parent.name
        else:
            # Parse the frontmatter
            frontmatter_text = parts[1].strip()
            frontmatter = yaml.safe_load(frontmatter_text)
            title = frontmatter.get("title", file.parent.name)

        # Generate relative path from the docs root directory
        relative_path = file.parent.relative_to(mkdocs_output_dir)
        path_for_link = str(relative_path).replace("\\", "/")

        # Format navigation entry
        entries.append(f"        - [{title}]({path_for_link}/{file.name})")

    return entries


def generate_community_insights_nav(mkdocs_output_dir: Path, mkdocs_nav_path: Path) -> None:
    user_stories_dir = mkdocs_output_dir / "docs" / "user-stories"
    community_talks_dir = mkdocs_output_dir / "docs" / "community-talks"

    user_stories_entries = _generate_navigation_entries(user_stories_dir, mkdocs_output_dir)
    community_talks_entries = _generate_navigation_entries(community_talks_dir, mkdocs_output_dir)

    user_stories_nav = "    - User Stories\n" + "\n".join(user_stories_entries)
    community_talks_nav = "    - Community Talks\n" + "\n".join(community_talks_entries)
    community_insights_nav = "- Community Insights\n" + user_stories_nav + "\n" + community_talks_nav

    # Read existing navigation template
    nav_content = mkdocs_nav_path.read_text()

    section_to_follow_marker = "- Blog"

    replacement_content = f"{community_insights_nav}\n{section_to_follow_marker}"
    updated_nav_content = nav_content.replace(section_to_follow_marker, replacement_content)

    # Write updated navigation to file
    mkdocs_nav_path.write_text(updated_nav_content)


def add_authors_info_to_user_stories(website_dir: Path) -> None:
    mkdocs_output_dir = website_dir / "mkdocs" / "docs" / "docs"
    user_stories_dir = mkdocs_output_dir / "user-stories"
    authors_yml = website_dir / "blogs_and_user_stories_authors.yml"

    all_authors_info = get_authors_info(authors_yml)

    add_authors_and_social_preview(website_dir, user_stories_dir, all_authors_info, "mkdocs")

    for file_path in user_stories_dir.glob("**/*.md"):
        content = file_path.read_text(encoding="utf-8")
        rel_path = f"/{file_path.relative_to(mkdocs_output_dir.parents[0])}"
        updated_content = transform_content_for_mkdocs(content, rel_path)
        file_path.write_text(updated_content, encoding="utf-8")


def main(force: bool) -> None:
    parser = create_base_argument_parser()
    args = parser.parse_args(["render"])
    args.dry_run = False
    args.quarto_bin = "quarto"
    args.notebooks = None

    # check if args.force is set
    if force and mkdocs_output_dir.exists():
        shutil.rmtree(mkdocs_output_dir)

    exclusion_list = [
        "docs/.gitignore",
        "docs/installation",
        "docs/user-guide/getting-started",
        "docs/user-guide/models/litellm-with-watsonx.md",
        "docs/contributor-guide/Migration-Guide.md",
    ]
    nav_exclusions = [""]

    files_to_copy = get_git_tracked_and_untracked_files_in_directory(mint_docs_dir)
    filtered_files = filter_excluded_files(files_to_copy, exclusion_list, website_dir)

    # Copy snippet files
    snippet_files = get_git_tracked_and_untracked_files_in_directory(website_dir / "snippets")
    copy_files(website_dir / "snippets", mkdocs_output_dir.parent / "snippets", snippet_files)

    copy_assets(website_dir)
    process_and_copy_files(mint_docs_dir, mkdocs_output_dir, filtered_files)

    snippets_dir_path = website_dir / "snippets"
    authors_yml_path = website_dir / "blogs_and_user_stories_authors.yml"

    process_blog_files(mkdocs_output_dir, authors_yml_path, snippets_dir_path)
    generate_mkdocs_navigation(website_dir, mkdocs_root_dir, nav_exclusions)

    if args.website_build_directory is None:
        args.website_build_directory = mkdocs_output_dir

    if args.notebook_directory is None:
        args.notebook_directory = mkdocs_root_dir / "../../notebook"

    metadata_yml_path = Path(args.website_build_directory) / "../../data/notebooks_metadata.yml"

    if not metadata_yml_path.exists() or (force and mkdocs_output_dir.exists()):
        process_notebooks_core(args, post_process_func, target_dir_func)

    # Render Notebooks Gallery HTML
    notebooks_md_path = mkdocs_output_dir / "use-cases" / "notebooks" / "Notebooks.md"
    inject_gallery_html(notebooks_md_path, metadata_yml_path)

    # Add Notebooks Navigation to Summary.md
    mkdocs_nav_path = mkdocs_root_dir / "docs" / "navigation_template.txt"
    add_notebooks_nav(mkdocs_nav_path, metadata_yml_path)

    # Render Community Gallery HTML
    community_md_path = mkdocs_output_dir / "use-cases" / "community-gallery" / "community-gallery.md"
    metadata_yml_path = Path(args.website_build_directory) / "../../data/gallery_items.yml"
    inject_gallery_html(community_md_path, metadata_yml_path)

    # Generate Navigation for User Stories
    docs_dir = mkdocs_root_dir / "docs"
    generate_community_insights_nav(docs_dir, mkdocs_nav_path)

    # Add Authors info to User Stories
    add_authors_info_to_user_stories(website_dir)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import re
import shutil
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from textwrap import dedent, indent
from typing import Literal, TypedDict, Union

from ..import_utils import optional_import_block, require_optional_import

with optional_import_block():
    import yaml


EDIT_URL_HTML = """
<div className="edit-url-container">
    <a className="edit-url" href="https://github.com/ag2ai/ag2/edit/main/{file_path}" target='_blank'><Icon icon="pen" iconType="solid" size="13px"/> Edit this page</a>
</div>
"""

build_system = Literal["mkdocs", "mintlify"]


class NavigationGroup(TypedDict):
    group: str
    pages: list[Union[str, "NavigationGroup"]]


def get_git_tracked_and_untracked_files_in_directory(directory: Path) -> list[Path]:
    """Get all files in the directory that are tracked by git or newly added."""
    proc = subprocess.run(
        ["git", "-C", str(directory), "ls-files", "--others", "--exclude-standard", "--cached"],
        capture_output=True,
        text=True,
        check=True,
    )
    return list({directory / p for p in proc.stdout.splitlines()})


def copy_files(src_dir: Path, dst_dir: Path, files_to_copy: list[Path]) -> None:
    """Copy files from src_dir to dst_dir."""
    for file in files_to_copy:
        if file.is_file():
            dst = dst_dir / file.relative_to(src_dir)
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(file, dst)


def copy_only_git_tracked_and_untracked_files(src_dir: Path, dst_dir: Path, ignore_dir: str | None = None) -> None:
    """Copy only the files that are tracked by git or newly added from src_dir to dst_dir."""
    tracked_and_new_files = get_git_tracked_and_untracked_files_in_directory(src_dir)

    if ignore_dir:
        ignore_dir_rel_path = src_dir / ignore_dir

        tracked_and_new_files = list({
            file for file in tracked_and_new_files if not any(parent == ignore_dir_rel_path for parent in file.parents)
        })

    copy_files(src_dir, dst_dir, tracked_and_new_files)


def remove_marker_blocks(content: str, marker_prefix: str) -> str:
    """Remove marker blocks from the content.

    Args:
        content: The source content to process
        marker_prefix: The marker prefix to identify blocks to remove (without the START/END suffix)

    Returns:
        Processed content with appropriate blocks handled
    """
    # First, remove blocks with the specified marker completely
    if f"{{/* {marker_prefix}-START */}}" in content:
        pattern = rf"\{{/\* {re.escape(marker_prefix)}-START \*/\}}.*?\{{/\* {re.escape(marker_prefix)}-END \*/\}}"
        content = re.sub(pattern, "", content, flags=re.DOTALL)

    # Now, remove markers but keep content for the other marker type
    other_prefix = (
        "DELETE-ME-WHILE-BUILDING-MKDOCS"
        if marker_prefix == "DELETE-ME-WHILE-BUILDING-MINTLIFY"
        else "DELETE-ME-WHILE-BUILDING-MINTLIFY"
    )

    # Remove start markers
    start_pattern = rf"\{{/\* {re.escape(other_prefix)}-START \*/\}}\s*"
    content = re.sub(start_pattern, "", content)

    # Remove end markers
    end_pattern = rf"\s*\{{/\* {re.escape(other_prefix)}-END \*/\}}"
    content = re.sub(end_pattern, "", content)

    # Fix any double newlines that might have been created
    content = re.sub(r"\n\s*\n\s*\n", "\n\n", content)

    return content


# Sort files by parent directory date (if exists) and name
def sort_files_by_date(file_path: Path) -> tuple[datetime, str]:
    dirname = file_path.parent.name
    try:
        # Extract date from directory name (first 3 parts)
        date_str = "-".join(dirname.split("-")[:3])
        date = datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        date = datetime.min
    return (date, dirname)


def construct_authors_html(authors_list: list[str], authors_dict: dict[str, dict[str, str]], build_system: str) -> str:
    """Constructs HTML for displaying author cards in a blog.

    Args:
        authors_list: list of author identifiers
        authors_dict: Dictionary containing author information keyed by author identifier
        build_system: The build system being used (mkdocs or mintlify)

    Returns:
        str: Formatted HTML string containing author cards
    """
    if not authors_list:
        return ""

    card_template_mintlify = """
        <Card href="{url}">
            <div class="col card">
              <div class="img-placeholder">
                <img noZoom src="{avatar}" />
              </div>
              <div>
                <p class="name">{name}</p>
                <p>{description}</p>
              </div>
            </div>
        </Card>"""

    card_template_mkdocs = """
        <div class="card">
            <div class="col card">
              <div class="img-placeholder">
                <img noZoom src="{avatar}" />
              </div>
              <div>
                <p class="name">{name}</p>
                <p>{description}</p>
              </div>
            </div>
        </div>
    """

    card_template = card_template_mintlify if build_system == "mintlify" else card_template_mkdocs

    authors_html = [card_template.format(**authors_dict[author]) for author in authors_list]

    author_label = "Author:" if len(authors_list) == 1 else "Authors:"
    authors_html_str = indent("".join(authors_html), "        ")

    retval = ""
    if build_system == "mintlify":
        retval = dedent(
            f"""
                <div class="blog-authors">
                <p class="authors">{author_label}</p>
                <CardGroup cols={{2}}>{authors_html_str}
                </CardGroup>
                </div>
            """
        )
    else:
        retval = dedent(
            f"""
                <div class="blog-authors">
                <p class="authors">{author_label}</p>
                <div class="card-group">
                    {authors_html_str}
                </div>
                </div>
            """
        )
    return retval


def separate_front_matter_and_content(file_path: Path) -> tuple[str, str]:
    """Separate front matter and content from a markdown file.

    Args:
        file_path (Path): Path to the mdx file
    """
    content = file_path.read_text(encoding="utf-8")

    if content.startswith("---"):
        front_matter_end = content.find("---", 3)
        front_matter = content[0 : front_matter_end + 3]
        content = content[front_matter_end + 3 :].strip()
        return front_matter, content

    return "", content


def ensure_edit_url(content: str, file_path: Path) -> str:
    """Ensure editUrl is present in the content.

    Args:
        content (str): Content of the file
        file_path (Path): Path to the file
    """
    html_placeholder = [line for line in EDIT_URL_HTML.splitlines() if line.strip() != ""][0]
    if html_placeholder in content:
        return content

    return content + EDIT_URL_HTML.format(file_path=file_path)


@require_optional_import("yaml", "docs")
def add_authors_and_social_preview(
    website_build_dir: Path,
    target_dir: Path,
    all_authors_info: dict[str, dict[str, str]],
    build_system: build_system = "mintlify",
) -> None:
    """Add authors info and social share image to mdx files in the target directory."""
    social_img_html = (
        """\n<div>
<img noZoom className="social-share-img"
  src="https://media.githubusercontent.com/media/ag2ai/ag2/refs/heads/main/website/static/img/cover.png"
  alt="social preview"
  style={{ position: 'absolute', left: '-9999px' }}
/>
</div>"""
        if build_system == "mintlify"
        else ""
    )

    target_file_extension = "mdx" if build_system == "mintlify" else "md"
    for file_path in target_dir.glob(f"**/*.{target_file_extension}"):
        try:
            front_matter_string, content = separate_front_matter_and_content(file_path)

            # Convert single author to list and handle authors
            front_matter = yaml.safe_load(front_matter_string[4:-3])
            authors = front_matter.get("authors", [])
            authors_list = [authors] if isinstance(authors, str) else authors

            # Generate authors HTML
            authors_html = (
                construct_authors_html(authors_list, all_authors_info, build_system)
                if '<div class="blog-authors">' not in content
                else ""
            )

            # Combine content
            new_content = f"{front_matter_string}\n{social_img_html}\n{authors_html}\n{content}"

            # ensure editUrl is present

            if build_system == "mintlify":
                rel_file_path = (
                    str(file_path.relative_to(website_build_dir.parent))
                    .replace("build/docs/", "website/docs/")
                    .replace("website/docs/blog/", "website/docs/_blogs/")
                )
                content_with_edit_url = ensure_edit_url(new_content, Path(rel_file_path))

                # replace the mkdocs excerpt marker
                content_with_edit_url = content_with_edit_url.replace(r"\<!-- more -->", "")

                file_path.write_text(f"{content_with_edit_url}\n", encoding="utf-8")

            else:
                file_path.write_text(f"{new_content}\n", encoding="utf-8")

        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            continue


@require_optional_import("yaml", "docs")
def get_authors_info(authors_yml: Path) -> dict[str, dict[str, str]]:
    try:
        all_authors_info = yaml.safe_load(authors_yml.read_text(encoding="utf-8"))["authors"]
    except (yaml.YAMLError, OSError) as e:
        print(f"Error reading authors file: {e}")
        sys.exit(1)

    return all_authors_info  # type: ignore [no-any-return]


@require_optional_import("yaml", "docs")
def render_gallery_html(gallery_file_path: Path) -> str:
    """Renders a gallery of items with tag filtering

    Args:
        gallery_file_path: Path to the YAML file containing gallery items

    Returns:
        HTML string for the gallery
    """
    try:
        # Load gallery items from YAML file
        with open(gallery_file_path) as file:
            gallery_items = yaml.safe_load(file)

        # Ensure gallery_items is a list
        if not isinstance(gallery_items, list):
            return f"<div class='error'>Error: YAML file did not contain a list, but a {type(gallery_items)}</div>"

        # Extract all unique tags from gallery items
        all_tags = []
        for item in gallery_items:
            if not isinstance(item, dict):
                continue

            if "tags" in item and item["tags"]:
                all_tags.extend(item["tags"])
        all_tags = sorted(set(all_tags))

        # Generate HTML directly
        html = '<div class="examples-gallery-container">'

        # Generate tag filter select
        html += '<select multiple class="tag-filter" data-placeholder="Filter by tags">'
        for tag in all_tags:
            html += f'<option value="{tag}">{tag}</option>'
        html += "</select>"

        # Generate gallery cards
        html += '<div class="gallery-cards">'

        for item in gallery_items:
            # Skip if item is not a dictionary
            if not isinstance(item, dict):
                continue

            image_url = item.get("image", "default.png")
            if image_url and not isinstance(image_url, str):
                # Handle case where image is not a string
                image_url = "default.png"

            if image_url and not image_url.startswith("http"):
                image_url = f"../../../../assets/img/gallery/{image_url}"

            # Handle default image
            if not image_url:
                image_url = "../../../../assets/img/gallery/default.png"

            # Tags HTML
            tags_html = ""
            if "tags" in item and item["tags"]:
                tags_html = '<div class="tags-container">'
                for tag in item["tags"]:
                    tags_html += f'<span class="tag" data-tag="{tag}">{tag}</span>'
                tags_html += "</div>"

            # Badges HTML
            badges_html = ""
            notebook_src = item.get("source", None)

            if notebook_src:
                colab_href = f"https://colab.research.google.com/github/ag2ai/ag2/blob/main/{notebook_src}"
                github_href = f"https://github.com/ag2ai/ag2/blob/main/{notebook_src}"
                badges_html = f"""
                <div class="badges">
                    <a style="margin-right: 5px" href="{colab_href}" target="_blank">
                        <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
                    </a>
                    <p class="hidden">{item.get("title", "")}</p>
                    <a href="{github_href}" target="_blank">
                        <img alt="GitHub" src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github"/>
                    </a>
                </div>
                """

            # Generate card HTML with safer access to attributes
            tags_str = ",".join(item.get("tags", [])) if isinstance(item.get("tags"), list) else ""

            # Generate HTML for image tag
            img_tag = (
                f'<img src="{image_url}" alt="{item.get("title", "")}" class="card-image">' if not notebook_src else ""
            )

            data_link_target = "_self" if notebook_src else "_blank"
            html += f"""
            <div class="card" data-link="{item.get("link", "#")}" data-rel-link="{item.get("rel_link", "#")}" data-tags="{tags_str}" data-link-target="{data_link_target}">
                <div class="card-container">
                    {img_tag}
                    <p class="card-title">{item.get("title", "")}</p>
                    {badges_html}
                    <p class="card-description">{item.get("description", item.get("title", ""))}</p>
                    {tags_html}
                </div>
            </div>
            """

        # Close containers
        html += """
            </div>
        </div>
        """

        return html

    except FileNotFoundError:
        return f"<div class='error'>Error: YAML file not found at path: {gallery_file_path}</div>"
    except yaml.YAMLError:
        return f"<div class='error'>Error: Invalid YAML format in file: {gallery_file_path}</div>"
    except Exception as e:
        return f"<div class='error'>Error: {str(e)}</div>"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import atexit
import logging
import uuid
from hashlib import md5
from pathlib import Path
from time import sleep
from types import TracebackType
from typing import Any, ClassVar

import docker
from docker.errors import ImageNotFound
from typing_extensions import Self

from ..code_utils import TIMEOUT_MSG, _cmd
from ..doc_utils import export_module
from .base import CodeBlock, CodeExecutor, CodeExtractor, CommandLineCodeResult
from .markdown_code_extractor import MarkdownCodeExtractor
from .utils import _get_file_name_from_content, silence_pip


def _wait_for_ready(container: Any, timeout: int = 60, stop_time: float = 0.1) -> None:
    elapsed_time = 0.0
    while container.status != "running" and elapsed_time < timeout:
        sleep(stop_time)
        elapsed_time += stop_time
        container.reload()
        continue
    if container.status != "running":
        raise ValueError("Container failed to start")


__all__ = ("DockerCommandLineCodeExecutor",)


@export_module("autogen.coding")
class DockerCommandLineCodeExecutor(CodeExecutor):
    DEFAULT_EXECUTION_POLICY: ClassVar[dict[str, bool]] = {
        "bash": True,
        "shell": True,
        "sh": True,
        "pwsh": True,
        "powershell": True,
        "ps1": True,
        "python": True,
        "javascript": False,
        "html": False,
        "css": False,
    }
    LANGUAGE_ALIASES: ClassVar[dict[str, str]] = {"py": "python", "js": "javascript"}

    def __init__(
        self,
        image: str = "python:3-slim",
        container_name: str | None = None,
        timeout: int = 60,
        work_dir: Path | str | None = None,
        bind_dir: Path | str | None = None,
        auto_remove: bool = True,
        stop_container: bool = True,
        execution_policies: dict[str, bool] | None = None,
        *,
        container_create_kwargs: dict[str, Any] | None = None,
    ):
        """(Experimental) A code executor class that executes code through
        a command line environment in a Docker container.

        The executor first saves each code block in a file in the working
        directory, and then executes the code file in the container.
        The executor executes the code blocks in the order they are received.
        Currently, the executor only supports Python and shell scripts.
        For Python code, use the language "python" for the code block.
        For shell scripts, use the language "bash", "shell", or "sh" for the code
        block.

        Args:
            image: Docker image to use for code execution. Defaults to "python:3-slim".
            container_name: Name of the Docker container which is created. If None, will autogenerate a name. Defaults to None.
            timeout: The timeout for code execution. Defaults to 60.
            work_dir: The working directory for the code execution. Defaults to Path(".").
            bind_dir: The directory that will be bound to the code executor container. Useful for cases where you want to spawn
                the container from within a container. Defaults to work_dir.
            auto_remove: If true, will automatically remove the Docker container when it is stopped. Defaults to True.
            stop_container: If true, will automatically stop the
                container when stop is called, when the context manager exits or when
                the Python process exits with atext. Defaults to True.
            execution_policies: A dictionary mapping language names to boolean values that determine
                whether code in that language should be executed. True means code in that language
                will be executed, False means it will only be saved to a file. This overrides the
                default execution policies. Defaults to None.
            container_create_kwargs: Optional dict forwarded verbatim to
                "docker.client.containers.create". Use it to set advanced Docker
                options (environment variables, GPU device_requests, port mappings, etc.).
                Values here override the class defaults when keys collide. Defaults to None.


        Raises:
            ValueError: On argument error, or if the container fails to start.
        """
        work_dir = work_dir if work_dir is not None else Path()

        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        if isinstance(work_dir, str):
            work_dir = Path(work_dir)
        work_dir.mkdir(exist_ok=True)

        if bind_dir is None:
            bind_dir = work_dir
        elif isinstance(bind_dir, str):
            bind_dir = Path(bind_dir)

        client = docker.from_env()
        # Check if the image exists
        try:
            client.images.get(image)
        except ImageNotFound:
            logging.info(f"Pulling image {image}...")
            # Let the docker exception escape if this fails.
            client.images.pull(image)

        if container_name is None:
            container_name = f"autogen-code-exec-{uuid.uuid4()}"

        # build kwargs for docker.create
        base_kwargs: dict[str, Any] = {
            "image": image,
            "name": container_name,
            "entrypoint": "/bin/sh",
            "tty": True,
            "auto_remove": auto_remove,
            "volumes": {str(bind_dir.resolve()): {"bind": "/workspace", "mode": "rw"}},
            "working_dir": "/workspace",
        }

        if container_create_kwargs:
            for k in ("entrypoint", "volumes", "working_dir", "tty"):
                if k in container_create_kwargs:
                    logging.warning(
                        "DockerCommandLineCodeExecutor: overriding default %s=%s",
                        k,
                        container_create_kwargs[k],
                    )
            base_kwargs.update(container_create_kwargs)

        # Start a container from the image, read to exec commands later
        self._container = client.containers.create(**base_kwargs)
        self._container.start()

        _wait_for_ready(self._container)

        def cleanup() -> None:
            try:
                container = client.containers.get(container_name)
                container.stop()
            except docker.errors.NotFound:
                pass
            atexit.unregister(cleanup)

        if stop_container:
            atexit.register(cleanup)

        self._cleanup = cleanup

        # Check if the container is running
        if self._container.status != "running":
            raise ValueError(f"Failed to start container from image {image}. Logs: {self._container.logs()}")

        self._timeout = timeout
        self._work_dir: Path = work_dir
        self._bind_dir: Path = bind_dir
        self.execution_policies = self.DEFAULT_EXECUTION_POLICY.copy()
        if execution_policies is not None:
            self.execution_policies.update(execution_policies)

    @property
    def timeout(self) -> int:
        """(Experimental) The timeout for code execution."""
        return self._timeout

    @property
    def work_dir(self) -> Path:
        """(Experimental) The working directory for the code execution."""
        return self._work_dir

    @property
    def bind_dir(self) -> Path:
        """(Experimental) The binding directory for the code execution container."""
        return self._bind_dir

    @property
    def code_extractor(self) -> CodeExtractor:
        """(Experimental) Export a code extractor that can be used by an agent."""
        return MarkdownCodeExtractor()

    def execute_code_blocks(self, code_blocks: list[CodeBlock]) -> CommandLineCodeResult:
        """(Experimental) Execute the code blocks and return the result.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.

        Returns:
            CommandlineCodeResult: The result of the code execution.
        """
        if len(code_blocks) == 0:
            raise ValueError("No code blocks to execute.")

        outputs = []
        files = []
        last_exit_code = 0
        for code_block in code_blocks:
            lang = self.LANGUAGE_ALIASES.get(code_block.language.lower(), code_block.language.lower())
            if lang not in self.DEFAULT_EXECUTION_POLICY:
                outputs.append(f"Unsupported language {lang}\n")
                last_exit_code = 1
                break

            execute_code = self.execution_policies.get(lang, False)
            code = silence_pip(code_block.code, lang)

            # Check if there is a filename comment
            try:
                filename = _get_file_name_from_content(code, self._work_dir)
            except ValueError:
                outputs.append("Filename is not in the workspace")
                last_exit_code = 1
                break

            if not filename:
                filename = f"tmp_code_{md5(code.encode()).hexdigest()}.{lang}"

            code_path = self._work_dir / filename
            with code_path.open("w", encoding="utf-8") as fout:
                fout.write(code)
            files.append(code_path)

            if not execute_code:
                outputs.append(f"Code saved to {code_path!s}\n")
                continue

            command = ["timeout", str(self._timeout), _cmd(lang), filename]
            result = self._container.exec_run(command)
            exit_code = result.exit_code
            output = result.output.decode("utf-8")
            if exit_code == 124:
                output += "\n" + TIMEOUT_MSG
            outputs.append(output)

            last_exit_code = exit_code
            if exit_code != 0:
                break

        code_file = str(files[0]) if files else None
        return CommandLineCodeResult(exit_code=last_exit_code, output="".join(outputs), code_file=code_file)

    def restart(self) -> None:
        """(Experimental) Restart the code executor."""
        self._container.restart()
        if self._container.status != "running":
            raise ValueError(f"Failed to restart container. Logs: {self._container.logs()}")

    def stop(self) -> None:
        """(Experimental) Stop the code executor."""
        self._cleanup()

    def __enter__(self) -> Self:
        return self

    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: TracebackType | None,
    ) -> None:
        self.stop()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import re

from ..code_utils import CODE_BLOCK_PATTERN, UNKNOWN, content_str, infer_lang
from ..doc_utils import export_module
from ..types import UserMessageImageContentPart, UserMessageTextContentPart
from .base import CodeBlock, CodeExtractor

__all__ = ("MarkdownCodeExtractor",)


@export_module("autogen.coding")
class MarkdownCodeExtractor(CodeExtractor):
    """(Experimental) A class that extracts code blocks from a message using Markdown syntax."""

    def extract_code_blocks(
        self, message: str | list[UserMessageTextContentPart | UserMessageImageContentPart] | None
    ) -> list[CodeBlock]:
        """(Experimental) Extract code blocks from a message. If no code blocks are found,
        return an empty list.

        Args:
            message (str): The message to extract code blocks from.

        Returns:
            List[CodeBlock]: The extracted code blocks or an empty list.
        """
        text = content_str(message)
        match = re.findall(CODE_BLOCK_PATTERN, text, flags=re.DOTALL)
        if not match:
            return []
        code_blocks = []
        for lang, code in match:
            if lang == "":
                lang = infer_lang(code)
            if lang == UNKNOWN:
                lang = ""
            code_blocks.append(CodeBlock(code=code, language=lang))
        return code_blocks
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT

import functools
import importlib
import inspect
from collections.abc import Callable
from dataclasses import dataclass, field
from importlib.abc import SourceLoader
from textwrap import dedent, indent
from typing import Any, Generic, TypeVar, Union

from typing_extensions import ParamSpec

T = TypeVar("T")
P = ParamSpec("P")


def _to_code(func: Union["FunctionWithRequirements[T, P]", Callable[P, T], "FunctionWithRequirementsStr"]) -> str:
    if isinstance(func, FunctionWithRequirementsStr):
        return func.func

    code = inspect.getsource(func)
    # Strip the decorator
    if code.startswith("@"):
        code = code[code.index("\n") + 1 :]
    return code


@dataclass
class Alias:
    name: str
    alias: str


@dataclass
class ImportFromModule:
    module: str
    imports: list[str | Alias]


Import = str | ImportFromModule | Alias


def _import_to_str(im: Import) -> str:
    if isinstance(im, str):
        return f"import {im}"
    elif isinstance(im, Alias):
        return f"import {im.name} as {im.alias}"
    else:

        def to_str(i: str | Alias) -> str:
            if isinstance(i, str):
                return i
            else:
                return f"{i.name} as {i.alias}"

        imports = ", ".join(map(to_str, im.imports))
        return f"from {im.module} import {imports}"


class _StringLoader(SourceLoader):
    def __init__(self, data: str):
        self.data = data

    def get_source(self, fullname: str) -> str:
        return self.data

    def get_data(self, path: str) -> bytes:
        return self.data.encode("utf-8")

    def get_filename(self, fullname: str) -> str:
        return "<not a real path>/" + fullname + ".py"


@dataclass
class FunctionWithRequirementsStr:
    func: str
    _compiled_func: Callable[..., Any]
    _func_name: str
    python_packages: list[str] = field(default_factory=list)
    global_imports: list[Import] = field(default_factory=list)

    def __init__(self, func: str, python_packages: list[str] = [], global_imports: list[Import] = []):
        self.func = func
        self.python_packages = python_packages
        self.global_imports = global_imports

        module_name = "func_module"
        loader = _StringLoader(func)
        spec = importlib.util.spec_from_loader(module_name, loader)
        if spec is None:
            raise ValueError("Could not create spec")
        module = importlib.util.module_from_spec(spec)
        if spec.loader is None:
            raise ValueError("Could not create loader")

        try:
            spec.loader.exec_module(module)
        except Exception as e:
            raise ValueError(f"Could not compile function: {e}") from e

        functions = inspect.getmembers(module, inspect.isfunction)
        if len(functions) != 1:
            raise ValueError("The string must contain exactly one function")

        self._func_name, self._compiled_func = functions[0]

    def __call__(self, *args: Any, **kwargs: Any) -> None:
        raise NotImplementedError("String based function with requirement objects are not directly callable")


@dataclass
class FunctionWithRequirements(Generic[T, P]):
    func: Callable[P, T]
    python_packages: list[str] = field(default_factory=list)
    global_imports: list[Import] = field(default_factory=list)

    @classmethod
    def from_callable(
        cls, func: Callable[P, T], python_packages: list[str] = [], global_imports: list[Import] = []
    ) -> "FunctionWithRequirements[T, P]":
        return cls(python_packages=python_packages, global_imports=global_imports, func=func)

    @staticmethod
    def from_str(
        func: str, python_packages: list[str] = [], global_imports: list[Import] = []
    ) -> FunctionWithRequirementsStr:
        return FunctionWithRequirementsStr(func=func, python_packages=python_packages, global_imports=global_imports)

    # Type this based on F
    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> T:
        return self.func(*args, **kwargs)


def with_requirements(
    python_packages: list[str] = [], global_imports: list[Import] = []
) -> Callable[[Callable[P, T]], FunctionWithRequirements[T, P]]:
    """Decorate a function with package and import requirements

    Args:
        python_packages (List[str], optional): Packages required to function. Can include version info.. Defaults to [].
        global_imports (List[Import], optional): Required imports. Defaults to [].

    Returns:
        Callable[[Callable[P, T]], FunctionWithRequirements[T, P]]: The decorated function
    """

    def wrapper(func: Callable[P, T]) -> FunctionWithRequirements[T, P]:
        func_with_reqs = FunctionWithRequirements(
            python_packages=python_packages, global_imports=global_imports, func=func
        )

        functools.update_wrapper(func_with_reqs, func)
        return func_with_reqs

    return wrapper


def _build_python_functions_file(
    funcs: list[FunctionWithRequirements[Any, P] | Callable[..., Any] | FunctionWithRequirementsStr],
) -> str:
    # First collect all global imports
    global_imports: set[str] = set()
    for func in funcs:
        if isinstance(func, (FunctionWithRequirements, FunctionWithRequirementsStr)):
            global_imports.update(map(_import_to_str, func.global_imports))

    content = "\n".join(global_imports) + "\n\n"

    for func in funcs:
        content += _to_code(func) + "\n\n"

    return content


def to_stub(func: Callable[..., Any] | FunctionWithRequirementsStr) -> str:
    """Generate a stub for a function as a string

    Args:
        func (Callable[..., Any]): The function to generate a stub for

    Returns:
        str: The stub for the function
    """
    if isinstance(func, FunctionWithRequirementsStr):
        return to_stub(func._compiled_func)

    content = f"def {func.__name__}{inspect.signature(func)}:\n"
    docstring = func.__doc__

    if docstring:
        docstring = dedent(docstring)
        docstring = '"""' + docstring + '"""'
        docstring = indent(docstring, "    ")
        content += docstring + "\n"

    content += "    ..."
    return content
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Original portions of this file are derived from https://github.com/microsoft/autogen under the MIT License.
# SPDX-License-Identifier: MIT
from .base import CodeBlock, CodeExecutor, CodeExtractor, CodeResult
from .docker_commandline_code_executor import DockerCommandLineCodeExecutor
from .factory import CodeExecutorFactory
from .local_commandline_code_executor import LocalCommandLineCodeExecutor
from .markdown_code_extractor import MarkdownCodeExtractor

__all__ = [
    "CodeBlock",
    "CodeExecutor",
    "CodeExecutorFactory",
    "CodeExtractor",
    "CodeResult",
    "DockerCommandLineCodeExecutor",
    "LocalCommandLineCodeExecutor",
    "MarkdownCodeExtractor",
]

# Try to import YepCode executor and add to __all__ if available
try:
    from .yepcode_code_executor import YepCodeCodeExecutor, YepCodeCodeResult  # noqa: F401

    __all__.extend(["YepCodeCodeExecutor", "YepCodeCodeResult"])
except ImportError:
    pass
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from ..doc_utils import export_module
from .base import CodeExecutionConfig, CodeExecutor

__all__ = ("CodeExecutorFactory",)


@export_module("autogen.coding")
class CodeExecutorFactory:
    """(Experimental) A factory class for creating code executors."""

    @staticmethod
    def create(code_execution_config: CodeExecutionConfig) -> CodeExecutor:
        """(Experimental) Get a code executor based on the code execution config.

        Args:
            code_execution_config (Dict): The code execution config,
                which is a dictionary that must contain the key "executor".
                The value of the key "executor" can be either a string
                or an instance of CodeExecutor, in which case the code
                executor is returned directly.

        Returns:
            CodeExecutor: The code executor.

        Raises:
            ValueError: If the code executor is unknown or not specified.
        """
        executor = code_execution_config.get("executor")
        if isinstance(executor, CodeExecutor):
            # If the executor is already an instance of CodeExecutor, return it.
            return executor
        if executor == "ipython-embedded":
            from .jupyter.embedded_ipython_code_executor import EmbeddedIPythonCodeExecutor

            return EmbeddedIPythonCodeExecutor(**code_execution_config.get("ipython-embedded", {}))
        elif executor == "commandline-local":
            from .local_commandline_code_executor import LocalCommandLineCodeExecutor

            return LocalCommandLineCodeExecutor(**code_execution_config.get("commandline-local", {}))
        elif executor == "yepcode":
            try:
                from .yepcode_code_executor import YepCodeCodeExecutor
            except ImportError as e:
                raise ImportError(
                    "Missing dependencies for YepCodeCodeExecutor. Please install with: pip install ag2[yepcode]"
                ) from e

            return YepCodeCodeExecutor(**code_execution_config.get("yepcode", {}))
        else:
            raise ValueError(f"Unknown code executor {executor}")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/ag2ai/ag2 are under the MIT License.
# SPDX-License-Identifier: MIT
# Will return the filename relative to the workspace path
import re
from pathlib import Path

filename_patterns = [
    re.compile(r"^<!-- (filename:)?(.+?) -->", re.DOTALL),
    re.compile(r"^/\* (filename:)?(.+?) \*/", re.DOTALL),
    re.compile(r"^// (filename:)?(.+?)$", re.DOTALL),
    re.compile(r"^# (filename:)?(.+?)$", re.DOTALL),
]


# Raises ValueError if the file is not in the workspace
def _get_file_name_from_content(code: str, workspace_path: Path) -> str | None:
    first_line = code.split("\n")[0].strip()
    # TODO - support other languages
    for pattern in filename_patterns:
        matches = pattern.match(first_line)
        if matches is not None:
            filename = matches.group(2).strip()

            # Handle relative paths in the filename
            path = Path(filename)
            if not path.is_absolute():
                path = workspace_path / path
            path = path.resolve()
            # Throws an error if the file is not in the workspace
            relative = path.relative_to(workspace_path.resolve())
            return str(relative)
    return None


def silence_pip(code: str, lang: str) -> str:
    """Apply -qqq flag to pip install commands."""
    if lang == "python":
        regex = r"^! ?pip install"
    elif lang in ["bash", "shell", "sh", "pwsh", "powershell", "ps1"]:
        regex = r"^pip install"
    else:
        return code

    # Find lines that start with pip install and make sure "-qqq" flag is added.
    lines = code.split("\n")
    for i, line in enumerate(lines):
        # use regex to find lines that start with pip install.
        match = re.search(regex, line)
        if match is not None and "-qqq" not in line:
            lines[i] = line.replace(match.group(0), match.group(0) + " -qqq")
    return "\n".join(lines)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import logging
import os
import re
import subprocess
import sys
import warnings
from collections.abc import Callable
from hashlib import md5
from pathlib import Path
from string import Template
from types import SimpleNamespace
from typing import Any, ClassVar

from typing_extensions import ParamSpec

from ..code_utils import PYTHON_VARIANTS, TIMEOUT_MSG, WIN32, _cmd
from ..doc_utils import export_module
from .base import CodeBlock, CodeExecutor, CodeExtractor, CommandLineCodeResult
from .func_with_reqs import (
    FunctionWithRequirements,
    FunctionWithRequirementsStr,
    _build_python_functions_file,
    to_stub,
)
from .markdown_code_extractor import MarkdownCodeExtractor
from .utils import _get_file_name_from_content, silence_pip

__all__ = ("LocalCommandLineCodeExecutor",)

A = ParamSpec("A")


@export_module("autogen.coding")
class LocalCommandLineCodeExecutor(CodeExecutor):
    SUPPORTED_LANGUAGES: ClassVar[list[str]] = [
        "bash",
        "shell",
        "sh",
        "pwsh",
        "powershell",
        "ps1",
        "python",
        "javascript",
        "html",
        "css",
    ]
    DEFAULT_EXECUTION_POLICY: ClassVar[dict[str, bool]] = {
        "bash": True,
        "shell": True,
        "sh": True,
        "pwsh": True,
        "powershell": True,
        "ps1": True,
        "python": True,
        "javascript": False,
        "html": False,
        "css": False,
    }

    FUNCTION_PROMPT_TEMPLATE: ClassVar[
        str
    ] = """You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.

For example, if there was a function called `foo` you could import it by writing `from $module_name import foo`

$functions"""

    def __init__(
        self,
        timeout: int = 60,
        virtual_env_context: SimpleNamespace | None = None,
        work_dir: Path | str = Path(),
        functions: list[FunctionWithRequirements[Any, A] | Callable[..., Any] | FunctionWithRequirementsStr] = [],
        functions_module: str = "functions",
        execution_policies: dict[str, bool] | None = None,
    ):
        """(Experimental) A code executor class that executes or saves LLM generated code a local command line
        environment.

        **This will execute or save LLM generated code on the local machine.**

        Each code block is saved as a file in the working directory. Depending on the execution policy,
        the code may be executed in a separate process.
        The code blocks are executed or save in the order they are received.
        Command line code is sanitized against a list of dangerous commands to prevent self-destructive commands from being executed,
        which could potentially affect the user's environment. Supported languages include Python, shell scripts (bash, shell, sh),
        PowerShell (pwsh, powershell, ps1), HTML, CSS, and JavaScript.
        Execution policies determine whether each language's code blocks are executed or saved only.

        ## Execution with a Python virtual environment
        A python virtual env can be used to execute code and install dependencies. This has the added benefit of not polluting the
        base environment with unwanted modules.
        ```python
        from autogen.code_utils import create_virtual_env
        from autogen.coding import LocalCommandLineCodeExecutor

        venv_dir = ".venv"
        venv_context = create_virtual_env(venv_dir)

        executor = LocalCommandLineCodeExecutor(virtual_env_context=venv_context)
        ```

        Args:
            timeout (int): The timeout for code execution, default is 60 seconds.
            virtual_env_context (Optional[SimpleNamespace]): The virtual environment context to use.
            work_dir (Union[Path, str]): The working directory for code execution, defaults to the current directory.
            functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any], FunctionWithRequirementsStr]]): A list of callable functions available to the executor.
            functions_module (str): The module name under which functions are accessible.
            execution_policies (Optional[Dict[str, bool]]): A dictionary mapping languages to execution policies (True for execution, False for saving only). Defaults to class-wide DEFAULT_EXECUTION_POLICY.
        """
        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        if isinstance(work_dir, str):
            work_dir = Path(work_dir)

        if not functions_module.isidentifier():
            raise ValueError("Module name must be a valid Python identifier")

        self._functions_module = functions_module

        work_dir.mkdir(exist_ok=True)

        self._timeout = timeout
        self._work_dir: Path = work_dir
        self._virtual_env_context: SimpleNamespace | None = virtual_env_context

        self._functions = functions
        # Setup could take some time so we intentionally wait for the first code block to do it.
        if len(functions) > 0:
            self._setup_functions_complete = False
        else:
            self._setup_functions_complete = True

        self.execution_policies = self.DEFAULT_EXECUTION_POLICY.copy()
        if execution_policies is not None:
            self.execution_policies.update(execution_policies)

    def format_functions_for_prompt(self, prompt_template: str = FUNCTION_PROMPT_TEMPLATE) -> str:
        """(Experimental) Format the functions for a prompt.

        The template includes two variables:
        - `$module_name`: The module name.
        - `$functions`: The functions formatted as stubs with two newlines between each function.

        Args:
            prompt_template (str): The prompt template. Default is the class default.

        Returns:
            str: The formatted prompt.
        """
        template = Template(prompt_template)
        return template.substitute(
            module_name=self._functions_module,
            functions="\n\n".join([to_stub(func) for func in self._functions]),
        )

    @property
    def functions_module(self) -> str:
        """(Experimental) The module name for the functions."""
        return self._functions_module

    @property
    def functions(
        self,
    ) -> list[FunctionWithRequirements[Any, A] | Callable[..., Any] | FunctionWithRequirementsStr]:
        """(Experimental) The functions that are available to the code executor."""
        return self._functions

    @property
    def timeout(self) -> int:
        """(Experimental) The timeout for code execution."""
        return self._timeout

    @property
    def work_dir(self) -> Path:
        """(Experimental) The working directory for the code execution."""
        return self._work_dir

    @property
    def code_extractor(self) -> CodeExtractor:
        """(Experimental) Export a code extractor that can be used by an agent."""
        return MarkdownCodeExtractor()

    @staticmethod
    def sanitize_command(lang: str, code: str) -> None:
        """Sanitize the code block to prevent dangerous commands.
        This approach acknowledges that while Docker or similar
        containerization/sandboxing technologies provide a robust layer of security,
        not all users may have Docker installed or may choose not to use it.
        Therefore, having a baseline level of protection helps mitigate risks for users who,
        either out of choice or necessity, run code outside of a sandboxed environment.
        """
        dangerous_patterns = [
            (r"\brm\s+-rf\b", "Use of 'rm -rf' command is not allowed."),
            (r"\bmv\b.*?\s+/dev/null", "Moving files to /dev/null is not allowed."),
            (r"\bdd\b", "Use of 'dd' command is not allowed."),
            (r">\s*/dev/sd[a-z][1-9]?", "Overwriting disk blocks directly is not allowed."),
            (r":\(\)\{\s*:\|\:&\s*\};:", "Fork bombs are not allowed."),
        ]
        if lang in ["bash", "shell", "sh"]:
            for pattern, message in dangerous_patterns:
                if re.search(pattern, code):
                    raise ValueError(f"Potentially dangerous command detected: {message}")

    def _setup_functions(self) -> None:
        func_file_content = _build_python_functions_file(self._functions)
        func_file = self._work_dir / f"{self._functions_module}.py"
        func_file.write_text(func_file_content)

        # Collect requirements
        lists_of_packages = [x.python_packages for x in self._functions if isinstance(x, FunctionWithRequirements)]
        flattened_packages = [item for sublist in lists_of_packages for item in sublist]
        required_packages = list(set(flattened_packages))
        if len(required_packages) > 0:
            logging.info("Ensuring packages are installed in executor.")
            py_executable = self._virtual_env_context.env_exe if self._virtual_env_context else sys.executable
            cmd = [py_executable, "-m", "pip", "install"] + required_packages
            try:
                result = subprocess.run(
                    cmd,
                    cwd=self._work_dir,
                    capture_output=True,
                    text=True,
                    timeout=float(self._timeout),
                    encoding="utf-8",
                )
            except subprocess.TimeoutExpired as e:
                raise ValueError("Pip install timed out") from e
            if result.returncode != 0:
                raise ValueError(f"Pip install failed. {result.stdout}, {result.stderr}")
        # Attempt to load the function file to check for syntax errors, imports etc.
        exec_result = self._execute_code_dont_check_setup([CodeBlock(code=func_file_content, language="python")])
        if exec_result.exit_code != 0:
            raise ValueError(f"Functions failed to load: {exec_result.output}")
        self._setup_functions_complete = True

    def execute_code_blocks(self, code_blocks: list[CodeBlock]) -> CommandLineCodeResult:
        """(Experimental) Execute the code blocks and return the result.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.

        Returns:
            CommandLineCodeResult: The result of the code execution.
        """
        if not self._setup_functions_complete:
            self._setup_functions()
        return self._execute_code_dont_check_setup(code_blocks)

    def _execute_code_dont_check_setup(self, code_blocks: list[CodeBlock]) -> CommandLineCodeResult:
        logs_all = ""
        file_names = []
        for code_block in code_blocks:
            lang, code = code_block.language, code_block.code
            lang = lang.lower()

            LocalCommandLineCodeExecutor.sanitize_command(lang, code)
            code = silence_pip(code, lang)

            if lang in PYTHON_VARIANTS:
                lang = "python"

            if WIN32 and lang in ["sh", "shell"]:
                lang = "ps1"

            if lang not in self.SUPPORTED_LANGUAGES:
                # In case the language is not supported, we return an error message.
                exitcode = 1
                logs_all += "\n" + f"unknown language {lang}"
                break

            execute_code = self.execution_policies.get(lang, False)
            try:
                # Check if there is a filename comment
                filename = _get_file_name_from_content(code, self._work_dir)
            except ValueError:
                return CommandLineCodeResult(exit_code=1, output="Filename is not in the workspace")

            if filename is None:
                # create a file with an automatically generated name
                code_hash = md5(code.encode()).hexdigest()
                filename = f"tmp_code_{code_hash}.{'py' if lang.startswith('python') else lang}"
            written_file = (self._work_dir / filename).resolve()
            with written_file.open("w", encoding="utf-8") as f:
                f.write(code)
            file_names.append(written_file)

            if not execute_code:
                # Just return a message that the file is saved.
                logs_all += f"Code saved to {written_file!s}\n"
                exitcode = 0
                continue

            program = _cmd(lang)
            cmd = [program, str(written_file.absolute())]
            env = os.environ.copy()

            if self._virtual_env_context:
                virtual_env_abs_path = os.path.abspath(self._virtual_env_context.bin_path)
                path_with_virtualenv = rf"{virtual_env_abs_path}{os.pathsep}{env['PATH']}"
                env["PATH"] = path_with_virtualenv
                if WIN32:
                    activation_script = os.path.join(virtual_env_abs_path, "activate.bat")
                    cmd = [activation_script, "&&", *cmd]

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self._work_dir,
                    capture_output=True,
                    text=True,
                    timeout=float(self._timeout),
                    env=env,
                    encoding="utf-8",
                )
            except subprocess.TimeoutExpired:
                logs_all += "\n" + TIMEOUT_MSG
                # Same exit code as the timeout command on linux.
                exitcode = 124
                break

            logs_all += result.stderr
            logs_all += result.stdout
            exitcode = result.returncode

            if exitcode != 0:
                break

        code_file = str(file_names[0]) if len(file_names) > 0 else None
        return CommandLineCodeResult(exit_code=exitcode, output=logs_all, code_file=code_file)

    def restart(self) -> None:
        """(Experimental) Restart the code executor."""
        warnings.warn("Restarting local command line code executor is not supported. No action is taken.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
"""YepCode code executor implementation."""

import os
from collections.abc import Callable
from typing import ClassVar

from pydantic import Field

from ..doc_utils import export_module
from .base import CodeBlock, CodeExecutor, CodeExtractor, CodeResult
from .markdown_code_extractor import MarkdownCodeExtractor

try:
    from dotenv import load_dotenv

    _load_dotenv: Callable[[], bool] | None = load_dotenv
except ImportError:
    _load_dotenv = None

try:
    from yepcode_run import YepCodeApiConfig, YepCodeRun
except ImportError:
    YepCodeRun = None
    YepCodeApiConfig = None


@export_module("autogen.coding")
class YepCodeCodeResult(CodeResult):
    """A code result class for YepCode executor."""

    execution_id: str | None = Field(default=None, description="The YepCode execution ID for this result.")


@export_module("autogen.coding")
class YepCodeCodeExecutor(CodeExecutor):
    """A code executor class that executes code using YepCode's serverless runtime.

    This executor runs code in YepCode's secure, production-grade sandboxes.
    It supports Python and JavaScript execution with access to any external library with automatic discovery and installation.

    The executor executes code blocks serially in the order they are received.
    Each code block is executed in a separate YepCode execution environment.
    Currently supports Python and JavaScript languages.

    Args:
        api_token (Optional[str]): YepCode API token. If None, will try to get from YEPCODE_API_TOKEN environment variable.
        timeout (int): The timeout for code execution in seconds. Default is 60.
        remove_on_done (bool): Whether to remove the execution after completion. Default is False.
        sync_execution (bool): Whether to wait for execution to complete. Default is True.

    Raises:
        ImportError: If yepcode-run package is not installed.
        ValueError: If YepCode API token is not provided or timeout is invalid.
        RuntimeError: If YepCode runner initialization fails.
    """

    SUPPORTED_LANGUAGES: ClassVar[list[str]] = ["python", "javascript"]

    def __init__(
        self,
        api_token: str | None = None,
        timeout: int = 60,
        remove_on_done: bool = False,
        sync_execution: bool = True,
    ):
        if YepCodeRun is None or YepCodeApiConfig is None:
            raise ImportError(
                "Missing dependencies for YepCodeCodeExecutor. Please install with: pip install ag2[yepcode]"
            )

        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        # Load environment variables from .env file if dotenv is available
        if _load_dotenv is not None:
            _load_dotenv()

        # Get API token from parameter or environment
        self._api_token = api_token or os.getenv("YEPCODE_API_TOKEN")
        if not self._api_token:
            raise ValueError(
                "YepCode API token is required. Provide it via api_token parameter or YEPCODE_API_TOKEN environment variable."
            )

        self._timeout = timeout
        self._remove_on_done = remove_on_done
        self._sync_execution = sync_execution

        try:
            config = YepCodeApiConfig(api_token=self._api_token)
            self._runner = YepCodeRun(config)
        except Exception as e:
            raise RuntimeError(f"Failed to initialize YepCode runner: {str(e)}") from e

    @property
    def code_extractor(self) -> CodeExtractor:
        """(Experimental) Export a code extractor that can be used by an agent."""
        return MarkdownCodeExtractor()

    @property
    def timeout(self) -> int:
        """The timeout for code execution."""
        return self._timeout

    def _normalize_language(self, language: str) -> str:
        """Normalize language name to YepCode format."""
        lang = language.lower()
        if lang in ["js", "javascript"]:
            return "javascript"
        elif lang in ["python", "py"]:
            return "python"
        else:
            return lang

    def execute_code_blocks(self, code_blocks: list[CodeBlock]) -> YepCodeCodeResult:
        """Execute the code blocks and return the result.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.

        Returns:
            YepCodeCodeResult: The result of the code execution.
        """
        if not code_blocks:
            return YepCodeCodeResult(exit_code=0, output="")

        outputs: list[str] = []
        last_execution_id: str | None = None

        for code_block in code_blocks:
            lang = self._normalize_language(code_block.language)

            if lang not in ["python", "javascript"]:
                return YepCodeCodeResult(
                    exit_code=1,
                    output=f"Unsupported language: {code_block.language}. Supported languages: {', '.join(self.SUPPORTED_LANGUAGES)}",
                )

            try:
                # Execute code using YepCode
                execution = self._runner.run(
                    code_block.code,
                    {
                        "language": lang,
                        "removeOnDone": self._remove_on_done,
                        "timeout": self._timeout * 1000,  # Convert to milliseconds
                    },
                )

                last_execution_id = execution.id

                if self._sync_execution:
                    # Wait for execution to complete
                    execution.wait_for_done()

                    logs_output = ""
                    # Get logs
                    if execution.logs:
                        logs_output = "\n\nExecution logs:\n" + "\n".join([
                            f"{log.timestamp} - {log.level}: {log.message}" for log in execution.logs
                        ])

                    # Check if execution was successful
                    if execution.error:
                        output = f"Execution failed with error:\n{execution.error}{logs_output}"

                        return YepCodeCodeResult(exit_code=1, output=output, execution_id=execution.id)

                    # Get output
                    output = ""
                    if execution.return_value:
                        output = f"Execution result:\n{execution.return_value}"

                    output += logs_output

                    outputs.append(output)
                else:
                    outputs.append(f"Execution started with ID: {execution.id}")

            except Exception as e:
                return YepCodeCodeResult(
                    exit_code=1,
                    output=f"Error executing code: {str(e)}",
                    execution_id=last_execution_id,
                )

        return YepCodeCodeResult(exit_code=0, output="\n===\n".join(outputs), execution_id=last_execution_id)

    def restart(self) -> None:
        """Restart the code executor."""
        pass
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import datetime
import json
import uuid
from dataclasses import dataclass
from types import TracebackType
from typing import Any, cast

import requests
from requests.adapters import HTTPAdapter, Retry
from typing_extensions import Self

from ...doc_utils import export_module
from ...import_utils import optional_import_block, require_optional_import
from .base import JupyterConnectionInfo

with optional_import_block():
    import websocket
    from websocket import WebSocket


@export_module("autogen.coding.jupyter")
class JupyterClient:
    def __init__(self, connection_info: JupyterConnectionInfo):
        """(Experimental) A client for communicating with a Jupyter gateway server.

        Args:
            connection_info (JupyterConnectionInfo): Connection information
        """
        self._connection_info = connection_info
        self._session = requests.Session()
        retries = Retry(total=5, backoff_factor=0.1)
        self._session.mount("http://", HTTPAdapter(max_retries=retries))

    def _get_headers(self) -> dict[str, str]:
        if self._connection_info.token is None:
            return {}
        return {"Authorization": f"token {self._connection_info.token}"}

    def _get_api_base_url(self) -> str:
        protocol = "https" if self._connection_info.use_https else "http"
        port = f":{self._connection_info.port}" if self._connection_info.port else ""
        return f"{protocol}://{self._connection_info.host}{port}"

    def _get_ws_base_url(self) -> str:
        port = f":{self._connection_info.port}" if self._connection_info.port else ""
        return f"ws://{self._connection_info.host}{port}"

    def list_kernel_specs(self) -> dict[str, dict[str, str]]:
        response = self._session.get(f"{self._get_api_base_url()}/api/kernelspecs", headers=self._get_headers())
        return cast(dict[str, dict[str, str]], response.json())

    def list_kernels(self) -> list[dict[str, str]]:
        response = self._session.get(f"{self._get_api_base_url()}/api/kernels", headers=self._get_headers())
        return cast(list[dict[str, str]], response.json())

    def start_kernel(self, kernel_spec_name: str) -> str:
        """Start a new kernel.

        Args:
            kernel_spec_name (str): Name of the kernel spec to start

        Returns:
            str: ID of the started kernel
        """
        response = self._session.post(
            f"{self._get_api_base_url()}/api/kernels",
            headers=self._get_headers(),
            json={"name": kernel_spec_name},
        )
        return cast(str, response.json()["id"])

    def delete_kernel(self, kernel_id: str) -> None:
        response = self._session.delete(
            f"{self._get_api_base_url()}/api/kernels/{kernel_id}", headers=self._get_headers()
        )
        response.raise_for_status()

    def restart_kernel(self, kernel_id: str) -> None:
        response = self._session.post(
            f"{self._get_api_base_url()}/api/kernels/{kernel_id}/restart", headers=self._get_headers()
        )
        response.raise_for_status()

    @require_optional_import("websocket", "jupyter-executor")
    def get_kernel_client(self, kernel_id: str) -> JupyterKernelClient:
        ws_url = f"{self._get_ws_base_url()}/api/kernels/{kernel_id}/channels"
        ws = websocket.create_connection(ws_url, header=self._get_headers())
        return JupyterKernelClient(ws)


@require_optional_import("websocket", "jupyter-executor")
class JupyterKernelClient:
    """(Experimental) A client for communicating with a Jupyter kernel."""

    @dataclass
    class ExecutionResult:
        @dataclass
        class DataItem:
            mime_type: str
            data: str

        is_ok: bool
        output: str
        data_items: list[DataItem]

    def __init__(self, websocket: WebSocket):  # type: ignore[no-any-unimported]
        self._session_id: str = uuid.uuid4().hex
        self._websocket: WebSocket = websocket  # type: ignore[no-any-unimported]

    def __enter__(self) -> Self:
        return self

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None
    ) -> None:
        self.stop()

    def stop(self) -> None:
        self._websocket.close()

    def _send_message(self, *, content: dict[str, Any], channel: str, message_type: str) -> str:
        timestamp = datetime.datetime.now().isoformat()
        message_id = uuid.uuid4().hex
        message = {
            "header": {
                "username": "autogen",
                "version": "5.0",
                "session": self._session_id,
                "msg_id": message_id,
                "msg_type": message_type,
                "date": timestamp,
            },
            "parent_header": {},
            "channel": channel,
            "content": content,
            "metadata": {},
            "buffers": {},
        }
        self._websocket.send_text(json.dumps(message))
        return message_id

    def _receive_message(self, timeout_seconds: float | None) -> dict[str, Any] | None:
        self._websocket.settimeout(timeout_seconds)
        try:
            data = self._websocket.recv()
            if isinstance(data, bytes):
                data = data.decode("utf-8")
            return cast(dict[str, Any], json.loads(data))
        except websocket.WebSocketTimeoutException:
            return None

    def wait_for_ready(self, timeout_seconds: float | None = None) -> bool:
        message_id = self._send_message(content={}, channel="shell", message_type="kernel_info_request")
        while True:
            message = self._receive_message(timeout_seconds)
            # This means we timed out with no new messages.
            if message is None:
                return False
            if (
                message.get("parent_header", {}).get("msg_id") == message_id
                and message["msg_type"] == "kernel_info_reply"
            ):
                return True

    def execute(self, code: str, timeout_seconds: float | None = None) -> ExecutionResult:
        message_id = self._send_message(
            content={
                "code": code,
                "silent": False,
                "store_history": True,
                "user_expressions": {},
                "allow_stdin": False,
                "stop_on_error": True,
            },
            channel="shell",
            message_type="execute_request",
        )

        text_output = []
        data_output = []
        while True:
            message = self._receive_message(timeout_seconds)
            if message is None:
                return JupyterKernelClient.ExecutionResult(
                    is_ok=False, output="ERROR: Timeout waiting for output from code block.", data_items=[]
                )

            # Ignore messages that are not for this execution.
            if message.get("parent_header", {}).get("msg_id") != message_id:
                continue

            msg_type = message["msg_type"]
            content = message["content"]
            if msg_type in ["execute_result", "display_data"]:
                for data_type, data in content["data"].items():
                    if data_type == "text/plain":
                        text_output.append(data)
                    elif data_type.startswith("image/") or data_type == "text/html":
                        data_output.append(self.ExecutionResult.DataItem(mime_type=data_type, data=data))
                    else:
                        text_output.append(json.dumps(data))
            elif msg_type == "stream":
                text_output.append(content["text"])
            elif msg_type == "error":
                # Output is an error.
                return JupyterKernelClient.ExecutionResult(
                    is_ok=False,
                    output=f"ERROR: {content['ename']}: {content['evalue']}\n{content['traceback']}",
                    data_items=[],
                )
            if msg_type == "status" and content["execution_state"] == "idle":
                break

        return JupyterKernelClient.ExecutionResult(
            is_ok=True, output="\n".join([str(output) for output in text_output]), data_items=data_output
        )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Original portions of this file are derived from https://github.com/microsoft/autogen under the MIT License.
# SPDX-License-Identifier: MIT

from .base import JupyterConnectable, JupyterConnectionInfo
from .docker_jupyter_server import DockerJupyterServer
from .embedded_ipython_code_executor import EmbeddedIPythonCodeExecutor
from .jupyter_client import JupyterClient
from .jupyter_code_executor import JupyterCodeExecutor
from .local_jupyter_server import LocalJupyterServer

__all__ = [
    "DockerJupyterServer",
    "EmbeddedIPythonCodeExecutor",
    "JupyterClient",
    "JupyterCodeExecutor",
    "JupyterConnectable",
    "JupyterConnectionInfo",
    "LocalJupyterServer",
]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import atexit
import io
import logging
import secrets
import uuid
from pathlib import Path
from types import TracebackType

import docker
from typing_extensions import Self

from ...doc_utils import export_module
from ..docker_commandline_code_executor import _wait_for_ready
from .base import JupyterConnectable, JupyterConnectionInfo
from .import_utils import require_jupyter_kernel_gateway_installed
from .jupyter_client import JupyterClient


@require_jupyter_kernel_gateway_installed()
@export_module("autogen.coding.jupyter")
class DockerJupyterServer(JupyterConnectable):
    DEFAULT_DOCKERFILE = """FROM quay.io/jupyter/docker-stacks-foundation

SHELL ["/bin/bash", "-o", "pipefail", "-c"]

USER ${NB_UID}
RUN mamba install --yes jupyter_kernel_gateway ipykernel && \
    mamba clean --all -f -y && \
    fix-permissions "${CONDA_DIR}" && \
    fix-permissions "/home/${NB_USER}"

ENV TOKEN="UNSET"
CMD python -m jupyter kernelgateway --KernelGatewayApp.ip=0.0.0.0 \
    --KernelGatewayApp.port=8888 \
    --KernelGatewayApp.auth_token="${TOKEN}" \
    --JupyterApp.answer_yes=true \
    --JupyterWebsocketPersonality.list_kernels=true

EXPOSE 8888

WORKDIR "${HOME}"
"""

    class GenerateToken:
        pass

    def __init__(
        self,
        *,
        custom_image_name: str | None = None,
        container_name: str | None = None,
        auto_remove: bool = True,
        stop_container: bool = True,
        docker_env: dict[str, str] = {},
        token: str | GenerateToken = GenerateToken(),
    ):
        """Start a Jupyter kernel gateway server in a Docker container.

        Args:
            custom_image_name (Optional[str], optional): Custom image to use. If this is None,
                then the bundled image will be built and used. The default image is based on
                quay.io/jupyter/docker-stacks-foundation and extended to include jupyter_kernel_gateway
            container_name (Optional[str], optional): Name of the container to start.
                A name will be generated if None.
            auto_remove (bool, optional): If true the Docker container will be deleted
                when it is stopped.
            stop_container (bool, optional): If true the container will be stopped,
                either by program exit or using the context manager
            docker_env (Dict[str, str], optional): Extra environment variables to pass
                to the running Docker container.
            token (Union[str, GenerateToken], optional): Token to use for authentication.
                If GenerateToken is used, a random token will be generated. Empty string
                will be unauthenticated.
        """
        if container_name is None:
            container_name = f"autogen-jupyterkernelgateway-{uuid.uuid4()}"

        client = docker.from_env()
        if custom_image_name is None:
            image_name = "autogen-jupyterkernelgateway"
            # Make sure the image exists
            try:
                client.images.get(image_name)
            except docker.errors.ImageNotFound:
                # Build the image
                # Get this script directory
                here = Path(__file__).parent
                dockerfile = io.BytesIO(self.DEFAULT_DOCKERFILE.encode("utf-8"))
                logging.info(f"Image {image_name} not found. Building it now.")
                client.images.build(path=here, fileobj=dockerfile, tag=image_name)
                logging.info(f"Image {image_name} built successfully.")
        else:
            image_name = custom_image_name
            # Check if the image exists
            try:
                client.images.get(image_name)
            except docker.errors.ImageNotFound:
                raise ValueError(f"Custom image {image_name} does not exist")

        if isinstance(token, DockerJupyterServer.GenerateToken):
            self._token = secrets.token_hex(32)
        else:
            self._token = token

        # Run the container
        env = {"TOKEN": self._token}
        env.update(docker_env)
        container = client.containers.run(
            image_name,
            detach=True,
            auto_remove=auto_remove,
            environment=env,
            publish_all_ports=True,
            name=container_name,
        )
        _wait_for_ready(container)
        container_ports = container.ports
        self._port = int(container_ports["8888/tcp"][0]["HostPort"])
        self._container_id = container.id

        def cleanup() -> None:
            try:
                inner_container = client.containers.get(container.id)
                inner_container.stop()
            except docker.errors.NotFound:
                pass

            atexit.unregister(cleanup)

        if stop_container:
            atexit.register(cleanup)

        self._cleanup_func = cleanup
        self._stop_container = stop_container

    @property
    def connection_info(self) -> JupyterConnectionInfo:
        return JupyterConnectionInfo(host="127.0.0.1", use_https=False, port=self._port, token=self._token)

    def stop(self) -> None:
        self._cleanup_func()

    def get_client(self) -> JupyterClient:
        return JupyterClient(self.connection_info)

    def __enter__(self) -> Self:
        return self

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None
    ) -> None:
        self.stop()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import atexit
import json
import secrets
import signal
import subprocess
import sys
from types import TracebackType

from typing_extensions import Self

from ...doc_utils import export_module
from .base import JupyterConnectable, JupyterConnectionInfo
from .import_utils import require_jupyter_kernel_gateway_installed
from .jupyter_client import JupyterClient


@require_jupyter_kernel_gateway_installed()
@export_module("autogen.coding.jupyter")
class LocalJupyterServer(JupyterConnectable):
    class GenerateToken:
        pass

    def __init__(
        self,
        ip: str = "127.0.0.1",
        port: int | None = None,
        token: str | GenerateToken = GenerateToken(),
        log_file: str = "jupyter_gateway.log",
        log_level: str = "INFO",
        log_max_bytes: int = 1048576,
        log_backup_count: int = 3,
    ):
        """Runs a Jupyter Kernel Gateway server locally.

        Args:
            ip (str, optional): IP address to bind to. Defaults to "127.0.0.1".
            port (Optional[int], optional): Port to use, if None it automatically selects a port. Defaults to None.
            token (Union[str, GenerateToken], optional): Token to use for Jupyter server. By default will generate a token. Using None will use no token for authentication. Defaults to GenerateToken().
            log_file (str, optional): File for Jupyter Kernel Gateway logs. Defaults to "jupyter_gateway.log".
            log_level (str, optional): Level for Jupyter Kernel Gateway logs. Defaults to "INFO".
            log_max_bytes (int, optional): Max logfile size. Defaults to 1048576.
            log_backup_count (int, optional): Number of backups for rotating log. Defaults to 3.
        """

        # Check Jupyter gateway server is installed
        try:
            subprocess.run(
                [sys.executable, "-m", "jupyter", "kernelgateway", "--version"],
                check=True,
                capture_output=True,
                text=True,
            )
        except subprocess.CalledProcessError:
            raise ValueError(
                "Jupyter gateway server is not installed. Please install it with `pip install jupyter_kernel_gateway`."
            )

        self.ip: str = ip

        if isinstance(token, LocalJupyterServer.GenerateToken):
            token = secrets.token_hex(32)

        self.token: str = token
        self._subprocess: subprocess.Popen[str]
        logging_config = {
            "handlers": {
                "file": {
                    "class": "logging.handlers.RotatingFileHandler",
                    "level": log_level,
                    "maxBytes": log_max_bytes,
                    "backupCount": log_backup_count,
                    "filename": log_file,
                }
            },
            "loggers": {"KernelGatewayApp": {"level": log_level, "handlers": ["file", "console"]}},
        }

        # Run Jupyter gateway server with detached subprocess
        args = [
            sys.executable,
            "-m",
            "jupyter",
            "kernelgateway",
            "--KernelGatewayApp.ip",
            ip,
            "--KernelGatewayApp.auth_token",
            token,
            "--JupyterApp.answer_yes",
            "true",
            "--JupyterApp.logging_config",
            json.dumps(logging_config),
            "--JupyterWebsocketPersonality.list_kernels",
            "true",
        ]
        if port is not None:
            args.extend(["--KernelGatewayApp.port", str(port)])
            args.extend(["--KernelGatewayApp.port_retries", "0"])
        self._subprocess = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        # Satisfy mypy, we know this is not None because we passed PIPE
        assert self._subprocess.stderr is not None
        # Read stderr until we see "is available at" or the process has exited with an error
        stderr = ""
        while True:
            result = self._subprocess.poll()
            if result is not None:
                stderr += self._subprocess.stderr.read()
                raise ValueError(f"Jupyter gateway server failed to start with exit code: {result}. stderr:\n{stderr}")
            line = self._subprocess.stderr.readline()
            stderr += line

            if "ERROR:" in line:
                error_info = line.split("ERROR:")[1]
                raise ValueError(f"Jupyter gateway server failed to start. {error_info}")

            if "is available at" in line:
                # We need to extract what port it settled on
                # Example output:
                #   Jupyter Kernel Gateway 3.0.0 is available at http://127.0.0.1:8890
                if port is None:
                    port = int(line.split(":")[-1])
                self.port: int = port

                break

        # Poll the subprocess to check if it is still running
        result = self._subprocess.poll()
        if result is not None:
            raise ValueError(
                f"Jupyter gateway server failed to start. Please check the logs ({log_file}) for more information."
            )

        atexit.register(self.stop)

    def stop(self) -> None:
        if self._subprocess.poll() is None:
            if sys.platform == "win32":
                self._subprocess.send_signal(signal.CTRL_C_EVENT)
            else:
                self._subprocess.send_signal(signal.SIGINT)
            self._subprocess.wait()

    @property
    def connection_info(self) -> JupyterConnectionInfo:
        return JupyterConnectionInfo(host=self.ip, use_https=False, port=self.port, token=self.token)

    def get_client(self) -> JupyterClient:
        return JupyterClient(self.connection_info)

    def __enter__(self) -> Self:
        return self

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None
    ) -> None:
        self.stop()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import base64
import json
import os
import re
import uuid
from pathlib import Path
from queue import Empty
from typing import Any

from pydantic import BaseModel, Field, field_validator

from ...doc_utils import export_module
from ...import_utils import optional_import_block, require_optional_import
from ..base import CodeBlock, CodeExtractor, IPythonCodeResult
from ..markdown_code_extractor import MarkdownCodeExtractor
from .import_utils import require_jupyter_kernel_gateway_installed

with optional_import_block():
    from jupyter_client import KernelManager  # type: ignore[attr-defined]
    from jupyter_client.kernelspec import KernelSpecManager

__all__ = ["EmbeddedIPythonCodeExecutor"]


@require_optional_import("jupyter_client", "jupyter-executor")
@require_jupyter_kernel_gateway_installed()
@export_module("autogen.coding.jupyter")
class EmbeddedIPythonCodeExecutor(BaseModel):
    """(Experimental) A code executor class that executes code statefully using an embedded
    IPython kernel managed by this class.

    **This will execute LLM generated code on the local machine.**

    Each execution is stateful and can access variables created from previous
    executions in the same session. The kernel must be installed before using
    this class. The kernel can be installed using the following command:
    `python -m ipykernel install --user --name {kernel_name}`
    where `kernel_name` is the name of the kernel to install.
    """

    timeout: int = Field(default=60, ge=1, description="The timeout for code execution.")
    kernel_name: str = Field(default="python3", description="The kernel name to use. Make sure it is installed.")
    output_dir: str = Field(default=".", description="The directory to save output files.")

    @field_validator("output_dir")
    @classmethod
    def _output_dir_must_exist(cls, value: str) -> str:
        if not os.path.exists(value):
            raise ValueError(f"Output directory {value} does not exist.")
        return value

    def __init__(self, **kwargs: Any):
        super().__init__(**kwargs)
        # Check if the kernel is installed.
        if self.kernel_name not in KernelSpecManager().find_kernel_specs():
            raise ValueError(
                f"Kernel {self.kernel_name} is not installed. "
                "Please first install it with "
                f"`python -m ipykernel install --user --name {self.kernel_name}`."
            )
        self._kernel_manager = KernelManager(kernel_name=self.kernel_name)
        self._kernel_manager.start_kernel()
        self._kernel_client = self._kernel_manager.client()
        self._kernel_client.start_channels()
        self._timeout = self.timeout
        self._kernel_name = self.kernel_name
        self._output_dir = Path(self.output_dir)

    @property
    def code_extractor(self) -> CodeExtractor:
        """(Experimental) Export a code extractor that can be used by an agent."""
        return MarkdownCodeExtractor()

    def execute_code_blocks(self, code_blocks: list[CodeBlock]) -> IPythonCodeResult:
        """(Experimental) Execute a list of code blocks and return the result.

        This method executes a list of code blocks as cells in an IPython kernel
        managed by this class.
        See: https://jupyter-client.readthedocs.io/en/stable/messaging.html
        for the message protocol.

        Args:
            code_blocks (List[CodeBlock]): A list of code blocks to execute.

        Returns:
            IPythonCodeResult: The result of the code execution.
        """
        self._kernel_client.wait_for_ready()
        outputs = []
        output_files = []
        for code_block in code_blocks:
            code = self._process_code(code_block.code)
            self._kernel_client.execute(code, store_history=True)
            while True:
                try:
                    msg = self._kernel_client.get_iopub_msg(timeout=self._timeout)
                    msg_type = msg["msg_type"]
                    content = msg["content"]
                    if msg_type in ["execute_result", "display_data"]:
                        for data_type, data in content["data"].items():
                            if data_type == "text/plain":
                                # Output is a text.
                                outputs.append(data)
                            elif data_type.startswith("image/"):
                                # Output is an image.
                                path = self._save_image(data)
                                outputs.append(f"Image data saved to {path}")
                                output_files.append(path)
                            elif data_type == "text/html":
                                # Output is an html.
                                path = self._save_html(data)
                                outputs.append(f"HTML data saved to {path}")
                                output_files.append(path)
                            else:
                                # Output raw data.
                                outputs.append(json.dumps(data))
                    elif msg_type == "stream":
                        # Output is a text.
                        outputs.append(content["text"])
                    elif msg_type == "error":
                        # Output is an error.
                        return IPythonCodeResult(
                            exit_code=1,
                            output=f"ERROR: {content['ename']}: {content['evalue']}\n{content['traceback']}",
                        )
                    if msg_type == "status" and content["execution_state"] == "idle":
                        break
                # handle time outs.
                except Empty:
                    return IPythonCodeResult(
                        exit_code=1,
                        output=f"ERROR: Timeout waiting for output from code block: {code_block.code}",
                    )
        # We return the full output.
        return IPythonCodeResult(
            exit_code=0, output="\n".join([str(output) for output in outputs]), output_files=output_files
        )

    def restart(self) -> None:
        """(Experimental) Restart a new session."""
        self._kernel_client.stop_channels()
        self._kernel_manager.shutdown_kernel()
        self._kernel_manager = KernelManager(kernel_name=self.kernel_name)
        self._kernel_manager.start_kernel()
        self._kernel_client = self._kernel_manager.client()
        self._kernel_client.start_channels()

    def _save_image(self, image_data_base64: str) -> str:
        """Save image data to a file."""
        image_data = base64.b64decode(image_data_base64)
        # Randomly generate a filename.
        filename = f"{uuid.uuid4().hex}.png"
        path = os.path.join(self.output_dir, filename)
        with open(path, "wb") as f:
            f.write(image_data)
        return os.path.abspath(path)

    def _save_html(self, html_data: str) -> str:
        """Save html data to a file."""
        # Randomly generate a filename.
        filename = f"{uuid.uuid4().hex}.html"
        path = os.path.join(self.output_dir, filename)
        with open(path, "w") as f:
            f.write(html_data)
        return os.path.abspath(path)

    def _process_code(self, code: str) -> str:
        """Process code before execution."""
        # Find lines that start with `! pip install` and make sure "-qqq" flag is added.
        lines = code.split("\n")
        for i, line in enumerate(lines):
            # use regex to find lines that start with `! pip install` or `!pip install`.
            match = re.search(r"^! ?pip install", line)
            if match is not None and "-qqq" not in line:
                lines[i] = line.replace(match.group(0), match.group(0) + " -qqq")
        return "\n".join(lines)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import base64
import json
import os
import uuid
from pathlib import Path
from types import TracebackType

from typing_extensions import Self

from ...doc_utils import export_module
from ..base import CodeBlock, CodeExecutor, CodeExtractor, IPythonCodeResult
from ..markdown_code_extractor import MarkdownCodeExtractor
from ..utils import silence_pip
from .base import JupyterConnectable, JupyterConnectionInfo
from .jupyter_client import JupyterClient


@export_module("autogen.coding.jupyter")
class JupyterCodeExecutor(CodeExecutor):
    def __init__(
        self,
        jupyter_server: JupyterConnectable | JupyterConnectionInfo,
        kernel_name: str = "python3",
        timeout: int = 60,
        output_dir: Path | str = Path(),
    ):
        """(Experimental) A code executor class that executes code statefully using
        a Jupyter server supplied to this class.

        Each execution is stateful and can access variables created from previous
        executions in the same session.

        Args:
            jupyter_server (Union[JupyterConnectable, JupyterConnectionInfo]): The Jupyter server to use.
            timeout (int): The timeout for code execution, by default 60.
            kernel_name (str): The kernel name to use. Make sure it is installed.
                By default, it is "python3".
            output_dir (str): The directory to save output files, by default ".".
        """
        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        if isinstance(output_dir, str):
            output_dir = Path(output_dir)

        if not output_dir.exists():
            raise ValueError(f"Output directory {output_dir} does not exist.")

        if isinstance(jupyter_server, JupyterConnectable):
            self._connection_info = jupyter_server.connection_info
        elif isinstance(jupyter_server, JupyterConnectionInfo):
            self._connection_info = jupyter_server
        else:
            raise ValueError("jupyter_server must be a JupyterConnectable or JupyterConnectionInfo.")

        self._jupyter_client = JupyterClient(self._connection_info)
        available_kernels = self._jupyter_client.list_kernel_specs()
        if kernel_name not in available_kernels["kernelspecs"]:
            raise ValueError(f"Kernel {kernel_name} is not installed.")

        self._kernel_id = self._jupyter_client.start_kernel(kernel_name)
        self._kernel_name = kernel_name
        self._jupyter_kernel_client = self._jupyter_client.get_kernel_client(self._kernel_id)
        self._timeout = timeout
        self._output_dir = output_dir

    @property
    def code_extractor(self) -> CodeExtractor:
        """(Experimental) Export a code extractor that can be used by an agent."""
        return MarkdownCodeExtractor()

    def execute_code_blocks(self, code_blocks: list[CodeBlock]) -> IPythonCodeResult:
        """(Experimental) Execute a list of code blocks and return the result.

        This method executes a list of code blocks as cells in the Jupyter kernel.
        See: https://jupyter-client.readthedocs.io/en/stable/messaging.html
        for the message protocol.

        Args:
            code_blocks (List[CodeBlock]): A list of code blocks to execute.

        Returns:
            IPythonCodeResult: The result of the code execution.
        """
        self._jupyter_kernel_client.wait_for_ready()
        outputs = []
        output_files = []
        for code_block in code_blocks:
            code = silence_pip(code_block.code, code_block.language)
            result = self._jupyter_kernel_client.execute(code, timeout_seconds=self._timeout)
            if result.is_ok:
                outputs.append(result.output)
                for data in result.data_items:
                    if data.mime_type == "image/png":
                        path = self._save_image(data.data)
                        outputs.append(f"Image data saved to {path}")
                        output_files.append(path)
                    elif data.mime_type == "text/html":
                        path = self._save_html(data.data)
                        outputs.append(f"HTML data saved to {path}")
                        output_files.append(path)
                    else:
                        outputs.append(json.dumps(data.data))
            else:
                return IPythonCodeResult(
                    exit_code=1,
                    output=f"ERROR: {result.output}",
                )

        return IPythonCodeResult(
            exit_code=0, output="\n".join([str(output) for output in outputs]), output_files=output_files
        )

    def restart(self) -> None:
        """(Experimental) Restart a new session."""
        self._jupyter_client.restart_kernel(self._kernel_id)
        self._jupyter_kernel_client = self._jupyter_client.get_kernel_client(self._kernel_id)

    def _save_image(self, image_data_base64: str) -> str:
        """Save image data to a file."""
        image_data = base64.b64decode(image_data_base64)
        # Randomly generate a filename.
        filename = f"{uuid.uuid4().hex}.png"
        path = os.path.join(self._output_dir, filename)
        with open(path, "wb") as f:
            f.write(image_data)
        return os.path.abspath(path)

    def _save_html(self, html_data: str) -> str:
        """Save html data to a file."""
        # Randomly generate a filename.
        filename = f"{uuid.uuid4().hex}.html"
        path = os.path.join(self._output_dir, filename)
        with open(path, "w") as f:
            f.write(html_data)
        return os.path.abspath(path)

    def stop(self) -> None:
        """Stop the kernel."""
        self._jupyter_client.delete_kernel(self._kernel_id)

    def __enter__(self) -> Self:
        return self

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None
    ) -> None:
        self.stop()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import subprocess
from collections.abc import Callable
from functools import lru_cache
from logging import getLogger
from typing import TypeVar

from ...import_utils import patch_object

logger = getLogger(__name__)

__all__ = ["require_jupyter_kernel_gateway_installed", "skip_on_missing_jupyter_kernel_gateway"]


@lru_cache
def is_jupyter_kernel_gateway_installed() -> bool:
    """Check if jupyter-kernel-gateway is installed."""
    try:
        subprocess.run(
            ["jupyter", "kernelgateway", "--version"],
            capture_output=True,
            check=True,
        )
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        logger.warning(
            "jupyter-kernel-gateway is required for JupyterCodeExecutor, please install it with `pip install ag2[jupyter-executor]`"
        )
        return False


T = TypeVar("T")


def require_jupyter_kernel_gateway_installed() -> Callable[[T], T]:
    """Decorator that checks if jupyter-kernel-gateway is installed before function execution.

    Returns:
        Callable[[T], T]: A decorator function that either:
            - Returns the original function unchanged if jupyter-kernel-gateway is installed
            - Returns a patched version of the function that will raise a helpful error indicating the missing dependency when called
    """
    if is_jupyter_kernel_gateway_installed():

        def decorator(o: T) -> T:
            return o

    else:

        def decorator(o: T) -> T:
            return patch_object(o, missing_modules={}, dep_target="jupyter-executor")

    return decorator


def skip_on_missing_jupyter_kernel_gateway() -> Callable[[T], T]:
    """Decorator to skip a test if an optional module is missing"""
    # Add pytest.mark.jupyter_executor decorator
    mark_name = "jupyter_executor"

    if is_jupyter_kernel_gateway_installed():

        def decorator(o: T) -> T:
            import pytest

            pytest_mark_o = getattr(pytest.mark, mark_name)(o)
            return pytest_mark_o  # type: ignore[no-any-return]

    else:

        def decorator(o: T) -> T:
            import pytest

            pytest_mark_o = getattr(pytest.mark, mark_name)(o)
            return pytest.mark.skip(  # type: ignore[return-value,no-any-return]
                reason="jupyter-kernel-gateway is required for JupyterCodeExecutor, please install it with `pip install ag2[jupyter-executor]`"
            )(pytest_mark_o)

    return decorator
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from dataclasses import dataclass
from typing import Protocol, runtime_checkable

from ...doc_utils import export_module


@dataclass
@export_module("autogen.coding.jupyter")
class JupyterConnectionInfo:
    """(Experimental)"""

    host: str
    """`str` - Host of the Jupyter gateway server"""
    use_https: bool
    """`bool` - Whether to use HTTPS"""
    port: int | None = None
    """`Optional[int]` - Port of the Jupyter gateway server. If None, the default port is used"""
    token: str | None = None
    """`Optional[str]` - Token for authentication. If None, no token is used"""


@runtime_checkable
@export_module("autogen.coding.jupyter")
class JupyterConnectable(Protocol):
    """(Experimental)"""

    @property
    def connection_info(self) -> JupyterConnectionInfo:
        """Return the connection information for this connectable."""
        pass
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

from collections.abc import Mapping
from typing import Any, Literal, Protocol, TypedDict, runtime_checkable

from pydantic import BaseModel, Field

from ..doc_utils import export_module
from ..types import UserMessageImageContentPart, UserMessageTextContentPart

__all__ = ("CodeBlock", "CodeExecutionConfig", "CodeExecutor", "CodeExtractor", "CodeResult")


@export_module("autogen.coding")
class CodeBlock(BaseModel):
    """(Experimental) A class that represents a code block."""

    code: str = Field(description="The code to execute.")

    language: str = Field(description="The language of the code.")


@export_module("autogen.coding")
class CodeResult(BaseModel):
    """(Experimental) A class that represents the result of a code execution."""

    exit_code: int = Field(description="The exit code of the code execution.")

    output: str = Field(description="The output of the code execution.")


@export_module("autogen.coding")
class CodeExtractor(Protocol):
    """(Experimental) A code extractor class that extracts code blocks from a message."""

    def extract_code_blocks(
        self, message: str | list[UserMessageTextContentPart | UserMessageImageContentPart] | None
    ) -> list[CodeBlock]:
        """(Experimental) Extract code blocks from a message.

        Args:
            message (str): The message to extract code blocks from.

        Returns:
            List[CodeBlock]: The extracted code blocks.
        """
        ...  # pragma: no cover


@runtime_checkable
@export_module("autogen.coding")
class CodeExecutor(Protocol):
    """(Experimental) A code executor class that executes code blocks and returns the result."""

    @property
    def code_extractor(self) -> CodeExtractor:
        """(Experimental) The code extractor used by this code executor."""
        ...  # pragma: no cover

    def execute_code_blocks(self, code_blocks: list[CodeBlock]) -> CodeResult:
        """(Experimental) Execute code blocks and return the result.

        This method should be implemented by the code executor.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.

        Returns:
            CodeResult: The result of the code execution.
        """
        ...  # pragma: no cover

    def restart(self) -> None:
        """(Experimental) Restart the code executor.

        This method should be implemented by the code executor.

        This method is called when the agent is reset.
        """
        ...  # pragma: no cover


class IPythonCodeResult(CodeResult):
    """(Experimental) A code result class for IPython code executor."""

    output_files: list[str] = Field(
        default_factory=list,
        description="The list of files that the executed code blocks generated.",
    )


CodeExecutionConfig = TypedDict(
    "CodeExecutionConfig",
    {
        "executor": Literal["ipython-embedded", "commandline-local", "yepcode"] | CodeExecutor,
        "last_n_messages": int | Literal["auto"],
        "timeout": int,
        "use_docker": bool | str | list[str],
        "work_dir": str,
        "ipython-embedded": Mapping[str, Any],
        "commandline-local": Mapping[str, Any],
        "yepcode": Mapping[str, Any],
    },
    total=False,
)


class CommandLineCodeResult(CodeResult):
    """(Experimental) A code result class for command line code executor."""

    code_file: str | None = Field(
        default=None,
        description="The file that the executed code block was saved to.",
    )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

import logging
import sqlite3
import uuid
from collections.abc import Callable
from typing import TYPE_CHECKING, Any, Literal, TypeVar

from .logger.base_logger import BaseLogger, LLMConfig
from .logger.logger_factory import LoggerFactory

if TYPE_CHECKING:
    from openai import AzureOpenAI, OpenAI
    from openai.types.chat import ChatCompletion

    from . import Agent, ConversableAgent, OpenAIWrapper
    from .oai.anthropic import AnthropicClient
    from .oai.bedrock import BedrockClient
    from .oai.cerebras import CerebrasClient
    from .oai.cohere import CohereClient
    from .oai.gemini import GeminiClient
    from .oai.groq import GroqClient
    from .oai.mistral import MistralAIClient
    from .oai.ollama import OllamaClient
    from .oai.together import TogetherClient

logger = logging.getLogger(__name__)

autogen_logger = None
is_logging = False

F = TypeVar("F", bound=Callable[..., Any])


def start(
    logger: BaseLogger | None = None,
    logger_type: Literal["sqlite", "file"] = "sqlite",
    config: dict[str, Any] | None = None,
) -> str:
    """Start logging for the runtime.

    Args:
        logger (BaseLogger):    A logger instance
        logger_type (str):      The type of logger to use (default: sqlite)
        config (dict):          Configuration for the logger
    Returns:
        session_id (str(uuid.uuid4)):       a unique id for the logging session
    """
    global autogen_logger
    global is_logging

    autogen_logger = logger or LoggerFactory.get_logger(logger_type=logger_type, config=config)

    try:
        session_id = autogen_logger.start()
        is_logging = True
    except Exception as e:
        logger.error(f"[runtime logging] Failed to start logging: {e}")
    finally:
        return session_id


def log_chat_completion(
    invocation_id: uuid.UUID,
    client_id: int,
    wrapper_id: int,
    agent: str | Agent,
    request: dict[str, float | str | list[dict[str, str]]],
    response: str | ChatCompletion,
    is_cached: int,
    cost: float,
    start_time: str,
) -> None:
    if autogen_logger is None:
        logger.error("[runtime logging] log_chat_completion: autogen logger is None")
        return

    autogen_logger.log_chat_completion(
        invocation_id, client_id, wrapper_id, agent, request, response, is_cached, cost, start_time
    )


def log_new_agent(agent: ConversableAgent, init_args: dict[str, Any]) -> None:
    if autogen_logger is None:
        logger.error("[runtime logging] log_new_agent: autogen logger is None")
        return

    autogen_logger.log_new_agent(agent, init_args)


def log_event(source: str | Agent, name: str, **kwargs: dict[str, Any]) -> None:
    if autogen_logger is None:
        logger.error("[runtime logging] log_event: autogen logger is None")
        return

    autogen_logger.log_event(source, name, **kwargs)


def log_function_use(agent: str | Agent, function: F, args: dict[str, Any], returns: any):
    if autogen_logger is None:
        logger.error("[runtime logging] log_function_use: autogen logger is None")
        return

    autogen_logger.log_function_use(agent, function, args, returns)


def log_new_wrapper(wrapper: OpenAIWrapper, init_args: dict[str, LLMConfig | list[LLMConfig]]) -> None:
    if autogen_logger is None:
        logger.error("[runtime logging] log_new_wrapper: autogen logger is None")
        return

    autogen_logger.log_new_wrapper(wrapper, init_args)


def log_new_client(
    client: (
        AzureOpenAI
        | OpenAI
        | CerebrasClient
        | GeminiClient
        | AnthropicClient
        | MistralAIClient
        | TogetherClient
        | GroqClient
        | CohereClient
        | OllamaClient
        | BedrockClient
    ),
    wrapper: OpenAIWrapper,
    init_args: dict[str, Any],
) -> None:
    if autogen_logger is None:
        logger.error("[runtime logging] log_new_client: autogen logger is None")
        return

    autogen_logger.log_new_client(client, wrapper, init_args)


def stop() -> None:
    global is_logging
    if autogen_logger:
        autogen_logger.stop()
    is_logging = False


def get_connection() -> None | sqlite3.Connection:
    if autogen_logger is None:
        logger.error("[runtime logging] get_connection: autogen logger is None")
        return None

    return autogen_logger.get_connection()


def logging_enabled() -> bool:
    return is_logging
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

import inspect
from collections.abc import Awaitable, Callable, Sequence
from copy import deepcopy
from typing import Annotated, Any, TypeVar, get_args, get_origin

from typing_extensions import ParamSpec

from .._compat import ConfigDict, create_model, get_config_base
from ..dependencies import Depends
from ..library import CustomField
from ..utils import (
    get_typed_signature,
    is_async_gen_callable,
    is_coroutine_callable,
    is_gen_callable,
)
from .model import CallModel, ResponseModel

CUSTOM_ANNOTATIONS = (Depends, CustomField)


P = ParamSpec("P")
T = TypeVar("T")


def build_call_model(
    call: Callable[P, T] | Callable[P, Awaitable[T]],
    *,
    cast: bool = True,
    use_cache: bool = True,
    is_sync: bool | None = None,
    extra_dependencies: Sequence[Depends] = (),
    pydantic_config: ConfigDict | None = None,
) -> CallModel[P, T]:
    name = getattr(call, "__name__", type(call).__name__)

    is_call_async = is_coroutine_callable(call) or is_async_gen_callable(call)
    if is_sync is None:
        is_sync = not is_call_async
    else:
        assert not (is_sync and is_call_async), f"You cannot use async dependency `{name}` at sync main"

    typed_params, return_annotation = get_typed_signature(call)
    if (is_call_generator := is_gen_callable(call) or is_async_gen_callable(call)) and (
        return_args := get_args(return_annotation)
    ):
        return_annotation = return_args[0]

    class_fields: dict[str, tuple[Any, Any]] = {}
    dependencies: dict[str, CallModel[..., Any]] = {}
    custom_fields: dict[str, CustomField] = {}
    positional_args: list[str] = []
    keyword_args: list[str] = []
    var_positional_arg: str | None = None
    var_keyword_arg: str | None = None

    for param_name, param in typed_params.parameters.items():
        dep: Depends | None = None
        custom: CustomField | None = None

        if param.annotation is inspect.Parameter.empty:
            annotation = Any

        elif get_origin(param.annotation) is Annotated:
            annotated_args = get_args(param.annotation)
            type_annotation = annotated_args[0]

            custom_annotations = []
            regular_annotations = []
            for arg in annotated_args[1:]:
                if isinstance(arg, CUSTOM_ANNOTATIONS):
                    custom_annotations.append(arg)
                else:
                    regular_annotations.append(arg)

            assert len(custom_annotations) <= 1, (
                f"Cannot specify multiple `Annotated` Custom arguments for `{param_name}`!"
            )

            next_custom = next(iter(custom_annotations), None)
            if next_custom is not None:
                if isinstance(next_custom, Depends):
                    dep = next_custom
                elif isinstance(next_custom, CustomField):
                    custom = deepcopy(next_custom)
                else:  # pragma: no cover
                    raise AssertionError("unreachable")

                annotation = param.annotation if regular_annotations else type_annotation
            else:
                annotation = param.annotation
        else:
            annotation = param.annotation

        default: Any
        if param.kind == inspect.Parameter.VAR_POSITIONAL:
            default = ()
            var_positional_arg = param_name
        elif param.kind == inspect.Parameter.VAR_KEYWORD:
            default = {}
            var_keyword_arg = param_name
        elif param.default is inspect.Parameter.empty:
            default = Ellipsis
        else:
            default = param.default

        if isinstance(default, Depends):
            assert not dep, "You can not use `Depends` with `Annotated` and default both"
            dep, default = default, Ellipsis

        elif isinstance(default, CustomField):
            assert not custom, "You can not use `CustomField` with `Annotated` and default both"
            custom, default = default, Ellipsis

        else:
            class_fields[param_name] = (annotation, default)

        if dep:
            if not cast:
                dep.cast = False

            dependencies[param_name] = build_call_model(
                dep.dependency,
                cast=dep.cast,
                use_cache=dep.use_cache,
                is_sync=is_sync,
                pydantic_config=pydantic_config,
            )

            if dep.cast is True:
                class_fields[param_name] = (annotation, Ellipsis)

            keyword_args.append(param_name)

        elif custom:
            assert not (is_sync and is_coroutine_callable(custom.use)), (
                f"You cannot use async custom field `{type(custom).__name__}` at sync `{name}`"
            )

            custom.set_param_name(param_name)
            custom_fields[param_name] = custom

            if custom.cast is False:
                annotation = Any

            if custom.required:
                class_fields[param_name] = (annotation, default)

            else:
                class_fields[param_name] = class_fields.get(param_name, (annotation | None, None))

            keyword_args.append(param_name)

        else:
            if param.kind is param.KEYWORD_ONLY:
                keyword_args.append(param_name)
            elif param.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                positional_args.append(param_name)

    func_model = create_model(  # type: ignore[call-overload]
        name,
        __config__=get_config_base(pydantic_config),
        **class_fields,
    )

    response_model: type[ResponseModel[T]] | None = None
    if cast and return_annotation and return_annotation is not inspect.Parameter.empty:
        response_model = create_model(  # type: ignore[call-overload,assignment]
            "ResponseModel",
            __config__=get_config_base(pydantic_config),
            response=(return_annotation, Ellipsis),
        )

    return CallModel(
        call=call,
        model=func_model,
        response_model=response_model,
        params=class_fields,
        cast=cast,
        use_cache=use_cache,
        is_async=is_call_async,
        is_generator=is_call_generator,
        dependencies=dependencies,
        custom_fields=custom_fields,
        positional_args=positional_args,
        keyword_args=keyword_args,
        var_positional_arg=var_positional_arg,
        var_keyword_arg=var_keyword_arg,
        extra_dependencies=[
            build_call_model(
                d.dependency,
                cast=d.cast,
                use_cache=d.use_cache,
                is_sync=is_sync,
                pydantic_config=pydantic_config,
            )
            for d in extra_dependencies
        ],
    )
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from .build import build_call_model
from .model import CallModel

__all__ = (
    "CallModel",
    "build_call_model",
)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from collections import namedtuple
from collections.abc import Awaitable, Callable, Generator, Iterable, Sequence
from contextlib import AsyncExitStack, ExitStack
from functools import partial
from inspect import Parameter, unwrap
from itertools import chain
from typing import (
    Any,
    Generic,
    TypeVar,
)

import anyio
from typing_extensions import ParamSpec

from .._compat import BaseModel, ExceptionGroup, get_aliases
from ..library import CustomField
from ..utils import (
    async_map,
    is_async_gen_callable,
    is_coroutine_callable,
    is_gen_callable,
    run_async,
    solve_generator_async,
    solve_generator_sync,
)

P = ParamSpec("P")
T = TypeVar("T")


PriorityPair = namedtuple("PriorityPair", ("call", "dependencies_number", "dependencies_names"))


class ResponseModel(BaseModel, Generic[T]):
    response: T


class CallModel(Generic[P, T]):
    call: Callable[P, T] | Callable[P, Awaitable[T]]
    is_async: bool
    is_generator: bool
    model: type[BaseModel] | None
    response_model: type[ResponseModel[T]] | None

    params: dict[str, tuple[Any, Any]]
    alias_arguments: tuple[str, ...]

    dependencies: dict[str, "CallModel[..., Any]"]
    extra_dependencies: Iterable["CallModel[..., Any]"]
    sorted_dependencies: tuple[tuple["CallModel[..., Any]", int], ...]
    custom_fields: dict[str, CustomField]
    keyword_args: tuple[str, ...]
    positional_args: tuple[str, ...]
    var_positional_arg: str | None
    var_keyword_arg: str | None

    # Dependencies and custom fields
    use_cache: bool
    cast: bool

    __slots__ = (
        "call",
        "is_async",
        "is_generator",
        "model",
        "response_model",
        "params",
        "alias_arguments",
        "keyword_args",
        "positional_args",
        "var_positional_arg",
        "var_keyword_arg",
        "dependencies",
        "extra_dependencies",
        "sorted_dependencies",
        "custom_fields",
        "use_cache",
        "cast",
    )

    @property
    def call_name(self) -> str:
        call = unwrap(self.call)
        return getattr(call, "__name__", type(call).__name__)

    @property
    def flat_params(self) -> dict[str, tuple[Any, Any]]:
        params = self.params
        for d in (*self.dependencies.values(), *self.extra_dependencies):
            params.update(d.flat_params)
        return params

    @property
    def flat_dependencies(
        self,
    ) -> dict[
        Callable[..., Any],
        tuple[
            "CallModel[..., Any]",
            tuple[Callable[..., Any], ...],
        ],
    ]:
        flat: dict[
            Callable[..., Any],
            tuple[
                CallModel[..., Any],
                tuple[Callable[..., Any], ...],
            ],
        ] = {}

        for i in (*self.dependencies.values(), *self.extra_dependencies):
            flat.update({
                i.call: (
                    i,
                    tuple(j.call for j in i.dependencies.values()),
                )
            })

            flat.update(i.flat_dependencies)

        return flat

    def __init__(
        self,
        /,
        call: Callable[P, T] | Callable[P, Awaitable[T]],
        model: type[BaseModel] | None,
        params: dict[str, tuple[Any, Any]],
        response_model: type[ResponseModel[T]] | None = None,
        use_cache: bool = True,
        cast: bool = True,
        is_async: bool = False,
        is_generator: bool = False,
        dependencies: dict[str, "CallModel[..., Any]"] | None = None,
        extra_dependencies: Iterable["CallModel[..., Any]"] | None = None,
        keyword_args: list[str] | None = None,
        positional_args: list[str] | None = None,
        var_positional_arg: str | None = None,
        var_keyword_arg: str | None = None,
        custom_fields: dict[str, CustomField] | None = None,
    ):
        self.call = call
        self.model = model

        if model:
            self.alias_arguments = get_aliases(model)
        else:  # pragma: no cover
            self.alias_arguments = ()

        self.keyword_args = tuple(keyword_args or ())
        self.positional_args = tuple(positional_args or ())
        self.var_positional_arg = var_positional_arg
        self.var_keyword_arg = var_keyword_arg
        self.response_model = response_model
        self.use_cache = use_cache
        self.cast = cast
        self.is_async = is_async or is_coroutine_callable(call) or is_async_gen_callable(call)
        self.is_generator = is_generator or is_gen_callable(call) or is_async_gen_callable(call)

        self.dependencies = dependencies or {}
        self.extra_dependencies = extra_dependencies or ()
        self.custom_fields = custom_fields or {}

        sorted_dep: list[CallModel[..., Any]] = []
        flat = self.flat_dependencies
        for calls in flat.values():
            _sort_dep(sorted_dep, calls, flat)

        self.sorted_dependencies = tuple((i, len(i.sorted_dependencies)) for i in sorted_dep if i.use_cache)
        for name in chain(self.dependencies.keys(), self.custom_fields.keys()):
            params.pop(name, None)
        self.params = params

    def _solve(
        self,
        /,
        *args: tuple[Any, ...],
        cache_dependencies: dict[
            Callable[P, T] | Callable[P, Awaitable[T]],
            T,
        ],
        dependency_overrides: dict[
            Callable[P, T] | Callable[P, Awaitable[T]], Callable[P, T] | Callable[P, Awaitable[T]]
        ]
        | None = None,
        **kwargs: dict[str, Any],
    ) -> Generator[
        tuple[
            Sequence[Any],
            dict[str, Any],
            Callable[..., Any],
        ],
        Any,
        T,
    ]:
        if dependency_overrides:
            call = dependency_overrides.get(self.call, self.call)
            assert self.is_async or not is_coroutine_callable(call), (
                f"You cannot use async dependency `{self.call_name}` at sync main"
            )

        else:
            call = self.call

        if self.use_cache and call in cache_dependencies:
            return cache_dependencies[call]

        kw: dict[str, Any] = {}

        for arg in self.keyword_args:
            if (v := kwargs.pop(arg, Parameter.empty)) is not Parameter.empty:
                kw[arg] = v

        if self.var_keyword_arg is not None:
            kw[self.var_keyword_arg] = kwargs
        else:
            kw.update(kwargs)

        for arg in self.positional_args:
            if args:
                kw[arg], args = args[0], args[1:]
            else:
                break

        keyword_args: Iterable[str]
        if self.var_positional_arg is not None:
            kw[self.var_positional_arg] = args
            keyword_args = self.keyword_args

        else:
            keyword_args = self.keyword_args + self.positional_args
            for arg in keyword_args:
                if not self.cast and arg in self.params:
                    kw[arg] = self.params[arg][1]

                if not args:
                    break

                if arg not in self.dependencies:
                    kw[arg], args = args[0], args[1:]

        solved_kw: dict[str, Any]
        solved_kw = yield args, kw, call

        args_: Sequence[Any]
        if self.cast:
            assert self.model, "Cast should be used only with model"
            casted_model = self.model(**solved_kw)

            kwargs_ = {arg: getattr(casted_model, arg, solved_kw.get(arg)) for arg in keyword_args}
            if self.var_keyword_arg:
                kwargs_.update(getattr(casted_model, self.var_keyword_arg, {}))

            if self.var_positional_arg is not None:
                args_ = [getattr(casted_model, arg, solved_kw.get(arg)) for arg in self.positional_args]
                args_.extend(getattr(casted_model, self.var_positional_arg, ()))
            else:
                args_ = ()

        else:
            kwargs_ = {arg: solved_kw.get(arg) for arg in keyword_args}

            args_ = tuple(map(solved_kw.get, self.positional_args)) if self.var_positional_arg is None else ()

        response: T
        response = yield args_, kwargs_, call

        if self.cast and not self.is_generator:
            response = self._cast_response(response)

        if self.use_cache:  # pragma: no branch
            cache_dependencies[call] = response

        return response

    def _cast_response(self, /, value: Any) -> Any:
        if self.response_model is not None:
            return self.response_model(response=value).response
        else:
            return value

    def solve(
        self,
        /,
        *args: Any,
        stack: ExitStack,
        cache_dependencies: dict[
            Callable[P, T] | Callable[P, Awaitable[T]],
            T,
        ],
        dependency_overrides: dict[
            Callable[P, T] | Callable[P, Awaitable[T]], Callable[P, T] | Callable[P, Awaitable[T]]
        ]
        | None = None,
        nested: bool = False,
        **kwargs: Any,
    ) -> T:
        cast_gen = self._solve(
            *args,
            cache_dependencies=cache_dependencies,
            dependency_overrides=dependency_overrides,
            **kwargs,
        )
        try:
            args, kwargs, _ = next(cast_gen)  # type: ignore[assignment]
        except StopIteration as e:
            cached_value: T = e.value
            return cached_value

        # Heat cache and solve extra dependencies
        for dep, _ in self.sorted_dependencies:
            dep.solve(
                *args,
                stack=stack,
                cache_dependencies=cache_dependencies,
                dependency_overrides=dependency_overrides,
                nested=True,
                **kwargs,
            )

        # Always get from cache
        for dep in self.extra_dependencies:
            dep.solve(
                *args,
                stack=stack,
                cache_dependencies=cache_dependencies,
                dependency_overrides=dependency_overrides,
                nested=True,
                **kwargs,
            )

        for dep_arg, dep in self.dependencies.items():
            kwargs[dep_arg] = dep.solve(
                stack=stack,
                cache_dependencies=cache_dependencies,
                dependency_overrides=dependency_overrides,
                nested=True,
                **kwargs,
            )

        for custom in self.custom_fields.values():
            if custom.field:
                custom.use_field(kwargs)
            else:
                kwargs = custom.use(**kwargs)

        final_args, final_kwargs, call = cast_gen.send(kwargs)

        if self.is_generator and nested:
            response = solve_generator_sync(
                *final_args,
                call=call,
                stack=stack,
                **final_kwargs,
            )

        else:
            response = call(*final_args, **final_kwargs)

        try:
            cast_gen.send(response)
        except StopIteration as e:
            value: T = e.value

            if not self.cast or nested or not self.is_generator:
                return value

            else:
                return map(self._cast_response, value)  # type: ignore[no-any-return, call-overload]

        raise AssertionError("unreachable")

    async def asolve(
        self,
        /,
        *args: Any,
        stack: AsyncExitStack,
        cache_dependencies: dict[
            Callable[P, T] | Callable[P, Awaitable[T]],
            T,
        ],
        dependency_overrides: dict[
            Callable[P, T] | Callable[P, Awaitable[T]], Callable[P, T] | Callable[P, Awaitable[T]]
        ]
        | None = None,
        nested: bool = False,
        **kwargs: Any,
    ) -> T:
        cast_gen = self._solve(
            *args,
            cache_dependencies=cache_dependencies,
            dependency_overrides=dependency_overrides,
            **kwargs,
        )
        try:
            args, kwargs, _ = next(cast_gen)  # type: ignore[assignment]
        except StopIteration as e:
            cached_value: T = e.value
            return cached_value

        # Heat cache and solve extra dependencies
        dep_to_solve: list[Callable[..., Awaitable[Any]]] = []
        try:
            async with anyio.create_task_group() as tg:
                for dep, subdep in self.sorted_dependencies:
                    solve = partial(
                        dep.asolve,
                        *args,
                        stack=stack,
                        cache_dependencies=cache_dependencies,
                        dependency_overrides=dependency_overrides,
                        nested=True,
                        **kwargs,
                    )
                    if not subdep:
                        tg.start_soon(solve)
                    else:
                        dep_to_solve.append(solve)
        except ExceptionGroup as exgr:
            for ex in exgr.exceptions:
                raise ex from None

        for i in dep_to_solve:
            await i()

        # Always get from cache
        for dep in self.extra_dependencies:
            await dep.asolve(
                *args,
                stack=stack,
                cache_dependencies=cache_dependencies,
                dependency_overrides=dependency_overrides,
                nested=True,
                **kwargs,
            )

        for dep_arg, dep in self.dependencies.items():
            kwargs[dep_arg] = await dep.asolve(
                stack=stack,
                cache_dependencies=cache_dependencies,
                dependency_overrides=dependency_overrides,
                nested=True,
                **kwargs,
            )

        custom_to_solve: list[CustomField] = []

        try:
            async with anyio.create_task_group() as tg:
                for custom in self.custom_fields.values():
                    if custom.field:
                        tg.start_soon(run_async, custom.use_field, kwargs)
                    else:
                        custom_to_solve.append(custom)

        except ExceptionGroup as exgr:
            for ex in exgr.exceptions:
                raise ex from None

        for j in custom_to_solve:
            kwargs = await run_async(j.use, **kwargs)

        final_args, final_kwargs, call = cast_gen.send(kwargs)

        if self.is_generator and nested:
            response = await solve_generator_async(
                *final_args,
                call=call,
                stack=stack,
                **final_kwargs,
            )
        else:
            response = await run_async(call, *final_args, **final_kwargs)

        try:
            cast_gen.send(response)
        except StopIteration as e:
            value: T = e.value

            if not self.cast or nested or not self.is_generator:
                return value

            else:
                return async_map(self._cast_response, value)  # type: ignore[return-value, arg-type]

        raise AssertionError("unreachable")


def _sort_dep(
    collector: list["CallModel[..., Any]"],
    items: tuple[
        "CallModel[..., Any]",
        tuple[Callable[..., Any], ...],
    ],
    flat: dict[
        Callable[..., Any],
        tuple[
            "CallModel[..., Any]",
            tuple[Callable[..., Any], ...],
        ],
    ],
) -> None:
    model, calls = items

    if model in collector:
        return

    if not calls:
        position = -1

    else:
        for i in calls:
            sub_model, _ = flat[i]
            if sub_model not in collector:  # pragma: no branch
                _sort_dep(collector, flat[i], flat)

        position = max(collector.index(flat[i][0]) for i in calls)

    collector.insert(position + 1, model)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from collections.abc import AsyncIterator, Callable, Iterator, Sequence
from contextlib import AsyncExitStack, ExitStack
from functools import partial, wraps
from typing import (
    Any,
    Protocol,
    TypeVar,
    cast,
    overload,
)

from typing_extensions import ParamSpec

from ._compat import ConfigDict
from .core import CallModel, build_call_model
from .dependencies import dependency_provider, model

P = ParamSpec("P")
T = TypeVar("T")


def Depends(  # noqa: N802
    dependency: Callable[P, T],
    *,
    use_cache: bool = True,
    cast: bool = True,
) -> Any:
    return model.Depends(
        dependency=dependency,
        use_cache=use_cache,
        cast=cast,
    )


class _InjectWrapper(Protocol[P, T]):
    def __call__(
        self,
        func: Callable[P, T],
        model: CallModel[P, T] | None = None,
    ) -> Callable[P, T]: ...


@overload
def inject(  # pragma: no cover
    func: None,
    *,
    cast: bool = True,
    extra_dependencies: Sequence[model.Depends] = (),
    pydantic_config: ConfigDict | None = None,
    dependency_overrides_provider: Any | None = dependency_provider,
    wrap_model: Callable[[CallModel[P, T]], CallModel[P, T]] = lambda x: x,
) -> _InjectWrapper[P, T]: ...


@overload
def inject(  # pragma: no cover
    func: Callable[P, T],
    *,
    cast: bool = True,
    extra_dependencies: Sequence[model.Depends] = (),
    pydantic_config: ConfigDict | None = None,
    dependency_overrides_provider: Any | None = dependency_provider,
    wrap_model: Callable[[CallModel[P, T]], CallModel[P, T]] = lambda x: x,
) -> Callable[P, T]: ...


def inject(
    func: Callable[P, T] | None = None,
    *,
    cast: bool = True,
    extra_dependencies: Sequence[model.Depends] = (),
    pydantic_config: ConfigDict | None = None,
    dependency_overrides_provider: Any | None = dependency_provider,
    wrap_model: Callable[[CallModel[P, T]], CallModel[P, T]] = lambda x: x,
) -> Callable[P, T] | _InjectWrapper[P, T]:
    decorator = _wrap_inject(
        dependency_overrides_provider=dependency_overrides_provider,
        wrap_model=wrap_model,
        extra_dependencies=extra_dependencies,
        cast=cast,
        pydantic_config=pydantic_config,
    )

    if func is None:
        return decorator

    else:
        return decorator(func)


def _wrap_inject(
    dependency_overrides_provider: Any | None,
    wrap_model: Callable[
        [CallModel[P, T]],
        CallModel[P, T],
    ],
    extra_dependencies: Sequence[model.Depends],
    cast: bool,
    pydantic_config: ConfigDict | None,
) -> _InjectWrapper[P, T]:
    if (
        dependency_overrides_provider
        and getattr(dependency_overrides_provider, "dependency_overrides", None) is not None
    ):
        overrides = dependency_overrides_provider.dependency_overrides
    else:
        overrides = None

    def func_wrapper(
        func: Callable[P, T],
        model: CallModel[P, T] | None = None,
    ) -> Callable[P, T]:
        if model is None:
            real_model = wrap_model(
                build_call_model(
                    call=func,
                    extra_dependencies=extra_dependencies,
                    cast=cast,
                    pydantic_config=pydantic_config,
                )
            )
        else:
            real_model = model

        if real_model.is_async:
            injected_wrapper: Callable[P, T]

            if real_model.is_generator:
                injected_wrapper = partial(solve_async_gen, real_model, overrides)  # type: ignore[assignment]

            else:

                @wraps(func)
                async def injected_wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
                    async with AsyncExitStack() as stack:
                        r = await real_model.asolve(
                            *args,
                            stack=stack,
                            dependency_overrides=overrides,
                            cache_dependencies={},
                            nested=False,
                            **kwargs,
                        )
                        return r

                    raise AssertionError("unreachable")

        else:
            if real_model.is_generator:
                injected_wrapper = partial(solve_gen, real_model, overrides)  # type: ignore[assignment]

            else:

                @wraps(func)
                def injected_wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
                    with ExitStack() as stack:
                        r = real_model.solve(
                            *args,
                            stack=stack,
                            dependency_overrides=overrides,
                            cache_dependencies={},
                            nested=False,
                            **kwargs,
                        )
                        return r

                    raise AssertionError("unreachable")

        return injected_wrapper

    return func_wrapper


class solve_async_gen:  # noqa: N801
    _iter: AsyncIterator[Any] | None = None

    def __init__(
        self,
        model: "CallModel[..., Any]",
        overrides: Any | None,
        *args: Any,
        **kwargs: Any,
    ):
        self.call = model
        self.args = args
        self.kwargs = kwargs
        self.overrides = overrides

    def __aiter__(self) -> "solve_async_gen":
        self._iter = None
        self.stack = AsyncExitStack()
        return self

    async def __anext__(self) -> Any:
        if self._iter is None:
            stack = self.stack = AsyncExitStack()
            await self.stack.__aenter__()
            self._iter = cast(
                AsyncIterator[Any],
                (
                    await self.call.asolve(
                        *self.args,
                        stack=stack,
                        dependency_overrides=self.overrides,
                        cache_dependencies={},
                        nested=False,
                        **self.kwargs,
                    )
                ).__aiter__(),
            )

        try:
            r = await self._iter.__anext__()
        except StopAsyncIteration as e:
            await self.stack.__aexit__(None, None, None)
            raise e
        else:
            return r


class solve_gen:  # noqa: N801
    _iter: Iterator[Any] | None = None

    def __init__(
        self,
        model: "CallModel[..., Any]",
        overrides: Any | None,
        *args: Any,
        **kwargs: Any,
    ):
        self.call = model
        self.args = args
        self.kwargs = kwargs
        self.overrides = overrides

    def __iter__(self) -> "solve_gen":
        self._iter = None
        self.stack = ExitStack()
        return self

    def __next__(self) -> Any:
        if self._iter is None:
            stack = self.stack = ExitStack()
            self.stack.__enter__()
            self._iter = cast(
                Iterator[Any],
                iter(
                    self.call.solve(
                        *self.args,
                        stack=stack,
                        dependency_overrides=self.overrides,
                        cache_dependencies={},
                        nested=False,
                        **self.kwargs,
                    )
                ),
            )

        try:
            r = next(self._iter)
        except StopIteration as e:
            self.stack.__exit__(None, None, None)
            raise e
        else:
            return r
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from collections.abc import Callable, Iterator
from contextlib import contextmanager
from typing import Any


class Provider:
    dependency_overrides: dict[Callable[..., Any], Callable[..., Any]]

    def __init__(self) -> None:
        self.dependency_overrides = {}

    def clear(self) -> None:
        self.dependency_overrides = {}

    def override(
        self,
        original: Callable[..., Any],
        override: Callable[..., Any],
    ) -> None:
        self.dependency_overrides[original] = override

    @contextmanager
    def scope(
        self,
        original: Callable[..., Any],
        override: Callable[..., Any],
    ) -> Iterator[None]:
        self.dependency_overrides[original] = override
        yield
        self.dependency_overrides.pop(original, None)


dependency_provider = Provider()
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from .model import Depends
from .provider import Provider, dependency_provider

__all__ = (
    "Depends",
    "Provider",
    "dependency_provider",
)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from collections.abc import Callable
from typing import Any


class Depends:
    use_cache: bool
    cast: bool

    def __init__(
        self,
        dependency: Callable[..., Any],
        *,
        use_cache: bool = True,
        cast: bool = True,
    ) -> None:
        self.dependency = dependency
        self.use_cache = use_cache
        self.cast = cast

    def __repr__(self) -> str:
        attr = getattr(self.dependency, "__name__", type(self.dependency).__name__)
        cache = "" if self.use_cache else ", use_cache=False"
        return f"{self.__class__.__name__}({attr}{cache})"
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from .dependencies import Provider, dependency_provider
from .use import Depends, inject

__all__ = (
    "Depends",
    "Provider",
    "dependency_provider",
    "inject",
)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from .model import CustomField

__all__ = ("CustomField",)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from abc import ABC
from typing import Any, TypeVar

Cls = TypeVar("Cls", bound="CustomField")


class CustomField(ABC):
    param_name: str | None
    cast: bool
    required: bool

    __slots__ = (
        "cast",
        "param_name",
        "required",
        "field",
    )

    def __init__(
        self,
        *,
        cast: bool = True,
        required: bool = True,
    ) -> None:
        self.cast = cast
        self.param_name = None
        self.required = required
        self.field = False

    def set_param_name(self: Cls, name: str) -> Cls:
        self.param_name = name
        return self

    def use(self, /, **kwargs: Any) -> dict[str, Any]:
        assert self.param_name, "You should specify `param_name` before using"
        return kwargs

    def use_field(self, kwargs: dict[str, Any]) -> None:
        raise NotImplementedError("You should implement `use_field` method.")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

import asyncio
import functools
import inspect
from collections.abc import AsyncGenerator, AsyncIterable, Awaitable, Callable
from contextlib import AbstractContextManager, AsyncExitStack, ExitStack, asynccontextmanager, contextmanager
from typing import TYPE_CHECKING, Annotated, Any, ForwardRef, TypeVar, cast, get_args, get_origin

import anyio
from typing_extensions import (
    ParamSpec,
)

from ._compat import evaluate_forwardref

if TYPE_CHECKING:
    from types import FrameType

P = ParamSpec("P")
T = TypeVar("T")


def asyncify(func: Callable[P, T]) -> Callable[P, Awaitable[T]]:
    if is_coroutine_callable(func):
        return cast(Callable[P, Awaitable[T]], func)

    async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
        return await run_in_threadpool(func, *args, **kwargs)

    return wrapper


async def run_async(
    func: Callable[P, T] | Callable[P, Awaitable[T]],
    *args: P.args,
    **kwargs: P.kwargs,
) -> T:
    if is_coroutine_callable(func):
        return await cast(Callable[P, Awaitable[T]], func)(*args, **kwargs)
    else:
        return await run_in_threadpool(cast(Callable[P, T], func), *args, **kwargs)


async def run_in_threadpool(func: Callable[P, T], *args: P.args, **kwargs: P.kwargs) -> T:
    if kwargs:
        func = functools.partial(func, **kwargs)
    return await anyio.to_thread.run_sync(func, *args)


async def solve_generator_async(
    *sub_args: Any, call: Callable[..., Any], stack: AsyncExitStack, **sub_values: Any
) -> Any:
    if is_gen_callable(call):
        cm = contextmanager_in_threadpool(contextmanager(call)(**sub_values))
    elif is_async_gen_callable(call):  # pragma: no branch
        cm = asynccontextmanager(call)(*sub_args, **sub_values)
    return await stack.enter_async_context(cm)


def solve_generator_sync(*sub_args: Any, call: Callable[..., Any], stack: ExitStack, **sub_values: Any) -> Any:
    cm = contextmanager(call)(*sub_args, **sub_values)
    return stack.enter_context(cm)


def get_typed_signature(call: Callable[..., Any]) -> tuple[inspect.Signature, Any]:
    signature = inspect.signature(call)

    locals = collect_outer_stack_locals()

    # We unwrap call to get the original unwrapped function
    call = inspect.unwrap(call)

    globalns = getattr(call, "__globals__", {})
    typed_params = [
        inspect.Parameter(
            name=param.name,
            kind=param.kind,
            default=param.default,
            annotation=get_typed_annotation(
                param.annotation,
                globalns,
                locals,
            ),
        )
        for param in signature.parameters.values()
    ]

    return inspect.Signature(typed_params), get_typed_annotation(
        signature.return_annotation,
        globalns,
        locals,
    )


def collect_outer_stack_locals() -> dict[str, Any]:
    frame = inspect.currentframe()

    frames: list[FrameType] = []
    while frame is not None:
        if "fast_depends" not in frame.f_code.co_filename:
            frames.append(frame)
        frame = frame.f_back

    locals = {}
    for f in frames[::-1]:
        locals.update(f.f_locals)

    return locals


def get_typed_annotation(
    annotation: Any,
    globalns: dict[str, Any],
    locals: dict[str, Any],
) -> Any:
    if isinstance(annotation, str):
        annotation = ForwardRef(annotation)

    if isinstance(annotation, ForwardRef):
        annotation = evaluate_forwardref(annotation, globalns, locals)

    if get_origin(annotation) is Annotated and (args := get_args(annotation)):
        solved_args = [get_typed_annotation(x, globalns, locals) for x in args]
        annotation.__origin__, annotation.__metadata__ = solved_args[0], tuple(solved_args[1:])

    return annotation


@asynccontextmanager
async def contextmanager_in_threadpool(
    cm: AbstractContextManager[T],
) -> AsyncGenerator[T, None]:
    exit_limiter = anyio.CapacityLimiter(1)
    try:
        yield await run_in_threadpool(cm.__enter__)
    except Exception as e:
        ok = bool(await anyio.to_thread.run_sync(cm.__exit__, type(e), e, None, limiter=exit_limiter))
        if not ok:  # pragma: no branch
            raise e
    else:
        await anyio.to_thread.run_sync(cm.__exit__, None, None, None, limiter=exit_limiter)


def is_gen_callable(call: Callable[..., Any]) -> bool:
    if inspect.isgeneratorfunction(call):
        return True
    dunder_call = getattr(call, "__call__", None)  # noqa: B004
    return inspect.isgeneratorfunction(dunder_call)


def is_async_gen_callable(call: Callable[..., Any]) -> bool:
    if inspect.isasyncgenfunction(call):
        return True
    dunder_call = getattr(call, "__call__", None)  # noqa: B004
    return inspect.isasyncgenfunction(dunder_call)


def is_coroutine_callable(call: Callable[..., Any]) -> bool:
    if inspect.isclass(call):
        return False

    if asyncio.iscoroutinefunction(call):
        return True

    dunder_call = getattr(call, "__call__", None)  # noqa: B004
    return asyncio.iscoroutinefunction(dunder_call)


async def async_map(func: Callable[..., T], async_iterable: AsyncIterable[Any]) -> AsyncIterable[T]:
    async for i in async_iterable:
        yield func(i)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

import sys
from typing import Any

from pydantic import BaseModel, create_model
from pydantic.version import VERSION as PYDANTIC_VERSION

__all__ = (
    "PYDANTIC_V2",
    "BaseModel",
    "ConfigDict",
    "ExceptionGroup",
    "create_model",
    "evaluate_forwardref",
    "get_config_base",
)


PYDANTIC_V2 = PYDANTIC_VERSION.startswith("2.")

default_pydantic_config = {"arbitrary_types_allowed": True}

evaluate_forwardref: Any
# isort: off
if PYDANTIC_V2:
    from pydantic import ConfigDict
    from pydantic._internal._typing_extra import (  # type: ignore[no-redef]
        eval_type_lenient as evaluate_forwardref,
    )

    def model_schema(model: type[BaseModel]) -> dict[str, Any]:
        return model.model_json_schema()

    def get_config_base(config_data: ConfigDict | None = None) -> ConfigDict:
        return config_data or ConfigDict(**default_pydantic_config)  # type: ignore[typeddict-item]

    def get_aliases(model: type[BaseModel]) -> tuple[str, ...]:
        return tuple(f.alias or name for name, f in model.model_fields.items())

    class CreateBaseModel(BaseModel):
        """Just to support FastStream < 0.3.7."""

        model_config = ConfigDict(arbitrary_types_allowed=True)

else:
    from pydantic.typing import evaluate_forwardref as evaluate_forwardref  # type: ignore[no-redef]
    from pydantic.config import get_config, ConfigDict, BaseConfig

    def get_config_base(config_data: ConfigDict | None = None) -> type[BaseConfig]:  # type: ignore[misc,no-any-unimported]
        return get_config(config_data or ConfigDict(**default_pydantic_config))  # type: ignore[typeddict-item,no-any-unimported,no-any-return]

    def model_schema(model: type[BaseModel]) -> dict[str, Any]:
        return model.schema()

    def get_aliases(model: type[BaseModel]) -> tuple[str, ...]:
        return tuple(f.alias or name for name, f in model.__fields__.items())  # type: ignore[attr-defined]

    class CreateBaseModel(BaseModel):  # type: ignore[no-redef]
        """Just to support FastStream < 0.3.7."""

        class Config:
            arbitrary_types_allowed = True


if sys.version_info < (3, 11):
    from exceptiongroup import ExceptionGroup as ExceptionGroup

else:
    ExceptionGroup = ExceptionGroup
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/https://github.com/Lancetnik/FastDepends are under the MIT License.
# SPDX-License-Identifier: MIT

from typing import Any

from ._compat import PYDANTIC_V2, create_model, model_schema
from .core import CallModel


def get_schema(
    call: CallModel[Any, Any],
    embed: bool = False,
    resolve_refs: bool = False,
) -> dict[str, Any]:
    assert call.model, "Call should has a model"
    params_model = create_model(  # type: ignore[call-overload]
        call.model.__name__, **call.flat_params
    )

    body: dict[str, Any] = model_schema(params_model)

    if not call.flat_params:
        body = {"title": body["title"], "type": "null"}

    if resolve_refs:
        pydantic_key = "$defs" if PYDANTIC_V2 else "definitions"
        body = _move_pydantic_refs(body, pydantic_key)
        body.pop(pydantic_key, None)

    if embed and len(body["properties"]) == 1:
        body = list(body["properties"].values())[0]

    return body


def _move_pydantic_refs(original: Any, key: str, refs: dict[str, Any] | None = None) -> Any:
    if not isinstance(original, dict):
        return original

    data = original.copy()

    if refs is None:
        raw_refs = data.get(key, {})
        refs = _move_pydantic_refs(raw_refs, key, raw_refs)

    name: str | None = None
    for k in data:
        if k == "$ref":
            name = data[k].replace(f"#/{key}/", "")

        elif isinstance(data[k], dict):
            data[k] = _move_pydantic_refs(data[k], key, refs)

        elif isinstance(data[k], list):
            for i in range(len(data[k])):
                data[k][i] = _move_pydantic_refs(data[k][i], key, refs)

    if name:
        assert refs, "Smth wrong"
        data = refs[name]

    return data
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from abc import ABC
from collections.abc import Callable
from copy import deepcopy
from typing import TYPE_CHECKING, Any, Literal, Union
from uuid import UUID

from pydantic import BaseModel, field_validator, model_serializer
from termcolor import colored

from autogen.agentchat.group import ContextVariables

from ..agentchat.agent import LLMMessageType
from ..code_utils import content_str
from ..import_utils import optional_import_block, require_optional_import
from ..oai.client import OpenAIWrapper
from .base_event import BaseEvent, wrap_event

with optional_import_block() as result:
    from PIL.Image import Image

IS_PIL_AVAILABLE = result.is_successful

if TYPE_CHECKING:
    from ..agentchat.agent import Agent
    from ..coding.base import CodeBlock


__all__ = [
    "ClearAgentsHistoryEvent",
    "ClearConversableAgentHistoryEvent",
    "ConversableAgentUsageSummaryEvent",
    "ConversableAgentUsageSummaryNoCostIncurredEvent",
    "ExecuteCodeBlockEvent",
    "ExecuteFunctionEvent",
    "FunctionCallEvent",
    "FunctionResponseEvent",
    "GenerateCodeExecutionReplyEvent",
    "GroupChatResumeEvent",
    "GroupChatRunChatEvent",
    "PostCarryoverProcessingEvent",
    "SelectSpeakerEvent",
    "SpeakerAttemptFailedMultipleAgentsEvent",
    "SpeakerAttemptFailedNoAgentsEvent",
    "SpeakerAttemptSuccessfulEvent",
    "TerminationAndHumanReplyNoInputEvent",
    "TerminationEvent",
    "TextEvent",
    "ToolCallEvent",
    "ToolResponseEvent",
]

EventRole = Literal["assistant", "function", "tool"]


class BasePrintReceivedEvent(BaseEvent, ABC):
    content: str | int | float | bool
    sender: str
    recipient: str

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        f(f"{colored(self.sender, 'yellow')} (to {self.recipient}):\n", flush=True)


@wrap_event
class FunctionResponseEvent(BasePrintReceivedEvent):
    name: str | None = None
    role: EventRole = "function"
    content: str | int | float | bool

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        id = self.name or "No id found"
        func_print = f"***** Response from calling {self.role} ({id}) *****"
        f(colored(func_print, "green"), flush=True)
        f(self.content, flush=True)
        f(colored("*" * len(func_print), "green"), flush=True)

        f("\n", "-" * 80, flush=True, sep="")


class ToolResponse(BaseModel):
    tool_call_id: str | None = None
    role: EventRole = "tool"
    content: str | int | float | bool

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        id = self.tool_call_id or "No id found"
        tool_print = f"***** Response from calling {self.role} ({id}) *****"
        f(colored(tool_print, "green"), flush=True)
        f(self.content, flush=True)
        f(colored("*" * len(tool_print), "green"), flush=True)


@wrap_event
class ToolResponseEvent(BasePrintReceivedEvent):
    role: EventRole = "tool"
    tool_responses: list[ToolResponse]
    content: str | int | float | bool

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        for tool_response in self.tool_responses:
            tool_response.print(f)
            f("\n", "-" * 80, flush=True, sep="")


class FunctionCall(BaseModel):
    name: str | None = None
    arguments: str | None = None

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        name = self.name or "(No function name found)"
        arguments = self.arguments or "(No arguments found)"

        func_print = f"***** Suggested function call: {name} *****"
        f(colored(func_print, "green"), flush=True)
        f(
            "Arguments: \n",
            arguments,
            flush=True,
            sep="",
        )
        f(colored("*" * len(func_print), "green"), flush=True)


@wrap_event
class FunctionCallEvent(BasePrintReceivedEvent):
    content: str | int | float | bool | None = None  # type: ignore [assignment]
    function_call: FunctionCall

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        if self.content is not None:
            f(self.content, flush=True)

        self.function_call.print(f)

        f("\n", "-" * 80, flush=True, sep="")


class ToolCall(BaseModel):
    id: str | None = None
    function: FunctionCall
    type: str

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        id = self.id or "No tool call id found"

        name = self.function.name or "(No function name found)"
        arguments = self.function.arguments or "(No arguments found)"

        func_print = f"***** Suggested tool call ({id}): {name} *****"
        f(colored(func_print, "green"), flush=True)
        f(
            "Arguments: \n",
            arguments,
            flush=True,
            sep="",
        )
        f(colored("*" * len(func_print), "green"), flush=True)


@wrap_event
class ToolCallEvent(BasePrintReceivedEvent):
    content: str | int | float | bool | None = None  # type: ignore [assignment]
    refusal: str | None = None
    role: EventRole | None = None
    audio: str | None = None
    function_call: FunctionCall | None = None
    tool_calls: list[ToolCall]

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        if self.content is not None:
            f(self.content, flush=True)

        for tool_call in self.tool_calls:
            tool_call.print(f)

        f("\n", "-" * 80, flush=True, sep="")


@wrap_event
class TextEvent(BasePrintReceivedEvent):
    content: str | int | float | bool | list[dict[str, str | dict[str, Any]]] | None = None  # type: ignore [assignment]

    @classmethod
    @require_optional_import("PIL", "unknown")
    def _replace_pil_image_with_placeholder(cls, image_url: dict[str, Any]) -> None:
        if "url" in image_url and isinstance(image_url["url"], Image):
            image_url["url"] = "<image>"

    @field_validator("content", mode="before")
    @classmethod
    def validate_and_encode_content(
        cls, content: str | int | float | bool | list[dict[str, str | dict[str, Any]]] | None
    ) -> str | int | float | bool | list[dict[str, str | dict[str, Any]]] | None:
        if not IS_PIL_AVAILABLE:
            return content

        if not isinstance(content, list):
            return content

        for item in content:
            if isinstance(item, dict) and "image_url" in item and isinstance(item["image_url"], dict):
                cls._replace_pil_image_with_placeholder(item["image_url"])

        return content

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print
        super().print(f)

        if self.content is not None:
            f(content_str(self.content), flush=True)  # type: ignore [arg-type]

        f("\n", "-" * 80, flush=True, sep="")


def create_received_event_model(
    *, uuid: UUID | None = None, event: dict[str, Any], sender: "Agent", recipient: "Agent"
) -> FunctionResponseEvent | ToolResponseEvent | FunctionCallEvent | ToolCallEvent | TextEvent:
    role = event.get("role")
    if role == "function":
        return FunctionResponseEvent(**event, sender=sender.name, recipient=recipient.name, uuid=uuid)
    if role == "tool":
        return ToolResponseEvent(**event, sender=sender.name, recipient=recipient.name, uuid=uuid)

    # Role is neither function nor tool

    if event.get("function_call"):
        return FunctionCallEvent(
            **event,
            sender=sender.name,
            recipient=recipient.name,
            uuid=uuid,
        )

    if event.get("tool_calls"):
        return ToolCallEvent(
            **event,
            sender=sender.name,
            recipient=recipient.name,
            uuid=uuid,
        )

    # Now message is a simple content message
    content = event.get("content")
    allow_format_str_template = (
        recipient.llm_config.get("allow_format_str_template", False) if recipient.llm_config else False  # type: ignore [attr-defined]
    )
    if content is not None and "context" in event:
        content = OpenAIWrapper.instantiate(
            content,  # type: ignore [arg-type]
            event["context"],
            allow_format_str_template,
        )

    return TextEvent(
        content=content,
        sender=sender.name,
        recipient=recipient.name,
        uuid=uuid,
    )


@wrap_event
class PostCarryoverProcessingEvent(BaseEvent):
    carryover: str | list[str | dict[str, Any] | Any]
    message: str
    verbose: bool = False

    sender: str
    recipient: str
    summary_method: str
    summary_args: dict[str, Any] | None = None
    max_turns: int | None = None

    def __init__(self, *, uuid: UUID | None = None, chat_info: dict[str, Any]):
        carryover = chat_info.get("carryover", "")
        message = chat_info.get("message")
        verbose = chat_info.get("verbose", False)

        sender = chat_info["sender"].name if hasattr(chat_info["sender"], "name") else chat_info["sender"]
        recipient = chat_info["recipient"].name if hasattr(chat_info["recipient"], "name") else chat_info["recipient"]
        summary_args = chat_info.get("summary_args")
        max_turns = chat_info.get("max_turns")

        # Fix Callable in chat_info
        summary_method = chat_info.get("summary_method", "")
        if callable(summary_method):
            summary_method = summary_method.__name__

        print_message = ""
        if isinstance(message, str):
            print_message = message
        elif callable(message):
            print_message = "Callable: " + message.__name__
        elif isinstance(message, dict):
            print_message = "Dict: " + str(message)
        elif message is None:
            print_message = "None"

        super().__init__(
            uuid=uuid,
            carryover=carryover,
            message=print_message,
            verbose=verbose,
            summary_method=summary_method,
            summary_args=summary_args,
            max_turns=max_turns,
            sender=sender,
            recipient=recipient,
        )

    @model_serializer
    def serialize_model(self) -> dict[str, Any]:
        return {
            "uuid": self.uuid,
            "chat_info": {
                "carryover": self.carryover,
                "message": self.message,
                "verbose": self.verbose,
                "sender": self.sender,
                "recipient": self.recipient,
                "summary_method": self.summary_method,
                "summary_args": self.summary_args,
                "max_turns": self.max_turns,
            },
        }

    def _process_carryover(self) -> str:
        if not isinstance(self.carryover, list):
            return self.carryover

        print_carryover = []
        for carryover_item in self.carryover:
            if isinstance(carryover_item, str):
                print_carryover.append(carryover_item)
            elif isinstance(carryover_item, dict) and "content" in carryover_item:
                print_carryover.append(str(carryover_item["content"]))
            else:
                print_carryover.append(str(carryover_item))

        return ("\n").join(print_carryover)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        print_carryover = self._process_carryover()

        f(colored("\n" + "*" * 80, "blue"), flush=True, sep="")
        f(
            colored(
                "Starting a new chat....",
                "blue",
            ),
            flush=True,
        )
        if self.verbose:
            f(colored("Event:\n" + self.message, "blue"), flush=True)
            f(colored("Carryover:\n" + print_carryover, "blue"), flush=True)
        f(colored("\n" + "*" * 80, "blue"), flush=True, sep="")


@wrap_event
class ClearAgentsHistoryEvent(BaseEvent):
    agent: str | None = None
    nr_events_to_preserve: int | None = None

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        agent: Union["Agent", str] | None = None,
        nr_events_to_preserve: int | None = None,
    ):
        return super().__init__(
            uuid=uuid,
            agent=agent.name if hasattr(agent, "name") else agent,
            nr_events_to_preserve=nr_events_to_preserve,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        if self.agent:
            if self.nr_events_to_preserve:
                f(f"Clearing history for {self.agent} except last {self.nr_events_to_preserve} events.")
            else:
                f(f"Clearing history for {self.agent}.")
        else:
            if self.nr_events_to_preserve:
                f(f"Clearing history for all agents except last {self.nr_events_to_preserve} events.")
            else:
                f("Clearing history for all agents.")


# todo: break into multiple events
@wrap_event
class SpeakerAttemptSuccessfulEvent(BaseEvent):
    mentions: dict[str, int]
    attempt: int
    attempts_left: int
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        mentions: dict[str, int],
        attempt: int,
        attempts_left: int,
        select_speaker_auto_verbose: bool | None = False,
    ):
        super().__init__(
            uuid=uuid,
            mentions=deepcopy(mentions),
            attempt=attempt,
            attempts_left=attempts_left,
            verbose=select_speaker_auto_verbose,
        )

    @model_serializer
    def serialize_model(self) -> dict[str, Any]:
        return {
            "uuid": self.uuid,
            "mentions": self.mentions,
            "attempt": self.attempt,
            "attempts_left": self.attempts_left,
            "select_speaker_auto_verbose": self.verbose,
        }

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        selected_agent_name = next(iter(self.mentions))
        f(
            colored(
                f">>>>>>>> Select speaker attempt {self.attempt} of {self.attempt + self.attempts_left} successfully selected: {selected_agent_name}",
                "green",
            ),
            flush=True,
        )


@wrap_event
class SpeakerAttemptFailedMultipleAgentsEvent(BaseEvent):
    mentions: dict[str, int]
    attempt: int
    attempts_left: int
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        mentions: dict[str, int],
        attempt: int,
        attempts_left: int,
        select_speaker_auto_verbose: bool | None = False,
    ):
        super().__init__(
            uuid=uuid,
            mentions=deepcopy(mentions),
            attempt=attempt,
            attempts_left=attempts_left,
            verbose=select_speaker_auto_verbose,
        )

    @model_serializer
    def serialize_model(self) -> dict[str, Any]:
        return {
            "uuid": self.uuid,
            "mentions": self.mentions,
            "attempt": self.attempt,
            "attempts_left": self.attempts_left,
            "select_speaker_auto_verbose": self.verbose,
        }

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f">>>>>>>> Select speaker attempt {self.attempt} of {self.attempt + self.attempts_left} failed as it included multiple agent names.",
                "red",
            ),
            flush=True,
        )


@wrap_event
class SpeakerAttemptFailedNoAgentsEvent(BaseEvent):
    mentions: dict[str, int]
    attempt: int
    attempts_left: int
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        mentions: dict[str, int],
        attempt: int,
        attempts_left: int,
        select_speaker_auto_verbose: bool | None = False,
    ):
        super().__init__(
            uuid=uuid,
            mentions=deepcopy(mentions),
            attempt=attempt,
            attempts_left=attempts_left,
            verbose=select_speaker_auto_verbose,
        )

    @model_serializer
    def serialize_model(self) -> dict[str, Any]:
        return {
            "uuid": self.uuid,
            "mentions": self.mentions,
            "attempt": self.attempt,
            "attempts_left": self.attempts_left,
            "select_speaker_auto_verbose": self.verbose,
        }

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f">>>>>>>> Select speaker attempt #{self.attempt} failed as it did not include any agent names.",
                "red",
            ),
            flush=True,
        )


@wrap_event
class GroupChatResumeEvent(BaseEvent):
    last_speaker_name: str
    events: list[LLMMessageType]
    verbose: bool | None = False

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        last_speaker_name: str,
        events: list["LLMMessageType"],
        silent: bool | None = False,
    ):
        super().__init__(uuid=uuid, last_speaker_name=last_speaker_name, events=events, verbose=not silent)

    @model_serializer
    def serialize_model(self) -> dict[str, Any]:
        return {
            "uuid": self.uuid,
            "last_speaker_name": self.last_speaker_name,
            "events": self.events,
            "silent": not self.verbose,
        }

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            f"Prepared group chat with {len(self.events)} events, the last speaker is",
            colored(self.last_speaker_name, "yellow"),
            flush=True,
        )


@wrap_event
class GroupChatRunChatEvent(BaseEvent):
    speaker: str
    verbose: bool | None = False

    def __init__(self, *, uuid: UUID | None = None, speaker: Union["Agent", str], silent: bool | None = False):
        super().__init__(uuid=uuid, speaker=speaker.name if hasattr(speaker, "name") else speaker, verbose=not silent)

    @model_serializer
    def serialize_model(self) -> dict[str, Any]:
        return {"uuid": self.uuid, "speaker": self.speaker, "silent": not self.verbose}

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored(f"\nNext speaker: {self.speaker}\n", "green"), flush=True)


@wrap_event
class TerminationAndHumanReplyNoInputEvent(BaseEvent):
    """When the human-in-the-loop is prompted but provides no input."""

    no_human_input_msg: str
    sender: str
    recipient: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        no_human_input_msg: str,
        sender: Union["Agent", str] | None = None,
        recipient: Union["Agent", str],
    ):
        sender = sender or "No sender"
        super().__init__(
            uuid=uuid,
            no_human_input_msg=no_human_input_msg,
            sender=sender.name if hasattr(sender, "name") else sender,
            recipient=recipient.name if hasattr(recipient, "name") else recipient,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored(f"\n>>>>>>>> {self.no_human_input_msg}", "red"), flush=True)


@wrap_event
class UsingAutoReplyEvent(BaseEvent):
    human_input_mode: str
    sender: str
    recipient: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        human_input_mode: str,
        sender: Union["Agent", str] | None = None,
        recipient: Union["Agent", str],
    ):
        sender = sender or "No sender"
        super().__init__(
            uuid=uuid,
            human_input_mode=human_input_mode,
            sender=sender.name if hasattr(sender, "name") else sender,
            recipient=recipient.name if hasattr(recipient, "name") else recipient,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored("\n>>>>>>>> USING AUTO REPLY...", "red"), flush=True)


@wrap_event
class TerminationEvent(BaseEvent):
    """When a workflow termination condition is met"""

    termination_reason: str
    sender: str
    recipient: str | None = None

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        sender: Union["Agent", str],
        recipient: Union["Agent", str] | None = None,
        termination_reason: str,
    ):
        super().__init__(
            uuid=uuid,
            termination_reason=termination_reason,
            sender=sender.name if hasattr(sender, "name") else sender,
            recipient=recipient.name if hasattr(recipient, "name") else recipient if recipient else None,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(colored(f"\n>>>>>>>> TERMINATING RUN ({str(self.uuid)}): {self.termination_reason}", "red"), flush=True)


@wrap_event
class ExecuteCodeBlockEvent(BaseEvent):
    code: str
    language: str
    code_block_count: int
    recipient: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        code: str,
        language: str,
        code_block_count: int,
        recipient: Union["Agent", str],
    ):
        super().__init__(
            uuid=uuid,
            code=code,
            language=language,
            code_block_count=code_block_count,
            recipient=recipient.name if hasattr(recipient, "name") else recipient,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f"\n>>>>>>>> EXECUTING CODE BLOCK {self.code_block_count} (inferred language is {self.language})...",
                "red",
            ),
            flush=True,
        )


@wrap_event
class ExecuteFunctionEvent(BaseEvent):
    func_name: str
    call_id: str | None = None
    arguments: dict[str, Any]
    recipient: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        func_name: str,
        call_id: str | None = None,
        arguments: dict[str, Any],
        recipient: Union["Agent", str],
    ):
        super().__init__(
            uuid=uuid,
            func_name=func_name,
            call_id=call_id,
            arguments=arguments,
            recipient=recipient.name if hasattr(recipient, "name") else recipient,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f"\n>>>>>>>> EXECUTING FUNCTION {self.func_name}...\nCall ID: {self.call_id}\nInput arguments: {self.arguments}",
                "magenta",
            ),
            flush=True,
        )


@wrap_event
class ExecutedFunctionEvent(BaseEvent):
    func_name: str
    call_id: str | None = None
    arguments: dict[str, Any] | None
    content: Any
    recipient: str
    is_exec_success: bool = True

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        func_name: str,
        call_id: str | None = None,
        arguments: dict[str, Any] | None,
        content: Any,
        recipient: Union["Agent", str],
        is_exec_success: bool = True,
    ):
        super().__init__(
            uuid=uuid,
            func_name=func_name,
            call_id=call_id,
            arguments=arguments,
            content=content,
            recipient=recipient.name if hasattr(recipient, "name") else recipient,
        )
        self.is_exec_success = is_exec_success

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                f"\n>>>>>>>> EXECUTED FUNCTION {self.func_name}...\nCall ID: {self.call_id}\nInput arguments: {self.arguments}\nOutput:\n{self.content}",
                "magenta",
            ),
            flush=True,
        )


@wrap_event
class SelectSpeakerEvent(BaseEvent):
    agents: list[str] | None = None

    def __init__(self, *, uuid: UUID | None = None, agents: list[Union["Agent", str]] | None = None):
        agents = [agent.name if hasattr(agent, "name") else agent for agent in agents] if agents else None
        super().__init__(uuid=uuid, agents=agents)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f("Please select the next speaker from the following list:")
        agents = self.agents or []
        for i, agent in enumerate(agents):
            f(f"{i + 1}: {agent}")


@wrap_event
class SelectSpeakerTryCountExceededEvent(BaseEvent):
    try_count: int
    agents: list[str] | None = None

    def __init__(self, *, uuid: UUID | None = None, try_count: int, agents: list[Union["Agent", str]] | None = None):
        agents = [agent.name if hasattr(agent, "name") else agent for agent in agents] if agents else None
        super().__init__(uuid=uuid, try_count=try_count, agents=agents)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"You have tried {self.try_count} times. The next speaker will be selected automatically.")


@wrap_event
class SelectSpeakerInvalidInputEvent(BaseEvent):
    agents: list[str] | None = None

    def __init__(self, *, uuid: UUID | None = None, agents: list[Union["Agent", str]] | None = None):
        agents = [agent.name if hasattr(agent, "name") else agent for agent in agents] if agents else None
        super().__init__(uuid=uuid, agents=agents)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"Invalid input. Please enter a number between 1 and {len(self.agents or [])}.")


@wrap_event
class ClearConversableAgentHistoryEvent(BaseEvent):
    agent: str
    recipient: str
    no_events_preserved: int

    def __init__(self, *, uuid: UUID | None = None, agent: Union["Agent", str], no_events_preserved: int | None = None):
        super().__init__(
            uuid=uuid,
            agent=agent.name if hasattr(agent, "name") else agent,
            recipient=agent.name if hasattr(agent, "name") else agent,
            no_events_preserved=no_events_preserved,
        )

    @model_serializer
    def serialize_model(self) -> dict[str, Any]:
        return {
            "uuid": self.uuid,
            "agent": self.agent,
            "no_events_preserved": self.no_events_preserved,
        }

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        for _ in range(self.no_events_preserved):
            f(f"Preserving one more event for {self.agent} to not divide history between tool call and tool response.")


@wrap_event
class ClearConversableAgentHistoryWarningEvent(BaseEvent):
    recipient: str

    def __init__(self, *, uuid: UUID | None = None, recipient: Union["Agent", str]):
        super().__init__(
            uuid=uuid,
            recipient=recipient.name if hasattr(recipient, "name") else recipient,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(
            colored(
                "WARNING: `nr_preserved_events` is ignored when clearing chat history with a specific agent.",
                "yellow",
            ),
            flush=True,
        )


@wrap_event
class GenerateCodeExecutionReplyEvent(BaseEvent):
    code_blocks: list[str]
    sender: str
    recipient: str

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        code_blocks: list[Union["CodeBlock", str]],
        sender: Union["Agent", str] | None = None,
        recipient: Union["Agent", str],
    ):
        code_blocks = [
            code_block.language if hasattr(code_block, "language") else code_block for code_block in code_blocks
        ]
        sender = sender or "No sender"

        super().__init__(
            uuid=uuid,
            code_blocks=code_blocks,
            sender=sender.name if hasattr(sender, "name") else sender,
            recipient=recipient.name if hasattr(recipient, "name") else recipient,
        )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        num_code_blocks = len(self.code_blocks)
        if num_code_blocks == 1:
            f(
                colored(
                    f"\n>>>>>>>> EXECUTING CODE BLOCK (inferred language is {self.code_blocks[0]})...",
                    "red",
                ),
                flush=True,
            )
        else:
            f(
                colored(
                    f"\n>>>>>>>> EXECUTING {num_code_blocks} CODE BLOCKS (inferred languages are [{', '.join(list(self.code_blocks))}])...",
                    "red",
                ),
                flush=True,
            )


@wrap_event
class ConversableAgentUsageSummaryNoCostIncurredEvent(BaseEvent):
    recipient: str

    def __init__(self, *, uuid: UUID | None = None, recipient: Union["Agent", str]):
        super().__init__(uuid=uuid, recipient=recipient.name if hasattr(recipient, "name") else recipient)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"No cost incurred from agent '{self.recipient}'.")


@wrap_event
class ConversableAgentUsageSummaryEvent(BaseEvent):
    recipient: str

    def __init__(self, *, uuid: UUID | None = None, recipient: Union["Agent", str]):
        super().__init__(uuid=uuid, recipient=recipient.name if hasattr(recipient, "name") else recipient)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(f"Agent '{self.recipient}':")


@wrap_event
class InputRequestEvent(BaseEvent):
    prompt: str
    password: bool = False
    respond: Callable[[str], None] | None = None

    type: str = "input_request"


@wrap_event
class AsyncInputRequestEvent(BaseEvent):
    prompt: str
    password: bool = False

    async def a_respond(self, response: "InputResponseEvent") -> None:
        pass


@wrap_event
class InputResponseEvent(BaseEvent):
    value: str


@wrap_event
class ErrorEvent(BaseEvent):
    error: Any


@wrap_event
class RunCompletionEvent(BaseEvent):
    summary: str
    history: list[LLMMessageType]
    cost: dict[str, Any]
    last_speaker: str | None
    context_variables: ContextVariables | None = None
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
from .base_event import BaseEvent, get_annotated_type_for_event_classes, wrap_event
from .helpers import deprecated_by

__all__ = ["BaseEvent", "deprecated_by", "get_annotated_type_for_event_classes", "wrap_event"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


import json
from collections.abc import Callable
from typing import Any
from uuid import UUID

from .base_event import BaseEvent, wrap_event


@wrap_event
class PrintEvent(BaseEvent):
    """Print message"""

    objects: list[str]
    """List of objects to print"""
    sep: str
    """Separator between objects"""
    end: str
    """End of the print"""

    def __init__(self, *objects: Any, sep: str = " ", end: str = "\n", flush: bool = False, uuid: UUID | None = None):
        objects_as_string = [self._to_json(x) for x in objects]

        super().__init__(uuid=uuid, objects=objects_as_string, sep=sep, end=end)

    def _to_json(self, obj: Any) -> str:
        if isinstance(obj, str):
            return obj

        if hasattr(obj, "model_dump_json"):
            return obj.model_dump_json()  # type: ignore [no-any-return]
        try:
            return json.dumps(obj)
        except Exception:
            return str(obj)
            # return repr(obj)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        f(*self.objects, sep=self.sep, end=self.end, flush=True)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
import logging
from collections.abc import Callable
from functools import wraps

from pydantic import BaseModel

logger = logging.getLogger(__name__)


def deprecated_by(
    new_class: type[BaseModel],
    param_mapping: dict[str, str] = None,
    default_params: dict[str, any] = None,
) -> Callable[[type[BaseModel]], Callable[..., BaseModel]]:
    param_mapping = param_mapping or {}
    default_params = default_params or {}

    def decorator(
        old_class: type[BaseModel],
        param_mapping: dict[str, str] = param_mapping,
        default_params: dict[str, any] = default_params,
    ) -> Callable[..., BaseModel]:
        @wraps(old_class)
        def wrapper(*args, **kwargs) -> BaseModel:
            logger.warning(
                f"{old_class.__name__} is deprecated by {new_class.__name__}. Please import it from {new_class.__module__} and use it instead."
            )
            # Translate old parameters to new parameters
            new_kwargs = {param_mapping.get(k, k): v for k, v in kwargs.items()}

            # Add default parameters if not already present
            for key, value in default_params.items():
                if key not in new_kwargs:
                    new_kwargs[key] = value

            # Pass the translated parameters to the new class
            return new_class(*args, **new_kwargs)

        return wrapper

    return decorator
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0


from abc import ABC
from collections.abc import Callable
from typing import Annotated, Any, Literal, Union
from uuid import UUID, uuid4

from pydantic import BaseModel, Field, create_model

from ..doc_utils import export_module

__all__ = ["BaseEvent", "get_annotated_type_for_event_classes", "get_event_classes", "wrap_event"]


@export_module("autogen.events")
class BaseEvent(BaseModel, ABC):
    uuid: UUID

    def __init__(self, uuid: UUID | None = None, **kwargs: Any) -> None:
        uuid = uuid or uuid4()
        super().__init__(uuid=uuid, **kwargs)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        """Print event

        Args:
            f (Optional[Callable[..., Any]], optional): Print function. If none, python's default print will be used.
        """
        ...


def camel2snake(name: str) -> str:
    return "".join(["_" + i.lower() if i.isupper() else i for i in name]).lstrip("_")


_event_classes: dict[str, type[BaseModel]] = {}


@export_module("autogen.events")
def wrap_event(event_cls: type[BaseEvent]) -> type[BaseModel]:
    """Wrap a event class with a type field to be used in a union type

    This is needed for proper serialization and deserialization of events in a union type.

    Args:
        event_cls (type[BaseEvent]): Event class to wrap
    """
    global _event_classes

    if not event_cls.__name__.endswith("Event"):
        raise ValueError("Event class name must end with 'Event'")

    type_name = camel2snake(event_cls.__name__)
    type_name = type_name[: -len("_event")]

    class WrapperBase(BaseModel):
        # these types are generated dynamically so we need to disable the type checker
        type: Literal[type_name] = type_name  # type: ignore[valid-type]
        content: event_cls  # type: ignore[valid-type]

        def __init__(self, *args: Any, **data: Any):
            if set(data.keys()) == {"type", "content"} and "content" in data:
                super().__init__(*args, **data)
            else:
                if "content" in data:
                    content = data.pop("content")
                    super().__init__(*args, content=event_cls(*args, **data, content=content), **data)
                else:
                    super().__init__(content=event_cls(*args, **data), **data)

        def print(self, f: Callable[..., Any] | None = None) -> None:
            self.content.print(f)  # type: ignore[attr-defined]

    wrapper_cls = create_model(event_cls.__name__, __base__=WrapperBase)

    # Preserve the original class's docstring and other attributes
    wrapper_cls.__doc__ = event_cls.__doc__
    wrapper_cls.__module__ = event_cls.__module__

    # Copy any other relevant attributes/metadata from the original class
    if hasattr(event_cls, "__annotations__"):
        wrapper_cls.__annotations__ = event_cls.__annotations__

    _event_classes[type_name] = wrapper_cls

    return wrapper_cls


@export_module("autogen.events")
def get_annotated_type_for_event_classes() -> type[Any]:
    # this is a dynamic type so we need to disable the type checker
    union_type = Union[tuple(_event_classes.values())]  # type: ignore[valid-type]  # noqa: UP007
    return Annotated[union_type, Field(discriminator="type")]  # type: ignore[return-value]


def get_event_classes() -> dict[str, type[BaseModel]]:
    return _event_classes
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from collections.abc import Callable
from typing import Any, Literal
from uuid import UUID

from pydantic import BaseModel

from .base_event import BaseEvent, wrap_event

__all__ = ["UsageSummaryEvent"]


class ModelUsageSummary(BaseModel):
    """Model usage summary."""

    model: str
    """Model name."""
    completion_tokens: int
    """Number of tokens used for completion."""
    cost: float
    """Cost of the completion."""
    prompt_tokens: int
    """Number of tokens used for prompt."""
    total_tokens: int
    """Total number of tokens used."""


class ActualUsageSummary(BaseModel):
    """Actual usage summary."""

    usages: list[ModelUsageSummary] | None = None
    """List of model usage summaries."""
    total_cost: float | None = None
    """Total cost."""


class TotalUsageSummary(BaseModel):
    """Total usage summary."""

    usages: list[ModelUsageSummary] | None = None
    """List of model usage summaries."""
    total_cost: float | None = None
    """Total cost."""


Mode = Literal["both", "total", "actual"]


def _change_usage_summary_format(
    actual_usage_summary: dict[str, Any] | None = None, total_usage_summary: dict[str, Any] | None = None
) -> dict[str, dict[str, Any]]:
    summary: dict[str, Any] = {}

    for usage_type, usage_summary in {"actual": actual_usage_summary, "total": total_usage_summary}.items():
        if usage_summary is None:
            summary[usage_type] = {"usages": None, "total_cost": None}
            continue

        usage_summary_altered_format: dict[str, list[dict[str, Any]]] = {"usages": []}
        for k, v in usage_summary.items():
            if isinstance(k, str) and isinstance(v, dict):
                current_usage = dict(v.items())
                current_usage["model"] = k
                usage_summary_altered_format["usages"].append(current_usage)
            else:
                usage_summary_altered_format[k] = v
        summary[usage_type] = usage_summary_altered_format

    return summary


@wrap_event
class UsageSummaryEvent(BaseEvent):
    """Usage summary message."""

    actual: ActualUsageSummary
    """Actual usage summary."""
    total: TotalUsageSummary
    """Total usage summary."""
    mode: Mode
    """Mode to display the usage summary."""

    def __init__(
        self,
        *,
        uuid: UUID | None = None,
        actual_usage_summary: dict[str, Any] | None = None,
        total_usage_summary: dict[str, Any] | None = None,
        mode: Mode = "both",
    ):
        # print(f"{actual_usage_summary=}")
        # print(f"{total_usage_summary=}")

        summary_dict = _change_usage_summary_format(actual_usage_summary, total_usage_summary)

        super().__init__(uuid=uuid, **summary_dict, mode=mode)

    def _print_usage(
        self,
        usage_summary: ActualUsageSummary | TotalUsageSummary,
        usage_type: str = "total",
        f: Callable[..., Any] | None = None,
    ) -> None:
        f = f or print
        word_from_type = "including" if usage_type == "total" else "excluding"
        if usage_summary.usages is None or len(usage_summary.usages) == 0:
            f("No actual cost incurred (all completions are using cache).", flush=True)
            return

        f(f"Usage summary {word_from_type} cached usage: ", flush=True)
        f(f"Total cost: {round(usage_summary.total_cost, 5)}", flush=True)  # type: ignore [arg-type]

        for usage in usage_summary.usages:
            f(
                f"* Model '{usage.model}': cost: {round(usage.cost, 5)}, prompt_tokens: {usage.prompt_tokens}, completion_tokens: {usage.completion_tokens}, total_tokens: {usage.total_tokens}",
                flush=True,
            )

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        if self.total.usages is None:
            f('No usage summary. Please call "create" first.', flush=True)
            return

        f("-" * 100, flush=True)
        if self.mode == "both":
            self._print_usage(self.actual, "actual", f)
            f()
            if self.total.model_dump_json() != self.actual.model_dump_json():
                self._print_usage(self.total, "total", f)
            else:
                f(
                    "All completions are non-cached: the total cost with cached completions is the same as actual cost.",
                    flush=True,
                )
        elif self.mode == "total":
            self._print_usage(self.total, "total", f)
        elif self.mode == "actual":
            self._print_usage(self.actual, "actual", f)
        else:
            raise ValueError(f'Invalid mode: {self.mode}, choose from "actual", "total", ["actual", "total"]')
        f("-" * 100, flush=True)


@wrap_event
class StreamEvent(BaseEvent):
    """Stream event."""

    content: str
    """Content of the event."""

    def __init__(self, *, uuid: UUID | None = None, content: str) -> None:
        super().__init__(uuid=uuid, content=content)

    def print(self, f: Callable[..., Any] | None = None) -> None:
        f = f or print

        # Set the terminal text color to green
        f("\033[32m", end="")

        f(self.content, end="", flush=True)

        # Reset the terminal text color
        f("\033[0m\n")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import inspect
import re
import sys
from abc import ABC, abstractmethod
from collections.abc import Callable, Generator, Iterable
from contextlib import contextmanager, suppress
from dataclasses import dataclass
from functools import wraps
from logging import getLogger
from pathlib import Path
from typing import Any, Generic, Optional, TypeVar

from packaging import version

from .fast_depends.utils import is_coroutine_callable

__all__ = [
    "optional_import_block",
    "patch_object",
    "require_optional_import",
    "run_for_optional_imports",
    "skip_on_missing_imports",
]

logger = getLogger(__name__)


@dataclass
class ModuleInfo:
    name: str
    min_version: str | None = None
    max_version: str | None = None
    min_inclusive: bool = False
    max_inclusive: bool = False

    def is_in_sys_modules(self) -> str | None:
        """Check if the module is installed and satisfies the version constraints

        Returns:
            None if the module is installed and satisfies the version constraints, otherwise a message indicating the issue.

        """
        if self.name not in sys.modules:
            return f"'{self.name}' is not installed."
        else:
            if hasattr(sys.modules[self.name], "__file__") and sys.modules[self.name].__file__ is not None:
                autogen_path = (Path(__file__).parent).resolve()
                test_path = (Path(__file__).parent.parent / "test").resolve()
                module_path = Path(sys.modules[self.name].__file__).resolve()  # type: ignore[arg-type]

                if str(autogen_path) in str(module_path) or str(test_path) in str(module_path):
                    # The module is in the autogen or test directory
                    # Aka similarly named module in the autogen or test directory
                    return f"'{self.name}' is not installed."

        # Ensure that the retrieved version is a string. Some packages might unexpectedly
        # have a __version__ attribute that is not a string (e.g., a module).
        raw_version_attr = (
            sys.modules[self.name].__version__ if hasattr(sys.modules[self.name], "__version__") else None
        )
        installed_version = raw_version_attr if isinstance(raw_version_attr, str) else None
        if installed_version is None and (self.min_version or self.max_version):
            return f"'{self.name}' is installed, but the version is not available."

        if installed_version:
            # Convert to version object for comparison
            installed_ver = version.parse(installed_version)

            if self.min_version:
                min_ver = version.parse(self.min_version)
                msg = f"'{self.name}' is installed, but the installed version {installed_version} is too low (required '{self}')."
                if not self.min_inclusive and installed_ver == min_ver:
                    return msg
                if self.min_inclusive and installed_ver < min_ver:
                    return msg

            if self.max_version:
                max_ver = version.parse(self.max_version)
                msg = f"'{self.name}' is installed, but the installed version {installed_version} is too high (required '{self}')."
                if not self.max_inclusive and installed_ver == max_ver:
                    return msg
                if self.max_inclusive and installed_ver > max_ver:
                    return msg

        return None

    def __repr__(self) -> str:
        s = self.name
        if self.min_version:
            s += f">={self.min_version}" if self.min_inclusive else f">{self.min_version}"
        if self.max_version:
            s += f"<={self.max_version}" if self.max_inclusive else f"<{self.max_version}"
        return s

    @classmethod
    def from_str(cls, module_info: str) -> "ModuleInfo":
        """Parse a string to create a ModuleInfo object

        Args:
            module_info (str): A string containing the module name and optional version constraints

        Returns:
            ModuleInfo: A ModuleInfo object with the parsed information

        Raises:
            ValueError: If the module information is invalid
        """
        pattern = re.compile(r"^(?P<name>[a-zA-Z0-9-_]+)(?P<constraint>.*)$")
        match = pattern.match(module_info.strip())

        if not match:
            raise ValueError(f"Invalid package information: {module_info}")

        name = match.group("name")
        constraints = match.group("constraint").strip()
        min_version = max_version = None
        min_inclusive = max_inclusive = False

        if constraints:
            constraint_pattern = re.findall(r"(>=|<=|>|<)([0-9\.]+)?", constraints)

            if not all(version for _, version in constraint_pattern):
                raise ValueError(f"Invalid module information: {module_info}")

            for operator, version in constraint_pattern:
                if operator == ">=":
                    min_version = version
                    min_inclusive = True
                elif operator == "<=":
                    max_version = version
                    max_inclusive = True
                elif operator == ">":
                    min_version = version
                    min_inclusive = False
                elif operator == "<":
                    max_version = version
                    max_inclusive = False
                else:
                    raise ValueError(f"Invalid package information: {module_info}")

        return ModuleInfo(
            name=name,
            min_version=min_version,
            max_version=max_version,
            min_inclusive=min_inclusive,
            max_inclusive=max_inclusive,
        )


class Result:
    def __init__(self) -> None:
        self._failed: bool | None = None

    @property
    def is_successful(self) -> bool:
        if self._failed is None:
            raise ValueError("Result not set")
        return not self._failed


@contextmanager
def optional_import_block() -> Generator[Result, None, None]:
    """Guard a block of code to suppress ImportErrors

    A context manager to temporarily suppress ImportErrors.
    Use this to attempt imports without failing immediately on missing modules.

    Example:
    ```python
    with optional_import_block():
        import some_module
        import some_other_module
    ```
    """
    result = Result()
    try:
        yield result
        result._failed = False
    except ImportError as e:
        # Ignore ImportErrors during this context
        logger.debug(f"Ignoring ImportError: {e}")
        result._failed = True


def get_missing_imports(modules: str | Iterable[str]) -> dict[str, str]:
    """Get missing modules from a list of module names

    Args:
        modules (Union[str, Iterable[str]]): Module name or list of module names

    Returns:
        List of missing module names
    """
    if isinstance(modules, str):
        modules = [modules]

    module_infos = [ModuleInfo.from_str(module) for module in modules]
    x = {m.name: m.is_in_sys_modules() for m in module_infos}
    return {k: v for k, v in x.items() if v}


T = TypeVar("T")
G = TypeVar("G", bound=Callable[..., Any] | type)
F = TypeVar("F", bound=Callable[..., Any])


class PatchObject(ABC, Generic[T]):
    def __init__(self, o: T, missing_modules: dict[str, str], dep_target: str):
        if not self.accept(o):
            raise ValueError(f"Cannot patch object of type {type(o)}")

        self.o = o
        self.missing_modules = missing_modules
        self.dep_target = dep_target

    @classmethod
    @abstractmethod
    def accept(cls, o: Any) -> bool: ...

    @abstractmethod
    def patch(self, except_for: Iterable[str]) -> T: ...

    def get_object_with_metadata(self) -> Any:
        return self.o

    @property
    def msg(self) -> str:
        o = self.get_object_with_metadata()
        plural = len(self.missing_modules) > 1
        fqn = f"{o.__module__}.{o.__name__}" if hasattr(o, "__module__") else o.__name__
        # modules_str = ", ".join([f"'{m}'" for m in self.missing_modules])
        msg = f"{'Modules' if plural else 'A module'} needed for {fqn} {'are' if plural else 'is'} missing:\n"
        for _, status in self.missing_modules.items():
            msg += f" - {status}\n"
        msg += f"Please install {'them' if plural else 'it'} using:\n'pip install ag2[{self.dep_target}]'"
        return msg

    def copy_metadata(self, retval: T) -> None:
        """Copy metadata from original object to patched object

        Args:
            retval: Patched object

        """
        o = self.o
        if hasattr(o, "__doc__"):
            retval.__doc__ = o.__doc__
        if hasattr(o, "__name__"):
            retval.__name__ = o.__name__  # type: ignore[attr-defined]
        if hasattr(o, "__module__"):
            retval.__module__ = o.__module__

    _registry: list[type["PatchObject[Any]"]] = []

    @classmethod
    def register(cls) -> Callable[[type["PatchObject[Any]"]], type["PatchObject[Any]"]]:
        def decorator(subclass: type["PatchObject[Any]"]) -> type["PatchObject[Any]"]:
            cls._registry.append(subclass)
            return subclass

        return decorator

    @classmethod
    def create(
        cls,
        o: T,
        *,
        missing_modules: dict[str, str],
        dep_target: str,
    ) -> Optional["PatchObject[T]"]:
        for subclass in cls._registry:
            if subclass.accept(o):
                return subclass(o, missing_modules, dep_target)
        return None


@PatchObject.register()
class PatchCallable(PatchObject[F]):
    @classmethod
    def accept(cls, o: Any) -> bool:
        return inspect.isfunction(o) or inspect.ismethod(o)

    def patch(self, except_for: Iterable[str]) -> F:
        if self.o.__name__ in except_for:
            return self.o

        f: Callable[..., Any] = self.o

        # @wraps(f.__call__)  # type: ignore[operator]
        @wraps(f)
        def _call(*args: Any, **kwargs: Any) -> Any:
            raise ImportError(self.msg)

        self.copy_metadata(_call)  # type: ignore[arg-type]

        return _call  # type: ignore[return-value]


@PatchObject.register()
class PatchStatic(PatchObject[F]):
    @classmethod
    def accept(cls, o: Any) -> bool:
        # return inspect.ismethoddescriptor(o)
        return isinstance(o, staticmethod)

    def patch(self, except_for: Iterable[str]) -> F:
        if hasattr(self.o, "__name__"):
            name = self.o.__name__
        elif hasattr(self.o, "__func__"):
            name = self.o.__func__.__name__
        else:
            raise ValueError(f"Cannot determine name for object {self.o}")
        if name in except_for:
            return self.o

        f: Callable[..., Any] = self.o.__func__  # type: ignore[attr-defined]

        @wraps(f)
        def _call(*args: Any, **kwargs: Any) -> Any:
            raise ImportError(self.msg)

        self.copy_metadata(_call)  # type: ignore[arg-type]

        return staticmethod(_call)  # type: ignore[return-value]

    def get_object_with_metadata(self) -> Any:
        return self.o.__func__  # type: ignore[attr-defined]


@PatchObject.register()
class PatchInit(PatchObject[F]):
    @classmethod
    def accept(cls, o: Any) -> bool:
        return inspect.ismethoddescriptor(o) and o.__name__ == "__init__"

    def patch(self, except_for: Iterable[str]) -> F:
        if self.o.__name__ in except_for:
            return self.o

        f: Callable[..., Any] = self.o

        @wraps(f)
        def _call(*args: Any, **kwargs: Any) -> Any:
            raise ImportError(self.msg)

        self.copy_metadata(_call)  # type: ignore[arg-type]

        return staticmethod(_call)  # type: ignore[return-value]

    def get_object_with_metadata(self) -> Any:
        return self.o


@PatchObject.register()
class PatchProperty(PatchObject[Any]):
    @classmethod
    def accept(cls, o: Any) -> bool:
        return inspect.isdatadescriptor(o) and hasattr(o, "fget")

    def patch(self, except_for: Iterable[str]) -> property:
        if not hasattr(self.o, "fget"):
            raise ValueError(f"Cannot patch property without getter: {self.o}")
        f: Callable[..., Any] = self.o.fget

        if f.__name__ in except_for:
            return self.o  # type: ignore[no-any-return]

        @wraps(f)
        def _call(*args: Any, **kwargs: Any) -> Any:
            raise ImportError(self.msg)

        self.copy_metadata(_call)

        return property(_call)

    def get_object_with_metadata(self) -> Any:
        return self.o.fget


@PatchObject.register()
class PatchClass(PatchObject[type[Any]]):
    @classmethod
    def accept(cls, o: Any) -> bool:
        return inspect.isclass(o)

    def patch(self, except_for: Iterable[str]) -> type[Any]:
        if self.o.__name__ in except_for:
            return self.o

        for name, member in inspect.getmembers(self.o):
            # Patch __init__ method if possible, but not other internal methods
            if name.startswith("__") and name != "__init__":
                continue
            patched = patch_object(
                member,
                missing_modules=self.missing_modules,
                dep_target=self.dep_target,
                fail_if_not_patchable=False,
                except_for=except_for,
            )
            with suppress(AttributeError):
                setattr(self.o, name, patched)

        return self.o


def patch_object(
    o: T,
    *,
    missing_modules: dict[str, str],
    dep_target: str,
    fail_if_not_patchable: bool = True,
    except_for: str | Iterable[str] | None = None,
) -> T:
    patcher = PatchObject.create(o, missing_modules=missing_modules, dep_target=dep_target)
    if fail_if_not_patchable and patcher is None:
        raise ValueError(f"Cannot patch object of type {type(o)}")

    except_for = except_for if except_for is not None else []
    except_for = [except_for] if isinstance(except_for, str) else except_for

    return patcher.patch(except_for=except_for) if patcher else o


def require_optional_import(
    modules: str | Iterable[str],
    dep_target: str,
    *,
    except_for: str | Iterable[str] | None = None,
) -> Callable[[T], T]:
    """Decorator to handle optional module dependencies

    Args:
        modules: Module name or list of module names required
        dep_target: Target name for pip installation (e.g. 'test' in pip install ag2[test])
        except_for: Name or list of names of objects to exclude from patching
    """
    missing_modules = get_missing_imports(modules)

    if not missing_modules:

        def decorator(o: T) -> T:
            return o

    else:

        def decorator(o: T) -> T:
            return patch_object(o, missing_modules=missing_modules, dep_target=dep_target, except_for=except_for)

    return decorator


def _mark_object(o: T, dep_target: str) -> T:
    import pytest

    markname = dep_target.replace("-", "_")
    pytest_mark_markname = getattr(pytest.mark, markname)
    pytest_mark_o = pytest_mark_markname(o)

    pytest_mark_o = pytest.mark.aux_neg_flag(pytest_mark_o)

    return pytest_mark_o  # type: ignore[no-any-return]


def run_for_optional_imports(modules: str | Iterable[str], dep_target: str) -> Callable[[G], G]:
    """Decorator to run a test if and only if optional modules are installed

    Args:
        modules: Module name or list of module names
        dep_target: Target name for pip installation (e.g. 'test' in pip install ag2[test])
    """
    # missing_modules = get_missing_imports(modules)
    # if missing_modules:
    #     raise ImportError(f"Missing module{'s' if len(missing_modules) > 1 else ''}: {', '.join(missing_modules)}. Install using 'pip install ag2[{dep_target}]'")

    def decorator(o: G) -> G:
        missing_modules = get_missing_imports(modules)

        if isinstance(o, type):
            wrapped = require_optional_import(modules, dep_target)(o)
        else:
            if is_coroutine_callable(o):

                @wraps(o)
                async def wrapped(*args: Any, **kwargs: Any) -> Any:
                    if missing_modules:
                        raise ImportError(
                            f"Missing module{'s' if len(missing_modules) > 1 else ''}: {', '.join(missing_modules)}. Install using 'pip install ag2[{dep_target}]'"
                        )
                    return await o(*args, **kwargs)

            else:

                @wraps(o)
                def wrapped(*args: Any, **kwargs: Any) -> Any:
                    if missing_modules:
                        raise ImportError(
                            f"Missing module{'s' if len(missing_modules) > 1 else ''}: {', '.join(missing_modules)}. Install using 'pip install ag2[{dep_target}]'"
                        )
                    return o(*args, **kwargs)

        pytest_mark_o: G = _mark_object(wrapped, dep_target)  # type: ignore[assignment]

        return pytest_mark_o

    return decorator


def skip_on_missing_imports(modules: str | Iterable[str], dep_target: str) -> Callable[[T], T]:
    """Decorator to skip a test if an optional module is missing

    Args:
        modules: Module name or list of module names
        dep_target: Target name for pip installation (e.g. 'test' in pip install ag2[test])
    """
    import pytest

    missing_modules = get_missing_imports(modules)

    if not missing_modules:

        def decorator(o: T) -> T:
            pytest_mark_o = _mark_object(o, dep_target)
            return pytest_mark_o  # type: ignore[no-any-return]

    else:

        def decorator(o: T) -> T:
            pytest_mark_o = _mark_object(o, dep_target)

            return pytest.mark.skip(  # type: ignore[return-value,no-any-return]
                f"Missing module{'s' if len(missing_modules) > 1 else ''}: {', '.join(missing_modules)}. Install using 'pip install ag2[{dep_target}]'"
            )(pytest_mark_o)

    return decorator
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from typing import Any

from .import_utils import optional_import_block, require_optional_import

with optional_import_block():
    from jsonschema import Draft7Validator, RefResolver

__all__ = ["resolve_json_references"]


@require_optional_import("jsonschema", "gemini")
def resolve_json_references(schema: dict[str, Any]) -> dict[str, Any]:
    """Resolve JSON references in the given schema.

    Args:
        schema (dict): The JSON schema with references.

    Returns:
        dict: The JSON schema with resolved references.
    """
    resolver = RefResolver.from_schema(schema)
    validator = Draft7Validator(schema, resolver=resolver)
    resolved_schema = validator.schema

    def resolve_refs(node: Any) -> Any:
        if isinstance(node, dict):
            if "$ref" in node:
                ref = node["$ref"]
                with resolver.resolving(ref) as resolved:
                    return resolve_refs(resolved)
            else:
                return {k: resolve_refs(v) for k, v in node.items()}
        elif isinstance(node, list):
            return [resolve_refs(item) for item in node]
        else:
            return node

    return resolve_refs(resolved_schema)  # type: ignore[no-any-return]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any

from .doc_utils import export_module

__all__ = [
    "AgentNameConflictError",
    "InvalidCarryOverTypeError",
    "ModelToolNotSupportedError",
    "NoEligibleSpeakerError",
    "SenderRequiredError",
    "UndefinedNextAgentError",
]


@export_module("autogen")
class AgentNameConflictError(Exception):  # noqa: N818
    def __init__(self, msg: str = "Found multiple agents with the same name.", *args: Any, **kwargs: Any):
        super().__init__(msg, *args, **kwargs)


@export_module("autogen")
class NoEligibleSpeakerError(Exception):  # noqa: N818
    """Exception raised for early termination of a GroupChat."""

    def __init__(self, message: str = "No eligible speakers."):
        self.message = message
        super().__init__(self.message)


@export_module("autogen")
class SenderRequiredError(Exception):  # noqa: N818
    """Exception raised when the sender is required but not provided."""

    def __init__(self, message: str = "Sender is required but not provided."):
        self.message = message
        super().__init__(self.message)


@export_module("autogen")
class InvalidCarryOverTypeError(Exception):  # noqa: N818
    """Exception raised when the carryover type is invalid."""

    def __init__(
        self, message: str = "Carryover should be a string or a list of strings. Not adding carryover to the message."
    ):
        self.message = message
        super().__init__(self.message)


@export_module("autogen")
class UndefinedNextAgentError(Exception):  # noqa: N818
    """Exception raised when the provided next agents list does not overlap with agents in the group."""

    def __init__(self, message: str = "The provided agents list does not overlap with agents in the group."):
        self.message = message
        super().__init__(self.message)


class ModelToolNotSupportedError(Exception):
    """Exception raised when attempting to use tools with models that do not support them."""

    def __init__(
        self,
        model: str,
    ):
        self.message = f"Tools are not supported with {model} models. Refer to the documentation at https://platform.openai.com/docs/guides/reasoning#limitations"
        super().__init__(self.message)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from __future__ import annotations

from collections.abc import Iterable
from typing import Literal

from .import_utils import optional_import_block

with optional_import_block() as result:
    from termcolor import colored

if not result.is_successful:
    # termcolor is an optional dependency - if it cannot be imported then no color is used.
    # Alternatively the envvar NO_COLOR can be used to disable color.
    # To allow for proper typing and for termcolor to be optional we need to re-define the types used in the lib here.
    # This is the direct function definition from termcolor.
    Attribute = Literal[
        "bold",
        "dark",
        "underline",
        "blink",
        "reverse",
        "concealed",
    ]

    Highlight = Literal[
        "on_black",
        "on_grey",
        "on_red",
        "on_green",
        "on_yellow",
        "on_blue",
        "on_magenta",
        "on_cyan",
        "on_light_grey",
        "on_dark_grey",
        "on_light_red",
        "on_light_green",
        "on_light_yellow",
        "on_light_blue",
        "on_light_magenta",
        "on_light_cyan",
        "on_white",
    ]

    Color = Literal[
        "black",
        "grey",
        "red",
        "green",
        "yellow",
        "blue",
        "magenta",
        "cyan",
        "light_grey",
        "dark_grey",
        "light_red",
        "light_green",
        "light_yellow",
        "light_blue",
        "light_magenta",
        "light_cyan",
        "white",
    ]

    def colored(
        text: object,
        color: Color | None = None,
        on_color: Highlight | None = None,
        attrs: Iterable[Attribute] | None = None,
        *,
        no_color: bool | None = None,
        force_color: bool | None = None,
    ) -> str:
        return str(text)


__all__ = ["colored"]
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import functools
import json
import re
import warnings
from collections.abc import Iterable
from contextvars import ContextVar
from pathlib import Path
from typing import Annotated, Any, Literal, TypeAlias

from pydantic import BaseModel, ConfigDict, Field
from typing_extensions import Self, deprecated

from autogen.doc_utils import export_module

from .entry import ApplicationConfig, LLMConfigEntry
from .types import ConfigEntries
from .utils import config_list_from_json, filter_config


# Meta class to allow LLMConfig.current and LLMConfig.default to be used as class properties
class MetaLLMConfig(type):
    def __init__(cls, *args: Any, **kwargs: Any) -> None:
        pass

    @property
    @deprecated(
        "`LLMConfig.current / .default` properties are deprecated. "
        "Pass config object to usage explicitly instead. "
        "Scheduled for removal in 0.11.0 version."
    )
    def current(cls) -> "LLMConfig":
        current_llm_config = LLMConfig.get_current_llm_config(llm_config=None)
        if current_llm_config is None:
            raise ValueError("No current LLMConfig set. Are you inside a context block?")
        return current_llm_config  # type: ignore[return-value]

    @property
    @deprecated(
        "`LLMConfig.current / .default` properties are deprecated. "
        "Pass config object to usage explicitly instead. "
        "Scheduled for removal in 0.11.0 version."
    )
    def default(cls) -> "LLMConfig":
        return cls.current


ConfigItem: TypeAlias = LLMConfigEntry | ConfigEntries | dict[str, Any]


@export_module("autogen")
class LLMConfig(metaclass=MetaLLMConfig):
    _current_llm_config: ContextVar["LLMConfig"] = ContextVar("current_llm_config")
    config_list: list[ConfigEntries]

    def __init__(
        self,
        *configs: ConfigItem,
        top_p: float | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        check_every_ms: int | None = None,
        allow_format_str_template: bool | None = None,
        response_format: str | dict[str, Any] | BaseModel | type[BaseModel] | None = None,
        timeout: int | None = None,
        seed: int | None = None,
        cache_seed: int | None = None,
        parallel_tool_calls: bool | None = None,
        tools: Iterable[Any] = (),
        functions: Iterable[Any] = (),
        routing_method: Literal["fixed_order", "round_robin"] | None = None,
        config_list: Annotated[
            Iterable[ConfigItem] | dict[str, Any],
            deprecated(
                "`LLMConfig(config_list=[{'model': ..., 'api_key': ...}, ...])` syntax is deprecated. "
                "Use `LLMConfig({'api_key': ..., 'model': ...}, ...)` instead. "
                "Scheduled for removal in 0.11.0 version."
            ),
        ] = (),
        **kwargs: Annotated[
            Any,
            deprecated(
                "`LLMConfig(api_key=..., model=...)` syntax is deprecated. "
                "Use `LLMConfig({'api_key': ..., 'model': ...})` instead. "
                "Scheduled for removal in 0.11.0 version."
            ),
        ],
    ) -> None:
        r"""Initializes the LLMConfig object.

        Args:
            *configs: A list of LLM configuration entries or dictionaries.
            config_list: A list of LLM configuration entries or dictionaries.
            temperature: The sampling temperature for LLM generation.
            check_every_ms: The interval (in milliseconds) to check for updates
            allow_format_str_template: Whether to allow format string templates.
            response_format: The format of the response (e.g., JSON, text).
            timeout: The timeout for LLM requests in seconds.
            seed: The random seed for reproducible results.
            cache_seed: The seed for caching LLM responses.
            parallel_tool_calls: Whether to enable parallel tool calls.
            tools: A list of tools available for the LLM.
            functions: A list of functions available for the LLM.
            max_tokens: The maximum number of tokens to generate.
            top_p: The nucleus sampling probability.
            routing_method: The method used to route requests (e.g., fixed_order, round_robin).
            **kwargs: Additional keyword arguments for\ future extensions.

        Examples:
            ```python
            # Example 1: create config from one model dictionary
            config = LLMConfig({
                "model": "gpt-5-mini",
                "api_key": os.environ["OPENAI_API_KEY"],
            })

            # Example 2: create config from list of dictionaries
            config = LLMConfig(
                {
                    "model": "gpt-5-mini",
                    "api_key": os.environ["OPENAI_API_KEY"],
                },
                {
                    "model": "gpt-4",
                    "api_key": os.environ["OPENAI_API_KEY"],
                },
            )

            # Example 3 (deprecated): create config from `kwargs` options
            config = LLMConfig(
                model="gpt-5-mini",
                api_key=os.environ["OPENAI_API_KEY"],
            )

            # Example 4 (deprecated): create config from `config_list` dictionary
            config = LLMConfig(
                config_list={
                    "model": "gpt-5-mini",
                    "api_key": os.environ["OPENAI_API_KEY"],
                }
            )

            # Example 5 (deprecated): create config from `config_list` list
            config = LLMConfig(
                config_list=[
                    {
                        "model": "gpt-5-mini",
                        "api_key": os.environ["OPENAI_API_KEY"],
                    },
                    {
                        "model": "gpt-5",
                        "api_key": os.environ["OPENAI_API_KEY"],
                    },
                ]
            )
            ```
        """
        if isinstance(config_list, dict):
            config_list = [config_list]

        if kwargs:
            warnings.warn(
                (
                    "`LLMConfig(api_key=..., model=...)` syntax is deprecated. "
                    "Use `LLMConfig({'api_key': ..., 'model': ...})` instead. "
                    "Scheduled for removal in 0.11.0 version."
                ),
                DeprecationWarning,
            )

        if config_list:
            warnings.warn(
                (
                    "`LLMConfig(config_list=[{'model': ..., 'api_key': ...}, ...])` syntax is deprecated. "
                    "Use `LLMConfig({'api_key': ..., 'model': ...}, ...)` instead. "
                    "Scheduled for removal in 0.11.0 version."
                ),
                DeprecationWarning,
            )

        app_config = ApplicationConfig(
            max_tokens=max_tokens,
            top_p=top_p,
            temperature=temperature,
        )

        application_level_options = app_config.model_dump(exclude_none=True)

        final_config_list: list[LLMConfigEntry | dict[str, Any]] = []
        for c in filter(bool, (*configs, *config_list, kwargs)):
            if isinstance(c, LLMConfigEntry):
                final_config_list.append(c.apply_application_config(app_config))
                continue

            else:
                final_config_list.append({
                    "api_type": "openai",  # default api_type
                    **application_level_options,
                    **c,
                })

        self._model = _LLMConfig(
            **application_level_options,
            config_list=final_config_list,
            check_every_ms=check_every_ms,
            seed=seed,
            allow_format_str_template=allow_format_str_template,
            response_format=response_format,
            timeout=timeout,
            cache_seed=cache_seed,
            tools=tools or [],
            functions=functions or [],
            parallel_tool_calls=parallel_tool_calls,
            routing_method=routing_method,
        )

    @classmethod
    def ensure_config(cls, config: "LLMConfig | ConfigItem | Iterable[ConfigItem]", /) -> "LLMConfig":
        """Transforms passed objects to LLMConfig object.

        Method to use for `Agent(llm_config={...})` cases.

        >>> LLMConfig.ensure_config(LLMConfig(...))
        LLMConfig(...)

        >>> LLMConfig.ensure_config(LLMConfigEntry(...))
        LLMConfig(LLMConfigEntry(...))

        >>> LLMConfig.ensure_config({"model": "gpt-o3"})
        LLMConfig(OpenAILLMConfigEntry(model="o3"))

        >>> LLMConfig.ensure_config([{"model": "gpt-o3"}, ...])
        LLMConfig(OpenAILLMConfigEntry(model="o3"), ...)

        >>> (deprecated) LLMConfig.ensure_config({"config_list": [{ "model": "gpt-o3" }, ...]})
        LLMConfig(OpenAILLMConfigEntry(model="o3"), ...)
        """
        if isinstance(config, LLMConfig):
            return config.copy()

        if isinstance(config, LLMConfigEntry):
            return LLMConfig(config)

        if isinstance(config, dict):
            if "config_list" in config:  # backport compatibility
                return LLMConfig(**config)
            return LLMConfig(config)

        return LLMConfig(*config)

    @deprecated(
        "`with llm_config: ...` context manager is deprecated. "
        "Pass config object to usage explicitly instead. "
        "Scheduled for removal in 0.11.0 version."
    )
    def __enter__(self) -> "LLMConfig":
        warnings.warn(
            (
                "`with llm_config: ...` context manager is deprecated. "
                "Pass config object to usage explicitly instead. "
                "Scheduled for removal in 0.11.0 version."
            ),
            DeprecationWarning,
        )

        self._token = LLMConfig._current_llm_config.set(self)
        return self

    def __exit__(self, exc_type: type[Exception], exc_val: Exception, exc_tb: Any) -> None:
        LLMConfig._current_llm_config.reset(self._token)

    @classmethod
    @deprecated(
        "`LLMConfig.current / .default` properties are deprecated. "
        "Pass config object to usage explicitly instead. "
        "Scheduled for removal in 0.11.0 version."
    )
    def get_current_llm_config(cls, llm_config: "LLMConfig | None" = None) -> "LLMConfig | None":
        warnings.warn(
            (
                "`LLMConfig.current / .default` properties are deprecated. "
                "Pass config object to usage explicitly instead. "
                "Scheduled for removal in 0.11.0 version."
            ),
            DeprecationWarning,
        )

        if llm_config is not None:
            return llm_config
        try:
            return (LLMConfig._current_llm_config.get()).copy()
        except LookupError:
            return None

    @classmethod
    def from_json(
        cls,
        *,
        env: str | None = None,
        path: str | Path | None = None,
        file_location: str | None = None,
        filter_dict: dict[str, list[str | None] | set[str | None]] | None = None,
        **kwargs: Any,
    ) -> Self:
        if env is None and path is None:
            raise ValueError("Either 'env' or 'path' must be provided")

        if env is not None and path is not None:
            raise ValueError("Only one of 'env' or 'path' can be provided")

        config_list = config_list_from_json(
            env_or_file=env if env is not None else str(path),
            file_location=file_location,
            filter_dict=filter_dict,
        )

        return cls(*config_list, **kwargs)

    def where(self, *, exclude: bool = False, **kwargs: Any) -> "LLMConfig":
        filtered_config_list = filter_config(
            config_list=[c.model_dump() for c in self.config_list],
            filter_dict=kwargs,
            exclude=exclude,
        )

        if len(filtered_config_list) == 0:
            raise ValueError(f"No config found that satisfies the filter criteria: {kwargs}")

        kwargs = self.model_dump()
        del kwargs["config_list"]

        return LLMConfig(*filtered_config_list, **kwargs)

    def model_dump(self, *args: Any, exclude_none: bool = True, **kwargs: Any) -> dict[str, Any]:
        d = self._model.model_dump(*args, exclude_none=exclude_none, **kwargs)
        return {k: v for k, v in d.items() if not (isinstance(v, list) and len(v) == 0)}

    def model_dump_json(self, *args: Any, exclude_none: bool = True, **kwargs: Any) -> str:
        # return self._model.model_dump_json(*args, exclude_none=exclude_none, **kwargs)
        d = self.model_dump(*args, exclude_none=exclude_none, **kwargs)
        return json.dumps(d)

    def model_validate(self, *args: Any, **kwargs: Any) -> Any:
        return self._model.model_validate(*args, **kwargs)

    @functools.wraps(BaseModel.model_validate_json)
    def model_validate_json(self, *args: Any, **kwargs: Any) -> Any:
        return self._model.model_validate_json(*args, **kwargs)

    @functools.wraps(BaseModel.model_validate_strings)
    def model_validate_strings(self, *args: Any, **kwargs: Any) -> Any:
        return self._model.model_validate_strings(*args, **kwargs)

    def __eq__(self, value: Any) -> bool:
        if not isinstance(value, LLMConfig):
            return NotImplemented
        return self._model == value._model

    def _getattr(self, o: object, name: str) -> Any:
        val = getattr(o, name)
        return val

    def get(self, key: str, default: Any | None = None) -> Any:
        val = getattr(self._model, key, default)
        return val

    def __getitem__(self, key: str) -> Any:
        try:
            return self._getattr(self._model, key)
        except AttributeError:
            raise KeyError(f"Key '{key}' not found in {self.__class__.__name__}")

    def __setitem__(self, key: str, value: Any) -> None:
        try:
            setattr(self._model, key, value)
        except ValueError:
            raise ValueError(f"'{self.__class__.__name__}' object has no field '{key}'")

    def __getattr__(self, name: Any) -> Any:
        try:
            return self._getattr(self._model, name)
        except AttributeError:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")

    def __setattr__(self, name: str, value: Any) -> None:
        if name == "_model":
            object.__setattr__(self, name, value)
        else:
            setattr(self._model, name, value)

    def __contains__(self, key: str) -> bool:
        return hasattr(self._model, key)

    def __repr__(self) -> str:
        d = self.model_dump()
        r = [f"{k}={repr(v)}" for k, v in d.items()]

        s = f"LLMConfig({', '.join(r)})"
        # Replace any keys ending with 'key' or 'token' values with stars for security
        s = re.sub(
            r"(['\"])(\w*(key|token))\1:\s*(['\"])([^'\"]*)(?:\4)", r"\1\2\1: \4**********\4", s, flags=re.IGNORECASE
        )
        return s

    def __copy__(self) -> "LLMConfig":
        options = self._model.model_dump(exclude={"config_list"})
        return LLMConfig(*self._model.config_list, **options)

    def __deepcopy__(self, memo: dict[int, Any] | None = None) -> "LLMConfig":
        return self.__copy__()

    def copy(self) -> "LLMConfig":
        return self.__copy__()

    def deepcopy(self, memo: dict[int, Any] | None = None) -> "LLMConfig":
        return self.__deepcopy__(memo)

    def __str__(self) -> str:
        return repr(self)

    def items(self) -> Iterable[tuple[str, Any]]:
        d = self.model_dump()
        return d.items()

    def keys(self) -> Iterable[str]:
        d = self.model_dump()
        return d.keys()

    def values(self) -> Iterable[Any]:
        d = self.model_dump()
        return d.values()

    _base_model_classes: dict[tuple[type["LLMConfigEntry"], ...], type[BaseModel]] = {}


class _LLMConfig(ApplicationConfig):
    check_every_ms: int | None
    seed: int | None
    allow_format_str_template: bool | None
    response_format: str | dict[str, Any] | BaseModel | type[BaseModel] | None
    timeout: int | None
    cache_seed: int | None
    parallel_tool_calls: bool | None

    tools: list[Any]
    functions: list[Any]

    config_list: list[
        Annotated[
            ConfigEntries,
            Field(discriminator="api_type"),
        ],
    ] = Field(..., min_length=1)

    routing_method: Literal["fixed_order", "round_robin"] | None

    # Following field is configuration for pydantic to disallow extra fields
    model_config = ConfigDict(extra="forbid")
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
from typing import Any, Protocol

from ..doc_utils import export_module


@export_module("autogen")
class ModelClient(Protocol):
    """A client class must implement the following methods:
    - create must return a response object that implements the ModelClientResponseProtocol
    - cost must return the cost of the response
    - get_usage must return a dict with the following keys:
        - prompt_tokens
        - completion_tokens
        - total_tokens
        - cost
        - model

    This class is used to create a client that can be used by OpenAIWrapper.
    The response returned from create must adhere to the ModelClientResponseProtocol but can be extended however needed.
    The message_retrieval method must be implemented to return a list of str or a list of messages from the response.
    """

    RESPONSE_USAGE_KEYS: list[str] = ["prompt_tokens", "completion_tokens", "total_tokens", "cost", "model"]

    class ModelClientResponseProtocol(Protocol):
        class Choice(Protocol):
            class Message(Protocol):
                content: str | dict[str, Any]

            message: Message

        choices: list[Choice]
        model: str

    def create(self, params: dict[str, Any]) -> ModelClientResponseProtocol: ...  # pragma: no cover

    def message_retrieval(
        self, response: ModelClientResponseProtocol
    ) -> list[str] | list["ModelClient.ModelClientResponseProtocol.Choice.Message"]:
        """Retrieve and return a list of strings or a list of Choice.Message from the response.

        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,
        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.
        """
        ...  # pragma: no cover

    def cost(self, response: ModelClientResponseProtocol) -> float: ...  # pragma: no cover

    @staticmethod
    def get_usage(response: ModelClientResponseProtocol) -> dict[str, Any]:
        """Return usage summary of the response using RESPONSE_USAGE_KEYS."""
        ...  # pragma: no cover
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from .client import ModelClient
from .config import LLMConfig

__all__ = (
    "LLMConfig",
    "ModelClient",
)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

from autogen.oai.anthropic import AnthropicLLMConfigEntry
from autogen.oai.bedrock import BedrockLLMConfigEntry
from autogen.oai.cerebras import CerebrasLLMConfigEntry
from autogen.oai.client import (
    AzureOpenAILLMConfigEntry,
    DeepSeekLLMConfigEntry,
    OpenAILLMConfigEntry,
    OpenAIResponsesLLMConfigEntry,
)
from autogen.oai.cohere import CohereLLMConfigEntry
from autogen.oai.gemini import GeminiLLMConfigEntry
from autogen.oai.groq import GroqLLMConfigEntry
from autogen.oai.mistral import MistralLLMConfigEntry
from autogen.oai.ollama import OllamaLLMConfigEntry
from autogen.oai.together import TogetherLLMConfigEntry

ConfigEntries = (
    AnthropicLLMConfigEntry
    | CerebrasLLMConfigEntry
    | BedrockLLMConfigEntry
    | AzureOpenAILLMConfigEntry
    | DeepSeekLLMConfigEntry
    | OpenAILLMConfigEntry
    | OpenAIResponsesLLMConfigEntry
    | CohereLLMConfigEntry
    | GeminiLLMConfigEntry
    | GroqLLMConfigEntry
    | MistralLLMConfigEntry
    | OllamaLLMConfigEntry
    | TogetherLLMConfigEntry
)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0
#
# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
# SPDX-License-Identifier: MIT
import json
import os
from pathlib import Path
from typing import Any


def config_list_from_json(
    env_or_file: str | Path,
    file_location: str | Path | None = "",
    filter_dict: dict[str, list[str | None] | set[str | None]] | None = None,
) -> list[dict[str, Any]]:
    """Retrieves a list of API configurations from a JSON stored in an environment variable or a file.

    This function attempts to parse JSON data from the given `env_or_file` parameter. If `env_or_file` is an
    environment variable containing JSON data, it will be used directly. Otherwise, it is assumed to be a filename,
    and the function will attempt to read the file from the specified `file_location`.

    The `filter_dict` parameter allows for filtering the configurations based on specified criteria. Each key in the
    `filter_dict` corresponds to a field in the configuration dictionaries, and the associated value is a list or set
    of acceptable values for that field. If a field is missing in a configuration and `None` is included in the list
    of acceptable values for that field, the configuration will still be considered a match.

    Args:
        env_or_file (str): The name of the environment variable, the filename, or the environment variable of the filename
            that containing the JSON data.
        file_location (str, optional): The directory path where the file is located, if `env_or_file` is a filename.
        filter_dict (dict, optional): A dictionary specifying the filtering criteria for the configurations, with
            keys representing field names and values being lists or sets of acceptable values for those fields.

    Example:
    ```python
    # Suppose we have an environment variable 'CONFIG_JSON' with the following content:
    # '[{"model": "gpt-3.5-turbo", "api_type": "azure"}, {"model": "gpt-4"}]'

    # We can retrieve a filtered list of configurations like this:
    filter_criteria = {"model": ["gpt-3.5-turbo"]}
    configs = config_list_from_json("CONFIG_JSON", filter_dict=filter_criteria)
    # The 'configs' variable will now contain only the configurations that match the filter criteria.
    ```

    Returns:
        List[Dict]: A list of configuration dictionaries that match the filtering criteria specified in `filter_dict`.

    Raises:
        FileNotFoundError: if env_or_file is neither found as an environment variable nor a file
    """
    env_str = os.environ.get(str(env_or_file))

    if env_str:
        # The environment variable exists. We should use information from it.
        if os.path.exists(env_str):  # noqa: SIM108
            # It is a file location, and we need to load the json from the file.
            json_str = Path(env_str).read_text()
        else:
            # Else, it should be a JSON string by itself.
            json_str = env_str
        config_list = json.loads(json_str)

    else:
        # The environment variable does not exist.
        # So, `env_or_file` is a filename. We should use the file location.
        config_list_path = Path(file_location) / env_or_file if file_location else Path(env_or_file)

        with open(config_list_path) as json_file:
            config_list = json.load(json_file)

    return filter_config(config_list, filter_dict)


def filter_config(
    config_list: list[dict[str, Any]],
    filter_dict: dict[str, list[str | None] | set[str | None]] | None,
    exclude: bool = False,
) -> list[dict[str, Any]]:
    """Filter configuration dictionaries based on specified criteria.

    This function filters a list of configuration dictionaries by applying ALL criteria specified in `filter_dict`.
    A configuration is included in the result if it satisfies every key-value constraint in the filter dictionary.
    For each filter key, the configuration's corresponding field value must match at least one of the acceptable
    values (OR logic within each criteria, AND logic between different criteria).

    Args:
        config_list (list of dict): A list of configuration dictionaries to be filtered.

        filter_dict (dict, optional): A dictionary specifying filter criteria where:
            - Keys are field names to check in each configuration dictionary
            - Values are lists/sets of acceptable values for that field
            - A configuration matches if ALL filter keys are satisfied AND for each key,
              the config's field value matches at least one acceptable value
            - If a filter value includes None, configurations missing that field will match
            - If None, no filtering is applied

        exclude (bool, optional): If False (default), return configurations that match the filter.
                                If True, return configurations that do NOT match the filter.

    Returns:
        list of dict: Filtered list of configuration dictionaries.

    Matching Logic:
        - **Between different filter keys**: AND logic (all criteria must be satisfied)
        - **Within each filter key's values**: OR logic (any acceptable value can match)
        - **For list-type config values**: Match if there's any intersection with acceptable values
        - **For scalar config values**: Match if the value is in the list of acceptable values
        - **Missing fields**: Only match if None is included in the acceptable values for that field

    Examples:
        ```python
        configs = [
            {"model": "gpt-3.5-turbo", "api_type": "openai"},
            {"model": "gpt-4", "api_type": "openai"},
            {"model": "gpt-3.5-turbo", "api_type": "azure", "api_version": "2024-02-01"},
            {"model": "gpt-4", "tags": ["premium", "latest"]},
        ]

        # Example 1: Single criterion - matches any model in the list
        filter_dict = {"model": ["gpt-4", "gpt-4o"]}
        result = filter_config(configs, filter_dict)
        # Returns: [{"model": "gpt-4", "api_type": "openai"}, {"model": "gpt-4", "tags": ["premium", "latest"]}]

        # Example 2: Multiple criteria - must satisfy ALL conditions
        filter_dict = {"model": ["gpt-3.5-turbo"], "api_type": ["azure"]}
        result = filter_config(configs, filter_dict)
        # Returns: [{"model": "gpt-3.5-turbo", "api_type": "azure", "api_version": "2024-02-01"}]

        # Example 3: Tag filtering with list intersection
        filter_dict = {"tags": ["premium"]}
        result = filter_config(configs, filter_dict)
        # Returns: [{"model": "gpt-4", "tags": ["premium", "latest"]}]

        # Example 4: Exclude matching configurations
        filter_dict = {"api_type": ["openai"]}
        result = filter_config(configs, filter_dict, exclude=True)
        # Returns configs that do NOT have api_type="openai"
        ```
    Note:
        - If `filter_dict` is empty or None, no filtering is applied and `config_list` is returned as is.
        - If a configuration dictionary in `config_list` does not contain a key specified in `filter_dict`,
          it is considered a non-match and is excluded from the result.

    """
    if filter_dict:
        return [
            item
            for item in config_list
            if all(_satisfies_criteria(item.get(key), values) != exclude for key, values in filter_dict.items())
        ]

    return config_list


def _satisfies_criteria(config_value: Any, criteria_values: Any) -> bool:
    """Check if a configuration field value satisfies the filter criteria.

    This helper function implements the matching logic between a single configuration
    field value and the acceptable values specified in the filter criteria. It handles
    both scalar and list-type configuration values with appropriate matching strategies.

    Args:
        config_value (Any): The value from a configuration dictionary field.
                           Can be None, a scalar value, or a list of values.
        criteria_values (Any): The acceptable values from the filter dictionary.
                              Can be a single value or a list/set of acceptable values.

    Returns:
        bool: True if the config_value satisfies the criteria, False otherwise.

    Matching Logic:
        - **None config values**: Always return False (missing fields don't match)
        - **List config values**:
            - If criteria is a list: Match if there's any intersection (set overlap)
            - If criteria is scalar: Match if the scalar is contained in the config list
        - **Scalar config values**:
            - If criteria is a list: Match if the config value is in the criteria list
            - If criteria is scalar: Match if the values are exactly equal

    Examples:
        ```python
        # List config value with list criteria (intersection matching)
        _satisfies_criteria(["gpt-4", "gpt-3.5"], ["gpt-4", "claude"])  # True (gpt-4 intersects)
        _satisfies_criteria(["tag1", "tag2"], ["tag3", "tag4"])  # False (no intersection)

        # List config value with scalar criteria (containment matching)
        _satisfies_criteria(["premium", "latest"], "premium")  # True (premium is in list)
        _satisfies_criteria(["tag1", "tag2"], "tag3")  # False (tag3 not in list)

        # Scalar config value with list criteria (membership matching)
        _satisfies_criteria("gpt-4", ["gpt-4", "gpt-3.5"])  # True (gpt-4 in criteria)
        _satisfies_criteria("claude", ["gpt-4", "gpt-3.5"])  # False (claude not in criteria)

        # Scalar config value with scalar criteria (equality matching)
        _satisfies_criteria("openai", "openai")  # True (exact match)
        _satisfies_criteria("openai", "azure")  # False (different values)

        # None config values (missing fields)
        _satisfies_criteria(None, ["gpt-4"])  # False (missing field)
        _satisfies_criteria(None, "gpt-4")  # False (missing field)
        ```

    Note:
        This is an internal helper function used by `filter_config()`. The function
        assumes that both parameters can be of various types and handles type
        checking internally to determine the appropriate matching strategy.
    """
    if config_value is None:
        return False

    if isinstance(config_value, list):
        if isinstance(criteria_values, list):
            return bool(set(config_value) & set(criteria_values))  # Non-empty intersection
        else:
            return criteria_values in config_value
    else:
        # In filter_dict, filter could be either a list of values or a single value.
        # For example, filter_dict = {"model": ["gpt-3.5-turbo"]} or {"model": "gpt-3.5-turbo"}
        if isinstance(criteria_values, list):
            return config_value in criteria_values
        return bool(config_value == criteria_values)
# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import re
from abc import ABC, abstractmethod
from collections.abc import Iterable, Mapping
from typing import Any

from httpx import Client as httpxClient
from pydantic import BaseModel, ConfigDict, Field, HttpUrl, SecretStr, ValidationInfo, field_serializer, field_validator
from typing_extensions import Required, Self, TypedDict

from .client import ModelClient


class LLMConfigEntryDict(TypedDict, total=False):
    api_type: Required[str]
    model: str
    max_tokens: int | None
    top_p: float | None
    temperature: float | None

    api_key: SecretStr | str | None
    api_version: str | None
    base_url: HttpUrl | str | None
    voice: str | None
    http_client: httpxClient | None
    model_client_cls: str | None
    response_format: str | dict[str, Any] | BaseModel | type[BaseModel] | None
    default_headers: Mapping[str, Any] | None
    tags: list[str]


class ApplicationConfig(BaseModel):
    max_tokens: int | None = Field(
        default=None,
        ge=0,
        description="The maximum number of tokens to generate before stopping.",
    )

    top_p: float | None = Field(
        default=None,
        ge=0,
        le=1,
        description=(
            "An alternative to sampling with temperature, called nucleus sampling, "
            "where the model considers the results of the tokens with top_p probability mass."
            "So 0.1 means only the tokens comprising the top 10% probability mass are considered."
            "You should either alter `temperature` or `top_p`, but not both."
        ),
    )

    temperature: float | None = Field(
        default=None,
        ge=0,
        le=1,
        description=(
            "Amount of randomness injected into the response. "
            "Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's "
            "maximum `temperature` for creative and generative tasks. "
            "Note that even with `temperature` of `0.0`, the results will not be fully deterministic."
        ),
    )


class LLMConfigEntry(ApplicationConfig, ABC):
    api_type: str
    model: str = Field(..., min_length=1)

    api_key: SecretStr | None = None
    api_version: str | None = None
    base_url: HttpUrl | None = None
    voice: str | None = None
    model_client_cls: str | None = None
    http_client: httpxClient | None = None
    response_format: str | dict[str, Any] | BaseModel | type[BaseModel] | None = None
    default_headers: Mapping[str, Any] | None = None
    tags: list[str] = Field(default_factory=list)

    # Following field is configuration for pydantic to disallow extra fields
    model_config = ConfigDict(extra="allow", arbitrary_types_allowed=True)

    def apply_application_config(self, application_config: ApplicationConfig) -> Self:
        """Apply application level configurations."""
        new_entry = self.model_copy()
        new_entry.max_tokens = new_entry.max_tokens or application_config.max_tokens
        new_entry.top_p = new_entry.top_p or application_config.top_p
        new_entry.temperature = new_entry.temperature or application_config.temperature
        return new_entry

    @abstractmethod
    def create_client(self) -> "ModelClient": ...

    @field_validator("base_url", mode="before")
    @classmethod
    def check_base_url(cls, v: HttpUrl | str | None, info: ValidationInfo) -> str | None:
        if v is None:  # Handle None case explicitly
            return None
        if not str(v).startswith("https://") and not str(v).startswith("http://"):
            return f"http://{str(v)}"
        return str(v)

    @field_serializer("base_url", when_used="unless-none")  # Ensure serializer also respects None
    def serialize_base_url(self, v: HttpUrl | None) -> str | None:
        return str(v) if v is not None else None

    @field_serializer("api_key", when_used="unless-none")
    def serialize_api_key(self, v: SecretStr) -> str:
        return v.get_secret_value()

    def model_dump(self, *args: Any, exclude_none: bool = True, **kwargs: Any) -> dict[str, Any]:
        return BaseModel.model_dump(self, exclude_none=exclude_none, *args, **kwargs)

    def model_dump_json(self, *args: Any, exclude_none: bool = True, **kwargs: Any) -> str:
        return BaseModel.model_dump_json(self, exclude_none=exclude_none, *args, **kwargs)

    def get(self, key: str, default: Any | None = None) -> Any:
        val = getattr(self, key, default)
        if isinstance(val, SecretStr):
            return val.get_secret_value()
        return val

    def __getitem__(self, key: str) -> Any:
        try:
            val = getattr(self, key)
            if isinstance(val, SecretStr):
                return val.get_secret_value()
            return val
        except AttributeError:
            raise KeyError(f"Key '{key}' not found in {self.__class__.__name__}")

    def __setitem__(self, key: str, value: Any) -> None:
        setattr(self, key, value)

    def __contains__(self, key: str) -> bool:
        return hasattr(self, key)

    def items(self) -> Iterable[tuple[str, Any]]:
        d = self.model_dump()
        return d.items()

    def keys(self) -> Iterable[str]:
        d = self.model_dump()
        return d.keys()

    def values(self) -> Iterable[Any]:
        d = self.model_dump()
        return d.values()

    def __repr__(self) -> str:
        # Override to eliminate none values from the repr
        d = self.model_dump()
        r = [f"{k}={repr(v)}" for k, v in d.items()]

        s = f"{self.__class__.__name__}({', '.join(r)})"

        # Replace any keys ending with '_key' or '_token' values with stars for security
        # This regex will match any key ending with '_key' or '_token' and its value, and replace the value with stars
        # It also captures the type of quote used (single or double) and reuses it in the replacement
        s = re.sub(r'(\w+_(key|token)\s*=\s*)([\'"]).*?\3', r"\1\3**********\3", s, flags=re.IGNORECASE)

        return s

    def __str__(self) -> str:
        return repr(self)
